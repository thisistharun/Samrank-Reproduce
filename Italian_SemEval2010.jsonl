{"file_name": "C-38", "text": "Un framework per l'architettura di overlay peer-to-peer guidati dal ricevitore ABSTRACT Questo documento presenta un framework semplice e scalabile per l'architettura di overlay peer-to-peer chiamato Peer-to-peer Receiverdriven Overlay -LRB- o PRO -RRB-. PRO \u00e8 progettato per applicazioni di streaming non interattive e il suo obiettivo di progettazione principale \u00e8 massimizzare la larghezza di banda fornita -LRB- e quindi fornire qualit\u00e0 -RRB- ai peer con larghezza di banda eterogenea e asimmetrica. Per raggiungere questo obiettivo, PRO adotta un approccio guidato dal ricevitore in cui ciascun ricevitore -LRB- o peer partecipante -RRB- -LRB- i -RRB- scopre indipendentemente altri peer nell'overlay attraverso il pettegolezzo e -LRB- ii -RRB- egoisticamente determina il miglior sottoinsieme di peer principali attraverso cui connettersi all'overlay per massimizzare la propria larghezza di banda fornita. I peer partecipanti formano un overlay non strutturato che \u00e8 intrinsecamente robusto per un tasso di abbandono elevato. Inoltre, ciascun ricevitore sfrutta la larghezza di banda controllata dalla congestione dei suoi genitori come segnale implicito per rilevare e reagire ai cambiamenti a lungo termine nella rete o nelle condizioni di sovrapposizione senza alcun coordinamento esplicito con gli altri peer partecipanti. La selezione indipendente dei genitori da parte di singoli pari convergono dinamicamente in un'efficiente struttura di sovrapposizione. 1. INTRODUZIONE eterogeneit\u00e0 e asimmetria della connettivit\u00e0 della larghezza di banda tra i peer partecipanti -LSB- 19 -RSB-. Affrontare le variazioni di larghezza di banda, l'eterogeneit\u00e0 e l'asimmetria sono particolarmente importanti nella progettazione di sovrapposizioni peer-to-peer per applicazioni di streaming poich\u00e9 la qualit\u00e0 fornita a ciascun peer \u00e8 direttamente determinata dalla sua connettivit\u00e0 di larghezza di banda a -LRB- altro peer -LRB- s -RRB- su -RRB- la sovrapposizione. Questo documento presenta un semplice framework per l'architettura di un overlay peer-to-peer basato sul ricevitore, chiamato PRO. La filosofia di progettazione principale di PRO \u00e8 che a ciascun peer dovrebbe essere consentito di determinare in modo indipendente ed egoistico il modo migliore per connettersi all'overlay al fine di massimizzare la propria qualit\u00e0 fornita. A tal fine, ciascun peer pu\u00f2 connettersi alla topologia overlay in pi\u00f9 punti -LRB-, ovvero ricevere contenuto attraverso pi\u00f9 peer principali -RRB-. Pertanto, i peer partecipanti formano un overlay non strutturato in grado di far fronte con grazia ad un elevato tasso di abbandono -LSB- 5 -RSB-. Inoltre, avere pi\u00f9 peer genitori soddisfa l\u2019eterogeneit\u00e0 e l\u2019asimmetria della larghezza di banda, migliorando al tempo stesso la resilienza rispetto alle dinamiche di partecipazione dei peer. PRO \u00e8 costituito da due componenti chiave: -LRB- i -RRB- Peer Discovery basato su gossip: ciascun peer scambia periodicamente messaggi -LRB- ovvero pettegolezzi -RRB- con altri peer noti per conoscere progressivamente un sottoinsieme di peer partecipanti nell'overlay che probabilmente saranno dei buoni genitori. -LRB- ii -RRB- Selezione dei genitori guidata dal destinatario: date le informazioni raccolte sugli altri pari partecipanti tramite il meccanismo di pettegolezzo,ciascun peer -LRB- o ricevitore -RRB- migliora gradualmente la propria qualit\u00e0 fornita selezionando dinamicamente un sottoinsieme appropriato di peer genitori che collettivamente massimizzano la larghezza di banda fornita al ricevitore. Poich\u00e9 la larghezza di banda disponibile da diversi peer partecipanti a un ricevitore -LRB- e la possibile correlazione tra loro -RRB- pu\u00f2 essere misurata solo su quel ricevitore, un approccio guidato dal ricevitore \u00e8 la soluzione naturale per massimizzare la larghezza di banda disponibile per peer eterogenei. Inoltre, la larghezza di banda disponibile dai peer principali funge da segnale implicito affinch\u00e9 un ricevitore possa rilevare e reagire ai cambiamenti nella rete o nelle condizioni di overlay senza alcun coordinamento esplicito con gli altri peer partecipanti. La selezione indipendente dei genitori da parte dei singoli peer porta a una sovrapposizione efficiente che massimizza la qualit\u00e0 fornita a ciascun peer. PRO incorpora diverse funzioni di smorzamento per garantire la stabilit\u00e0 del rivestimento nonostante le azioni non coordinate di diversi colleghi. PRO fa parte di un'architettura pi\u00f9 ampia che abbiamo sviluppato per lo streaming peer-to-peer. Pertanto, PRO e PALS sono entrambi guidati dal ricevitore ma si completano a vicenda. Pi\u00f9 specificamente, PRO determina un sottoinsieme appropriato di peer principali che massimizzano collettivamente la larghezza di banda fornita a ciascun ricevitore mentre PALS coordina lo streaming \"in-time\" di diversi segmenti di contenuto multimediale da questi genitori nonostante le variazioni imprevedibili nella loro larghezza di banda disponibile. Questa divisione delle funzionalit\u00e0 offre una grande flessibilit\u00e0 perch\u00e9 disaccoppia la costruzione della sovrapposizione dal meccanismo di consegna. In questo articolo ci concentreremo principalmente sul meccanismo di costruzione overlay, o PRO. Il resto di questo documento \u00e8 organizzato come segue: Nella Sezione 2, rivisitiamo il problema della costruzione dell'overlay per lo streaming peerto-peer e identifichiamo i suoi due componenti chiave ed esploriamo il loro spazio di progettazione. Presentiamo il nostro quadro proposto nella Sezione 3. Nelle Sezioni 4 e 5, i componenti chiave del nostro quadro sono descritti in maggiore dettaglio. Infine, la Sezione 6 conclude il documento e presenta i nostri piani futuri. 6. CONCLUSIONI E LAVORO FUTURO In questo articolo, abbiamo presentato un semplice framework guidato dal ricevitore per l'architettura di strutture overlay peer-to-pee chiamato PRO. PRO consente a ciascun peer di determinare egoisticamente e in modo indipendente il modo migliore per connettersi all'overlay per massimizzarne le prestazioni. Pertanto, PRO dovrebbe essere in grado di massimizzare la qualit\u00e0 fornita ai peer con connettivit\u00e0 a larghezza di banda eterogenea e asimmetrica. Sia la scoperta dei pari che la selezione dei pari in questo framework sono scalabili. Inoltre, PRO utilizza la larghezza di banda controllata dalla congestione come segnale implicito per rilevare i colli di bottiglia condivisi tra i genitori esistenti, nonch\u00e9 i cambiamenti nelle condizioni di rete o di sovrapposizione per rimodellare adeguatamente la struttura. Abbiamo descritto la struttura di base e i suoi componenti chiave e abbiamo abbozzato le nostre soluzioni di paglia. Questo \u00e8 un punto di partenza per il nostro lavoro su PRO. Attualmente stiamo valutando vari aspetti di questo quadro tramite simulazione,ed esplorare lo spazio di progettazione dei componenti chiave. Stiamo anche prototipando questo quadro per condurre esperimenti nel mondo reale sul Planet-Lab in un prossimo futuro.", "keyphrases": ["flusso peer-to-peer", "controllo della congestione", "approccio orientato alla ricezione", "sovrapposizione guidata dalla ricezione", "sistema di distribuzione", "progetto", "misura", "struttura efficace sovrapposta", "pro", "sottoinsieme appropriato del peer genitore", "scoperta tra pari basata su gossip", "selezione genitore basata sulla ricezione"]}
{"file_name": "H-17", "text": "Politiche di potatura per indici invertiti a due livelli con garanzia di correttezza ABSTRACT I motori di ricerca Web mantengono indici invertiti su larga scala che vengono interrogati migliaia di volte al secondo da utenti desiderosi di informazioni. Per far fronte all'enorme quantit\u00e0 di carichi di query, i motori di ricerca riducono il proprio indice per conservare i documenti che probabilmente verranno restituiti come risultati migliori e utilizzano questo indice ridotto per calcolare i primi gruppi di risultati. Sebbene questo approccio possa migliorare le prestazioni riducendo la dimensione dell'indice, se calcoliamo i migliori risultati solo dall'indice ridotto potremmo notare un significativo degrado nella qualit\u00e0 del risultato: se un documento dovrebbe essere tra i primi risultati ma non \u00e8 stato incluso in l'indice sfoltito, verr\u00e0 posizionato dietro i risultati calcolati dall'indice sfoltito. Considerata la forte concorrenza nel mercato della ricerca online, questo fenomeno \u00e8 chiaramente indesiderabile. In questo articolo studiamo come evitare qualsiasi degrado della qualit\u00e0 dei risultati dovuto all'ottimizzazione delle prestazioni basata sulla potatura, realizzandone comunque la maggior parte dei vantaggi. Il nostro contributo consiste in una serie di modifiche alle tecniche di potatura per la creazione dell'indice sfoltito e un nuovo algoritmo di calcolo dei risultati che garantisce che le pagine con la corrispondenza pi\u00f9 elevata siano sempre posizionate nei primi risultati di ricerca, anche se stiamo calcolando il primo batch dall'indice sfoltito. indice per la maggior parte del tempo. Mostriamo anche come determinare la dimensione ottimale di un indice ridotto e valutiamo sperimentalmente i nostri algoritmi su una raccolta di 130 milioni di pagine Web. 1. INTRODUZIONE Secondo un recente studio -LSB- 13 -RSB-, si stima che \u2217 Lavoro svolto mentre l'autore era presso il Dipartimento di Informatica dell'UCLA. \u2020 Questo lavoro \u00e8 parzialmente supportato dalle sovvenzioni NSF, IIS-0534784, IIS0347993 e CNS-0626702. A causa di questa immensa quantit\u00e0 di informazioni disponibili, gli utenti stanno diventando sempre pi\u00f9 dipendenti dai motori di ricerca Web per individuare informazioni rilevanti sul Web. In genere, i motori di ricerca Web, analogamente ad altre applicazioni di recupero delle informazioni, utilizzano una struttura dati denominata indice invertito. Un indice invertito consente il recupero efficiente dei documenti -LRB- o delle pagine Web -RRB- che contengono una determinata parola chiave. Nella maggior parte dei casi, una query eseguita dall'utente pu\u00f2 contenere migliaia o addirittura milioni di documenti corrispondenti. Per evitare di sovraccaricare gli utenti con una quantit\u00e0 enorme di risultati, i motori di ricerca presentano i risultati in gruppi di 10-20 documenti rilevanti. L'utente esamina quindi il primo gruppo di risultati e, se non trova la risposta che sta cercando, potrebbe potenzialmente richiedere di visualizzare il gruppo successivo o decidere di inviare una nuova query. Uno studio recente -LSB- 16 -RSB- ha indicato che circa l'80% degli utenti esamina al massimo i primi 3 lotti di risultati. Cio\u00e8, l'80% degli utenti in genere visualizza al massimo dai 30 ai 60 risultati per ogni query che invia a un motore di ricerca. Allo stesso tempo, date le dimensioni del Web,l'indice invertito mantenuto dai motori di ricerca pu\u00f2 diventare molto grande. Una soluzione naturale a questo problema \u00e8 creare un piccolo indice su un sottoinsieme dei documenti che probabilmente verranno restituiti come risultati migliori -LRB- utilizzando, ad esempio, le tecniche di potatura in -LSB- 7, 20 -RSB- -RRB- e calcola il primo gruppo di risposte utilizzando l'indice ridotto. Anche se \u00e8 stato dimostrato che questo approccio offre un miglioramento significativo delle prestazioni, porta anche a un notevole degrado della qualit\u00e0 dei risultati della ricerca, poich\u00e9 le risposte migliori vengono calcolate solo dall'indice ridotto -LSB- 7, 20 -RSB-. Cio\u00e8, anche se una pagina dovesse essere posizionata come la pagina con la corrispondenza pi\u00f9 alta secondo la metrica di ranking di un motore di ricerca, la pagina potrebbe essere posizionata dietro quelle contenute nell'indice sfoltito se la pagina non \u00e8 diventata parte dell'indice sfoltito per vari motivi -LSB- 7, 20 -RSB-. Data la forte concorrenza tra i motori di ricerca di oggi, questo degrado \u00e8 chiaramente indesiderabile e deve essere affrontato, se possibile. In questo documento studieremo come evitare qualsiasi degrado della qualit\u00e0 della ricerca dovuto all'ottimizzazione delle prestazioni di cui sopra, realizzandone comunque la maggior parte dei vantaggi. Cio\u00e8, presentiamo una serie di semplici modifiche -LRB- ma importanti -RRB- nelle tecniche di potatura per la creazione dell'indice pruned. Il nostro contributo principale \u00e8 un nuovo algoritmo di calcolo delle risposte che garantisce che le pagine con la migliore corrispondenza -LRB- secondo la metrica di ranking del motore di ricerca -RRB- siano sempre posizionate in cima ai risultati di ricerca, anche se stiamo calcolando la prima batch di risposte dall'indice ridotto per la maggior parte del tempo. Queste tecniche di potatura avanzate e gli algoritmi di calcolo delle risposte vengono esplorati nel context dell'architettura cluster comunemente impiegata dai motori di ricerca odierni. Infine, studieremo e presenteremo come i motori di ricerca possono ridurre al minimo il costo operativo della risposta alle domande fornendo allo stesso tempo risultati di ricerca di alta qualit\u00e0. Figura 1: -LRB- a -RRB- Il motore di ricerca replica il suo intero indice IF per aumentare la capacit\u00e0 di risposta alle query. -LRB- b -RRB- Nel primo livello, i piccoli IP pindex gestiscono la maggior parte delle query. Quando l'IP non pu\u00f2 rispondere a una query, viene reindirizzato al 2\u00b0 livello, dove viene utilizzato l'indice completo IF per calcolare la risposta. 6. LAVORI CORRELATI -LSB- 3, 30 -RSB- forniscono una buona panoramica dell'indicizzazione inversa nei motori di ricerca Web e nei sistemi IR. Studi sperimentali e analisi di vari schemi di partizionamento per un indice invertito sono presentati in -LSB- 6, 23, 33 -RSB-. Gli algoritmi di potatura presentati in questo articolo sono indipendenti dallo schema di partizionamento utilizzato. Tuttavia, -LSB- 1, 5, 7, 27 -RSB- non considerano alcuna qualit\u00e0 indipendente dalla query -LRB- come PageRank -RRB- nella funzione di classificazione. -LSB- 32 -RSB- presenta un quadro generico per il calcolo delle risposte top-k approssimative con alcuni limiti probabilistici sulla qualit\u00e0 dei risultati. Il nostro lavoro si estende essenzialmente -LSB- 1, 2, 4, 7, 20, 27,31 -RSB- proponendo meccanismi per fornire la garanzia di correttezza dei risultati top-k calcolati. I motori di ricerca utilizzano vari metodi di memorizzazione nella cache come mezzo per ridurre i costi associati alle query -LSB- 18, 19, 21, 31 -RSB-. Questo filo di lavoro \u00e8 anche ortogonale al nostro perch\u00e9 uno schema di memorizzazione nella cache pu\u00f2 funzionare sopra il nostro p-index per ridurre al minimo il costo di calcolo della risposta. Le esatte funzioni di classificazione utilizzate dagli attuali motori di ricerca sono segreti gelosamente custoditi. In generale, tuttavia, le classifiche si basano sulla pertinenza dipendente dalla query e sulla \"qualit\u00e0\" del documento indipendente dalla query. '' Allo stesso modo, ci sono una serie di lavori che misurano la `` qualit\u00e0 '' dei documenti, tipicamente catturati attraverso l'analisi basata sui collegamenti -LSB- 17, 28, 26 -RSB-. Poich\u00e9 il nostro lavoro non assume una forma particolare di funzione di classificazione, \u00e8 complementare a questo corpus di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.\u00e8 complementare a questo corpo di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.\u00e8 complementare a questo corpo di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.", "keyphrases": ["motore di ricerca web", "indice invertito su larga scala", "carico di domanda", "indice di prugna", "mercato della ricerca online", "degrado della qualit\u00e0 dei risultati", "la base di prugna ha prestazioni ottimali", "tecnica della prugna", "algoritmo di calcolo dei risultati", "pagina con la corrispondenza migliore", "risultato della ricerca migliore", "dimensione ottimale"]}
{"file_name": "J-25", "text": "Scommesse in stile booleano: una struttura per la negoziazione di titoli basata su formule logiche ABSTRACT Sviluppiamo una struttura per la negoziazione di titoli composti: strumenti finanziari che pagano in base ai risultati di dichiarazioni arbitrarie in logica proposizionale. L'acquisto o la vendita di titoli - che possono essere considerati come scommesse su o contro un particolare risultato futuro - consente agli agenti sia di coprire il rischio che di trarre profitto -LRB- in aspettativa -RRB- su previsioni soggettive. Un mercato mobiliare composto consente agli agenti di piazzare scommesse su combinazioni booleane arbitrarie di eventi, consentendo loro di raggiungere pi\u00f9 da vicino la loro esposizione ottimale al rischio e consentendo al mercato nel suo insieme di raggiungere pi\u00f9 da vicino l\u2019ottimo sociale. Il compromesso per consentire tale espressivit\u00e0 sta nella complessit\u00e0 dei problemi di ottimizzazione degli agenti e del banditore. Sviluppiamo e motiviamo il concetto di mercato mobiliare composto, presentando il quadro attraverso una serie di definizioni formali ed esempi. Analizziamo poi nel dettaglio il problema di abbinamento del banditore. Mostriamo che, con n eventi, il problema del matching \u00e8 co-NP-completo nel caso divisibile e \u03a3p2-completo nel caso indivisibile. Mostriamo che quest'ultimo risultato di durezza vale anche in presenza di severe restrizioni linguistiche sulle offerte. Con eventi log n, il problema \u00e8 polinomiale nel caso divisibile e NP-completo nel caso indivisibile. Discuteremo brevemente gli algoritmi di abbinamento e i casi speciali trattabili. 1. INTRODUZIONE I mercati mobiliari consentono effettivamente ai trader di piazzare scommesse sui risultati di proposte future incerte. Il valore economico dei mercati mobiliari \u00e8 duplice. In primo luogo, consentono ai trader di coprire i rischi o di assicurarsi contro risultati indesiderati. Ad esempio, il proprietario di un titolo potrebbe acquistare un'opzione put -LRB- il diritto di vendere il titolo a un prezzo particolare -RRB- per assicurarsi contro una flessione del titolo. In secondo luogo, i mercati mobiliari consentono agli operatori di speculare o di ottenere un profitto soggettivo atteso quando i prezzi di mercato non riflettono la loro valutazione della probabilit\u00e0 di risultati futuri. Ad esempio, un trader potrebbe acquistare un'opzione call se ritiene che vi sia un'elevata probabilit\u00e0 che il prezzo del titolo sottostante salga, indipendentemente dall'esposizione al rischio derivante dalle variazioni del prezzo del titolo. Poich\u00e9 i trader possono ottenere un profitto se riescono a effettuare valutazioni probabilistiche efficaci, spesso i prezzi nei mercati finanziari forniscono previsioni aggregate molto accurate di eventi futuri -LSB- 10, 29, 27, 28 -RSB-. I mercati dei titoli reali hanno strutture di rendimento complesse con vari fattori scatenanti. Tuttavia, questi possono tutti essere modellati come raccolte di titoli Arrow-Debreu pi\u00f9 elementari o atomici -LSB- 1, 8, 20 -RSB-. Una unit\u00e0 di un titolo Arrow-Debreu paga un dollaro se e solo se -LRB- se e solo -RRB- si verifica un evento binario corrispondente; non paga nulla se l'evento non si verifica. Quindi, ad esempio, un'unit\u00e0 di un titolo denominata -LRB- Acme100 -RRB- potrebbe pagare $ 1 se e solo se le azioni di Acme fossero superiori a $ 100 il 4 gennaio,2004. Un'opzione su azioni Acme, come verrebbe definita in una borsa finanziaria, pu\u00f2 essere considerata come un portafoglio di tali titoli atomici.1 In questo articolo, sviluppiamo e analizziamo una struttura per la negoziazione in mercati di titoli composti con profitti condizionati da combinazioni logiche di eventi, compresi i condizionali. Ad esempio, dati gli eventi binari A, B e C, un trader potrebbe fare un'offerta per acquistare tre unit\u00e0 di un titolo indicato -LRB- A n B \u00af VC -RRB- che paga $ 1 se e solo l'evento composto A n B \u00af VC avviene per trenta centesimi ciascuno. Dato un insieme di tali offerte, il banditore deve affrontare un complesso problema di abbinamento per decidere quali offerte sono accettate per quante unit\u00e0 e a quale prezzo. In genere, il banditore non cerca di assumersi alcun rischio, ma si limita a confrontare le transazioni accettabili tra gli offerenti, ma consideriamo anche formulazioni alternative in cui il banditore agisce come un market maker disposto ad accettare alcuni rischi. Esaminiamo la complessit\u00e0 computazionale del problema di abbinamento del banditore. Sia la lunghezza della descrizione di tutti i titoli disponibili O -LRB- n -RRB-. Con n eventi, il problema di abbinamento \u00e8 co-NP-completo nel caso divisibile ed Ep2-completo nel caso indivisibile. Questa durezza completa Ep2 rimane anche quando la lingua delle offerte \u00e8 significativamente limitata. Con eventi log n, il problema \u00e8 polinomiale nel caso divisibile e NP-completo nel caso indivisibile. La sezione 2 presenta alcune informazioni di base, motivazioni e lavoro correlato necessari. La sezione 3 descrive formalmente il nostro quadro per i titoli composti e definisce il problema di abbinamento del banditore. La sezione 4 discute brevemente gli algoritmi naturali per risolvere il problema dell'abbinamento. La sezione 5 dimostra i nostri risultati centrali sulla complessit\u00e0 computazionale. La sezione 6 discute la possibilit\u00e0 di casi speciali trattabili. La sezione 7 si conclude con una sintesi e alcune idee sulle direzioni future. 2. PRELIMINARI 2.1 Context e notazione In questo mondo semplice ci sono quattro possibili stati futuri -- tutte le possibili combinazioni dei risultati degli eventi binari: colpito n acme100, colpito n acme100, colpito n acme100, colpito n acme100. Il rischio di copertura pu\u00f2 essere pensato come un\u2019azione di spostamento di denaro tra vari possibili stati futuri. Ad esempio, insur1Tecnicamente, un'opzione \u00e8 un portafoglio di infiniti titoli atomici, sebbene possa essere modellato approssimativamente con un numero finito. Trasferire denaro dalla propria casa dai futuri stati in cui \u00e8 colpito non \u00e8 vero per gli stati in cui si trova. La vendita di un titolo denominato -LRB-acme100 -RRB- - che paga $ 1 se e solo se si verifica l'evento acme100 - trasferisce denaro dagli stati futuri in cui il prezzo di Acme \u00e8 superiore a $ 100 il 4 gennaio agli stati in cui non lo \u00e8. La speculazione \u00e8 anche un atto di trasferimento di denaro tra stati futuri, sebbene solitamente associato alla massimizzazione del rendimento atteso piuttosto che alla riduzione del rischio. Ad esempio, scommettere su una squadra di calcio sposta denaro dallo stato \"la squadra perde\" allo stato \"la squadra vince\".Tutti i possibili risultati futuri formano uno spazio degli stati \u03a9, costituito da stati mutuamente esclusivi ed esaustivi \u03c9 E \u03a9. Spesso un modo pi\u00f9 naturale di pensare ai possibili risultati futuri \u00e8 come uno spazio degli eventi A di eventi AEA linearmente indipendenti che possono sovrapporsi arbitrariamente. Quindi nel nostro esempio del giocattolo colpito n acme100 \u00e8 uno dei quattro stati disgiunti, mentre colpito \u00e8 uno dei due eventi. Si noti che un insieme di n eventi linearmente indipendenti definisce uno spazio degli stati \u03a9 di dimensione 2 '' costituito da tutte le possibili combinazioni di risultati di eventi. Al contrario, qualsiasi spazio di stato \u03a9 pu\u00f2 essere fattorizzato in eventi -LSB- log l\u03a9ll. Supponiamo che A copra in modo esaustivo tutti i risultati futuri significativi -LRB-, cio\u00e8 copra tutte le eventualit\u00e0 contro le quali gli agenti potrebbero voler proteggersi e/o speculare su -RRB-. Quindi l'esistenza di 2 titoli linearmente indipendenti \u2013 chiamati mercato completo \u2013 consente agli agenti di distribuire arbitrariamente la loro ricchezza tra gli stati futuri.2 Un agente pu\u00f2 creare qualsiasi copertura o speculazione desideri. In condizioni classiche, gli agenti che commerciano in un mercato completo formano un equilibrio in cui il rischio \u00e8 allocato in modo Pareto ottimale. Se il mercato \u00e8 incompleto, cio\u00e8 consiste di meno di 2 '' titoli linearmente indipendenti, allora in generale gli agenti non possono costruire coperture arbitrarie e le allocazioni di equilibrio potrebbero non essere ottimali -LSB- 1, 8, 19, 20 -RSB-. Negli scenari del mondo reale, il numero di eventi significativi n \u00e8 elevato e quindi il numero di garanzie richieste per la completezza \u00e8 intrattabile. Non esiste n\u00e9 esister\u00e0 mai un mercato veramente completo. Una delle motivazioni alla base dei mercati mobiliari composti \u00e8 quella di fornire un meccanismo che supporti il \u200b\u200bmassimo trasferimento del rischio utilizzando il minor numero di transazioni possibile. I titoli composti consentono un elevato grado di espressivit\u00e0 nella costruzione delle offerte. Il compromesso per una maggiore espressivit\u00e0 \u00e8 una maggiore complessit\u00e0 computazionale, sia dal punto di vista dell'offerente che del banditore. 2.2 Lavoro correlato La ricerca per ridurre il numero di strumenti finanziari necessari per supportare un'allocazione ottimale del rischio risale al lavoro originale di Arrow -LSB- 1 -RSB-. Il requisito sopra indicato di \"solo\" 2 \"titoli linearmente indipendenti \u00e8 di per s\u00e9 una riduzione della formulazione pi\u00f9 semplice. In un'economia con k beni standard, il mercato completo pi\u00f9 semplice contiene k \u2022 2 titoli, ciascuno dei quali frutta in un bene con una realizzazione statale. Arrow -LSB- 1 -RSB- ha dimostrato che \u00e8 completo anche un mercato in cui titoli e beni sono sostanzialmente separati, con 2'' titoli che pagano in un unico bene numerario pi\u00f9 k mercati spot nei beni standard. Per i nostri scopi, dobbiamo considerare solo il mercato dei titoli. 2Per titoli linearmente indipendenti intendiamo che i vettori dei payoff in tutti gli stati futuri di questi titoli sono linearmente indipendenti. Varian -LSB- 34 -RSB- mostra che un mercato completo pu\u00f2 essere costruito utilizzando meno di 2n titoli,sostituzione dei titoli mancanti con opzioni. Tuttavia, il numero di strumenti finanziari linearmente indipendenti \u2013 titoli pi\u00f9 opzioni \u2013 deve essere 2n per garantire la completezza. Gli autori mostrano che in alcuni casi il mercato pu\u00f2 essere strutturato e \u201ccompattato\u201d in analogia alle rappresentazioni di rete bayesiana delle distribuzioni di probabilit\u00e0 congiunte -LSB- 23 -RSB-. Essi mostrano che, se le indipendenze neutrali al rischio di tutti gli agenti concordano con le indipendenze codificate nella struttura del mercato, allora il mercato \u00e8 operativamente completo. Per insiemi di agenti tutti con costante avversione assoluta al rischio, \u00e8 sufficiente un accordo sulle indipendenze di Markov. Bossaerts, Fine e Ledyard -LSB- 2 -RSB- sviluppano un meccanismo che chiamano negoziazione a valore combinato -LRB- CVT -RRB- che consente ai trader di ordinare un portafoglio arbitrario di titoli in un'unica offerta, anzich\u00e9 suddividere l'ordine in una sequenza di offerte su singoli titoli. Se l'ordine di portafoglio viene accettato, tutte le negoziazioni implicite sui singoli titoli vengono eseguite simultaneamente, eliminando cos\u00ec il cosiddetto rischio di esecuzione che i prezzi cambino nel mezzo di una sequenza pianificata di ordini. Gli autori conducono esperimenti di laboratorio dimostrando che, anche nei mercati sottili in cui il normale trading sequenziale fallisce, la CVT supporta una determinazione dei prezzi e un\u2019allocazione efficienti. Si noti che la CVT differisce in modo significativo dalla negoziazione di titoli composti. CVT consente la negoziazione istantanea di qualsiasi combinazione lineare di titoli, mentre i titoli composti consentono titoli pi\u00f9 espressivi in \u200b\u200bgrado di codificare combinazioni booleane non lineari di eventi. Ad esempio, CVT pu\u00f2 consentire a un agente di ordinare titoli -LRB- A -RRB- e -LRB- B -RRB- in un pacchetto che ripaga come una combinazione lineare di A e B,3 ma CVT non consentir\u00e0 il costruzione di un titolo composto -LRB- A n B -RRB- che paga $ 1 se e solo se si verificano sia A che B, oppure un titolo composto -LRB- AIB -RRB-. Le aste combinatorie consentono agli offerenti di assegnare valori distinti a tutti i possibili pacchetti di beni anzich\u00e9 solo ai singoli beni. I titoli composti differiscono dalle aste combinatorie per concetto e complessit\u00e0. I titoli composti consentono agli offerenti di costruire una scommessa arbitraria su uno qualsiasi dei 22n possibili eventi composti esprimibili come funzioni logiche degli n eventi base, condizionata a qualsiasi altro dei 22n eventi composti. Gli agenti ottimizzano in base alle proprie probabilit\u00e0 soggettive e all'attitudine al rischio -LRB- e, in generale, alle loro convinzioni sulle convinzioni e utilit\u00e0 di altri agenti, ad infinitum -RRB-. Il problema centrale del banditore \u00e8 identificare le opportunit\u00e0 di arbitraggio: cio\u00e8 abbinare le scommesse senza assumersi alcun rischio. Le aste combinatorie, invece, consentono offerte su uno qualsiasi dei 2n panieri di n beni. l\u2019incertezza \u2013 e quindi il rischio \u2013 non viene considerata. Il problema centrale del banditore \u00e8 massimizzare il benessere sociale. Si noti inoltre che i problemi risiedono in diverse classi di complessit\u00e0.Mentre la chiusura di un'asta combinatoria \u00e8 polinomiale nel caso divisibile e NP-completa nel caso indivisibile, l'abbinamento in un mercato di titoli composto \u00e8 NP-completo nel caso divisibile ed Ep2-completo nel caso indivisibile. Infatti, anche il problema di decidere se due offerte su titoli composti coincidono, anche nel caso divisibile, \u00e8 NP-completo -LRB- vedi Sezione 5.2 -RRB-. In un certo senso \u00e8 possibile tradurre il nostro problema di abbinamento per titoli composti in un problema analogo per la compensazione di scambi combinatori bilaterali -LSB- 31 -RSB- di dimensione esponenziale. Nello specifico, se consideriamo il profitto in un particolare stato come un bene, allora i titoli composti possono essere visti come pacchetti di quantit\u00e0 frazionarie -LRB- di tali beni -RRB-. Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Affrontiamo questi problemi di seguito.Affrontiamo questi problemi di seguito.", "keyphrases": ["scommessa combinatoria", "effetto probabile valutare", "combinazione logica arbitraria", "composto sicuro", "rete bayesiana", "commercio a valore combinato", "algoritmo approssimativo", "vettore di profitto", "caso trattabile", "base sicura"]}
{"file_name": "J-15", "text": "Scomposizione generalizzata del valore e aste multiattributo strutturate ABSTRACT I meccanismi di asta multiattributo generalmente rimangono agnostici riguardo alle preferenze dei trader o presuppongono forme altamente restrittive, come la piena additivit\u00e0. Le preferenze reali spesso mostrano dipendenze tra attributi, ma possono possedere una struttura che pu\u00f2 essere utilmente sfruttata per snellire la comunicazione e semplificare il funzionamento di un'asta multiattributo. Sviluppiamo tale struttura utilizzando la teoria delle funzioni di valore misurabili, una rappresentazione dell'utilit\u00e0 cardinale basata su un ordine sottostante sulle differenze di preferenza. Un insieme di relazioni di indipendenza condizionale locale su tali differenze supporta una rappresentazione generalizzata delle preferenze additive, che scompone l\u2019utilit\u00e0 attraverso gruppi sovrapposti di attributi correlati. Introduciamo un meccanismo d'asta iterativo che mantiene i prezzi su cluster locali di attributi piuttosto che sull'intero spazio delle configurazioni congiunte. Quando le preferenze dei trader sono coerenti con la struttura additiva generalizzata dell'asta, il meccanismo produce allocazioni approssimativamente ottimali, a prezzi VCG approssimativi. 1. INTRODUZIONE I meccanismi di negoziazione multiattributo estendono i tradizionali meccanismi basati esclusivamente sul prezzo facilitando la negoziazione su una serie di attributi predefiniti che rappresentano vari aspetti non legati al prezzo dell'operazione. Invece di negoziare su un bene o servizio completamente definito, un meccanismo multiattributo ritarda l\u2019impegno verso configurazioni specifiche finch\u00e9 non vengono identificati i candidati pi\u00f9 promettenti. Ad esempio, l'ufficio acquisti di un'azienda pu\u00f2 utilizzare un'asta multiattributo per selezionare un fornitore di dischi rigidi. Per tenere conto delle preferenze dei trader, il meccanismo dell'asta deve estrarre informazioni valutative su un dominio complesso di configurazioni multidimensionali. Costruire e comunicare una specificazione completa delle preferenze pu\u00f2 rappresentare un grave onere anche per un numero moderato di attributi, pertanto le aste pratiche multiattributo devono accogliere specifiche parziali o supportare l'espressione compatta delle preferenze assumendo una forma semplificata. La forma multiattributo di gran lunga pi\u00f9 popolare da adottare \u00e8 la pi\u00f9 semplice: una rappresentazione additiva in cui il valore complessivo \u00e8 una combinazione lineare di valori associati a ciascun attributo. Ad esempio, diverse proposte recenti per aste iterative multiattributo -LSB- 2, 3, 8, 19 -RSB- richiedono rappresentazioni di preferenze aggiuntive. Tale additivit\u00e0 riduce esponenzialmente la complessit\u00e0 della specificazione delle preferenze -LRB- rispetto al caso discreto generale -RRB-, ma preclude l'espressione di qualsiasi interdipendenza tra gli attributi. In pratica, tuttavia, le interdipendenze tra gli attributi naturali sono abbastanza comuni. In tali casi una funzione di valore additivo potrebbe non essere in grado di fornire nemmeno un\u2019approssimazione ragionevole delle preferenze reali. D\u2019altro canto, i modelli pienamente generali sono intrattabili,ed \u00e8 ragionevole aspettarsi che le preferenze multiattributo mostrino una certa struttura. Il nostro obiettivo, quindi, \u00e8 identificare le rappresentazioni strutturate pi\u00f9 sottili ma pi\u00f9 ampiamente applicabili e sfruttare queste propriet\u00e0 delle preferenze nei meccanismi di scambio. Proponiamo un meccanismo d'asta iterativo basato proprio su tale struttura di preferenza flessibile. Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo periodo di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo termine di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 delle preferenze. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo termine di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.", "keyphrases": ["asta", "asta multiattributo", "preferisco l'handl", "teoria della funzione valore misura", "meccanismo dell'asta iter", "mvf", "gau", "asta base gai"]}
{"file_name": "I-7", "text": "Impegno ed estorsione * ABSTRACT Assumere impegni, ad esempio attraverso promesse e minacce, consente a un giocatore di sfruttare i punti di forza della propria posizione strategica cos\u00ec come i punti deboli di quella dei suoi avversari. Quali impegni un giocatore pu\u00f2 assumere con credibilit\u00e0 dipende dalle circostanze. In alcuni, un giocatore pu\u00f2 impegnarsi solo nell'esecuzione di un'azione, in altri pu\u00f2 impegnarsi in modo condizionale rispetto alle azioni degli altri giocatori. Alcune situazioni consentono addirittura impegni su impegni o impegni verso azioni randomizzate. Esploriamo le propriet\u00e0 formali di questi tipi di impegno condizionato -LRB- -RRB- e le loro interrelazioni. Per evitare incoerenze tra gli impegni condizionati, assumiamo un ordine in cui i giocatori assumono i propri impegni. Centrale nelle nostre analisi \u00e8 la nozione di estorsione, che definiamo, per un dato ordine di giocatori, come un profilo che contiene, per ciascun giocatore, un impegno ottimale dati gli impegni dei giocatori che si sono impegnati in precedenza. Su questa base, indaghiamo per diversi tipi di impegno se sia vantaggioso impegnarsi prima piuttosto che dopo, e in che modo i risultati ottenuti attraverso le estorsioni si collegano all\u2019induzione a ritroso e all\u2019efficienza paretiana. 1. INTRODUZIONE Da un lato, il minimo che ci si pu\u00f2 aspettare dalla teoria dei giochi \u00e8 che fornisca una risposta alla domanda: quali azioni massimizzano l'utilit\u00e0 attesa di un agente in situazioni di processo decisionale interattivo. Da questa prospettiva, il modello formale di un gioco in forma strategica delinea solo le caratteristiche strategiche di una situazione interattiva. Oltre alla semplice scelta ed esecuzione di un'azione da una serie di azioni, possono esserci anche altri percorsi aperti a un agente. Ad esempio, la situazione strategica del territorio pu\u00f2 essere tale che una promessa, una minaccia o una combinazione di entrambe sarebbero pi\u00f9 conduttrici ai suoi fini. Allo stesso modo, una minaccia riesce a scoraggiare un agente solo se si pu\u00f2 far credere a quest\u2019ultimo che il minacciatore \u00e8 tenuto a mettere in atto la minaccia, nel caso in cui questa venga ignorata. In questo senso, le promesse e le minacce implicano essenzialmente un impegno da parte di chi le fa, limitando cos\u00ec di proposito la sua libert\u00e0 di scelta. Promesse e minacce sintetizzano uno dei fenomeni fondamentali e a prima vista forse pi\u00f9 sorprendenti della teoria dei giochi: pu\u00f2 accadere che un giocatore possa migliorare la propria posizione strategica limitando la propria libert\u00e0 di azione. Per impegni intendiamo tali limitazioni del proprio spazio di azione. L\u2019azione stessa potrebbe essere vista come l\u2019impegno finale. Compiere una particolare azione significa farlo escludendo tutte le altre azioni. Gli impegni assumono forme diverse e pu\u00f2 dipendere dalle circostanze quali possono essere assunti in modo credibile e quali no. Oltre a impegnarsi semplicemente a compiere un'azione, un agente potrebbe subordinare il suo impegno alle azioni di altri agenti, come fa, ad esempio, il rapitore, quando promette di liberare un ostaggio dietro pagamento di un riscatto,minacciando di tagliargli un altro dito, altrimenti. Alcune situazioni consentono addirittura impegni su impegni o impegni verso azioni randomizzate. Concentrandosi sulla selezione delle azioni piuttosto che sugli impegni, potrebbe sembrare che la concezione della teoria dei giochi come mera teoria delle decisioni interattive sia troppo ristretta. A questo riguardo, il punto di vista di Schelling potrebbe sembrare mostrare una comprensione pi\u00f9 completa di ci\u00f2 che la teoria dei giochi cerca di realizzare. Si potrebbe obiettare che gli impegni potrebbero essere visti come le azioni di un gioco pi\u00f9 ampio. -LSB-... -RSB- Quello che vogliamo \u00e8 una teoria che sistematizzi lo studio dei vari ingredienti universali che compongono la struttura-mossa dei giochi; un modello troppo astratto li mancher\u00e0. -LSB- 9, pp. 156-7 -RSB- La nostra preoccupazione riguarda queste tattiche di impegno, anche se la nostra analisi \u00e8 limitata a situazioni in cui i giocatori possono impegnarsi in un dato ordine e dove assumiamo gli impegni che i giocatori possono assumere sono dati. Nonostante l'avvertimento di Schelling per un quadro troppo astratto, il nostro approccio si baser\u00e0 sulla nozione formale di estorsione, che proporremo nella Sezione 4 come tattica uniforme per una classe completa di situazioni in cui gli impegni possono essere assunti in sequenza. Su questa base affrontiamo questioni come l'utilit\u00e0 di certi tipi di impegno in diverse situazioni -LRB- giochi strategici -RRB- o se sia meglio impegnarsi presto piuttosto che tardi. Forniamo anche un quadro per la valutazione di questioni pi\u00f9 generali di teoria dei giochi come la relazione tra le estorsioni e l\u2019induzione all\u2019indietro o l\u2019efficienza paretiana. Ad esempio, si \u00e8 sostenuto che gli impegni siano importanti per l'interazione degli agenti software nonch\u00e9 per la progettazione dei meccanismi. Nel primo caso, l\u2019incapacit\u00e0 di riprogrammare un agente software al volo pu\u00f2 essere vista come un impegno a rispettarne le specifiche e quindi sfruttata per rafforzare la propria posizione strategica in un context multiagente. Un meccanismo, d'altro canto, potrebbe essere visto come un insieme di impegni che orientano il comportamento dei giocatori in un certo modo desiderato -LRB- vedi, ad esempio, -LSB- 2 -RSB- -RRB-. Questi giochi analizzano situazioni in cui un leader si impegna in una strategia pura o mista e un numero di seguaci, che poi agiscono simultaneamente. Dopo aver discusso brevemente il lavoro correlato nella Sezione 2, presentiamo il quadro formale della teoria dei giochi, in cui definiamo le nozioni di tipo di impegno cos\u00ec come gli impegni condizionati e incondizionati -LRB- Sezione 3 -RRB-. Nella sezione 4 proponiamo il concetto generico di estorsione, che per ciascuna tipologia di impegno coglie l'idea di un profilo di impegno ottimale. La sezione 5 esamina brevemente alcune altre tipologie di impegno, come gli impegni condizionali induttivi, misti e misti. 2. LAVORI CORRELATI L'impegno \u00e8 un concetto centrale nella teoria dei giochi. La possibilit\u00e0 di assumere impegni distingue la teoria dei giochi cooperativa da quella non cooperativa -LSB- 4, 6 -RSB-. I giochi di leadership, come accennato nell\u2019introduzione,analizzare l'impegno verso strategie pure o miste in quello che \u00e8 essenzialmente un context a due giocatori -LSB- 15, 16 -RSB-. In modo informale, Schelling -LSB- 9 -RSB- ha sottolineato l'importanza delle promesse, delle minacce e simili per una corretta comprensione dell'interazione sociale. A un livello pi\u00f9 formale, le minacce sono presenti anche nella teoria della contrattazione. Il gioco delle minacce di Nash -LSB- 5 -RSB- e le minacce razionali di Harsanyi -LSB- 3 -RSB- sono due importanti primi esempi. Inoltre, gli impegni hanno giocato un ruolo significativo nella teoria della selezione dell'equilibrio -LRB- vedi, ad esempio, -LSB- 13 -RSB-. Negli ultimi anni, la teoria dei giochi \u00e8 diventata quasi indispensabile come strumento di ricerca per l\u2019informatica e la ricerca su agenti -LRB- multi -RRB-. Gli impegni non sono affatto passati inosservati -LRB- vedi Figura 1: Impegnarsi in una strategia dominata pu\u00f2 essere vantaggioso. ad esempio, -LSB- 1, 11 -RSB- -RRB-. Recentemente anche gli aspetti strategici degli impegni hanno attirato l'attenzione degli informatici. Pertanto, Conitzer e Sandholm -LSB- 2 -RSB- hanno studiato la complessit\u00e0 computazionale del calcolo della strategia ottimale a cui impegnarsi in giochi in forma normale e bayesiani. Sandholm e Lesser -LSB- 8 -RSB- impiegano impegni livellati per la progettazione di sistemi multiagente in cui gli accordi contrattuali non sono completamente vincolanti. Un altro collegamento tra impegni e informatica \u00e8 stato sottolineato da Samet -LSB- 7 -RSB- e Tennenholtz -LSB- 12 -RSB-. Il loro punto di partenza \u00e8 la constatazione che i programmi possono essere utilizzati per formulare impegni condizionati ai programmi di altri sistemi. Il nostro approccio \u00e8 simile all'impostazione Stackleberg in quanto presupponiamo un ordine in cui i giocatori si impegnano. Noi, tuttavia, consideriamo diverse tipologie di impegno, tra cui gli impegni condizionati, e proponiamo un concetto di soluzione generico. 6. SOMMARIO E CONCLUSIONE In alcune situazioni gli agenti possono rafforzare la loro posizione strategica impegnandosi in una particolare linea d'azione. Esistono vari tipi di impegno, ad esempio puro, misto e condizionato. Il tipo di impegno che un agente \u00e8 in grado di assumere dipende essenzialmente dalla situazione considerata. Se gli agenti commettono in un ordine particolare, esiste una tattica comune all'assunzione di impegni di qualsiasi tipo, che abbiamo formalizzato mediante il concetto di estorsione. Questo concetto generico di estorsione pu\u00f2 essere analizzato in abstracto. Inoltre, sulla base di esso \u00e8 possibile confrontare formalmente e sistematicamente le diverse tipologie di impegno. Abbiamo visto che il tipo di impegno che un agente pu\u00f2 assumere ha un profondo impatto su ci\u00f2 che un agente pu\u00f2 ottenere in una situazione simile a un gioco. In alcune situazioni un giocatore \u00e8 molto aiutato se \u00e8 nella posizione di impegnarsi condizionatamente, mentre in altre sarebbero pi\u00f9 proficui impegni misti. Ci\u00f2 solleva la questione delle caratteristiche formali caratteristiche delle situazioni in cui \u00e8 vantaggioso per un giocatore poter assumere impegni di un determinato tipo.Un altro problema che lasciamo per la ricerca futura \u00e8 la complessit\u00e0 computazionale nel trovare un\u2019estorsione per i diversi tipi di impegno.", "keyphrases": ["commettere", "credibile", "teoria del gioco", "decidere", "posizione strategica", "libert\u00e0 di azione", "sistema multiag", "distribuire il calcolo", "mercato degli elettroni", "estorcere", "insieme di Stackleberg", "impegno in condizioni ottimali", "tipo di commit sequenziale", "indurre ipotesi", "pareto efficace", "pareto effici condit extort"]}
{"file_name": "J-10", "text": "Comprendere il comportamento degli utenti nella segnalazione di feedback online SOMMARIO Le recensioni online sono diventate sempre pi\u00f9 popolari come mezzo per giudicare la qualit\u00e0 di vari prodotti e servizi. Il lavoro precedente ha dimostrato che resoconti contraddittori e pregiudizi degli utenti sottostanti rendono difficile giudicare il vero valore di un servizio. In questo documento, esaminiamo i fattori sottostanti che influenzano il comportamento degli utenti quando riportano il feedback. Esaminiamo due fonti di informazione oltre alle valutazioni numeriche: prove linguistiche provenienti dal commento testuale che accompagna una revisione e modelli nella sequenza temporale dei resoconti. Innanzitutto mostriamo che i gruppi di utenti che discutono ampiamente di una determinata funzionalit\u00e0 hanno maggiori probabilit\u00e0 di concordare una valutazione comune per quella funzionalit\u00e0. In secondo luogo, mostriamo che la valutazione di un utente riflette in parte la differenza tra la qualit\u00e0 reale e le aspettative di qualit\u00e0 precedenti desunte dalle recensioni precedenti. Entrambi ci forniscono un modo meno rumoroso per produrre stime di valutazione e rivelare le ragioni alla base dei pregiudizi degli utenti. Le nostre ipotesi sono state convalidate da prove statistiche provenienti dalle recensioni degli hotel sul sito TripAdvisor. 1. MOTIVAZIONI considerano attentamente il feedback online quando prendono decisioni di acquisto e sono disposti a pagare premi di reputazione per prodotti o servizi che hanno una buona reputazione. Analisi recenti, tuttavia, sollevano importanti questioni riguardanti la capacit\u00e0 dei forum esistenti di riflettere la reale qualit\u00e0 di un prodotto. In assenza di incentivi chiari, gli utenti con una prospettiva moderata non si preoccuperanno di esprimere le proprie opinioni, il che porta a un campione di recensioni non rappresentativo. In queste circostanze, utilizzare la media aritmetica per prevedere la qualit\u00e0 -LRB- come fa effettivamente la maggior parte dei forum -RRB- fornisce all'utente tipico uno stimatore con un'elevata varianza che spesso \u00e8 falsa. Migliorare il modo in cui aggreghiamo le informazioni disponibili dalle recensioni online richiede una profonda comprensione dei fattori sottostanti che influenzano il comportamento di valutazione degli utenti. Hu et al. -LSB- 12 -RSB- propongono il \"modello Brag-and-Moan\" in cui gli utenti valutano solo se la loro utilit\u00e0 del prodotto -LRB- ricavata da una distribuzione normale -RRB- cade al di fuori di un intervallo mediano. Gli autori concludono che il modello spiega la distribuzione empirica dei report e offre spunti su modi pi\u00f9 intelligenti per stimare la reale qualit\u00e0 del prodotto. Nel presente articolo estendiamo questa linea di ricerca e tentiamo di spiegare ulteriori fatti sul comportamento degli utenti quando riportano feedback online. Utilizzando le recensioni effettive degli hotel dal sito TripAdvisor2, prendiamo in considerazione due ulteriori fonti di informazioni oltre alle valutazioni numeriche di base inviate dagli utenti. La prima \u00e8 una semplice evidenza linguistica derivante dalla revisione testuale che solitamente accompagna le valutazioni numeriche. Abbiamo scoperto che gli utenti che commentano pi\u00f9 spesso la stessa funzionalit\u00e0 hanno maggiori probabilit\u00e0 di concordare una valutazione numerica comune per quella particolare funzionalit\u00e0. Intuitivamente, lunghi commenti rivelano all'utente l'importanza della funzionalit\u00e0.Poich\u00e9 le persone tendono ad essere pi\u00f9 informate sugli aspetti che considerano importanti, si potrebbe presumere che gli utenti che discutono una determinata funzionalit\u00e0 in modo pi\u00f9 dettagliato abbiano maggiore autorit\u00e0 nella valutazione di tale funzionalit\u00e0. Successivamente esaminiamo la relazione tra una recensione. Figura 1: la pagina di TripAdvisor che mostra le recensioni per un famoso hotel di Boston. Il nome dell'hotel e gli annunci pubblicitari sono stati deliberatamente cancellati. e le recensioni che lo hanno preceduto. Un esame delle recensioni online mostra che le valutazioni fanno spesso parte dei thread di discussione, in cui un post non \u00e8 necessariamente indipendente dagli altri post. Si possono vedere, ad esempio, utenti che si sforzano di contraddire o di concordare con veemenza con le osservazioni degli utenti precedenti. Analizzando la sequenza temporale dei report, concludiamo che le revisioni passate influenzano i report futuri, poich\u00e9 creano alcune aspettative precedenti riguardo alla qualit\u00e0 del servizio. La percezione soggettiva dell'utente \u00e8 influenzata dal divario tra l'aspettativa precedente e l'effettiva prestazione del servizio -LSB- 17, 18, 16, 21 -RSB- che si rifletter\u00e0 successivamente nella valutazione dell'utente. Proponiamo un modello che cattura la dipendenza dei rating dalle aspettative precedenti e lo convalida utilizzando i dati empirici che abbiamo raccolto. Entrambi i risultati possono essere utilizzati per migliorare il modo in cui i meccanismi di reputazione aggregano le informazioni provenienti dalle singole recensioni. Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.", "keyphrases": ["recensione in linea", "meccanico di reputazione", "stima della qualit\u00e0 caratteristica per caratteristica", "assenza di chiaro incentivo", "utilit\u00e0 del prodotto", "modello di vantarsi e lamentarsi", "valutare", "ottimo probabilmente bimodale", "distribuzione a forma di U", "orientamento semantico della valutazione del prodotto", "correlare", "ampio arco di tempo"]}
{"file_name": "C-1", "text": "Discovery scalabile dei servizi di rete basato su UDDI * ABSTRACT Il rilevamento efficiente dei servizi di rete \u00e8 essenziale per il successo del grid computing. La standardizzazione delle griglie basate sui servizi web ha comportato la necessit\u00e0 di implementare meccanismi scalabili di scoperta dei servizi web nelle griglie. Anche se UDDI \u00e8 stato di fatto lo standard industriale per la scoperta dei servizi web, ha imposto requisiti di stretta replica tra i registri e la mancanza del controllo autonomo ne ha gravemente ostacolato la diffusione e l\u2019utilizzo diffuso. Con l'avvento del grid computing, il problema della scalabilit\u00e0 dell'UDDI diventer\u00e0 un ostacolo che ne impedir\u00e0 l'implementazione nelle reti. In questo articolo presentiamo la nostra architettura distribuita di rilevamento dei servizi Web, denominata DUDE -LRB- Distributed UDDI Deployment Engine -RRB-. DUDE sfrutta DHT -LRB- Distributed Hash Tables -RRB- come meccanismo di incontro tra pi\u00f9 registri UDDI. DUDE consente ai consumatori di interrogare pi\u00f9 registri, consentendo allo stesso tempo alle organizzazioni di avere un controllo autonomo sui propri registri. . Sulla base del prototipo preliminare su PlanetLab, riteniamo che l'architettura DUDE possa supportare una distribuzione efficace dei registri UDDI rendendo cos\u00ec l'UDDI pi\u00f9 robusto e risolvendo anche i suoi problemi di ridimensionamento. Inoltre, l'architettura DUDE per la distribuzione scalabile pu\u00f2 essere applicata oltre UDDI a qualsiasi meccanismo di Grid Service Discovery. 1. INTRODUZIONE La scoperta efficiente dei servizi di rete \u00e8 essenziale per il successo del grid computing. meccanismi di scoperta da implementare nelle griglie. I servizi di Grid Discovery offrono la possibilit\u00e0 di monitorare e scoprire risorse e servizi sulle griglie. Forniscono la possibilit\u00e0 di interrogare e sottoscrivere informazioni su risorse/servizi. Lo stato dei dati deve essere mantenuto in uno stato soft in modo che le informazioni pi\u00f9 recenti siano sempre disponibili. Le informazioni raccolte devono essere fornite a una variet\u00e0 di sistemi allo scopo di utilizzare la griglia o fornire informazioni sintetiche. Tuttavia, il problema fondamentale \u00e8 la necessit\u00e0 di essere scalabili per gestire enormi quantit\u00e0 di dati provenienti da pi\u00f9 fonti. La comunit\u00e0 dei servizi web ha affrontato la necessit\u00e0 di rilevamento dei servizi, prima che le reti fossero anticipate, tramite uno standard industriale chiamato UDDI. Tuttavia, anche se UDDI \u00e8 stato di fatto lo standard industriale per il rilevamento dei servizi web, ha imposto requisiti di stretta replica tra i registri e la mancanza di controllo autonomo, tra le altre cose ha gravemente ostacolato la sua diffusione e utilizzo diffuso -LSB- 7 -RSB- . Con l'avvento del grid computing, il problema della scalabilit\u00e0 con UDDI diventer\u00e0 un ostacolo che ne impedir\u00e0 l'implementazione nelle reti. Questo documento affronta il problema della scalabilit\u00e0 e un modo per trovare servizi su pi\u00f9 registri in UDDI sviluppando un'architettura di rilevamento dei servizi Web distribuiti. La distribuzione della funzionalit\u00e0 UDDI pu\u00f2 essere ottenuta in diversi modi e forse utilizzando diverse infrastrutture/piattaforme di calcolo distribuite -LRB- ad esempio CORBA, DCE, ecc. -RRB-.In questo articolo esploriamo come sfruttare la tecnologia Distributed Hash Table -LRB- DHT -RRB- per sviluppare un'architettura scalabile di rilevamento dei servizi Web distribuiti. Un DHT \u00e8 un sistema distribuito peer-to-peer -LRB- P2P -RRB- che forma un overlay strutturato che consente un routing pi\u00f9 efficiente rispetto alla rete sottostante. Il primo fattore motivante \u00e8 la semplicit\u00e0 intrinseca dell'astrazione put/get fornita dai DHT, che semplifica la creazione rapida di applicazioni sui DHT. Altre piattaforme/middleware di elaborazione distribuita, pur fornendo pi\u00f9 funzionalit\u00e0, hanno un sovraccarico e una complessit\u00e0 molto pi\u00f9 elevati. Il secondo fattore motivante deriva dal fatto che i DHT sono uno strumento relativamente nuovo per costruire applicazioni distribuite e vorremmo testarne il potenziale applicandolo al problema della distribuzione di UDDI. Nella sezione successiva, forniamo una breve panoramica dei servizi di informazione di rete, UDDI e le sue limitazioni, seguita da una panoramica dei DHT nella sezione 3. La sezione 4 descrive la nostra architettura proposta con dettagli sui casi d'uso. Nella Sezione 5, l'Articolo 2 descrive la nostra attuale implementazione, seguita dai nostri risultati nella Sezione 6. La Sezione 7 discute il lavoro correlato in quest'area e la Sezione 8 contiene le nostre osservazioni conclusive. 2. BACKGROUND 2.1 Grid Service Discovery Il grid computing \u00e8 basato su standard che utilizzano la tecnologia dei servizi web. Nell'architettura presentata in -LSB-6 -RSB-, la funzione di rilevamento del servizio \u00e8 assegnata ad un servizio Grid specializzato chiamato Registro. La sua funzione di base lo rende simile al registro UDDI. Per ottenere la scalabilit\u00e0, i servizi Index di diversi contenitori Globus possono registrarsi tra loro in modo gerarchico per aggregare i dati. Nello specifico, questo approccio non si adatta bene ai sistemi che cercano di sfruttare la convergenza del grid e del peer-to-peer computing -LSB- 5 -RSB-. 2.2 UDDI Al di l\u00e0 del grid computing, il problema del service discovery deve essere affrontato pi\u00f9 in generale nella comunit\u00e0 dei servizi web. Ancora una volta, la scalabilit\u00e0 \u00e8 una delle principali preoccupazioni poich\u00e9 milioni di acquirenti alla ricerca di servizi specifici hanno bisogno di trovare tutti i potenziali venditori del servizio che possano soddisfare le loro esigenze. Sebbene esistano diversi modi per farlo, i comitati per gli standard dei servizi Web soddisfano questo requisito attraverso una specifica denominata UDDI -LRB- Universal Description, Discovery, and Integration -RRB-. Un registro UDDI consente a un'azienda di inserire tre tipi di informazioni in un registro UDDI: pagine bianche, pagine gialle e pagine verdi. L'intento di UDDI \u00e8 quello di funzionare come un registro per i servizi proprio come le Pagine Gialle sono un registro per le imprese. Proprio come nelle Pagine Gialle, le aziende registrano se stesse e i loro servizi in diverse categorie. In UDDI, le Pagine Bianche sono un elenco delle entit\u00e0 aziendali. Le pagine verdi rappresentano le informazioni tecniche necessarie per usufruire di un determinato servizio. Pertanto, esplorando un registro UDDI,uno sviluppatore dovrebbe essere in grado di individuare un servizio e un'azienda e scoprire come richiamare il servizio. Quando fu inizialmente offerto l'UDDI, offriva molto potenziale. Tuttavia, oggi scopriamo che l'UDDI non \u00e8 stato ampiamente utilizzato in Internet. In effetti, gli unici usi conosciuti dell'UDDI sono quelli conosciuti come registri UDDI privati \u200b\u200ball'interno dei confini di un'azienda. I lettori possono fare riferimento a -LSB- 7 -RSB- per un articolo recente che discute le carenze di UDDI e le propriet\u00e0 di un registro di servizio ideale. Il miglioramento dello standard UDDI continua a pieno ritmo e la versione 3 dell'UDDI -LRB- V3 -RRB- \u00e8 stata recentemente approvata come standard OASIS. Tuttavia, l\u2019UDDI oggi presenta problemi che non sono stati affrontati, come la scalabilit\u00e0 e l\u2019autonomia dei singoli registri. UDDI V3 fornisce un supporto pi\u00f9 ampio per ambienti multi-registro basati sulla portabilit\u00e0 delle chiavi Consentendo di registrare nuovamente le chiavi in \u200b\u200bpi\u00f9 registri, viene effettivamente abilitata la capacit\u00e0 di collegare registri in varie topologie. Tuttavia, a questo punto non viene fornita alcuna descrizione normativa di queste topologie nella specifica UDDI. I miglioramenti all'interno di UDDI V3 che consentono il supporto per ambienti multiregistro sono significativi e aprono la possibilit\u00e0 di ulteriori ricerche su come possono essere distribuiti ambienti multiregistro. Uno scenario di distribuzione consigliato proposto dalla specifica UDDI V3.0.2 consiste nell'utilizzare i registri aziendali UDDI come registri root ed \u00e8 possibile abilitarlo utilizzando la nostra soluzione. 2.3 Tabelle Hash Distribuite Una Tabella Hash Distribuita -LRB- DHT -RRB- \u00e8 un sistema distribuito peer-to-peer -LRB- P2P -RRB- che forma un overlay strutturato che consente un routing pi\u00f9 efficiente rispetto alla rete sottostante. Mantiene una raccolta di coppie chiave-valore sui nodi che partecipano a questa struttura grafica. Per la nostra distribuzione, una chiave \u00e8 l'hash di una parola chiave dal nome o dalla descrizione di un servizio. Ci saranno pi\u00f9 valori per questa chiave, uno per ciascun servizio contenente la parola chiave. Proprio come qualsiasi altra struttura dati della tabella hash, fornisce una semplice interfaccia composta da operazioni put -LRB- -RRB- e get -LRB- -RRB-. Ci\u00f2 deve essere fatto con robustezza a causa della natura transitoria dei nodi nei sistemi P2P. Le chiavi DHT sono ottenute da un ampio spazio identificatore. Una funzione hash, come MD5 o SHA-1, viene applicata al nome di un oggetto per ottenere la sua chiave DHT. I nodi in un DHT vengono anche mappati nello stesso spazio degli identificatori applicando la funzione hash al loro identificatore, come l'indirizzo IP e il numero di porta o la chiave pubblica. Lo spazio degli identificatori viene assegnato ai nodi in modo distribuito e deterministico, in modo che l'instradamento e la ricerca possano essere eseguiti in modo efficiente. I nodi di un DHT mantengono collegamenti con alcuni degli altri nodi del DHT. Lo schema di questi collegamenti \u00e8 noto come geometria del DHT. Ad esempio, nel Bamboo DHT -LSB- 11 -RSB-, e nel Pastry DHT -LSB- 8 -RSB- su cui si basa il Bamboo,i nodi mantengono i collegamenti ai nodi vicini e ad altri nodi distanti trovati in una tabella di instradamento. La tabella di routing consente un routing overlay efficiente. Per ottenere un routing o una ricerca coerente, una chiave DHT deve essere instradata al nodo con l'identificatore numericamente pi\u00f9 vicino. Per i dettagli su come vengono costruite e mantenute le tabelle di routing si rimanda il lettore a -LSB- 8, 11 -RSB-. 5. LAVORI CORRELATI Un quadro per la scoperta di servizi basati su QoS nelle griglie \u00e8 stato proposto in -LSB- 18 -RSB-. UDDIe, un registro UDDI esteso per la pubblicazione e la scoperta di servizi basati su parametri QoS, \u00e8 proposto in -LSB- 19 -RSB-. Il nostro lavoro \u00e8 complementare poich\u00e9 ci concentriamo su come federare i registri UDDI e affrontare il problema della scalabilit\u00e0 con UDDI. Il proxy DUDE pu\u00f2 pubblicare le propriet\u00e0 del servizio supportate da UDDIe nel DHT e supportare query di intervallo utilizzando le tecniche proposte per tali query su DHT. Quindi potremo offrire i vantaggi di scalabilit\u00e0 della nostra attuale soluzione ai registri UDDI e UDDIe. La scoperta dei servizi che soddisfano i requisiti di QoS e di prezzo \u00e8 stata studiata nel context di un'economia di rete, in modo che i pianificatori della rete possano utilizzare vari modelli di mercato come i mercati delle materie prime e le aste. A questo scopo \u00e8 stata proposta la Grid Market Directory -LSB- 20 -RSB-. Le descrizioni delle risorse e delle richieste sono espresse in RDF Schema, un linguaggio di markup semantico. Le regole del matchmaking sono espresse in TRIPLE, un linguaggio basato sulla logica del corno. Sebbene la nostra attuale implementazione si concentri sulla versione 2 di UDDI, in futuro prenderemo in considerazione estensioni semantiche a UDDI, WS-Discovery -LSB- 16 -RSB- e altri standard di Grid computing come Monitoring and Discovery Service -LRB- MDS -RRB- -LSB - 10 -RSB-. Quindi l'estensione pi\u00f9 semplice del nostro lavoro potrebbe comportare l'utilizzo del DHT per effettuare una ricerca iniziale basata sulla sintassi per identificare i registri locali che devono essere contattati. La convergenza del grid computing e del P2P \u00e8 stata esplorata in -LSB- 5 -RSB-. Un servizio UDDI federato -LSB- 4 -RSB- \u00e8 stato costruito sul sistema di pubblicazione-sottoscrizione PlanetP -LSB- 3 -RSB- per comunit\u00e0 P2P non strutturate. Il focus di questo lavoro \u00e8 stato la gestibilit\u00e0 del servizio federato. Il servizio UDDI \u00e8 trattato come un servizio applicativo ai sensi dell'Articolo 2 da gestire nel loro framework. Pertanto non affrontano il problema della scalabilit\u00e0 in UDDI e utilizzano invece la semplice replica. In -LSB- 21 -RSB-, gli autori descrivono un sistema di estensione UDDI -LRB- UX -RRB- che lancia una query federata solo se i risultati trovati localmente non sono adeguati. Sebbene il server UX sia posizionato come intermediario in modo simile al proxy UDDI descritto nel nostro framework DUDE, si concentra maggiormente sul framework QoS e non tenta di implementare un meccanismo di federazione senza soluzione di continuit\u00e0 come il nostro approccio basato su DHT. In -LSB- 22 -RSB- D2HT descrive un framework di scoperta costruito su DHT. Tuttavia, abbiamo scelto di utilizzare UDDI oltre a DHT. 6. CONCLUSIONI E LAVORI FUTURI In questo documento,abbiamo descritto un'architettura distribuita per supportare la scoperta su larga scala di servizi web. La nostra architettura consentir\u00e0 alle organizzazioni di mantenere un controllo autonomo sui propri registri UDDI e allo stesso tempo consentir\u00e0 ai clienti di interrogare pi\u00f9 registri contemporaneamente. Sulla base dei test iniziali del prototipo, riteniamo che l'architettura DUDE possa supportare una distribuzione efficace dei registri UDDI, rendendo cos\u00ec l'UDDI pi\u00f9 robusto e risolvendo anche i suoi problemi di ridimensionamento. Il documento ha risolto i problemi di scalabilit\u00e0 con UDDI ma non preclude l'applicazione di questo approccio ad altri meccanismi di rilevamento dei servizi. Un esempio di un altro meccanismo di rilevamento dei servizi che potrebbe trarre vantaggio da un simile approccio \u00e8 MDS di Globus Toolkit. Inoltre, intendiamo indagare altri aspetti della scoperta del servizio di rete che estendono questo lavoro. Inoltre, prevediamo di rivisitare le API del servizio per una soluzione Grid Service Discovery sfruttando le soluzioni e le specifiche disponibili, nonch\u00e9 il lavoro presentato in questo documento.", "keyphrases": ["scoperta del servizio di rete", "uddi", "distribuire il servizio web discoveri architecture", "dht base uddi registri gerarchici", "distribuire il problema", "codice dht di bamb\u00f9", "ricerca senza distinzione tra maiuscole e minuscole", "queri", "prefisso dell'avail pi\u00f9 lungo", "servizio qo-base discoveri", "controllo autonomo", "registro uddi", "problema scalabile", "stato molle"]}
{"file_name": "J-22", "text": "Scommesse sulle permutazioni ABSTRACT Consideriamo uno scenario di scommesse sulle permutazioni, in cui le persone scommettono sull'ordine finale di n candidati: ad esempio, il risultato di una corsa di cavalli. Esaminiamo il problema del banditore di abbinare senza rischi le scommesse o, equivalentemente, di trovare opportunit\u00e0 di arbitraggio tra le scommesse proposte. Richiedere agli offerenti di elencare esplicitamente gli ordini su cui vorrebbero scommettere \u00e8 sia innaturale che intrattabile, perch\u00e9 il numero di ordinamenti \u00e8 n! e il numero di sottoinsiemi di ordinamenti \u00e8 2n! . Proponiamo due linguaggi espressivi per le scommesse che sembrano naturali per gli offerenti ed esaminiamo in ciascun caso la complessit\u00e0 computazionale del problema del banditore. Le scommesse sui sottoinsiemi consentono ai trader di scommettere che un candidato finir\u00e0 classificato in un sottoinsieme di posizioni nell'ordine finale, ad esempio \"il cavallo A finir\u00e0 nelle posizioni 4, 9 o 13-21\", oppure che una posizione sar\u00e0 preso da un sottoinsieme di candidati, ad esempio \"il cavallo A, B o D finir\u00e0 nella posizione 2\". Per le scommesse sui sottoinsiemi, mostriamo che il problema del banditore pu\u00f2 essere risolto in tempo polinomiale se gli ordini sono divisibili. Le scommesse di coppia consentono ai trader di scommettere se un candidato finir\u00e0 per classificarsi pi\u00f9 in alto rispetto a un altro candidato, ad esempio \"il cavallo A batter\u00e0 il cavallo B\". Dimostriamo che il problema del banditore diventa NP-difficile per le scommesse sulle coppie. Identifichiamo una condizione sufficiente per l'esistenza di un match di scommesse di coppia che pu\u00f2 essere verificato in tempo polinomiale. Mostriamo anche che un algoritmo greedy naturale fornisce una scarsa approssimazione per ordini indivisibili. 1. INTRODUZIONE Acquistare o vendere un titolo finanziario \u00e8 in effetti una scommessa sul valore del titolo. Ad esempio, acquistare un'azione significa scommettere che il valore dell'azione \u00e8 maggiore del suo prezzo corrente. Ogni trader valuta il profitto atteso per decidere la quantit\u00e0 da acquistare o vendere in base alle proprie informazioni e alla valutazione soggettiva della probabilit\u00e0. L'interazione collettiva di tutte le scommesse porta ad un equilibrio che riflette l'aggregazione di tutte le informazioni e le convinzioni dei trader. Considera l'acquisto di un titolo al prezzo di cinquantadue centesimi, che paga 1 dollaro se e solo se un democratico vince le elezioni presidenziali americane del 2008. In questo caso di un titolo contingente all'evento, il prezzo \u2013 il valore di mercato del titolo \u2013 corrisponde direttamente alla probabilit\u00e0 stimata dell'evento. Quasi tutte le borse finanziarie e di scommesse esistenti accoppiano partner commerciali bilaterali. Ad esempio, un trader disposto ad accettare una perdita di x dollari se un democratico non vince in cambio di un profitto di y dollari se un democratico vince viene abbinato a un secondo trader disposto ad accettare il contrario. Tuttavia in molti scenari, anche se non esistono accordi bilaterali tra i commercianti, potrebbero essere possibili accordi multilaterali. Proponiamo uno scambio in cui i trader hanno una notevole flessibilit\u00e0 per esprimere le loro scommesse in modo naturale e conciso,ed esaminare la complessit\u00e0 computazionale del conseguente problema di abbinamento del banditore nell'identificazione di accordi bilaterali e multilaterali. In particolare, ci concentreremo su un context in cui i trader scommettono sull'esito di una competizione tra n candidati. Ad esempio, supponiamo che ci siano n candidati in un'elezione -LRB- oppure n cavalli in una corsa, ecc. -RRB- e quindi n! eventuali ordinamenti dei candidati dopo il conteggio dei voti finali. Come vedremo, il problema del matching pu\u00f2 essere impostato come un programma lineare o intero, a seconda che gli ordini siano rispettivamente divisibili o indivisibili. Tentare di ridurre il problema a un problema di corrispondenza bilaterale creando esplicitamente n! titoli, uno per ogni eventuale ordinamento finale, risulta essere macchinoso per gli operatori e computazionalmente impraticabile anche per n. Inoltre, l'attenzione dei trader sarebbe distribuita tra n! scelte indipendenti, facendo sembrare remota la probabilit\u00e0 che due trader convergano nello stesso momento e nello stesso luogo. Esiste un compromesso tra l'espressivit\u00e0 del linguaggio di offerta e la complessit\u00e0 computazionale del problema di abbinamento. Vogliamo offrire ai trader il linguaggio di offerta pi\u00f9 espressivo possibile mantenendo la fattibilit\u00e0 computazionale. Esploriamo due linguaggi di offerta che sembrano naturali dal punto di vista del trader. Le scommesse sui sottoinsiemi, descritte nella Sezione 3.2, consentono ai trader di scommettere su quali posizioni nella classifica cadr\u00e0 un candidato, ad esempio \"il candidato D finir\u00e0 nella posizione 1, 3-5 o 10\". Simmetricamente, i trader possono anche scommettere su quali candidati cadranno in una determinata posizione. Nella Sezione 4, deriviamo un algoritmo tempo-polinomiale per abbinare le scommesse del sottoinsieme -LRB- divisibili -RRB-. Le scommesse di coppia, descritte nella Sezione 3.3, consentono ai trader di scommettere sulla classifica finale di due candidati qualsiasi, ad esempio \"il candidato D sconfigger\u00e0 il candidato R\". Nella Sezione 5, mostriamo che l'abbinamento ottimale delle scommesse sulla coppia -LRB- divisibile o indivisibile -RRB- \u00e8 NP-difficile, attraverso una riduzione dal problema dell'insieme dell'arco di feedback minimo non ponderato. Forniamo anche una condizione sufficiente polinomialmente verificabile per l'esistenza di una coppia di scommesse e mostriamo che un algoritmo greedy offre una scarsa approssimazione per le scommesse su coppie indivisibili. 2. BACKGROUND E LAVORO CORRELATO Consideriamo le scommesse a permutazione, ovvero le scommesse sull'esito di una competizione tra n candidati. Il risultato finale o ES dello stato \u00e8 una classifica ordinale degli n candidati. Ad esempio, i candidati potrebbero essere i cavalli in una corsa e il risultato l'elenco dei cavalli in ordine crescente in base al tempo finale. Lo spazio degli stati S contiene tutti gli n! permutazioni mutuamente esclusive ed esaustive dei candidati. In pratica in pista, ciascuno di questi diversi tipi di scommesse viene elaborato in pool o gruppi separati. Descriviamo invece uno scambio centrale in cui tutte le scommesse sul risultato vengono elaborate insieme, aggregando cos\u00ec la liquidit\u00e0 e garantendo che l'inferenza informativa avvenga automaticamente. Idealmente,vorremmo consentire ai trader di scommettere su qualsiasi propriet\u00e0 dell'ordine finale che preferiscono, espressa esattamente nella lingua che preferiscono. In pratica, consentire un linguaggio troppo flessibile crea un onere computazionale per il banditore che tenta di abbinare i commercianti disponibili. Esploriamo il compromesso tra l'espressivit\u00e0 del linguaggio di offerta e la complessit\u00e0 computazionale del problema di abbinamento. Consideriamo un quadro in cui le persone propongono di acquistare titoli che pagano 1 dollaro se e solo se alcune propriet\u00e0 dell'ordine finale sono vere. I trader dichiarano il prezzo che sono disposti a pagare per azione e il numero di azioni che vorrebbero acquistare. Un ordine divisibile consente al trader di ricevere meno azioni di quelle richieste, purch\u00e9 venga rispettato il vincolo di prezzo; un ordine indivisibile \u00e8 un ordine tutto o niente. titoli, uno per ogni stato s ES -LRB- o di fatto qualsiasi insieme di n! titoli linearmente indipendenti -RRB-. Questo \u00e8 il cosiddetto mercato completo dei titoli Arrow-Debreu -LSB- 1 -RSB- per il nostro context. In pratica, i trader non vogliono avere a che fare con la specificazione di basso livello di ordini completi: le persone pensano in modo pi\u00f9 naturale in termini di propriet\u00e0 di alto livello degli ordini. Inoltre, operando n! titoli \u00e8 irrealizzabile in pratica da un punto di vista computazionale al crescere di n. Un linguaggio di offerta molto semplice potrebbe consentire ai trader di scommettere solo su chi vince la competizione, come avviene nel pool di \"vincite\" negli ippodromi. Il corrispondente problema di abbinamento \u00e8 polinomiale, tuttavia il linguaggio non \u00e8 molto espressivo. Un trader che crede che A sconfigger\u00e0 B, ma che nessuno dei due vincer\u00e0 a titolo definitivo, non pu\u00f2 impartire utilmente le sue informazioni al mercato. Lo spazio dei prezzi del mercato rivela le stime collettive delle probabilit\u00e0 di vincita ma nient\u2019altro. Il nostro obiettivo \u00e8 trovare linguaggi quanto pi\u00f9 espressivi e intuitivi possibile e rivelare quante pi\u00f9 informazioni possibili, pur mantenendo la fattibilit\u00e0 computazionale. Il nostro lavoro \u00e8 in diretta analogia con il lavoro di Fortnow et. Mentre esploriamo la combinatoria delle permutazioni, Fortnow et. al. esplorare la combinatoria booleana. Gli autori considerano uno spazio degli stati dei 2n possibili risultati di n variabili binarie. I trader esprimono le scommesse secondo la logica booleana. Gli autori mostrano che l'abbinamento divisibile \u00e8 co-NP-completo e l'abbinamento indivisibile \u00e8 p2-completo. Hanson -LSB- 9 -RSB- descrive un meccanismo di regole di punteggio di mercato che pu\u00f2 consentire di scommettere su un numero combinatorio di risultati. Il mercato inizia con una distribuzione di probabilit\u00e0 congiunta su tutti i risultati. Funziona come una versione sequenziale di una regola di punteggio. Qualsiasi trader pu\u00f2 modificare la distribuzione di probabilit\u00e0 purch\u00e9 accetti di pagare il trader pi\u00f9 recente in base alla regola del punteggio. Il market maker paga l\u2019ultimo trader. Pertanto, si assume il rischio e pu\u00f2 subire perdite. I meccanismi delle regole di punteggio di mercato hanno la caratteristica di limitare la perdita del market maker nel caso peggiore. Tuttavia, gli aspetti computazionali su come far funzionare il meccanismo non sono stati completamente esplorati.I nostri meccanismi hanno un banditore che non si assume alcun rischio e abbina solo gli ordini. Le aste combinatorie consentono agli offerenti di attribuire valori distinti a pacchetti di beni piuttosto che solo a singoli beni. L\u2019incertezza e il rischio in genere non vengono presi in considerazione e il problema centrale del banditore \u00e8 massimizzare il benessere sociale. I nostri meccanismi consentono ai trader di costruire scommesse per un evento con n! risultati. Vengono presi in considerazione l'incertezza e il rischio e il problema del banditore \u00e8 esplorare le opportunit\u00e0 di arbitraggio e abbinare le scommesse senza rischi. 6. CONCLUSIONE Consideriamo uno scenario di scommesse a permutazione, in cui i trader scommettono sull'ordine finale di n candidati. Sebbene sia innaturale e intrattabile consentire ai trader di scommettere direttamente sul n! diversi ordinamenti finali, proponiamo due linguaggi espressivi per le scommesse, le scommesse sui sottoinsiemi e le scommesse sulle coppie. In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppie. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppia. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppia. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.", "keyphrases": ["scommessa permuta", "scommessa del sottoinsieme", "partner commerciale bilaterale", "algoritmo polinomi-tempo", "informare aggreg", "combinatore di permutazione", "mercato delle scommesse a coppie", "grafico bipartito", "feedback minimo", "algoritmo greedi", "Trasformata polinomistica complessa"]}
{"file_name": "I-31", "text": "Ragionare sul giudizio e sull'aggregazione delle preferenze \u25e6 ABSTRACT Gli agenti che devono raggiungere accordi con altri agenti devono ragionare su come le loro preferenze, giudizi e credenze potrebbero essere aggregati con quelli degli altri attraverso i meccanismi di scelta sociale che governano le loro interazioni. Il campo dell'aggregazione del giudizio emergente di recente studia l'aggregazione da una prospettiva logica e considera come pi\u00f9 insiemi di formule logiche possono essere aggregati in un unico insieme coerente. Come caso speciale, si pu\u00f2 vedere che l'aggregazione dei giudizi sussume l'aggregazione delle preferenze classica. Presentiamo una logica modale che intende supportare il ragionamento sugli scenari di aggregazione dei giudizi -LRB- e quindi, come caso speciale, sull'aggregazione delle preferenze -RRB-: il linguaggio logico \u00e8 interpretato direttamente nelle regole di aggregazione dei giudizi. Presentiamo una solida e completa assiomatizzazione di tali regole. Mostriamo che la logica pu\u00f2 esprimere regole di aggregazione come il voto a maggioranza; propriet\u00e0 delle regole come l'indipendenza; e risultati come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet - che sono derivabili come teoremi formali della logica. La logica \u00e8 parametrizzata in modo tale da poter essere utilizzata come quadro generale per confrontare le propriet\u00e0 logiche di diversi tipi di aggregazione, inclusa l'aggregazione delle preferenze classica. 1. INTRODUZIONE In questo articolo, siamo interessati ai formalismi di rappresentazione della conoscenza per sistemi in cui gli agenti hanno bisogno di aggregare le loro preferenze, giudizi, credenze, ecc.. Ad esempio, un agente potrebbe aver bisogno di ragionare sul voto a maggioranza in un gruppo di cui fa parte. un membro di. L'aggregazione delle preferenze - che combina le relazioni di preferenza degli individui su un insieme di alternative in una relazione di preferenza che rappresenta le preferenze congiunte del gruppo mediante le cosiddette funzioni di benessere sociale - \u00e8 stata ampiamente studiata nella teoria della scelta sociale -LSB-2 -RSB- . Il campo recentemente emergente dell'aggregazione dei giudizi studia l'aggregazione da una prospettiva logica e discute come, dato un insieme coerente di formule logiche per ciascun agente, che rappresentano le credenze o i giudizi dell'agente, possiamo aggregarli in un unico insieme coerente di formule. A tal fine sono state sviluppate diverse regole di aggregazione dei giudizi. Come caso speciale, si pu\u00f2 vedere che l'aggregazione dei giudizi sussume l'aggregazione delle preferenze -LSB- 5 -RSB-. In questo articolo presentiamo una logica, chiamata Logica di Aggregazione del Giudizio -LRB-jal -RRB-, per ragionare sull'aggregazione dei giudizi. Le formule della logica vengono interpretate come affermazioni sulle regole di aggregazione dei giudizi e diamo un'assiomatizzazione solida e completa di tutte queste regole. L'assiomatizzazione \u00e8 parametrizzata in modo tale da poterla istanziare per ottenere una gamma di diverse logiche di aggregazione del giudizio. Ad esempio, un esempio \u00e8 un'assiomatizzazione, nel nostro linguaggio, di tutte le funzioni di benessere sociale - quindi otteniamo anche una logica di aggregazione delle preferenze classica.E questo \u00e8 uno dei principali contributi di questo articolo: identifichiamo le propriet\u00e0 logiche dell'aggregazione del giudizio e possiamo confrontare le propriet\u00e0 logiche di diverse classi di aggregazione del giudizio - e dell'aggregazione generale del giudizio e dell'aggregazione delle preferenze in particolare. Naturalmente una logica \u00e8 interessante solo finch\u00e9 \u00e8 espressiva. Uno degli obiettivi di questo articolo \u00e8 indagare le capacit\u00e0 rappresentazionali e logiche di cui un agente ha bisogno per il giudizio e l'aggregazione delle preferenze; cio\u00e8, che tipo di linguaggio logico potrebbe essere utilizzato per rappresentare e ragionare sull'aggregazione dei giudizi? Il linguaggio di rappresentazione della conoscenza di un agente dovrebbe essere in grado di esprimere: regole comuni di aggregazione come il voto a maggioranza; propriet\u00e0 comunemente discusse delle regole di aggregazione del giudizio e delle funzioni di benessere sociale come l'indipendenza; paradossi comunemente usati per illustrare l'aggregazione dei giudizi e l'aggregazione delle preferenze, vale a dire. rispettivamente il paradosso discorsivo e il paradosso di Condorcet ; e altre propriet\u00e0 importanti come il teorema di Arrow. Da questo esempio sembra che un linguaggio formale per i SWF dovrebbe essere in grado di esprimere: \u2022 Propriet\u00e0 delle relazioni di preferenza per agenti diversi e propriet\u00e0 di diverse relazioni di preferenza per lo stesso agente nella stessa formula. \u2022 Confronto tra diverse relazioni di preferenza. \u2022 La relazione di preferenza risultante dall'applicazione di un SWF ad altre relazioni di preferenza. Da questi punti potrebbe sembrare che un tale linguaggio sarebbe piuttosto complesso -LRB- in particolare, questi requisiti sembrano escludere una logica modale proposizionale standard -RRB-. Nella sezione successiva esamineremo le basi dell'aggregazione dei giudizi e dell'aggregazione delle preferenze e menzioneremo alcune propriet\u00e0 comunemente discusse delle regole di aggregazione dei giudizi e delle funzioni di benessere sociale. Le formule di JAL sono interpretate direttamente dalle regole di aggregazione dei giudizi e quindi ne rappresentano le propriet\u00e0. Nella Sezione 4 dimostriamo che la logica pu\u00f2 esprimere propriet\u00e0 comunemente discusse delle regole di aggregazione del giudizio, come il paradosso discorsivo. Diamo un'assiomatizzazione solida e completa della logica nella Sezione 5, partendo dal presupposto che l'agenda su cui gli agenti esprimono giudizi sia finita. Come accennato in precedenza, l'aggregazione delle preferenze pu\u00f2 essere vista come un caso speciale di aggregazione del giudizio e nella Sezione 6 introduciamo un'interpretazione alternativa delle formule JAL direttamente nelle funzioni di benessere sociale. Otteniamo anche una solida e completa assiomatizzazione della logica per l'aggregazione delle preferenze. Le sezioni 7 e 8 discutono il lavoro correlato e concludono. 7. LAVORI CORRELATI Le logiche formali legate alla scelta sociale si sono concentrate principalmente sulla rappresentazione logica delle preferenze quando l'insieme di alternative \u00e8 ampio e sulle propriet\u00e0 computazionali del calcolo delle preferenze aggregate per una data rappresentazione -LSB- 6, 7, 8 -RSB- . Un'eccezione notevole e recente \u00e8 un quadro logico per l'aggregazione dei giudizi sviluppato da Marc Pauly in -LSB- 10 -RSB-,al fine di poter caratterizzare le relazioni logiche tra diverse regole di aggregazione dei giudizi. La logica modale della freccia -LSB- 11 -RSB- \u00e8 progettata per ragionare su qualsiasi oggetto che pu\u00f2 essere rappresentato graficamente come una freccia e dispone di vari operatori modali per esprimere propriet\u00e0 e relazioni tra queste frecce. Nella logica di aggregazione delle preferenze jal -LRB- LK -RRB- abbiamo interpretato le formule in coppie di alternative - che possono essere viste come frecce. Pertanto, -LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.-LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.-LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.Teorema di Arrow e paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.Teorema di Arrow e paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.", "keyphrases": ["la conoscenza rappresenta formale", "funzione di assistenza sociale", "assiomatis completa", "sintassi e semantica di jal", "parla del paradosso", "regola del giudizio aggregato", "teorema della freccia", "esprimere", "non dittatura", "unanime", "preferire l'aggregato", "logica della freccia", "jal"]}
{"file_name": "C-27", "text": "Un sistema di localizzazione ad alta precisione e basso costo per reti di sensori wireless SOMMARIO Il problema della localizzazione dei nodi di sensori wireless \u00e8 stato a lungo considerato molto difficile da risolvere, se si considerano le realt\u00e0 degli ambienti del mondo reale. In questo articolo descriviamo, progettiamo, implementiamo e valutiamo formalmente un nuovo sistema di localizzazione, chiamato Spotlight. Il nostro sistema utilizza le propriet\u00e0 spazio-temporali di eventi ben controllati nella rete -LRB-, ad esempio la luce -RRB-, per ottenere le posizioni dei nodi sensore. Dimostriamo che \u00e8 possibile ottenere un'elevata precisione nella localizzazione senza l'ausilio di hardware costoso sui nodi sensore, come richiesto da altri sistemi di localizzazione. Valutiamo le prestazioni del nostro sistema nelle implementazioni di mote Mica2 e XSM. Attraverso le valutazioni delle prestazioni di un sistema reale distribuito all'aperto, otteniamo un errore di localizzazione di 20 cm. Una rete di sensori, con qualsiasi numero di nodi, dispiegata in un\u2019area di 2500 m2, pu\u00f2 essere localizzata in meno di 10 minuti, utilizzando un dispositivo che costa meno di 1000 dollari. Per quanto ne sappiamo, questo \u00e8 il primo rapporto di un sub- errore di localizzazione del misuratore, ottenuto in un ambiente esterno, senza dotare i nodi sensore wireless di hardware di rilevamento specializzato. 1. INTRODUZIONE Recentemente, i sistemi di reti di sensori wireless sono stati utilizzati in molte applicazioni promettenti tra cui la sorveglianza militare, il monitoraggio degli habitat, il monitoraggio della fauna selvatica, ecc. -LSB- 12 -RSB- -LSB- 22 -RSB- -LSB- 33 -RSB- -LSB - 36 -RSB-. Sebbene molti servizi middleware, per supportare queste applicazioni, siano stati progettati e implementati con successo, la localizzazione, ovvero l'individuazione della posizione dei nodi sensore, rimane una delle sfide di ricerca pi\u00f9 difficili da risolvere praticamente. Un GPS di bordo -LSB- 23 -RSB- \u00e8 una tipica soluzione di fascia alta, che richiede hardware sofisticato per ottenere la sincronizzazione temporale ad alta risoluzione con i satelliti. I vincoli in termini di potenza e costi per i piccoli nodi di sensori ne impediscono l\u2019adozione come soluzione praticabile. Altre soluzioni richiedono dispositivi per nodo in grado di eseguire la scansione tra i nodi vicini. Le difficolt\u00e0 di questi approcci sono duplici. Innanzitutto, a causa dei vincoli di forma e alimentazione, la portata effettiva di tali dispositivi \u00e8 molto limitata. Ad esempio, la portata effettiva dei trasduttori a ultrasuoni utilizzati nel sistema Cricket \u00e8 inferiore a 2 metri quando l'emettitore e il ricevitore non sono uno di fronte all'altro -LSB- 26 -RSB-. In secondo luogo, poich\u00e9 la maggior parte dei nodi sensori sono statici, ovvero non \u00e8 previsto che la posizione cambi, non \u00e8 conveniente dotare questi sensori di circuiti speciali solo per una localizzazione una tantum. Per superare queste limitazioni, sono stati proposti molti schemi di localizzazione senza raggio d'azione. La maggior parte di questi schemi stimano la posizione dei nodi sensore sfruttando le informazioni di connettivit\u00e0 radio tra i nodi vicini. Questi approcci eliminano la necessit\u00e0 di hardware specializzato ad alto costo, a scapito di una localizzazione meno accurata. Inoltre,le caratteristiche della propagazione radio variano nel tempo e dipendono dall'ambiente, imponendo quindi elevati costi di calibrazione per gli schemi di localizzazione senza portata. La nostra risposta a questa sfida \u00e8 un sistema di localizzazione chiamato Spotlight. Questo sistema utilizza un'architettura asimmetrica, in cui i nodi sensore non necessitano di hardware aggiuntivo, oltre a quello di cui dispongono attualmente. Tutto l'hardware e i calcoli sofisticati risiedono su un singolo dispositivo Spotlight. Il dispositivo Spotlight utilizza una sorgente di luce laser orientabile, illuminando i nodi sensore posizionati all'interno di un terreno noto. Allo stesso tempo, poich\u00e9 \u00e8 necessario un solo sofisticato dispositivo per localizzare l\u2019intera rete, il costo ammortizzato \u00e8 molto inferiore al costo per aggiungere componenti hardware ai singoli sensori. 2. LAVORI CORRELATI Il problema della localizzazione \u00e8 un problema di ricerca fondamentale in molti settori. Gli errori di localizzazione segnalati sono dell'ordine di decine di centimetri, quando si utilizza hardware specializzato, ad esempio telemetro laser o ultrasuoni. A causa del costo elevato e del fattore di forma non trascurabile dell'hardware, queste soluzioni non possono essere semplicemente applicate alle reti di sensori. L'RSSI \u00e8 stata una soluzione interessante per stimare la distanza tra il mittente e il destinatario. Il sistema RADAR -LSB- 2 -RSB- utilizza l'RSSI per costruire un archivio centralizzato di intensit\u00e0 del segnale in varie posizioni rispetto a un insieme di nodi beacon. La posizione di un utente mobile \u00e8 stimata entro pochi metri. In un approccio simile, MoteTrack -LSB- 17 -RSB- distribuisce i valori RSSI di riferimento ai nodi beacon. Sono state proposte anche soluzioni che utilizzano RSSI e non richiedono nodi beacon -LSB- 5 -RSB- -LSB- 14 -RSB- -LSB- 24 -RSB- -LSB- 26 -RSB- -LSB- 29 -RSB-. Tutti condividono l'idea di utilizzare un beacon mobile. I nodi sensore che ricevono i beacon applicano diversi algoritmi per dedurre la loro posizione. In -LSB- 29 -RSB-, Sichitiu propone una soluzione in cui i nodi che ricevono il beacon costruiscono, in base al valore RSSI, un vincolo sulla loro stima di posizione. In -LSB-24 -RSB-, Pathirana et al. formulare il problema della localizzazione come stima on-line in un sistema dinamico non lineare e proporre un filtro di Kalman esteso robusto per risolverlo. Elnahrawy -LSB- 8 -RSB- fornisce una prova evidente dei limiti intrinseci dell'accuratezza della localizzazione utilizzando RSSI, in ambienti interni. Una tecnica di misurazione pi\u00f9 precisa utilizza la differenza di tempo tra un segnale radio e un'onda acustica, per ottenere distanze a coppie tra i nodi sensore. Questo approccio produce errori di localizzazione minori, al costo di hardware aggiuntivo. Il sistema di supporto alla localizzazione Cricket -LSB- 25 -RSB- pu\u00f2 raggiungere una granularit\u00e0 di localizzazione di decine di centimetri con ricetrasmettitori a ultrasuoni a corto raggio. AHLoS, proposto da Savvides et al. -LSB- 27 -RSB-, utilizza tecniche di misurazione del tempo di arrivo -LRB- ToA -RRB- che richiedono hardware esteso e la risoluzione di sistemi di equazioni non lineari relativamente grandi.In -LSB- 30 -RSB-, Simon et al. implementare un sistema distribuito -LRB- utilizzando il raggio acustico -RRB- che localizza un cecchino in un terreno urbano. La gamma acustica per la localizzazione \u00e8 utilizzata anche da Kwon et al. -LSB-15 -RSB-. Gli errori di localizzazione riportati variano da 2,2 m a 9,5 m, a seconda del tipo -LRB- centralizzato o distribuito -RRB- dell'algoritmo Least Square Scaling utilizzato. Per le reti di sensori wireless, la distanza \u00e8 un'opzione difficile. Tuttavia, l'elevata precisione di localizzazione ottenibile da questi schemi \u00e8 molto desiderabile. Per superare le sfide poste dagli schemi di localizzazione basati sul raggio d'azione, quando applicati alle reti di sensori, in passato \u00e8 stato proposto e valutato un approccio diverso. Questo approccio \u00e8 chiamato range-free e tenta di ottenere informazioni sulla posizione dalla vicinanza a un insieme di nodi beacon noti. Bulusu et al. proporre in -LSB- 4 -RSB- uno schema di localizzazione, chiamato Centroide, in cui ciascun nodo si localizza al centroide dei suoi nodi beacon prossimi. Il Global Coordinate System -LSB-20 -RSB-, sviluppato al MIT, utilizza la conoscenza a priori della densit\u00e0 dei nodi nella rete, per stimare la distanza media del salto. La famiglia DV - * di schemi di localizzazione -LSB- 21 -RSB-, utilizza il conteggio dei salti dai nodi beacon noti ai nodi della rete per dedurre la distanza. La maggior parte degli schemi di localizzazione senza raggio d'azione sono stati valutati in simulazioni o ambienti controllati. Langendoen e Reijers presentano uno studio dettagliato e comparativo di diversi schemi di localizzazione in -LSB- 16 -RSB-. Per quanto ne sappiamo, Spotlight \u00e8 il primo sistema di localizzazione senza portata che funziona molto bene in un ambiente esterno. Il nostro sistema richiede una linea di vista tra un singolo dispositivo e i nodi dei sensori e la mappa del terreno in cui si trova il campo dei sensori. Il sistema Spotlight ha una portata effettiva lunga -LRB- 1000 metri -RRB- e non richiede alcuna infrastruttura o hardware aggiuntivo per i nodi sensore. Il sistema Spotlight unisce i vantaggi e non soffre degli svantaggi delle due classi di localizzazione. 7. CONCLUSIONI E LAVORO FUTURO In questo articolo abbiamo presentato la progettazione, l'implementazione e la valutazione di un sistema di localizzazione per reti di sensori wireless, chiamato Spotlight. La nostra soluzione di localizzazione non richiede alcun hardware aggiuntivo per i nodi sensore, oltre a quello gi\u00e0 esistente. Tutta la complessit\u00e0 del sistema \u00e8 racchiusa in un unico dispositivo Spotlight. Il nostro sistema di localizzazione \u00e8 riutilizzabile, ovvero i costi possono essere ammortizzati attraverso diverse implementazioni e le sue prestazioni non sono influenzate dal numero di nodi di sensori nella rete. I nostri risultati sperimentali, ottenuti da un sistema reale distribuito all'aperto, mostrano che l'errore di localizzazione \u00e8 inferiore a 20 cm. Questo errore \u00e8 attualmente lo stato dell'arte,anche per i sistemi di localizzazione basati sulla distanza ed \u00e8 inferiore del 75% rispetto all'errore ottenuto quando si utilizzano dispositivi GPS o quando l'implementazione manuale dei nodi sensore \u00e8 un'opzione fattibile -LSB- 31 -RSB-. Come lavoro futuro, vorremmo esplorare l'autocalibrazione e l'autotuning del sistema Spotlight. La precisione del sistema pu\u00f2 essere ulteriormente migliorata se viene riportata la distribuzione dell'evento, invece di una singola marcatura temporale. Una generalizzazione potrebbe essere ottenuta riformulando il problema come un problema di stima angolare che fornisce gli elementi costitutivi per tecniche di localizzazione pi\u00f9 generali.", "keyphrases": ["rete di sensori wireless", "Locale", "locale della base di chiamata", "schema senza chiamata", "trasmettere", "eseguire", "accurati", "errore locale", "rete di sensori", "sistema di faretti", "tecnica locale", "distribuire"]}
{"file_name": "C-17", "text": "Problemi di implementazione di un sistema di conferenza VoIP in un ambiente di conferenza virtuale ABSTRACT I servizi in tempo reale sono stati supportati in generale su reti a commutazione di circuito. Le tendenze recenti favoriscono i servizi portati su reti a commutazione di pacchetto. Per le conferenze audio, dobbiamo considerare molti problemi: scalabilit\u00e0, qualit\u00e0 dell'applicazione per conferenze, controllo della sala e carico sui client/server, solo per citarne alcuni. In questo articolo descriviamo un framework di servizi audio progettato per fornire un ambiente di conferenza virtuale -LRB- VCE -RRB-. Il sistema \u00e8 progettato per accogliere un gran numero di utenti finali che parlano contemporaneamente e si diffondono su Internet. Il framework \u00e8 basato su Conference Server -LSB- 14 -RSB-, che facilitano la gestione dell'audio, mentre sfruttiamo le capacit\u00e0 SIP per scopi di segnalazione. La selezione del cliente si basa su un quantificatore recente chiamato \"Numero di volume\" che aiuta a imitare una conferenza fisica faccia a faccia. Trattiamo i problemi di implementazione della soluzione proposta sia in termini di scalabilit\u00e0 che di interattivit\u00e0, spiegando al contempo le tecniche che utilizziamo per ridurre il traffico. Abbiamo implementato un'applicazione Conference Server -LRB- CS -RRB- su una rete a livello di campus presso il nostro Istituto. 1. INTRODUZIONE Internet di oggi utilizza la suite di protocolli IP che \u00e8 stata progettata principalmente per il trasporto di dati e fornisce il miglior sforzo di consegna dei dati. I vincoli di ritardo e le caratteristiche separano i dati tradizionali da un lato dalle applicazioni voce e video dall'altro. Pertanto, man mano che su Internet vengono distribuite applicazioni voce e video sempre pi\u00f9 sensibili al fattore tempo, l'inadeguatezza di Internet viene messa a nudo. Inoltre, cerchiamo di portare i servizi telefonici su Internet. Tra questi, la struttura per conferenze virtuali -LRB- teleconferenza -RRB- \u00e8 all'avanguardia. Le conferenze audio e video su Internet sono popolari -LSB- 25 -RSB- per i numerosi vantaggi che comportano -LSB- 3,6 -RSB-. Chiaramente la larghezza di banda necessaria per una teleconferenza su Internet aumenta rapidamente con il numero dei partecipanti; ridurre la larghezza di banda senza compromettere la qualit\u00e0 audio \u00e8 una sfida nella telefonia Internet. C'\u00e8 molta discussione tra la comunit\u00e0 HCI e CSCW sull'uso dell'etnometodologia per la progettazione di applicazioni CSCW. L'approccio di base \u00e8 quello di fornire una larghezza di banda pi\u00f9 ampia, pi\u00f9 strutture e meccanismi di controllo pi\u00f9 avanzati, in attesa di una migliore qualit\u00e0 dell'interazione. Questo approccio ignora l'utilit\u00e0 funzionale dell'ambiente utilizzato per la collaborazione. Pertanto, \u00e8 necessario adottare un approccio che consideri entrambi gli aspetti: quello tecnico e quello funzionale. In questo lavoro non parliamo di videoconferenza; la sua inclusione non apporta vantaggi significativi alla qualit\u00e0 della conferenza -LSB- 4 -RSB-. La nostra attenzione \u00e8 rivolta agli ambienti audio virtuali. Descriviamo innanzitutto le sfide incontrate nelle conferenze audio virtuali. Poi esamineremo le motivazioni seguite dalla letteratura rilevante. Nella sezione 5,spieghiamo l'architettura del nostro sistema. La sezione 6 comprende la descrizione dei vari algoritmi utilizzati nella nostra configurazione. Affrontiamo i problemi di distribuzione. Segue una discussione sulle prestazioni. Concludiamo affrontando alcune questioni attuative. 4. LAVORI CORRELATI Lo standard SIP definito in RFC 3261 -LSB- 22 -RSB- e nelle estensioni successive come -LSB- 21 -RSB- non offre servizi di controllo delle conferenze come il controllo della sala o la votazione e non prescrive come una Fig 1. Esempio di conferenza: 3 domini contenenti le entit\u00e0 necessarie affinch\u00e9 la conferenza possa aver luogo. la conferenza deve essere gestita. Tuttavia SIP pu\u00f2 essere utilizzato per avviare una sessione che utilizza qualche altro protocollo di controllo della conferenza. La specifica SIP principale supporta molti modelli per conferenze -LSB- 26, 23 -RSB-. Nei modelli basati su server, un server mescola i flussi multimediali, mentre in una conferenza senza server, la miscelazione viene effettuata sui sistemi finali. SDP -LSB- 7 -RSB- pu\u00f2 essere utilizzato per definire le capacit\u00e0 multimediali e fornire altre informazioni sulla conferenza. Considereremo ora alcuni modelli di conferenza in SIP che sono stati proposti di recente -LSB- 23 -RSB-. Innanzitutto, esaminiamo i modelli senza server. Nell'End-System Mixing, solo un client -LRB- SIP UA -RRB- gestisce la segnalazione e il mixaggio multimediale per tutti gli altri, il che chiaramente non \u00e8 scalabile e causa problemi quando quel particolare client lascia la conferenza. Ci\u00f2 porta a un numero crescente di hop per le foglie remote e non \u00e8 scalabile. Un'altra opzione potrebbe essere quella di utilizzare il multicast per le conferenze, ma il multicast non \u00e8 abilitato su Internet ed \u00e8 attualmente possibile solo su una LAN. Tra i modelli basati su server, in una conferenza Dial-In, gli UA si connettono a un server centrale che gestisce tutto il mixaggio. Questo modello non \u00e8 scalabile poich\u00e9 \u00e8 limitato dalla potenza di elaborazione del server e dalla larghezza di banda della rete. Le conferenze centralizzate ad hoc e i server per conferenze in uscita presentano meccanismi e problemi simili. I modelli ibridi che coinvolgono segnalazione centralizzata e media distribuiti, con questi ultimi che utilizzano unicast o multicast, sollevano problemi di scalabilit\u00e0 come prima. Tuttavia un vantaggio \u00e8 che il controllo della conferenza pu\u00f2 essere una soluzione di terze parti. La perdita di spazialismo quando si mescolano e l'aumento della larghezza di banda quando non lo fanno sono problemi aperti. Uno studio correlato -LSB- 19 -RSB- dello stesso autore propone un'architettura di conferenza per ambienti virtuali collaborativi -LRB- CVE -RRB- ma non fornisce l'angolo di scalabilit\u00e0 senza la disponibilit\u00e0 del multicasting. Tenendo presenti i limiti dei sistemi di conferenza proposti, descriveremo ora nei dettagli la nostra proposta. 9. CONCLUSIONE In questo articolo abbiamo presentato una discussione su un ambiente di conferenza virtuale esclusivamente vocale. Abbiamo sostenuto che la natura distribuita della distribuzione la rende scalabile. L'interattivit\u00e0 si ottiene adattando un recente schema di selezione del flusso basato sul Loudness Number. Pertanto, vi \u00e8 un utilizzo significativamente efficace della larghezza di banda.In questo modo il discorso improvvisato in una teleconferenza virtuale tramite VoIP diventa realt\u00e0, come in una vera conferenza faccia a faccia. Il traffico nella WAN -LRB- Internet -RRB- \u00e8 limitato superiormente dal quadrato del numero di domini, ulteriormente ridotto utilizzando algoritmi euristici, che \u00e8 molto inferiore al numero totale di client nella conferenza. Ci\u00f2 \u00e8 dovuto all'utilizzo di un Conference Server locale per ciascun dominio. Le tecniche VAD aiutano a ridurre ulteriormente il traffico. L'utilizzo dello standard SIP per la segnalazione rende questa soluzione altamente interoperabile. Abbiamo implementato un'applicazione CS su una rete a livello di campus. Riteniamo che questa nuova generazione di ambienti di conferenza virtuale guadagner\u00e0 maggiore popolarit\u00e0 in futuro poich\u00e9 la loro facilit\u00e0 di implementazione \u00e8 garantita grazie a tecnologie prontamente disponibili e strutture scalabili.", "keyphrases": ["sistema di conferenza voip", "rete a commutazione di pacchetto", "struttura dei servizi audio", "ambiente di conferenza virtuale", "conferire server", "numero forte", "miscela parziale", "rilevamento attivo vocale", "sono sufficienti tre altoparlanti simultanei", "tecnica vad"]}
{"file_name": "H-30", "text": "Espansione dei concetti latenti utilizzando campi casuali di Markov ABSTRACT L'espansione delle query, sotto forma di feedback di pseudo-rilevanza o feedback di pertinenza, \u00e8 una tecnica comune utilizzata per migliorare l'efficacia del recupero. La maggior parte degli approcci precedenti hanno ignorato questioni importanti, come il ruolo delle caratteristiche e l\u2019importanza della modellazione delle dipendenze dei termini. In questo articolo proponiamo una robusta tecnica di espansione delle query basata sul modello di campo casuale di Markov per il recupero delle informazioni. La tecnica, chiamata espansione dei concetti latenti, fornisce un meccanismo per modellare le dipendenze dei termini durante l'espansione. Inoltre, l\u2019uso di caratteristiche arbitrarie all\u2019interno del modello fornisce un potente quadro per andare oltre le semplici caratteristiche di occorrenza dei termini che sono implicitamente utilizzate dalla maggior parte delle altre tecniche di espansione. Valutiamo la nostra tecnica rispetto ai modelli di pertinenza, una tecnica di espansione delle query di modellazione del linguaggio all'avanguardia. Il nostro modello dimostra miglioramenti coerenti e significativi nell'efficacia del recupero su diversi set di dati TREC. Descriviamo anche come la nostra tecnica pu\u00f2 essere utilizzata per generare concetti multitermine significativi per attivit\u00e0 quali suggerimento/riformulazione di query. 1. INTRODUZIONE possibilmente una narrazione pi\u00f9 lunga. Una grande quantit\u00e0 di informazioni viene persa durante il processo di conversione dal bisogno informativo alla query effettiva. Per questo motivo c\u2019\u00e8 stato un forte interesse verso le tecniche di espansione delle query. Tali tecniche vengono utilizzate per aumentare la query originale per produrre una rappresentazione che rifletta meglio il bisogno informativo sottostante. Le tecniche di espansione delle query sono state ben studiate per vari modelli in passato e hanno dimostrato di migliorare significativamente l'efficacia sia nel feedback di pertinenza che nel feedback di pseudo-rilevanza -LSB- 12, 21, 28, 29 -RSB-. Il modello MRF generalizza l'unigramma, il bigramma e altri vari modelli di dipendenza -LSB- 14 -RSB-. La maggior parte dei modelli di dipendenza a termine del passato non sono riusciti a mostrare miglioramenti consistenti e significativi rispetto alle linee di base dell'unigramma, con poche eccezioni -LSB- 8 -RSB-. Fino ad ora, il modello \u00e8 stato utilizzato esclusivamente per classificare i documenti in risposta a una determinata query. In questo lavoro, mostriamo come il modello pu\u00f2 essere esteso e utilizzato per l'espansione delle query utilizzando una tecnica che chiamiamo espansione dei concetti latenti -LRB- LCE -RRB-. Ci sono tre contributi principali del nostro lavoro. Innanzitutto, LCE fornisce un meccanismo per combinare la dipendenza dai termini con l'espansione delle query. Le precedenti tecniche di espansione delle query si basano su modelli \"bag of word\". Pertanto, eseguendo l'espansione della query utilizzando il modello MRF, siamo in grado di studiare la dinamica tra la dipendenza dai termini e l'espansione della query. Successivamente, come mostreremo, il modello MRF consente di utilizzare caratteristiche arbitrarie all'interno del modello. In passato le tecniche di espansione delle query utilizzavano implicitamente solo le funzionalit\u00e0 di occorrenza dei termini. Utilizzando set di funzionalit\u00e0 pi\u00f9 robusti, \u00e8 possibile produrre termini di espansione migliori che discriminino meglio tra documenti rilevanti e non rilevanti. Finalmente,il nostro approccio proposto fornisce senza soluzione di continuit\u00e0 un meccanismo per generare concetti sia a termine singolo che a termine multiplo. La maggior parte delle tecniche precedenti, per impostazione predefinita, generano termini in modo indipendente. Ci sono stati diversi approcci che fanno uso di concetti generalizzati, tuttavia tali approcci erano in qualche modo euristici e realizzati al di fuori del modello -LSB- 19, 28 -RSB-. Il nostro approccio \u00e8 sia formalmente motivato che una naturale estensione del modello sottostante. Nella Sezione 2 descriviamo i relativi approcci di espansione delle query. La sezione 3 fornisce una panoramica del modello MRF e descrive in dettaglio la nostra tecnica di espansione dei concetti latenti proposta. Nella Sezione 4 valutiamo il nostro modello proposto e analizziamo i risultati. 2. LAVORI CORRELATI Uno degli approcci classici e pi\u00f9 ampiamente utilizzati per l'espansione delle query \u00e8 l'algoritmo di Rocchio -LSB- 21 -RSB-. L'approccio di Rocchio, che \u00e8 stato sviluppato all'interno del modello dello spazio vettoriale, ripondera il vettore di query originale spostando i pesi verso l'insieme di documenti rilevanti o pseudo-rilevanti e lontano dai documenti non rilevanti. Sfortunatamente, non \u00e8 possibile applicare formalmente l'approccio di Rocchio a un modello di recupero statistico, come la modellazione del linguaggio per il recupero delle informazioni. Sono state sviluppate numerose tecniche formalizzate di espansione delle query per il framework di modellazione del linguaggio, incluso il feedback basato su modelli di Zhai e Lafferty e i modelli di rilevanza di Lavrenko e Croft -LSB- 12, 29 -RSB-. Entrambi gli approcci tentano di utilizzare documenti pseudo-rilevanti o rilevanti per stimare un modello di query migliore. Il feedback basato sul modello trova il modello che meglio descrive i documenti rilevanti prendendo in considerazione un modello di rumore di fondo -LRB- -RRB-. Ci\u00f2 separa il modello di contenuto dal modello di sfondo. Il modello di contenuto viene quindi interpolato con il modello di query originale per formare la query espansa. L\u2019altra tecnica, i modelli di rilevanza, \u00e8 pi\u00f9 strettamente legata al nostro lavoro. Entriamo quindi nei dettagli del modello. Proprio come il feedback basato su modelli, i modelli di pertinenza stimano un modello di query migliorato. L\u2019unica differenza tra i due approcci \u00e8 che i modelli di rilevanza non modellano esplicitamente i documenti rilevanti o pseudo-rilevanti. Invece, modellano una nozione di rilevanza pi\u00f9 generalizzata, come mostreremo ora. Data una query Q, un modello di rilevanza \u00e8 una distribuzione multinomiale, P -LRB- \u00b7 | Q -RRB-, che codifica la verosimiglianza di ciascun termine data la query come prova. Viene calcolato come: dove RQ \u00e8 l'insieme di documenti rilevanti o pseudorilevanti per interrogare Q. Questi presupposti blandi rendono il calcolo del posteriore bayesiano pi\u00f9 pratico. Dopo che il modello \u00e8 stato stimato, i documenti vengono classificati ritagliando il modello di pertinenza scegliendo i k termini pi\u00f9 probabili da P -LRB- \u00b7 | Q -RRB-. Questa distribuzione ritagliata viene quindi interpolata con il modello di query originale di massima verosimiglianza -LSB- 1 -RSB-. Questo pu\u00f2 essere pensato come un'espansione della query originale di k termini ponderati. Per tutto il resto di questo lavoro,ci riferiamo a questa istanziazione dei modelli di rilevanza come RM3. \u00c8 stato svolto relativamente poco lavoro nell'area dell'espansione delle query nel context dei modelli di dipendenza -LSB-9 -RSB-. Tuttavia, ci sono stati diversi tentativi di espansione utilizzando concetti multitermine. Il metodo di analisi del context locale di Xu e Croft -LRB- LCA -RRB- combinava il recupero a livello di passaggio con l'espansione dei concetti, dove i concetti erano singoli termini e frasi -LSB- 28 -RSB-. I concetti di espansione sono stati scelti e ponderati utilizzando una metrica basata su statistiche di co-occorrenza. Papka e Allan studiano l'utilizzo del feedback sulla pertinenza per eseguire l'espansione del concetto multi-termine per l'instradamento dei documenti -LSB- 19 -RSB-. I risultati hanno mostrato che la combinazione di concetti a termine singolo e multitermine ad ampia finestra ha migliorato significativamente l'efficacia. 5. CONCLUSIONI In questo articolo abbiamo proposto una robusta tecnica di espansione delle query chiamata espansione dei concetti latenti. La tecnica ha dimostrato di essere un'estensione naturale del modello del campo casuale di Markov per il recupero delle informazioni e una generalizzazione dei modelli di rilevanza. Abbiamo dimostrato che la tecnica pu\u00f2 essere utilizzata per produrre concetti di espansione multi-termine di alta qualit\u00e0, ben formati e rilevanti per l'attualit\u00e0. I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.", "keyphrases": ["la robusta queri espande la tecnica", "modello linguistico queri espande la tecnica", "feedback rilevante", "feedback pseudo-rilevante", "informare il recupero", "approccio basato sul modello linguistico", "ricerca sul web", "queri si espande", "mrf", "algoritmo di rocchio", "quadro del modello linguistico", "rm3", "percorso dei documenti", "recupero ad hoc", "modello MRF", "distribuzione rilev"]}
{"file_name": "I-10", "text": "SMILE: apprendimento incrementale multi-agente valido ;--RRB- * ABSTRACT Questo articolo affronta il problema dell'apprendimento collaborativo in un sistema multi-agente. Qui ogni agente pu\u00f2 aggiornare in modo incrementale le sue credenze B -LRB- la rappresentazione concettuale -RRB- in modo che sia in un certo senso mantenuta coerente con l'intero insieme di informazioni K -LRB- gli esempi -RRB- che ha ricevuto dall'ambiente o altri agenti. Estendiamo questa nozione di consistenza -LRB- o solidit\u00e0 -RRB- all'intera MAS e discutiamo come ottenere che, in ogni momento, una stessa rappresentazione concettuale coerente sia presente in ciascun agente. Il protocollo corrispondente viene applicato all'apprendimento supervisionato dei concetti. Il metodo risultante SMILE -LRB- che sta per Sound Multiagent Incremental LEarning -RRB- \u00e8 qui descritto e sperimentato. Sorprendentemente, alcune formule booleane difficili vengono apprese meglio, a parit\u00e0 di set di apprendimento, da un sistema multi-agente piuttosto che da un singolo agente. 1. INTRODUZIONE Questo articolo affronta il problema dell'apprendimento collaborativo di concetti in un sistema multi-agente. -LSB- 6 -RSB- introduce una caratterizzazione dell'apprendimento nel sistema multi-agente in base al livello di consapevolezza degli agenti. Al livello 1, gli agenti imparano * L'autore principale di questo articolo \u00e8 uno studente. nel sistema senza tener conto della presenza di altri agenti, se non attraverso la modificazione apportata all'ambiente dalla loro azione. Il livello 2 implica l'interazione diretta tra gli agenti in quanto possono scambiarsi messaggi per migliorare il proprio apprendimento. In questo articolo ci concentreremo sul livello 2, studiando l'interazione diretta tra gli agenti coinvolti in un processo di apprendimento. Si presuppone che ogni agente sia in grado di apprendere in modo incrementale dai dati che riceve, il che significa che ogni agente pu\u00f2 aggiornare il suo insieme di credenze B per mantenerlo coerente con l'intero insieme di informazioni K che ha ricevuto dall'ambiente o da altri agenti. Inoltre, supponiamo che almeno una parte Bc delle credenze di ciascun agente sia comune a tutti gli agenti e debba rimanere tale. Pertanto, un aggiornamento di questo insieme comune Bc da parte dell'agente r deve provocare un aggiornamento di Bc per l'intera comunit\u00e0 di agenti. Ci porta a definire quale sia la mass-coerenza di un agente rispetto alla comunit\u00e0. Il processo di aggiornamento delle convinzioni della comunit\u00e0 quando uno dei suoi membri riceve nuove informazioni pu\u00f2 quindi essere definito come il processo di mantenimento della coerenza che garantisce che ogni agente nella comunit\u00e0 rimanga macoerente. Questo processo di mantenimento della coerenza di massa di un agente che ottiene nuove informazioni gli conferisce il ruolo di colui che apprende e implica la comunicazione con altri agenti che agiscono come critici. Tuttavia, gli agenti non sono specializzati e possono a loro volta essere studenti o critici, nessuno di loro \u00e8 vincolato a un ruolo specifico. Le informazioni vengono distribuite tra gli agenti, ma possono essere ridondanti. Non esiste una memoria centrale. Il lavoro qui descritto ha la sua origine in un precedente lavoro riguardante l'apprendimento in un sistema multi-agente intenzionale utilizzando un formalismo BDI -LSB- 6 -RSB-. In quell'opera,gli agenti avevano dei piani, ciascuno dei quali era associato a un context che definiva in quali condizioni poteva essere attivato. I piani -LRB- ciascuno dei quali aveva il proprio context -RRB- erano comuni all'intero insieme di agenti della comunit\u00e0. Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.uno studente incrementale di concetti multi agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.uno studente incrementale di concetti multi agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento concettuale. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento concettuale. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.", "keyphrases": ["apprendimento multi-agente", "il concetto di collaborazione impara", "processo di apprendimento", "conoscenza", "ma-consistono", "incrementare l'apprendimento", "agente", "aggiornamento meccanico", "sincronizzato"]}
{"file_name": "H-31", "text": "Uno studio sul modello di generazione di query di Poisson per il recupero delle informazioni ABSTRACT Sono state proposte molte varianti di modelli linguistici per il recupero delle informazioni. La maggior parte dei modelli esistenti si basa sulla distribuzione multinomiale e assegna un punteggio ai documenti in base alla probabilit\u00e0 delle query calcolata sulla base di un modello probabilistico di generazione delle query. In questo articolo proponiamo e studiamo una nuova famiglia di modelli di generazione di query basati sulla distribuzione di Poisson. Mostriamo che mentre nelle loro forme pi\u00f9 semplici, la nuova famiglia di modelli e i modelli multinomiali esistenti sono equivalenti, si comportano diversamente per molti metodi di livellamento. Mostriamo che il modello di Poisson presenta diversi vantaggi rispetto al modello multinomiale, tra cui il livellamento per termine naturalmente accomodante e la possibilit\u00e0 di una modellazione dello sfondo pi\u00f9 accurata. Presentiamo diverse varianti del nuovo modello corrispondenti a diversi metodi di livellamento e le valutiamo su quattro raccolte di test TREC rappresentative. I risultati mostrano che mentre i loro modelli di base hanno prestazioni comparabili, il modello di Poisson pu\u00f2 sovraperformare il modello multinomiale con livellamento per termine. Le prestazioni possono essere ulteriormente migliorate con lo smoothing a due stadi. 1. INTRODUZIONE Essendo un nuovo tipo di modelli di recupero probabilistico, i modelli linguistici hanno dimostrato di essere efficaci per molti compiti di recupero -LSB- 21, 28, 14, 4 -RSB-. Possiamo quindi classificare i documenti in base alla probabilit\u00e0 di generare la query. Praticamente tutti i modelli linguistici di generazione delle query esistenti sono basati sulla distribuzione multinomiale -LSB- 19, 6, 28 -RSB- o sulla distribuzione multivariata di Bernoulli -LSB- 21, 18 -RSB-. La distribuzione multinomiale \u00e8 particolarmente apprezzata e si \u00e8 dimostrata piuttosto efficace. Si noti che l'assenza di termini viene catturata indirettamente anche in un modello multinomiale attraverso il vincolo che tutte le probabilit\u00e0 dei termini devono sommarsi a 1. In questo articolo proponiamo e studiamo una nuova famiglia di modelli di generazione di query basati sulla distribuzione di Poisson. In questa nuova famiglia di modelli, modelliamo la frequenza di ciascun termine in modo indipendente con una distribuzione di Poisson. Per assegnare un punteggio a un documento, stimeremmo prima un modello di Poisson multivariato basato sul documento, quindi gli daremmo un punteggio in base alla probabilit\u00e0 della query data dal modello di Poisson stimato. In un certo senso, il modello di Poisson combina il vantaggio del multinomiale nel modellare la frequenza dei termini e il vantaggio del modello Bernoulli multivariato nell\u2019accettare il livellamento per termine. Come nel lavoro esistente sui modelli linguistici multinomiali, lo smoothing \u00e8 fondamentale per questa nuova famiglia di modelli. Deriviamo diversi metodi di livellamento per il modello di Poisson in parallelo a quelli utilizzati per le distribuzioni multinomiali e confrontiamo i corrispondenti modelli di recupero con quelli basati su distribuzioni multinomiali. Abbiamo scoperto che mentre con alcuni metodi di livellamento, il nuovo modello e il modello multinomiale portano esattamente alla stessa formula, con altri metodi di livellamento divergono e il modello di Poisson apporta maggiore flessibilit\u00e0 per il livellamento. In particolare,una differenza fondamentale \u00e8 che il modello di Poisson pu\u00f2 naturalmente adattarsi al livellamento pertermine, cosa difficile da ottenere con un modello multinomiale senza una svolta euristica della semantica di un modello generativo. Sfruttiamo questo potenziale vantaggio per sviluppare un nuovo algoritmo di livellamento dipendente dal termine per il modello di Poisson e dimostrare che questo nuovo algoritmo di livellamento pu\u00f2 migliorare le prestazioni rispetto agli algoritmi di livellamento indipendenti dal termine utilizzando il modello di Poisson o quello multinomiale. Questo vantaggio \u00e8 evidente sia per lo smoothing a uno stadio che per quello a due stadi. Un altro potenziale vantaggio del modello di Poisson \u00e8 che il corrispondente modello di fondo per lo smussamento pu\u00f2 essere migliorato utilizzando un modello misto che ha una formula in forma chiusa. \u00c8 stato dimostrato che questo nuovo modello di sfondo supera le prestazioni del modello di sfondo standard e riduce la sensibilit\u00e0 delle prestazioni di recupero al parametro di livellamento. Nella Sezione 2, introduciamo la nuova famiglia di modelli di generazione di query con distribuzione di Poisson e presentiamo vari metodi di livellamento che portano a diverse funzioni di recupero. Nella Sezione 3 confrontiamo analiticamente il modello linguistico di Poisson con il modello linguistico multinomiale, dal punto di vista del recupero. Progettiamo quindi esperimenti empirici per confrontare le due famiglie di modelli linguistici nella Sezione 4. Discutiamo il lavoro correlato nel paragrafo 5 e concludiamo nel paragrafo 6. 5. LAVORO CORRELATO Per quanto ne sappiamo, non \u00e8 stato effettuato alcuno studio sui modelli di generazione di query basati sulla distribuzione di Poisson. I modelli linguistici hanno dimostrato di essere efficaci per molti compiti di recupero -LSB- 21, 28, 14, 4 -RSB-. Il pi\u00f9 popolare e fondamentale \u00e8 il modello linguistico di generazione di query -LSB- 21, 13 -RSB-. Tutti i modelli linguistici di generazione delle query esistenti si basano sulla distribuzione multinomiale -LSB- 19, 6, 28, 13 -RSB- o sulla distribuzione multivariata di Bernoulli -LSB- 21, 17, 18 -RSB-. Introduciamo una nuova famiglia di modelli linguistici, basati sulla distribuzione di Poisson. La distribuzione di Poisson \u00e8 stata precedentemente studiata nei modelli di generazione di documenti -LSB- 16, 22, 3, 24 -RSB-, portando allo sviluppo di una delle formule di recupero pi\u00f9 efficaci BM25 -LSB- 23 -RSB-. -LSB- 24 -RSB- studia la derivazione parallela di tre diversi modelli di recupero che \u00e8 legata al nostro confronto tra Poisson e multinomiale. Tuttavia, il modello di Poisson nel loro articolo rientra ancora nel quadro della generazione dei documenti e inoltre non tiene conto della variazione della lunghezza del documento. -LSB- 26 -RSB- introduce un modo per cercare empiricamente un modello esponenziale per i documenti. Le miscele di Poisson -LSB- 3 -RSB- come 2-Poisson -LSB- 22 -RSB-, multinomiale negativo e KMixture -LSB- 9 -RSB- di Katz hanno dimostrato di essere efficaci per modellare e recuperare documenti. Ancora una volta, nessuno di questo lavoro esplora la distribuzione di Poisson nel framework di generazione delle query. Il livellamento del modello linguistico -LSB- 2, 28, 29 -RSB- e le strutture di fondo -LSB- 15, 10, 25, 27 -RSB- sono stati studiati con modelli linguistici multinomiali.-LSB- 7 -RSB- mostra analiticamente che il livellamento specifico del termine potrebbe essere utile. Mostriamo che il modello linguistico di Poisson \u00e8 naturale per accogliere il livellamento per termine senza torsione euristica della semantica di un modello generativo, ed \u00e8 in grado di modellare in modo efficiente e migliore lo sfondo misto, sia analiticamente che empiricamente. 6. CONCLUSIONI Presentiamo una nuova famiglia di modelli linguistici di generazione di query per il recupero basati sulla distribuzione di Poisson. Deriviamo diversi metodi di smoothing per questa famiglia di modelli, incluso lo smoothing a stadio singolo e lo smoothing a due stadi. Confrontiamo i nuovi modelli con i popolari modelli di recupero multinomiale sia analiticamente che sperimentalmente. La nostra analisi mostra che, sebbene i nostri nuovi modelli e i modelli multinomiali siano equivalenti sotto alcuni presupposti, sono generalmente diversi con alcune importanti differenze. In particolare, mostriamo che Poisson ha un vantaggio rispetto al multinomiale nello smoothing per termine naturalmente accomodante. Sfruttiamo questa propriet\u00e0 per sviluppare un nuovo algoritmo di livellamento per termine per i modelli linguistici di Poisson, che ha dimostrato di sovraperformare il livellamento indipendente dai termini sia per i modelli di Poisson che per quelli multinomiali. Inoltre, mostriamo che un modello di fondo misto per Poisson pu\u00f2 essere utilizzato per migliorare le prestazioni e la robustezza rispetto al modello di fondo di Poisson standard. Il nostro lavoro apre molte direzioni interessanti per ulteriori esplorazioni in questa nuova famiglia di modelli. Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.", "keyphrases": ["distribuzione multinomi", "modello probabilistico del queri gener", "distribuzione del veleno", "liscio a due stadi", "multivari distribuzione bernoullu", "riconoscimento vocale", "termine frequenza", "perterm liscio", "nuovo algoritmo uniforme dipendente dai termini", "insieme di vocabolari", "processo di poisson omogeneo", "singolo pseudo termine"]}
{"file_name": "I-35", "text": "Gestione distribuita delle norme nei sistemi multi-agente regolamentati * SOMMARIO Le norme sono ampiamente riconosciute come mezzo per coordinare i sistemi multi-agente. La gestione distribuita delle norme \u00e8 una questione impegnativa e osserviamo una mancanza di realizzazioni computazionali realmente distribuite di modelli normativi. Per regolare il comportamento degli agenti autonomi che prendono parte a molteplici attivit\u00e0 correlate, proponiamo un modello normativo, la Struttura Normativa -LRB- NS -RRB-, un artefatto che si basa sulla propagazione di posizioni normative -LRB- obblighi , divieti, permessi -RRB-, come conseguenze delle azioni degli agenti. All'interno di un NS possono sorgere conflitti a causa della natura dinamica del MAS e della concorrenza delle azioni degli agenti. Tuttavia, garantire la libert\u00e0 da conflitti di un NS in fase di progettazione \u00e8 computazionalmente intrattabile. Lo dimostriamo formalizzando la nozione di conflitto, fornendo una mappatura delle NS nelle reti di Petri colorate e prendendo in prestito risultati teorici ben noti da quel campo. Poich\u00e9 \u00e8 necessaria la risoluzione dei conflitti online, presentiamo un algoritmo trattabile da impiegare in modo distribuito. Dimostreremo quindi che questo algoritmo \u00e8 fondamentale per l'attuazione distribuita di una NS. 1. INTRODUZIONE Una caratteristica fondamentale dei sistemi multi-agente aperti e regolamentati in cui interagiscono agenti autonomi, \u00e8 che gli agenti partecipanti sono tenuti a rispettare le convenzioni del sistema. Le norme possono essere utilizzate per modellare tali convenzioni e quindi come mezzo per regolare il comportamento osservabile degli agenti -LSB- 6, 29 -RSB-. Sul tema delle norme sono numerosi i contributi di sociologi, filosofi e logici -LRB- ad es. -LSB- 15, 28 -RSB- -RRB-. Tuttavia, ci sono pochissime proposte per la realizzazione computazionale di modelli normativi - il modo in cui le norme possono essere integrate nella progettazione e nell'esecuzione dei MAS. Per quanto ne sappiamo, nessuna proposta supporta veramente l'attuazione distribuita di ambienti normativi. Nel nostro articolo affrontiamo questo problema e proponiamo mezzi per gestire gli impegni contrastanti in sistemi aperti, regolamentati e multiagente in modo distribuito. Il tipo di MAS regolamentato che immaginiamo consiste in attivit\u00e0 multiple, simultanee e correlate in cui gli agenti interagiscono. Ogni agente pu\u00f2 partecipare contemporaneamente a diverse attivit\u00e0 e passare da un'attivit\u00e0 all'altra. Le azioni di un agente all'interno di un'attivit\u00e0 possono avere conseguenze sotto forma di posizioni normative -LRB- cio\u00e8 obblighi, permessi e divieti -RRB- -LSB- 26 -RSB- che possono limitare il suo comportamento futuro. Partiamo dal presupposto che gli agenti possano scegliere di non adempiere a tutti i loro obblighi e quindi possano essere sanzionati dalla MAS. Si noti che, quando le attivit\u00e0 vengono distribuite, le posizioni normative devono fluire dalle attivit\u00e0 in cui vengono generate a quelle in cui hanno effetto. Poich\u00e9 in un MAS aperto e regolamentato non \u00e8 possibile incorporare aspetti normativi nella progettazione degli agenti,adottiamo l'idea che la MAS dovrebbe essere integrata con un insieme separato di norme che regolino ulteriormente il comportamento degli agenti partecipanti. Per modellare la separazione delle preoccupazioni tra il livello di coordinamento -LRB- interazioni degli agenti -RRB- e il livello normativo -LRB- propagazione di posizioni normative -RRB-, proponiamo un artefatto chiamato Struttura normativa -LRB- NS -RRB -. All'interno di un NS possono sorgere conflitti a causa della natura dinamica del MAS e della concorrenza delle azioni degli agenti. Ad esempio, un agente pu\u00f2 essere obbligato e vietato di compiere la stessa azione in un'attivit\u00e0. Tuttavia, garantire la libert\u00e0 da conflitti di un NS in fase di progettazione \u00e8 computazionalmente intrattabile. Dimostriamo questo formalizzando la nozione di conflitto, fornendo una mappatura delle NS in reti di Petri colorate -LRB-CPN -RRB- e prendendo in prestito risultati teorici ben noti dal campo dei CPN. Riteniamo che sia necessario il rilevamento e la risoluzione dei conflitti online. Quindi, presentiamo un algoritmo trattabile per la risoluzione dei conflitti. Questo algoritmo \u00e8 fondamentale per l'attuazione distribuita di una NS. Il documento \u00e8 organizzato come segue. Nella Sezione 2 descriviamo in dettaglio uno scenario che servir\u00e0 da esempio per tutto il documento. Successivamente, nella Sezione 3 definiamo formalmente l'artefatto della struttura normativa. Successivamente, nella Sezione 4 formalizziamo la nozione di conflitto per analizzare successivamente la complessit\u00e0 del rilevamento dei conflitti in termini di CPN nella Sezione 5. La Sezione 6 descrive la gestione computazionale dei NS descrivendo la loro attuazione e presentando un algoritmo per la risoluzione dei conflitti. Infine, commenteremo il lavoro correlato, trarremo conclusioni e riferiremo sul lavoro futuro nella Sezione 7. 7. LAVORO CORRELATO E CONCLUSIONI I nostri contributi in questo documento sono triplici. In primo luogo, introduciamo un approccio per la gestione e il ragionamento sulle norme in modo distribuito. A nostra conoscenza, c\u2019\u00e8 poco lavoro pubblicato in questa direzione. In -LSB- 8, 21 -RSB-, vengono presentate due lingue per l'applicazione distribuita delle norme in MAS. Tuttavia, in entrambi i lavori, ciascun agente dispone di un'interfaccia di messaggi locale che inoltra messaggi legali secondo una serie di norme. Poich\u00e9 queste interfacce sono locali per ciascun agente, le norme possono essere espresse solo in termini di azioni di quell\u2019agente. Questo \u00e8 un grave svantaggio, ad esempio quando \u00e8 necessario attivare un obbligo verso un agente a causa di un determinato messaggio di un altro. Il secondo contributo \u00e8 la proposta di un assetto normativo. La nozione \u00e8 fruttuosa perch\u00e9 consente la separazione delle preoccupazioni normative e procedurali. L'impianto normativo da noi proposto rende evidente la somiglianza tra la propagazione delle posizioni normative e la propagazione 642 Sesta Intl.. Conf. congiunta. su Agenti Autonomi e Sistemi Multi-Agente -LRB- AAMAS 07 -RRB- di token in Reti di Petri Colorate. Questa somiglianza suggerisce prontamente una mappatura tra i due, e fornisce le basi per un\u2019opportuna trattazione analitica della struttura normativa, in generale,e la complessit\u00e0 del rilevamento dei conflitti, in particolare. In -LSB-5 -RSB-, le conversazioni vengono prima progettate e analizzate a livello di CPN e successivamente tradotte in protocolli. Lin et al. -LSB- 20 -RSB- mappa gli schemi di conversazione sui CPN. A nostra conoscenza, l\u2019uso di questa rappresentazione a supporto del rilevamento dei conflitti nei MAS regolamentati non \u00e8 stato segnalato altrove. Infine, presentiamo un meccanismo distribuito per risolvere i conflitti normativi. Sartor -LSB- 25 -RSB- tratta i conflitti normativi dal punto di vista della teoria giuridica e suggerisce un modo per ordinare le norme coinvolte. La sua idea \u00e8 implementata in -LSB- 12 -RSB- ma richiede una risorsa centrale per il mantenimento della norma. L'approccio al rilevamento e alla risoluzione dei conflitti \u00e8 un adattamento ed estensione del lavoro sui grafici di istanziazione riportati in -LSB- 17 -RSB- e un algoritmo correlato in -LSB- 27 -RSB-. I tre contributi che presentiamo in questo articolo aprono molte possibilit\u00e0 per il lavoro futuro. Ci aspettiamo che tale accoppiamento doter\u00e0 le istituzioni elettroniche di un ambiente normativo pi\u00f9 flessibile e pi\u00f9 espressivo. Dal punto di vista teorico, intendiamo utilizzare tecniche di analisi dei CPN per caratterizzare classi di CPN -LRB- ad esempio, aciclici, simmetrici, ecc. -RRB- corrispondenti a famiglie di strutture normative che sono suscettibili di rilevamento di conflitti offline trattabili. La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.", "keyphrases": ["algoritmo", "attivo", "scenario", "presupposto normativo", "protocollo", "scena normale", "regola di transito normativo", "struttura della norma", "grafico bipartito", "vietare", "consentire la sovrapposizione", "gettone", "conflitto"]}
{"file_name": "I-33", "text": "Un percorso formale dalle norme istituzionali alle strutture organizzative ABSTRACT Finora, il modo in cui le istituzioni e le organizzazioni sono state utilizzate nello sviluppo di sistemi aperti non \u00e8 spesso andato oltre un'utile euristica. Per sviluppare sistemi che implementino effettivamente istituzioni e organizzazioni, i metodi formali dovrebbero prendere il posto di quelli euristici. L'articolo presenta una semantica formale per la nozione di istituzione e le sue componenti -LRB- norme astratte e concrete, empowerment degli agenti, ruoli -RRB- e definisce una relazione formale tra istituzioni e strutture organizzative. Di conseguenza, viene mostrato come le norme istituzionali possano essere raffinate in costrutti \u2013 strutture organizzative \u2013 che sono pi\u00f9 vicini a un sistema implementato. Viene inoltre mostrato come tale processo di raffinamento possa essere pienamente formalizzato e sia quindi suscettibile di verifica rigorosa. 1. INTRODUZIONE L\u2019opportunit\u00e0 di un \u201ctrasferimento tecnologico\u201d dal campo della teoria organizzativa e sociale all\u2019intelligenza artificiale distribuita e ai sistemi multiagente -LRB- MASs -RRB- \u00e8 stata a lungo sostenuta -LRB- -LSB- 8 -RSB- -RRB -. Nei MAS l'applicazione delle metafore organizzative e istituzionali alla progettazione dei sistemi si \u00e8 rivelata utile per lo sviluppo di metodologie e strumenti. In molti casi, tuttavia, l'applicazione di questi apparati concettuali equivale a mere euristiche che guidano la progettazione di alto livello dei sistemi. trattati formalmente, cio\u00e8 una volta che nozioni come norma, ruolo, struttura, ecc. ottengono una semantica formale. Scopo del presente contributo \u00e8 colmare questa lacuna rispetto alla nozione di istituzione fornendo i fondamenti formali per l\u2019applicazione della metafora istituzionale e per la sua relazione con quella organizzativa. Il risultato principale del lavoro consiste nel mostrare come i vincoli astratti -LRB- istituzioni -RRB- possano essere passo dopo passo raffinati in descrizioni strutturali concrete -LRB- strutture organizzative -RRB- del sistema da implementare, colmando cos\u00ec il divario tra norme astratte e specifiche concrete del sistema. Concretamente, nella Sezione 2, viene presentato un quadro logico che fornisce una semantica formale per le nozioni di istituzione, norma, ruolo, e che supporta la spiegazione delle caratteristiche chiave delle istituzioni come la traduzione di norme astratte in norme concrete e implementabili, la empowerment istituzionale degli agenti e alcuni aspetti della progettazione dell\u2019applicazione delle norme. Nella sezione 3 il quadro viene ampliato per trattare la nozione di infrastruttura di un'istituzione. Il quadro esteso viene poi studiato in relazione al formalismo per rappresentare le strutture organizzative presentato in -LSB- 11 -RSB-. Nella Sezione 4 seguono alcune conclusioni. 4. CONCLUSIONI Il contributo mirava a fornire un'analisi formale completa della metafora istituzionale e della sua relazione con quella organizzativa. Lo strumento formale predominante \u00e8 stata la logica descrittiva.I TBox sono stati utilizzati per rappresentare le specifiche delle istituzioni -LRB- Definizione 3 -RRB- e delle loro infrastrutture -LRB- Definizione 6 -RRB-, fornendo quindi una semantica del sistema di transizione per una serie di nozioni istituzionali -LRB- Esempi 1-7 - RRB-. I multigrafi sono stati quindi utilizzati per rappresentare la specificazione delle strutture organizzative -LRB- Definizione 6 -RRB-. L'ultimo risultato presentato riguarda la definizione di una corrispondenza formale tra le specifiche dell'istituzione e dell'organizzazione -LRB- Definizione 7 -RRB-, che fornisce un modo formale per il passaggio tra i due paradigmi. Tutto sommato, questi risultati forniscono un modo per mettere in relazione le specifiche astratte del sistema -LRB- cio\u00e8 le istituzioni come insiemi di norme -RRB- con le specifiche che sono pi\u00f9 vicine a un sistema implementato -LRB- cio\u00e8 le strutture organizzative -RRB-.", "keyphrases": ["metodo formale", "norma dell'istituto", "vincolo astratto", "formale per repres organiz structur", "entiti", "propriet\u00e0", "logica descrittiva", "logica dinamica", "assioma terminologico", "ruolo", "infrastruttura"]}
{"file_name": "C-36", "text": "Controllo degli accessi basato sulla crittografia nelle reti di pubblicazione/sottoscrizione dinamiche multidominio ABSTRACT I sistemi di pubblicazione/sottoscrizione forniscono un'infrastruttura di comunicazione distribuita su vasta area efficiente, basata sugli eventi e su un'ampia area. \u00c8 probabile che i sistemi di pubblicazione/iscrizione su larga scala utilizzino componenti della rete di trasporto di eventi di propriet\u00e0 di organizzazioni cooperanti ma indipendenti. Con l\u2019aumento del numero di partecipanti alla rete, la sicurezza diventa una preoccupazione crescente. Questo documento estende il lavoro precedente per presentare e valutare un'infrastruttura di pubblicazione/sottoscrizione multidominio sicura che supporta e applica un controllo di accesso capillare sui singoli attributi dei tipi di eventi. L'aggiornamento delle chiavi ci consente di garantire la sicurezza in avanti e all'indietro quando i broker di eventi si uniscono e lasciano la rete. Dimostriamo che i costi di tempo e spazio possono essere ridotti al minimo mediante un'attenta considerazione delle tecniche di crittografia e mediante l'uso della memorizzazione nella cache per ridurre le decrittografie non necessarie. Mostriamo che il nostro approccio ha un sovraccarico di comunicazione complessivo inferiore rispetto agli approcci esistenti per ottenere lo stesso grado di controllo sulla sicurezza nelle reti di pubblicazione/sottoscrizione. 1. INTRODUZIONE La pubblicazione/sottoscrizione \u00e8 particolarmente adatta come meccanismo di comunicazione per la creazione di applicazioni distribuite guidate da eventi su scala Internet. dei partecipanti deriva dal disaccoppiamento tra editori e abbonati inserendo tra loro un servizio di distribuzione di eventi asincrono. Nei sistemi di pubblicazione/sottoscrizione realmente su scala Internet, il servizio di distribuzione degli eventi includer\u00e0 un ampio insieme di nodi broker interconnessi che coprono un'ampia area geografica -LRB- e quindi di rete -RRB-. Sebbene le capacit\u00e0 di comunicazione dei sistemi di pubblicazione/sottoscrizione siano ben collaudate, \u00e8 probabile che l'estensione di pi\u00f9 domini amministrativi richieda considerazioni di sicurezza. Poich\u00e9 la sicurezza e il controllo degli accessi sono quasi l\u2019antitesi del disaccoppiamento, finora relativamente poche ricerche di tipo public/subscribe si sono concentrate sulla sicurezza. Il nostro obiettivo generale di ricerca \u00e8 sviluppare reti di pubblicazione/sottoscrizione su scala Internet che forniscano una distribuzione sicura ed efficiente di eventi, tolleranza agli errori e autoriparazione nell'infrastruttura di distribuzione e una comoda interfaccia per gli eventi. In -LSB- 12 -RSB- Pesonen et al. proporre un'architettura di controllo degli accessi multidominio e basata sulle capacit\u00e0 per i sistemi di pubblicazione/sottoscrizione. L'architettura fornisce un meccanismo per autorizzare i client di eventi a pubblicare e sottoscrivere tipi di eventi. I privilegi del client vengono controllati dal broker locale a cui il client si connette per accedere al sistema di pubblicazione/sottoscrizione. L'approccio implementa il controllo degli accessi ai margini della rete dei broker e presuppone che tutti i broker siano affidabili per applicare correttamente le policy di controllo degli accessi. Qualsiasi broker dannoso, compromesso o non autorizzato \u00e8 libero di leggere e scrivere qualsiasi evento che lo attraversa nel suo percorso dagli editori agli abbonati. Proponiamo di applicare il controllo degli accessi all'interno della rete dei broker crittografando il contenuto degli eventi,e quella politica detta i controlli sulle chiavi di crittografia necessarie. Con il contenuto dell'evento crittografato solo i broker autorizzati ad accedere alle chiavi di crittografia sono in grado di accedere al contenuto dell'evento -LRB- ovvero pubblicare, iscriversi o filtrare -RRB-. Spostiamo effettivamente l'applicazione del controllo degli accessi dai broker ai gestori delle chiavi di crittografia. Ci aspettiamo che il controllo degli accessi debba essere applicato in un sistema di pubblicazione/sottoscrizione multidominio quando pi\u00f9 organizzazioni formano un sistema di pubblicazione/sottoscrizione condiviso ma eseguono pi\u00f9 applicazioni indipendenti. Il controllo degli accessi potrebbe essere necessario anche quando una singola organizzazione \u00e8 composta da pi\u00f9 sottodomini che forniscono dati riservati tramite il sistema di pubblicazione/sottoscrizione a livello di organizzazione. Entrambi i casi richiedono il controllo degli accessi perch\u00e9 la distribuzione degli eventi in un'infrastruttura dinamica di pubblicazione/sottoscrizione basata su una rete di broker condivisa potrebbe portare all'instradamento degli eventi attraverso domini non autorizzati lungo i loro percorsi dagli editori agli abbonati. Ci sono due vantaggi particolari nella condivisione dell'infrastruttura di pubblicazione/sottoscrizione, entrambi legati alla rete di broker. Innanzitutto, i broker di condivisione creeranno una rete fisicamente pi\u00f9 grande che fornir\u00e0 una maggiore portata geografica. In secondo luogo, l\u2019aumento dell\u2019interconnettivit\u00e0 dei broker consentir\u00e0 al sistema di pubblicazione/sottoscrizione di fornire una maggiore tolleranza agli errori. La Figura 1 mostra la rete di pubblicazione/sottoscrizione multidominio che utilizziamo come esempio in questo documento. Questo dominio contiene una serie di telecamere a circuito chiuso che pubblicano informazioni sui movimenti dei veicoli nell'area di Londra. Abbiamo incluso il detective Smith come abbonato in questo dominio. Dominio del servizio di tassazione della congestione. Le tariffe applicate quotidianamente ai veicoli che hanno attraversato la zona a traffico limitato di Londra sono emesse da sistemi all'interno di questo dominio. I dati di riconoscimento della targa di origine provengono dalle telecamere del dominio della polizia metropolitana. Il fatto che i CCS siano autorizzati a leggere solo un sottoinsieme dei dati degli eventi del veicolo eserciter\u00e0 alcune delle caratteristiche chiave del controllo di accesso al sistema di pubblicazione/sottoscrizione applicabile presentato in questo documento. Dominio PITO. \u00c8 il proprietario del tipo di evento in questo particolare scenario. La crittografia tutela la riservatezza degli eventi nel caso in cui vengano trasportati attraverso domini non autorizzati. Tuttavia, crittografare interi eventi significa che i broker non autorizzati non possono prendere decisioni di routing efficienti. Il nostro approccio consiste nell'applicare la crittografia ai singoli attributi degli eventi. In questo modo la nostra politica di controllo degli accessi multidominio funziona con una granularit\u00e0 pi\u00f9 precisa: editori e abbonati possono essere autorizzati ad accedere a un sottoinsieme degli attributi disponibili. Nei casi in cui vengono utilizzati eventi non crittografati per l'instradamento, possiamo ridurre il numero totale di eventi inviati attraverso il sistema senza rivelare i valori degli attributi sensibili. Preserviamo cos\u00ec la privacy degli automobilisti consentendo comunque al CCS di svolgere il proprio lavoro utilizzando l'infrastruttura condivisa di pubblicazione/iscrizione.L'investigatore ottiene un'ordinanza del tribunale che la autorizza a iscriversi agli eventi relativi alla targa specifica relativa al suo caso. Gli attuali sistemi di controllo degli accessi di pubblicazione/sottoscrizione rafforzano la sicurezza ai margini della rete di broker dove i client si connettono ad essa. Tuttavia questo approccio spesso non sar\u00e0 accettabile nei sistemi su scala Internet. Proponiamo di rafforzare la sicurezza all'interno della rete dei broker e ai margini a cui si connettono i client degli eventi, crittografando il contenuto dell'evento. Le pubblicazioni verranno crittografate con chiavi di crittografia specifiche del tipo di evento. Controllando l'accesso alle chiavi di crittografia, possiamo controllare l'accesso ai tipi di eventi. L'approccio proposto consente agli event broker di instradare gli eventi anche quando hanno accesso solo a un sottoinsieme delle potenziali chiavi di crittografia. Introduciamo i sistemi di pubblicazione/sottoscrizione decentralizzati e la relativa crittografia nella Sezione 2. Nella Sezione 3 presentiamo il nostro modello per crittografare il contenuto dell'evento sia a livello di evento che di attributo. La sezione 4 tratta la gestione delle chiavi di crittografia nei sistemi di pubblicazione/sottoscrizione multidominio. Infine la Sezione 6 discute il lavoro correlato nella protezione dei sistemi di pubblicazione/sottoscrizione e la Sezione 7 fornisce osservazioni conclusive. 2. BACKGROUND In questa sezione forniamo una breve introduzione ai sistemi di pubblicazione/sottoscrizione decentralizzati. Indichiamo le nostre ipotesi sui sistemi di pubblicazione/sottoscrizione multidominio e descriviamo come queste ipotesi influenzano gli sviluppi che abbiamo apportato dal nostro lavoro precedentemente pubblicato. 2.1 Sistemi di pubblicazione/sottoscrizione decentralizzati Un sistema di pubblicazione/sottoscrizione include editori, abbonati e un servizio eventi. Gli editori pubblicano eventi, gli abbonati si iscrivono agli eventi di loro interesse e il servizio eventi \u00e8 responsabile della fornitura degli eventi pubblicati a tutti gli abbonati i cui interessi corrispondono all'evento specificato. Il servizio eventi in un sistema di pubblicazione/sottoscrizione decentralizzato \u00e8 distribuito su un numero di nodi broker. Insieme, questi broker formano una rete responsabile del mantenimento dei percorsi di instradamento necessari dagli editori agli abbonati. I clienti -LRB-, editori e abbonati -RRB- si connettono a un broker locale, di cui il cliente si fida completamente. Nella nostra discussione ci riferiamo ai client hosting broker come editori hosting broker -LRB- PHB -RRB- o abbonati hosting broker -LRB- SHB -RRB- a seconda che il cliente connesso sia un editore o Figura 1: Una visione d'insieme del nostro distribuzione di pubblicazione/sottoscrizione multidominio rispettivamente a un abbonato. Un broker locale di solito fa parte dello stesso dominio del cliente oppure \u00e8 di propriet\u00e0 di un fornitore di servizi di cui il cliente si fida. Una rete di broker pu\u00f2 avere una topologia statica -LRB- ad esempio Siena -LSB- 3 -RSB- e Gryphon -LSB- 14 -RSB- -RRB- o una topologia dinamica -LRB- ad esempio Scribe -LSB- 4 -RSB- ed Hermes -LSB- 13 -RSB- -RRB-. L\u2019approccio proposto funzioner\u00e0 in entrambi i casi.Una topologia statica consente all'amministratore di sistema di creare domini fidati e in questo modo migliorare l'efficienza del routing evitando crittografie non necessarie -LRB- vedi Sez. Il nostro lavoro si basa sul sistema Hermes. Hermes \u00e8 un middleware di pubblicazione/sottoscrizione basato sui contenuti che include un forte supporto per il tipo di evento. In altre parole, ogni pubblicazione \u00e8 un'istanza di un particolare tipo di evento predefinito. Le pubblicazioni vengono verificate presso l'intermediario locale di ciascun editore. Il nostro schema di crittografia a livello di attributo presuppone che gli eventi siano tipizzati. Hermes utilizza una rete overlay strutturata come trasporto e quindi ha una topologia dinamica. Una pubblicazione Hermes \u00e8 costituita da un identificatore del tipo di evento e da un insieme di coppie di valori di attributo. L'identificatore del tipo \u00e8 l'hash SHA-1 del nome del tipo di evento. Viene utilizzato per instradare la pubblicazione attraverso la rete del broker di eventi. Nasconde convenientemente il tipo di pubblicazione, cio\u00e8 ai broker viene impedito di vedere quali eventi li attraversano a meno che non siano a conoscenza del nome e dell'identificatore specifico del tipo di evento. 2.2 Tipi di eventi sicuri Pesonen et al. ha introdotto tipi di eventi sicuri in -LSB- 11 -RSB-, la cui integrit\u00e0 e autenticit\u00e0 pu\u00f2 essere confermata controllando le firme digitali. Un utile effetto collaterale dei tipi di eventi sicuri sono i tipi di eventi e i nomi degli attributi univoci a livello globale. \u00c8 possibile fare riferimento a questi nomi tramite le policy di controllo degli accessi. In questo documento utilizziamo il nome sicuro del tipo di evento o dell'attributo per fare riferimento alla chiave di crittografia utilizzata per crittografare l'evento o l'attributo. 2.3 Controllo degli accessi basato sulle capacit\u00e0 Pesonen et al. ha proposto un'architettura di controllo degli accessi basata sulle capacit\u00e0 per sistemi di pubblicazione/sottoscrizione multidominio in -LSB- 12 -RSB-. Il modello tratta i tipi di eventi come risorse a cui editori, abbonati e broker di eventi desiderano accedere. Il proprietario del tipo di evento \u00e8 responsabile della gestione del controllo degli accessi per un tipo di evento emettendo certificati di autorizzazione Simple Public Key Infrastructure -LRB-SPKI -RRB- che garantiscono al titolare l'accesso al tipo di evento specificato. Ad esempio, agli editori autorizzati verr\u00e0 rilasciato un certificato di autorizzazione che specifica che l'editore, identificato tramite chiave pubblica, \u00e8 autorizzato a pubblicare istanze del tipo di evento specificato nel certificato. Sfruttiamo il meccanismo di controllo degli accessi sopra menzionato in questo documento controllando l'accesso alle chiavi di crittografia utilizzando gli stessi certificati di autorizzazione. Cio\u00e8, un editore autorizzato a pubblicare un determinato tipo di evento, \u00e8 anche autorizzato ad accedere alle chiavi di crittografia utilizzate per proteggere eventi di quel tipo. 4. 2.4 Modello di minaccia L'obiettivo del meccanismo proposto \u00e8 quello di imporre il controllo dell'accesso per i partecipanti autorizzati al sistema. Nel nostro caso il primo livello di controllo dell'accesso viene applicato quando il partecipante tenta di unirsi alla rete di pubblicazione/sottoscrizione. Ai broker di eventi non autorizzati non \u00e8 consentito unirsi alla rete dei broker. Allo stesso modo, ai client di eventi non autorizzati non \u00e8 consentito connettersi a un broker di eventi.Tutte le connessioni nella rete del broker tra broker di eventi e client di eventi utilizzano Transport Layer Security -LRB- TLS -RRB- -LSB- 5 -RSB- per impedire l'accesso non autorizzato al livello di trasporto. L'architettura del sistema di pubblicazione/sottoscrizione implica che i client degli eventi devono connettersi ai broker di eventi per poter accedere al sistema di pubblicazione/sottoscrizione. Pertanto presumiamo che questi client non costituiscano una minaccia. Il client dell'evento si affida completamente al broker di eventi locale per l'accesso alla rete del broker. Pertanto il client dell'evento non \u00e8 in grado di accedere ad alcun evento senza l'assistenza del broker locale. I broker invece sono in grado di analizzare tutti gli eventi del sistema che li attraversano. Un broker pu\u00f2 analizzare sia il traffico degli eventi sia il numero e i nomi degli attributi che vengono popolati in un evento -LRB- nel caso della crittografia a livello di attributo -RRB-. Esistono approcci praticabili per prevenire l'analisi del traffico inserendo eventi casuali nel flusso di eventi al fine di produrre un modello di traffico uniforme. 6. LAVORI CORRELATI Wang et al. hanno classificato i vari problemi di sicurezza che dovranno essere affrontati in futuro nei sistemi di pubblicazione/sottoscrizione in -LSB- 20 -RSB-. Il documento \u00e8 una panoramica completa dei problemi di sicurezza nei sistemi di pubblicazione/sottoscrizione e come tale cerca di attirare l'attenzione sui problemi piuttosto che fornire soluzioni. Bacon et al. in -LSB- 1 -RSB- esaminare l'uso del controllo degli accessi basato sui ruoli in sistemi di pubblicazione/sottoscrizione distribuiti multidominio. Opyrchal e Prakash affrontano il problema della riservatezza degli eventi nell'ultimo collegamento tra l'abbonato e l'SHB in -LSB- 10 -RSB-. Affermano correttamente che un approccio di comunicazione di gruppo sicuro non \u00e8 fattibile in un ambiente come quello di pubblicazione/sottoscrizione che ha appartenenze a gruppi altamente dinamici. Nel nostro lavoro assumiamo che SHB sia abbastanza potente da mantenere una connessione protetta TLS per ciascun abbonato locale. Sia Srivatsa et al. -LSB- 19 -RSB- e Raiciu et al. -LSB- 16 -RSB- presenti meccanismi per proteggere la riservatezza dei messaggi nelle infrastrutture di pubblicazione/sottoscrizione decentralizzate. Rispetto al nostro lavoro, entrambi i documenti mirano a fornire i mezzi per proteggere l'integrit\u00e0 e la riservatezza dei messaggi, mentre l'obiettivo del nostro lavoro \u00e8 applicare il controllo degli accessi all'interno della rete dei broker. Raiciu et al. presuppongono nel loro lavoro che nessuno dei broker nella rete sia affidabile e quindi tutti gli eventi siano crittografati dall'editore all'abbonato e che tutta la corrispondenza sia basata su eventi crittografati. Al contrario, presupponiamo che alcuni dei broker sul percorso di una pubblicazione siano affidabili per accedere a quella pubblicazione e siano quindi in grado di implementare la corrispondenza degli eventi. Partiamo inoltre dal presupposto che l'editore e i broker di hosting dell'abbonato siano sempre affidabili per accedere alla pubblicazione. Infine, Fiege et al. affrontare l'argomento correlato della visibilit\u00e0 degli eventi in -LSB- 6 -RSB-.Sebbene il lavoro si sia concentrato sull\u2019utilizzo degli ambiti come meccanismo per strutturare sistemi basati su eventi su larga scala, la nozione di visibilit\u00e0 degli eventi \u00e8 in qualche modo in sintonia con il controllo degli accessi. 7. CONCLUSIONI La crittografia del contenuto degli eventi pu\u00f2 essere utilizzata per applicare una politica di controllo degli accessi mentre gli eventi sono in transito nella rete di broker di un sistema di pubblicazione/sottoscrizione multi-dominio. La crittografia a livello di attributo pu\u00f2 essere implementata per applicare politiche di controllo degli accessi a grana fine. Oltre a fornire il controllo dell'accesso a livello di attributo, la crittografia degli attributi consente ai broker parzialmente autorizzati di implementare l'instradamento basato sul contenuto in base agli attributi a loro accessibili.", "keyphrases": ["sistema di pubblicazione/sottoscrizione sicuro", "distribuire il controllo degli accessi", "dominio amministrativo multiplo", "crittografia degli attributi", "multidominio", "spese generali generali", "distribuire l'applicazione di distribuzione del sistema", "eseguire", "crittografare", "servizio tariffazione congestione"]}
{"file_name": "C-29", "text": "Implementazione e valutazione delle prestazioni di CONFLEX-G: programma di ricerca spaziale conformazionale molecolare abilitato alla griglia con OmniRPC ABSTRACT CONFLEX-G \u00e8 la versione abilitata alla griglia di un programma di ricerca spaziale conformazionale molecolare chiamato CONFLEX. Abbiamo implementato CONFLEX-G utilizzando un sistema RPC a griglia chiamato OmniRPC. In questo articolo riportiamo le prestazioni di CONFLEX-G in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare molte conformazioni di grandi biomolecole, CONFLEX-G genera strutture di prova delle molecole e assegna compiti per ottimizzare una struttura di prova con un metodo affidabile di meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato per supportare le applicazioni di ricerca parametrica. In questo modello, quando la procedura di inizializzazione \u00e8 definita nel modulo RPC, il modulo viene inizializzato automaticamente al momento dell'invocazione richiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare comunicazioni e inizializzazioni non necessarie ad ogni chiamata in CONFLEX-G. CONFLEXG pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e pu\u00f2 sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di un cluster di pi\u00f9 cluster nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha raggiunto un'accelerazione di 56,5 volte nel caso della molecola 1BL1, dove la molecola \u00e8 costituita da un gran numero di atomi, e ogni ottimizzazione della struttura di prova richiede tempo significativo. Anche lo squilibrio del carico del tempo di ottimizzazione della struttura di prova pu\u00f2 causare un degrado delle prestazioni. 1. INTRODUZIONE Recentemente, il concetto di griglia computazionale ha iniziato ad attirare un notevole interesse nel campo del network computing ad alte prestazioni. CONFLEX \u00e8 uno dei programmi di ricerca dello spazio conformazionale pi\u00f9 efficienti e affidabili -LSB- 1 -RSB-. Abbiamo applicato questo programma alla parallelizzazione utilizzando il calcolo globale. Le prestazioni del CONFLEX parallelizzato consentono l'esplorazione della regione a bassa energia dello spazio conformazionale di piccoli peptidi entro un tempo trascorso disponibile utilizzando un cluster PC locale. Poich\u00e9 l'ottimizzazione della struttura di prova in CONFLEX viene calcolata tramite la meccanica molecolare, la ricerca nello spazio conformazionale pu\u00f2 essere eseguita rapidamente rispetto a quella utilizzando il calcolo dell'orbitale molecolare. Sebbene sia stata utilizzata la versione parallelizzata di CONFLEX per calcolare in parallelo l'ottimizzazione della struttura, che richiede oltre il 90% dell'elaborazione nella ricerca della conformazione molecolare, con questo metodo da solo non \u00e8 stato possibile ottenere un miglioramento sufficiente nell'accelerazione. Ci\u00f2 richiede le vaste risorse informatiche di un ambiente di grid computing. In questo articolo descriviamo CONFLEX-G, un programma di ricerca conformazionale molecolare abilitato alla griglia, utilizzando OmniRPC e riportiamo le sue prestazioni in una griglia di diversi cluster di PC distribuiti geograficamente. Il prototipo CONFLEX-G assegna l'ottimizzazione delle strutture di prova di calcolo, che \u00e8 un compito che richiede molto tempo, ai nodi di lavoro nell'ambiente di rete al fine di ottenere un throughput elevato.Inoltre, confrontiamo le prestazioni di CONFLEX-G in un cluster di PC locale con quelle in un banco di prova a griglia. OmniRPC -LSB- 2, 3, 4 -RSB- \u00e8 un'implementazione thread-safe di Ninf RPC -LSB- 5, 6 -RSB- che \u00e8 una struttura Grid RPC per il calcolo dell'ambiente grid. Diversi sistemi adottano il concetto di RPC come modello di base per il grid Environment Computing, tra cui Ninf-G -LSB- 7 -RSB-, NetSolve -LSB- 8 -RSB- e CORBA -LSB- 9 -RSB-. Il sistema RPCstyle fornisce un'interfaccia di programmazione intuitiva e facile da usare, che consente agli utenti del sistema grid di creare facilmente applicazioni abilitate alla griglia. Per supportare la programmazione parallela, un client RPC pu\u00f2 inviare richieste di chiamata asincrone a un diverso computer remoto per sfruttare il parallelismo a livello di rete tramite OmniRPC. In questo articolo proponiamo il modello di persistenza OmniRPC a un sistema Grid RPC e ne dimostriamo l'efficacia. Per supportare un'applicazione tipica per un ambiente grid, come un'applicazione di ricerca parametrica, in cui la stessa funzione viene eseguita con parametri di input diversi sullo stesso set di dati. Nell'attuale sistema GridRPC -LSB- 10 -RSB-, i dati impostati dalla chiamata precedente non possono essere utilizzati dalle chiamate successive. Questo articolo dimostra che CONFLEX-G \u00e8 in grado di sfruttare le enormi risorse informatiche di un ambiente grid e di ricercare conformeri molecolari su larga scala. Dimostriamo CONFLEX-G sul nostro banco di prova a griglia utilizzando la proteina reale come molecola campione. La funzionalit\u00e0 OmniRPC del modulo inizializzabile automatico -LRB- AIM -RRB- consente al sistema di calcolare in modo efficiente numerosi conformeri. Inoltre, utilizzando OmniRPC, l'utente pu\u00f2 parallelizzare l'applicazione esistente e spostarsi dal cluster all'ambiente grid senza modificare il codice del programma e compilare il programma. Inoltre, l'utente pu\u00f2 facilmente creare un ambiente di rete privato. Una panoramica Figura 1: Algoritmo di ricerca dello spazio conformazionale nel CONFLEX originale. del sistema CONFLEX \u00e8 presentato nella Sezione 2, e l'implementazione e la progettazione di CONFLEX-G sono descritte nella Sezione 3. Riportiamo i risultati sperimentali ottenuti utilizzando CONFLEX-G e discutiamo le sue prestazioni nella Sezione 4. Nella Sezione 6, presentiamo le conclusioni e discutiamo argomenti per studi futuri. 5. LAVORI CORRELATI Recentemente \u00e8 stato sviluppato un algoritmo che risolve i problemi di parallelizzazione e comunicazione in processori scarsamente connessi da utilizzare per la simulazione. Ci\u00f2 ci ha permesso di simulare il ripiegamento per la prima volta e di esaminare direttamente le malattie correlate al ripiegamento. SETI@home[14] \u00e8 un programma per cercare la vita aliena analizzando i segnali dei radiotelescopi utilizzando i dati dei radiotelescopi in trasformata di Fourier provenienti da telescopi di diversi siti. SETI@home affronta problemi estremamente paralleli, in cui il calcolo pu\u00f2 essere facilmente suddiviso tra pi\u00f9 computer. I blocchi di dati del radiotelescopio possono essere facilmente assegnati a diversi computer. Tuttavia, le competenze e l'impegno necessari per sviluppare un'applicazione grid potrebbero non essere necessari per OmniRPC.Nimrod/G -LSB- 15 -RSB- \u00e8 uno strumento per la modellazione parametrica distribuita e implementa una task farm parallela per simulazioni che richiedono diversi parametri di input variabili. Nimrod \u00e8 stato applicato ad applicazioni tra cui la bioinformatica, la ricerca operativa e la modellazione molecolare per la progettazione di farmaci. NetSolve -LSB- 8 -RSB- \u00e8 una struttura RPC simile a OmniRPC e Ninf, che fornisce un'interfaccia di programmazione simile e un meccanismo di bilanciamento automatico del carico. Matsuoka et al. -LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.-LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.-LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.", "keyphrases": ["riflesso-g", "omnirpc", "ricerca nello spazio conforme", "biomolecola", "modulo rpc", "procedura initi", "MPU", "gruppo di PC", "calcolo della griglia", "sistema rpc a griglia", "meccanica molecolare", "modulo inizializzazione automatica"]}
{"file_name": "C-9", "text": "EDAS: fornire un ambiente per servizi adattivi decentralizzati ABSTRACT Poich\u00e9 l'idea di virtualizzazione della potenza di calcolo, dello storage e della larghezza di banda diventa sempre pi\u00f9 importante, il grid computing si evolve e viene applicato a un numero crescente di applicazioni. L'ambiente per i servizi adattivi decentralizzati -LRB- EDAS -RRB- fornisce un'infrastruttura simile a una griglia per servizi a lungo termine accessibili agli utenti -LRB- ad es. server web, repository di codice sorgente ecc. -RRB-. Mira a supportare l'esecuzione autonoma e l'evoluzione dei servizi in termini di scalabilit\u00e0 e distribuzione consapevole delle risorse. EDAS offre modelli di servizio flessibili basati su oggetti mobili distribuiti che vanno da uno scenario client-server tradizionale a un approccio completamente basato su peer-to-peer. La gestione automatica e dinamica delle risorse consente un utilizzo ottimizzato delle risorse disponibili riducendo al minimo la complessit\u00e0 amministrativa. 1. INTRODUZIONE Le infrastrutture per il grid computing mirano a virtualizzare un gruppo di computer, server e storage come un unico grande sistema informatico. La gestione delle risorse \u00e8 una questione chiave in tali sistemi, necessaria per una distribuzione efficiente e automatizzata dei compiti sulla rete. Tali infrastrutture di rete sono spesso implementate a livello aziendale, ma progetti come SETI@home -LSB- 1 -RSB- hanno dimostrato la fattibilit\u00e0 anche di reti pi\u00f9 decentralizzate. Le attuali infrastrutture di grid computing non forniscono un supporto sufficiente per l'esecuzione di servizi distribuiti, accessibili agli utenti e a lungo termine poich\u00e9 sono progettate per risolvere compiti ad alta intensit\u00e0 di calcolo o di dati con un insieme di parametri pi\u00f9 o meno fissi. Invece un'infrastruttura per servizi a lungo termine deve collocare i servizi in base alla domanda attuale e ai requisiti futuri stimati. La migrazione, tuttavia, \u00e8 costosa poich\u00e9 deve essere trasferito l'intero stato di un servizio. Inoltre, un servizio non replicato non \u00e8 accessibile durante la migrazione. Pertanto la gestione delle risorse deve evitare la migrazione, se possibile. Inoltre deve essere fornito un concetto di servizio che eviti in primo luogo il sovraccarico e in secondo luogo inibisca l'indisponibilit\u00e0 del servizio se la migrazione non pu\u00f2 essere evitata. EDAS -LSB- 2 -RSB- mira a fornire un'infrastruttura simile a una griglia per servizi a lungo termine accessibili agli utenti che consenta l'adattamento dinamico in fase di esecuzione, fornisce un'infrastruttura di gestione e offre supporto a livello di sistema per scalabilit\u00e0 e guasti tolleranza. I nodi possono unirsi e abbandonare dinamicamente l'infrastruttura e tutte le attivit\u00e0 di gestione, in particolare la gestione delle risorse, sono decentralizzate. L'ambiente \u00e8 basato sulla nostra infrastruttura middleware AspectIX -LSB- 3 -RSB-, che supporta direttamente la riconfigurazione dinamica dei servizi basata su QoS. La gestione delle risorse si concentra sull'esecuzione di servizi che hanno un tempo operativo lungo, potenzialmente infinito. Questi servizi sono organizzati in progetti. Ogni progetto ha un ambito di esecuzione distribuito chiamato ambiente di servizio. Un tale ambiente probabilmente abbraccia pi\u00f9 istituzioni.Ciascuna istituzione rappresenta un dominio amministrativo in grado di supportare un progetto con un insieme fisso di risorse. Il nostro approccio supporta la gestione adattiva delle risorse di tutti i progetti nell'ambito di un'istituzione basata su un algoritmo ispirato agli algoritmi diffusivi per il bilanciamento del carico decentralizzato -LSB- 4 -RSB-. Non \u00e8 noto come suddividere in modo ottimale queste risorse per i servizi poich\u00e9 la domanda di risorse dei servizi pu\u00f2 cambiare nel tempo o addirittura fluttuare frequentemente. Per fornire le risorse necessarie, il nostro approccio ridedica automaticamente in modo uniforme le risorse libere o non necessarie tra le istanze del servizio su progetti e nodi. Nei casi in cui la ridedicazione non \u00e8 possibile, viene avviata la migrazione del servizio impegnativo. In un'infrastruttura di rete di servizi a lungo termine, la replica attiva presenta diversi vantaggi: le repliche possono unirsi e lasciare il gruppo di oggetti e pertanto \u00e8 possibile migrare le repliche senza indisponibilit\u00e0 del servizio. Infine, pu\u00f2 essere tollerata una certa quantit\u00e0 di arresti anomali dei nodi. La Sezione 4 spiega i concetti di autogestione e ridedicazione della gestione distribuita delle risorse adattive. La sezione 5 descrive il quadro per i servizi adattivi decentralizzati. La Sezione 6 descrive il lavoro correlato e infine la Sezione 7 conclude il documento. 6. LAVORI CORRELATI Infrastrutture grid come Globus-Toolkit -LSB- 11 -RSB- forniscono servizi e meccanismi per ambienti eterogenei distribuiti per combinare risorse su richiesta per risolvere compiti ad alta intensit\u00e0 di consumo di risorse e di calcolo. A causa di questo orientamento si concentrano su diversi modelli di servizio e non forniscono alcun supporto per la mobilit\u00e0 degli oggetti, se non addirittura supportando un approccio a oggetti distribuiti. Ma, cosa pi\u00f9 importante, seguono un diverso approccio di gestione delle risorse poich\u00e9 mirano all\u2019esecuzione parallela di un gran numero di attivit\u00e0 a breve e medio termine. Gli oggetti replicati attivamente sono forniti da Jgroup -LSB- 14 -RSB- basato su RMI. Oltre a questo middleware di base \u00e8 stato implementato un livello di gestione della replica chiamato ARM -LSB- 15 -RSB-. JGroup si concentra sulla replica attiva degli oggetti ma manca del supporto per servizi pi\u00f9 flessibili come fa EDAS. ARM pu\u00f2 essere paragonato a EDAS ma non supporta alcuna distribuzione consapevole delle risorse. Fog -LSB- 16 -RSB- e Globe -LSB- 17 -RSB- sono ambienti middleware di base che supportano l'approccio agli oggetti frammentati. Globe considera la replica e la memorizzazione nella cache. Entrambi i sistemi non supportano la distribuzione consapevole delle risorse. 7. CONCLUSIONE E LAVORI IN CORSO Sulla base del modello a oggetti frammentato e dell'architettura dell'ambiente EDAS, i servizi adattivi decentralizzati possono essere facilmente progettati, implementati ed eseguiti. Come descritto, la gestione delle risorse pu\u00f2 essere scomposta in due problemi principali che devono essere risolti. Controllo e gestione dei limiti delle risorse, inclusa la garanzia che le risorse assegnate siano disponibili -LRB- anche nel context di crash del nodo -RRB- e il posizionamento autonomo dei servizi. Per entrambi i problemi offriamo una soluzione,un ambiente di simulazione attualmente implementato ne verificher\u00e0 la fattibilit\u00e0. In una fase successiva la gestione delle risorse sar\u00e0 integrata in un prototipo gi\u00e0 implementato dell'architettura EDAS. Come descritto, abbiamo gi\u00e0 una prima implementazione del quadro per i servizi adattivi decentralizzati. Questo quadro deve essere esteso per interagire agevolmente con la gestione delle risorse e l'architettura EDAS. In una fase finale dobbiamo implementare alcuni servizi che verifichino l'usabilit\u00e0 dell'intero progetto EDAS.", "keyphrases": ["decentrare adattare il servizio", "gestione delle risorse", "ambiente domestico", "infrastruttura", "cliente", "servizio a lungo termine", "eda", "limite locale", "limite globale", "risorsa", "nodo"]}
{"file_name": "H-14", "text": "Studiare l'uso di destinazioni popolari per migliorare l'interazione della ricerca sul Web ABSTRACT Presentiamo una nuova funzionalit\u00e0 di interazione della ricerca sul Web che, per una determinata query, fornisce collegamenti a siti Web visitati frequentemente da altri utenti con esigenze di informazioni simili. Queste destinazioni popolari integrano i risultati di ricerca tradizionali, consentendo la navigazione diretta verso risorse autorevoli per l'argomento della query. Le destinazioni vengono identificate utilizzando la cronologia del comportamento di ricerca e di navigazione di molti utenti per un periodo di tempo prolungato, il cui comportamento collettivo fornisce una base per l'autorit\u00e0 della fonte informatica. Descriviamo uno studio sugli utenti che ha confrontato il suggerimento delle destinazioni con il suggerimento precedentemente proposto di query correlate, nonch\u00e9 con la ricerca Web tradizionale e non assistita. I risultati mostrano che la ricerca potenziata dai suggerimenti di destinazione supera gli altri sistemi per le attivit\u00e0 esplorative, con le migliori prestazioni ottenute dall'estrazione del comportamento passato degli utenti con granularit\u00e0 a livello di query. 1. INTRODUZIONE Il problema del miglioramento delle query inviate ai sistemi di Information Retrieval -LRB- IR -RRB- \u00e8 stato ampiamente studiato nella ricerca IR -LSB- 4 -RSB- -LSB- 11 -RSB-. Formulazioni di query alternative, note come suggerimenti di query, possono essere offerte agli utenti dopo una query iniziale, consentendo loro di modificare le specifiche delle loro esigenze fornite al sistema, con conseguente miglioramento delle prestazioni di recupero. La recente popolarit\u00e0 dei motori di ricerca Web ha consentito suggerimenti di query che si basano sul comportamento di riformulazione delle query di molti utenti per fornire consigli sulle query basati sulle precedenti interazioni dell'utente -LSB- 10 -RSB-. Sfruttare i processi decisionali di molti utenti per la riformulazione delle query ha le sue radici nell'indicizzazione adattiva -LSB- 8 -RSB-. Tuttavia, gli approcci basati sull\u2019interazione per suggerire query possono essere meno potenti quando il bisogno di informazione \u00e8 esplorativo, poich\u00e9 gran parte dell\u2019attivit\u00e0 dell\u2019utente per tali bisogni di informazione pu\u00f2 verificarsi oltre le interazioni con i motori di ricerca. Nei casi in cui la ricerca diretta rappresenta solo una frazione del comportamento di ricerca di informazioni degli utenti, l'utilit\u00e0 dei clic di altri utenti nello spazio dei risultati di primo livello potrebbe essere limitata, poich\u00e9 non copre il successivo comportamento di navigazione. Allo stesso tempo, la navigazione dell'utente che segue le interazioni dei motori di ricerca fornisce l'approvazione implicita delle risorse Web preferite dagli utenti, il che pu\u00f2 essere particolarmente utile per attivit\u00e0 di ricerca esplorativa. Pertanto, proponiamo di sfruttare una combinazione di ricerche passate e comportamenti degli utenti di navigazione per migliorare le interazioni di ricerca Web degli utenti. I plug-in del browser e i log del server proxy forniscono accesso ai modelli di navigazione degli utenti che trascendono le interazioni con i motori di ricerca. In lavori precedenti, tali dati sono stati utilizzati per migliorare il posizionamento dei risultati di ricerca da Agichtein et al. -LSB-1 -RSB-. Radlinski e Joachims -LSB- 13 -RSB- hanno utilizzato tale intelligenza collettiva dell'utente per migliorare la precisione del recupero utilizzando sequenze di riformulazioni di query consecutive,tuttavia il loro approccio non considera le interazioni degli utenti oltre la pagina dei risultati di ricerca. In questo articolo presentiamo uno studio sugli utenti di una tecnica che sfrutta il comportamento di ricerca e navigazione di molti utenti per suggerire pagine Web popolari, da ora in poi denominate destinazioni, oltre ai normali risultati di ricerca. Le destinazioni potrebbero non essere tra i primi risultati, non contenere i termini cercati o addirittura non essere indicizzate dal motore di ricerca. Si tratta invece di pagine in cui altri utenti finiscono frequentemente dopo aver inviato query identiche o simili e poi allontanandosi dai risultati di ricerca inizialmente cliccati. Ipotizziamo che le destinazioni popolari tra un gran numero di utenti possano catturare l'esperienza collettiva dell'utente per esigenze di informazione, e i nostri risultati supportano questa ipotesi. In -LSB- 19 -RSB-, Wexelblat e Maes descrivono un sistema per supportare la navigazione all'interno del dominio in base ai percorsi di navigazione di altri utenti. Tuttavia, non siamo a conoscenza di tali principi applicati alla ricerca sul Web. Forse l'esempio pi\u00f9 vicino al teletrasporto \u00e8 l'offerta da parte dei motori di ricerca di numerose scorciatoie all'interno del dominio sotto il titolo di un risultato di ricerca. Sebbene questi possano essere basati sul comportamento dell'utente e possibilmente sulla struttura del sito, l'utente risparmia al massimo un clic da questa funzione. Al contrario, l'approccio da noi proposto pu\u00f2 trasportare gli utenti in luoghi molti clic oltre il risultato della ricerca, risparmiando tempo e offrendo loro una prospettiva pi\u00f9 ampia sulle informazioni correlate disponibili. Lo studio condotto sugli utenti esamina l'efficacia dell'inclusione di collegamenti a destinazioni popolari come funzionalit\u00e0 aggiuntiva dell'interfaccia nelle pagine dei risultati dei motori di ricerca. Confronteremo due varianti di questo approccio con il suggerimento di query correlate e ricerca sul Web non assistita e cercheremo risposte a domande su: -LRB- i -RRB- preferenza dell'utente ed efficacia della ricerca per elementi noti e attivit\u00e0 di ricerca esplorativa e -LRB- ii -RRB- la distanza preferita tra query e destinazione utilizzata per identificare le destinazioni popolari dai registri di comportamento passati. I risultati indicano che suggerire destinazioni popolari agli utenti che tentano attivit\u00e0 esplorative fornisce i migliori risultati negli aspetti chiave dell\u2019esperienza di ricerca di informazioni, mentre fornire suggerimenti di perfezionamento delle query \u00e8 pi\u00f9 auspicabile per attivit\u00e0 con elementi noti. Nella Sezione 2 descriviamo l'estrazione dei percorsi di ricerca e di navigazione dai registri delle attivit\u00e0 degli utenti e il loro utilizzo nell'identificazione delle principali destinazioni per le nuove query. La Sezione 3 descrive il disegno dello studio sugli utenti, mentre le Sezioni 4 e 5 presentano rispettivamente i risultati dello studio e la loro discussione. 6. CONCLUSIONI Abbiamo presentato un nuovo approccio per migliorare l'interazione degli utenti nella ricerca sul Web fornendo collegamenti a siti Web visitati di frequente da utenti che hanno effettuato ricerche in passato con esigenze di informazioni simili. \u00c8 stato condotto uno studio sugli utenti in cui abbiamo valutato l'efficacia della tecnica proposta rispetto a un sistema di perfezionamento delle query e alla ricerca Web non assistita. I risultati del nostro studio hanno rivelato che:I sistemi -LRB- i -RRB- che suggeriscono perfezionamenti delle query sono stati preferiti per attivit\u00e0 di elementi noti, i sistemi -LRB- ii -RRB- che offrono destinazioni popolari sono stati preferiti per attivit\u00e0 di ricerca esplorativa e le destinazioni -LRB- iii -RRB- dovrebbero essere estratte da la fine dei percorsi delle query, non dei percorsi delle sessioni. Nel complesso, i suggerimenti di destinazioni popolari hanno influenzato strategicamente le ricerche in un modo non ottenibile con gli approcci di suggerimento delle query, offrendo un nuovo modo per risolvere i problemi di informazione e migliorare l'esperienza di ricerca di informazioni per molti ricercatori sul Web.", "keyphrases": ["destinazione popolare", "la ricerca web interagisce", "domande improvvisate", "recuperare eseguire", "riguardare queri", "esperienza di ricerca di informazioni", "sentiero queri", "percorso della sessione", "approccio basato sulla ricerca", "valutazione su base logaritmica"]}
{"file_name": "I-20", "text": "Calcolo dell'indice di potenza di Banzhaf nei giochi di flusso di rete SOMMARIO L'aggregazione delle preferenze viene utilizzata in una variet\u00e0 di applicazioni multiagente e, di conseguenza, la teoria del voto \u00e8 diventata un argomento importante nella ricerca sui sistemi multiagente. Tuttavia, gli indici di potere -LRB- che riflettono quanto \u201cpotere reale\u201d ha un elettore in un sistema di voto ponderato -RRB- hanno ricevuto relativamente poca attenzione, sebbene siano stati a lungo studiati nelle scienze politiche e in economia. L'indice di potenza Banzhaf \u00e8 uno dei pi\u00f9 popolari; \u00e8 anche ben definito per qualsiasi semplice gioco di coalizione. In questo articolo, esaminiamo la complessit\u00e0 computazionale del calcolo dell'indice di potenza di Banzhaf all'interno di un particolare dominio multiagente, un gioco di flusso di rete. Gli agenti controllano i bordi di un grafico; una coalizione vince se riesce a inviare un flusso di una determinata dimensione da un vertice sorgente a un vertice destinazione. Il potere relativo di ciascun edge/agente riflette la sua importanza nel consentire tale flusso e nelle reti del mondo reale potrebbe essere utilizzato, ad esempio, per allocare risorse per la manutenzione di parti della rete. Mostriamo che il calcolo dell'indice di potenza Banzhaf di ciascun agente in questo dominio di flusso di rete \u00e8 #P - completo. Mostriamo anche che per alcuni domini di flusso di rete ristretti esiste un algoritmo polinomiale per calcolare gli indici di potenza Banzhaf degli agenti. 1. INTRODUZIONE Qual \u00e8 la complessit\u00e0 del processo? La complessit\u00e0 pu\u00f2 essere utilizzata per difendersi da fenomeni indesiderati? La complessit\u00e0 del calcolo impedisce l\u2019implementazione realistica di una tecnica? Le applicazioni pratiche del voto tra agenti automatizzati sono gi\u00e0 diffuse. Infatti, per vedere la generalit\u00e0 dello scenario di voto automatizzato -LRB- -RRB-, si consideri la moderna ricerca sul web. In questo articolo, consideriamo un argomento che \u00e8 stato meno studiato nel context del voto automatizzato degli agenti, vale a dire gli indici di potere. Un indice di potere \u00e8 una misura del potere che un sottogruppo, o equivalentemente un elettore in un ambiente di voto ponderato, ha sulle decisioni di un gruppo pi\u00f9 ampio. L\u2019indice di potere di Banzhaf \u00e8 una delle misure pi\u00f9 popolari del potere di voto e, sebbene sia stato utilizzato principalmente per misurare il potere nei giochi di voto ponderato, \u00e8 ben definito per qualsiasi semplice gioco di coalizione. Esaminiamo alcuni aspetti computazionali dell'indice di potenza di Banzhaf in un ambiente specifico, vale a dire un gioco di flusso di rete. In questo gioco, una coalizione di agenti vince se riesce a inviare un flusso di dimensione k da un vertice sorgente s a un vertice bersaglio t, con la potenza relativa di ciascun bordo che riflette il suo significato nel consentire tale flusso. Mostriamo che il calcolo dell'indice di potenza Banzhaf di ciascun agente in questo dominio di flusso della rete generale \u00e8 #P - completo. giochi di nectivity su grafi a strati limitati -RRB-, esiste un algoritmo polinomiale per calcolare l'indice di potenza Banzhaf di un agente. Il documento procede come segue. Nella Sezione 2 forniamo alcune informazioni di base sui giochi di coalizione e sull'indice di potere di Banzhaf, e nella Sezione 3 introduciamo il nostro specifico gioco di flusso di rete.Nella Sezione 4 discutiamo l'indice di potenza di Banzhaf nei giochi di flusso di rete, presentando il nostro risultato di complessit\u00e0 nel caso generale. Nella Sezione 5 consideriamo un caso ristretto del gioco del flusso di rete e presentiamo i risultati. Nella Sezione 6 discuteremo il lavoro correlato e concluderemo nella Sezione 7. 6. LAVORO CORRELATO La misurazione del potere dei singoli giocatori nei giochi di coalizione \u00e8 stata studiata per molti anni. Gli indici pi\u00f9 popolari suggeriti per tale misurazione sono l\u2019indice Banzhaf -LSB- 1 -RSB- e l\u2019indice Shapley-Shubik -LSB- 19 -RSB-. Nel suo articolo fondamentale, Shapley -LSB- 18 -RSB- ha considerato i giochi di coalizione e l'equa allocazione dell'utilit\u00e0 acquisita dalla grande coalizione -LRB- la coalizione di tutti gli agenti -RRB- ai suoi membri. L'indice Shapley-Shubik -LSB- 19 -RSB- \u00e8 l'applicazione diretta del valore di Shapley a semplici giochi di coalizione. L'indice Banzhaf \u00e8 emerso direttamente dallo studio del voto negli organi decisionali. L\u2019indice Banzhaf normalizzato misura la percentuale di coalizioni in cui un giocatore \u00e8 uno swinger, rispetto a tutte le coalizioni vincenti. Questo indice \u00e8 simile all'indice Banzhaf discusso nella Sezione 1, ed \u00e8 definito come: L'indice Banzhaf \u00e8 stato analizzato matematicamente in -LSB- 3 -RSB-, dove \u00e8 stato dimostrato che questa normalizzazione manca di alcune propriet\u00e0 desiderabili, e l'indice Banzhaf pi\u00f9 naturale viene introdotto l'indice Sia l\u2019indice Shapley-Shubik che quello Banzhaf sono stati ampiamente studiati, e Straffin -LSB- 20 -RSB- ha dimostrato che ciascun indice riflette condizioni specifiche in un organo di voto. -LSB- 11 -RSB- considera questi due indici insieme a molti altri e descrive gli assiomi che caratterizzano i diversi indici. L'implementazione ingenua di un algoritmo per calcolare l'indice Banzhaf di un agente i enumera tutte le coalizioni contenenti i. Esistono 2n \u2212 1 coalizioni di questo tipo, quindi la prestazione \u00e8 esponenziale nel numero di agenti. -LSB- 12 -RSB- contiene un'indagine sugli algoritmi per il calcolo degli indici di potere dei giochi a maggioranza ponderata. Deng e Papadimitriou -LSB- 2 -RSB- mostrano che il calcolo del valore di Shapley nei giochi a maggioranza ponderata \u00e8 #P - completo, utilizzando una riduzione da KNAPSACK. Poich\u00e9 il valore di Shapley di qualsiasi gioco semplice ha lo stesso valore del suo indice di Shapley-Shubik, ci\u00f2 dimostra che il calcolo dell'indice di Shapley-Shubik nei giochi a maggioranza ponderata \u00e8 #Pcompleto. Matsui e Matsui -LSB- 13 -RSB- hanno dimostrato che il calcolo degli indici Banzhaf e Shapley-Shubik nei giochi di voto ponderato \u00e8 NP-completo. Il problema degli indici di potenza di calcolo nei giochi semplici dipende dalla rappresentazione scelta del gioco. Poich\u00e9 il numero di possibili coalizioni \u00e8 esponenziale nel numero di agenti, il calcolo degli indici di potere in polinomio temporale nel numero di agenti pu\u00f2 essere ottenuto solo in domini specifici. In questo articolo abbiamo considerato il dominio del flusso di rete, dove una coalizione di agenti deve raggiungere un flusso oltre un certo valore. Il gioco del flusso di rete che abbiamo definito \u00e8 un gioco semplice. -LSB-10,9 -RSB- hanno considerato un dominio di flusso di rete simile, in cui ciascun agente controlla un bordo di un grafo di flusso di rete. Tuttavia, hanno introdotto un gioco non semplice, in cui il valore raggiunto da una coalizione di agenti \u00e8 il flusso totale massimo. Hanno dimostrato che alcune famiglie di giochi a flusso di rete e giochi simili hanno nuclei non vuoti. 7. CONCLUSIONI E DIREZIONI FUTURE Abbiamo considerato giochi di flusso di rete, dove una coalizione di agenti vince se riesce a inviare un flusso di pi\u00f9 di un certo valore k tra due vertici. Abbiamo valutato il potere relativo di ciascun agente in questo scenario utilizzando l'indice Banzhaf. Questo indice di potere pu\u00f2 essere utilizzato per decidere come allocare le risorse di manutenzione nelle reti del mondo reale, al fine di massimizzare la nostra capacit\u00e0 di mantenere un certo flusso di informazioni tra due siti. Sebbene l'indice di Banzhaf ci permetta teoricamente di misurare il potere degli agenti nel gioco del flusso di rete, abbiamo dimostrato che il problema del calcolo dell'indice di Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.abbiamo dimostrato che il problema del calcolo dell'indice Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.abbiamo dimostrato che il problema del calcolo dell'indice Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.", "keyphrases": ["preferire l'aggregato", "applicazione multiag", "voto teorico", "indice di potenza banzhaf", "analisi di algoritmi e problemi complessi", "teoria della scelta sociale", "voto dell'agente automatico", "gioco del flusso di rete", "modello probabilistico", "collegare il gioco"]}
{"file_name": "J-7", "text": "Il ruolo della compatibilit\u00e0 nella diffusione delle tecnologie attraverso i social network ABSTRACT In molti contesti, le tecnologie concorrenti \u2013 ad esempio i sistemi operativi, i sistemi di messaggistica istantanea o i formati di documenti \u2013 possono essere viste adottare un livello limitato di compatibilit\u00e0 tra loro; in altre parole, la difficolt\u00e0 nell\u2019utilizzare pi\u00f9 tecnologie \u00e8 in equilibrio tra i due estremi dell\u2019impossibilit\u00e0 e dell\u2019interoperabilit\u00e0 senza sforzo. Ci sono una serie di ragioni per cui questo fenomeno si verifica, molte delle quali \u2013 basate su considerazioni legali, sociali o aziendali \u2013 sembrano sfidare modelli matematici concisi. Nonostante ci\u00f2, mostriamo che i vantaggi di una compatibilit\u00e0 limitata possono emergere in un modello molto semplice di diffusione nei social network, offrendo cos\u00ec una spiegazione di base di questo fenomeno in termini puramente strategici. Il nostro approccio si basa sul lavoro sulla diffusione delle innovazioni nella letteratura economica, che cerca di modellare come una nuova tecnologia A potrebbe diffondersi attraverso una rete sociale di individui che sono attualmente utenti della tecnologia B. Consideriamo diversi modi per catturare la compatibilit\u00e0 di A e B, concentrandosi principalmente su un modello in cui gli utenti possono scegliere di adottare A, adottare B o, a un costo aggiuntivo, adottare sia A che B. Caratterizziamo come la capacit\u00e0 di A di diffondersi dipenda sia dalla sua qualit\u00e0 relativa a B, e anche questo costo aggiuntivo derivante dall\u2019adozione di entrambe, e trovare alcune sorprendenti propriet\u00e0 di non monotonicit\u00e0 nella dipendenza da questi parametri: in alcuni casi, affinch\u00e9 una tecnologia sopravviva all\u2019introduzione di un\u2019altra, il costo dell\u2019adozione di entrambe le tecnologie deve essere bilanciato entro un intervallo ristretto e intermedio. Estendiamo il quadro anche al caso di pi\u00f9 tecnologie, dove scopriamo che un semplice Questo lavoro \u00e8 stato supportato in parte dai finanziamenti NSF CCF0325453, IIS-0329064, CNS-0403340 e BCS-0537606, un Google Research Grant, un Yahoo ! Research Alliance Grant, l'Istituto per le scienze sociali alla Cornell e la Fondazione John D. e Catherine T. MacArthur. Il modello cattura il fenomeno di due aziende che adottano una \u201calleanza strategica\u201d limitata per difendersi da una nuova, terza tecnologia. 1. INTRODUZIONE Giochi di diffusione e di coordinamento in rete. Tali problemi sorgono, ad esempio, nell'adozione di nuove tecnologie, nell'emergere di nuove norme sociali o convenzioni organizzative, o nella diffusione dei linguaggi umani -LSB- 2, 14, 15, 16, 17 -RSB-. Una linea di ricerca attiva in economia e sociologia matematica si occupa di modellare questi tipi di processi di diffusione come un gioco di coordinazione giocato su una rete sociale -LSB- 1, 5, 7, 13, 19 -RSB-. Inizieremo discutendo uno dei modelli di diffusione pi\u00f9 basilari della teoria dei giochi, proposto in un influente articolo di Morris -LSB- 13 -RSB-, che costituir\u00e0 il punto di partenza per il nostro lavoro qui. Lo descriviamo nei termini del seguente scenario di adozione della tecnologia, sebbene esistano molti altri esempi che potrebbero servire allo stesso scopo.Si noti che A \u00e8 la tecnologia \u201cmigliore\u201d se q < 21, nel senso che i profitti AA supererebbero i guadagni BB, mentre A \u00e8 la tecnologia peggiore se q > 21. Una serie di intuizioni qualitative possono essere derivate da una diffusione modello anche a questo livello di semplicit\u00e0. Nello specifico, consideriamo una rete G e lasciamo che tutti i nodi giochino inizialmente a B. Supponiamo ora che un piccolo numero di nodi inizi invece ad adottare la strategia A. Compatibilit\u00e0, interoperabilit\u00e0 e bilinguismo. Un pezzo importante che probabilmente manca nei modelli di diffusione di base della teoria dei giochi, tuttavia, \u00e8 un quadro pi\u00f9 dettagliato di ci\u00f2 che sta accadendo al confine di coesistenza, dove la forma base del modello presuppone nodi che adottano A collegati a nodi che adottano B. In questi contesti motivanti per i modelli, ovviamente, si vedono molto spesso regioni di interfaccia in cui gli individui diventano essenzialmente \"bilingui\". ''Nel caso della diffusione del linguaggio umano, questa bilinguit\u00e0 \u00e8 intesa letteralmente: le regioni geografiche dove c'\u00e8 una sostanziale interazione con parlanti di due lingue diverse tendono ad avere abitanti che le parlano entrambe. Da questo punto di vista, \u00e8 naturale chiedersi come si comportano i modelli di diffusione una volta estesi in modo che alcuni nodi possano essere bilingui in questo senso molto generale, adottando entrambe le strategie a un certo costo per loro stessi. Cosa potremmo imparare da una simile estensione? Per cominciare, ha il potenziale per fornire una prospettiva preziosa sulla questione della compatibilit\u00e0 e dell\u2019incompatibilit\u00e0 che \u00e8 alla base della concorrenza tra le aziende tecnologiche. Esiste un\u2019ampia letteratura su come la compatibilit\u00e0 tra tecnologie influisca sulla concorrenza tra imprese, e in particolare su come l\u2019incompatibilit\u00e0 possa essere una decisione strategica vantaggiosa per alcuni partecipanti in un mercato -LSB- 3, 4, 8, 9, 12 -RSB-. Mentre questi modelli esistenti di compatibilit\u00e0 catturano gli effetti di rete nel senso che gli utenti sul mercato preferiscono utilizzare una tecnologia pi\u00f9 diffusa, non catturano il fenomeno di rete pi\u00f9 dettagliato rappresentato dalla diffusione \u2013 cio\u00e8 che ogni utente include la sua visione locale in la decisione, in base a ci\u00f2 che stanno facendo i suoi vicini di rete sociale. Un modello di diffusione che incorpori tali estensioni potrebbe fornire informazioni sulla struttura dei confini nella rete tra le tecnologie; potrebbe potenzialmente offrire una base teorica dei grafi su come l\u2019incompatibilit\u00e0 possa avvantaggiare una tecnologia esistente, rafforzando questi confini e prevenendo l\u2019incursione di una tecnologia nuova e migliore. Il presente lavoro: Diffusione con comportamento bilingue. In questo articolo sviluppiamo una serie di modelli di diffusione che incorporano nozioni di compatibilit\u00e0 e bilinguismo e scopriamo che alcuni fenomeni inaspettati emergono anche da versioni molto semplici dei modelli. Inizieremo con il modo forse pi\u00f9 semplice di estendere il modello di Morris discusso sopra per incorporare il comportamento bilingue. Consideriamo ancora l'esempio dei sistemi IM A e B, con la struttura dei payoff come prima,ma supponiamo ora che ciascun nodo possa adottare una terza strategia, denotata AB, in cui decide di utilizzare sia A che B. Infine, chi adotta AB paga una penalit\u00e0 di costo fisso pari a c -LRB- cio\u00e8 -- c viene aggiunto a il suo profitto totale -RRB- rappresenta il costo di dover mantenere entrambe le tecnologie. Pertanto, in questo modello, ci sono due parametri che possono essere variati: le qualit\u00e0 relative delle due tecnologie -LRB- codificate da q -RRB-, e il costo di essere bilingue, che riflette un tipo di incompatibilit\u00e0 -LRB- codificato da c -RRB-. Introduciamo anche un ulteriore bit di notazione che sar\u00e0 utile nelle sezioni successive: definiamo r = c / \u0394, la penalit\u00e0 fissa per l'adozione di AB, scalata in modo che sia un costo per arco. Nel modello di Morris, dove le uniche opzioni strategiche sono A e B, un parametro chiave \u00e8 la soglia di contagio di G, denominata q \u2217 -LRB- G -RRB-: questo \u00e8 l\u2019apice di q per il quale A pu\u00f2 diventare epidemico in G con parametro q nella struttura dei payoff. Un risultato centrale di -LSB- 13 -RSB- \u00e8 che 21 \u00e8 la massima soglia di contagio possibile per qualsiasi grafico: supG q \u2217 -LRB- G -RRB- = 21. In effetti, esistono grafici in cui la soglia di contagio \u00e8 altrettanto grande come 21 -LRB- inclusa la linea infinita -- l'unico grafo 2-regolare infinito connesso -RRB- ; d'altro canto si pu\u00f2 dimostrare che non esiste un grafico con una soglia di contagio maggiore di quella della Figura 1: La regione del piano -LRB- q, r -RRB- per la quale la tecnologia A pu\u00f2 diventare epidemica sulla linea infinita. I nostri risultati. -LRB- Troviamo forme analoghe che diventano ancora pi\u00f9 complesse per altre semplici strutture di grafi infiniti ; si vedano ad esempio le Figure 3 e 4. -RRB- In particolare, ci\u00f2 significa che per valori di q prossimi ma inferiori a 21, la strategia A pu\u00f2 diventare epidemica sulla linea infinita se r \u00e8 sufficientemente piccolo o sufficientemente grande, ma non se r assume valori in qualche intervallo intermedio. In altre parole, la strategia B -LRB- che rappresenta la tecnologia peggiore, poich\u00e9 q < 21 -RRB- sopravviver\u00e0 se e solo se il costo dell'essere bilingue \u00e8 calibrato per rientrare in questo intervallo medio. Ci\u00f2 riflette una compatibilit\u00e0 limitata \u2013 ovvero che potrebbe essere nell\u2019interesse di una tecnologia dominante rendere difficile ma non troppo difficile l\u2019uso di una nuova tecnologia \u2013 e troviamo sorprendente che emerga da un modello di base su tale base. una semplice struttura di rete. Viene naturale chiedersi se esista un'interpretazione qualitativa di come ci\u00f2 emerga dal modello, ed in effetti non \u00e8 difficile dare un'interpretazione del genere, come segue. Quando r \u00e8 molto piccolo, \u00e8 economico per i nodi adottare AB come strategia, e cos\u00ec AB si diffonde attraverso l'intera rete. Una volta che AB \u00e8 ovunque, gli aggiornamenti con la risposta migliore fanno s\u00ec che tutti i nodi passino ad A, poich\u00e9 ottengono gli stessi vantaggi di interazione senza pagare la penalit\u00e0 di r. Quando r \u00e8 molto grande, i nodi all'interfaccia, con un vicino A e uno B, troveranno troppo costoso scegliere AB, quindi sceglieranno A -LRB- la tecnologia migliore -RRB-,e quindi A si diffonder\u00e0 passo dopo passo attraverso la rete. Quando r assume un valore intermedio, un nodo v all'interfaccia, con un vicino A e un vicino B, trover\u00e0 pi\u00f9 vantaggioso adottare AB come strategia. Quindi, questo valore intermedio di r consente la formazione di un \"confine\" di AB tra gli adottanti di A e gli adottanti di B. Ma se ha il giusto equilibrio nel valore di r, allora le adozioni di A arrivano a un fermarsi a un confine bilingue dove i nodi adottano AB. Andando oltre i grafici specifici G, troviamo che questa non convessit\u00e0 vale anche in un senso molto pi\u00f9 generale, considerando la regione epidemica generale \u03a9 = UG\u03a9 -LRB- G -RRB-. Per ogni dato valore di \u0394, la regione \u03a9 \u00e8 un'unione complicata di poligoni limitati e illimitati, e non abbiamo una semplice descrizione in forma chiusa per essa. Tuttavia, possiamo mostrare tramite un argomento di funzione potenziale che nessun punto -LRB- q, r -RRB- con q > 21 appartiene a \u03a9. Inoltre, possiamo dimostrare l'esistenza di un punto -LRB- q, r -RRB- E ~ \u03a9 per il quale q < 21. D'altra parte, considerando la regione epidemica per la linea infinita mostra che -LRB- 21, r -RRB- E \u03a9 per r = 0 e per r sufficientemente grande. Quindi n\u00e9 \u03a9 n\u00e9 il suo complemento sono convessi nel quadrante positivo. Infine, estendiamo anche una caratterizzazione fornita da Morris per la soglia di contagio -LSB- 13 -RSB-, producendo una caratterizzazione un po' pi\u00f9 complessa della regione \u03a9 -LRB- G -RRB-. Nel context di Morris, senza una strategia AB, ha dimostrato che A non pu\u00f2 diventare epidemico con il parametro q se e solo se ogni insieme cofinito di nodi contiene un sottoinsieme S che funziona come una \"comunit\u00e0\" ben connessa: ogni nodo in S ha almeno una frazione -LRB- 1 -- q -RRB- dei suoi vicini in S. In altre parole, le comunit\u00e0 molto unite sono gli ostacoli naturali alla diffusione nel suo ambiente. Con la strategia AB come ulteriore opzione, una struttura pi\u00f9 complessa diventa l'ostacolo: mostriamo che A non pu\u00f2 diventare epidemico con parametri -LRB- q, r -RRB- se e solo se ogni insieme cofinito contiene una struttura costituita da un insieme strettamente comunit\u00e0 -knit con un particolare tipo di \"interfaccia\" di nodi vicini. Mostriamo che tale struttura consente ai nodi di adottare AB all'interfaccia e B all'interno della comunit\u00e0 stessa, impedendo l'ulteriore diffusione di A ; e viceversa, questo \u00e8 l\u2019unico modo per bloccare la diffusione di A. Ulteriori estensioni. Un altro modo per modellare la compatibilit\u00e0 e l'interoperabilit\u00e0 nei modelli di diffusione \u00e8 attraverso i termini \"fuori diagonale\" che rappresentano il profitto per le interazioni tra un nodo che adotta A e un nodo che adotta B. Invece di impostarli su 0, possiamo considerare di impostarli su un valore x < min -LRB- q, 1 -- q -RRB-. Troviamo che nel caso di due tecnologie, il modello non diventa pi\u00f9 generale, in quanto qualsiasi esempio di questo tipo \u00e8 equivalente, mediante un riscalamento di q e r, a uno in cui x = 0. Inoltre,utilizzando la nostra caratterizzazione della regione \u03a9 -LRB- G -RRB- in termini di comunit\u00e0 e interfacce, mostriamo un risultato di monotonicit\u00e0: se A pu\u00f2 diventare epidemico su un grafo G con parametri -LRB- q, r, x -RRB-, e poi x viene aumentato, allora A pu\u00f2 ancora diventare epidemico con i nuovi parametri. Consideriamo anche l'effetto di questi termini fuori diagonale in un'estensione a k > 2 tecnologie concorrenti; per le tecnologie X e Y, sia qX il profitto derivante da un'interazione XX su un bordo e qXY il profitto derivante da un'interazione XY su un bordo. Consideriamo un context in cui due tecnologie B e C, che inizialmente coesistono con qBC = 0, affrontano l'introduzione di una terza, migliore tecnologia A in un insieme finito di nodi. Mostriamo un esempio in cui B e C sopravvivono entrambi in equilibrio se impostano qBC in un particolare intervallo di valori, ma non se impostano qBC troppo basso o troppo alto per rientrare in questo intervallo. Pertanto, anche in un modello di diffusione di base con tre tecnologie, si trovano casi in cui due imprese hanno un incentivo ad adottare una \u201calleanza strategica\u201d limitata, aumentando parzialmente la loro interoperabilit\u00e0 per difendersi da un nuovo concorrente nel mercato. 6. COMPATIBILIT\u00c0 LIMITATA Consideriamo ora alcuni ulteriori modi per modellare la compatibilit\u00e0 e l'interoperabilit\u00e0. Consideriamo innanzitutto due tecnologie, come nelle sezioni precedenti, e introduciamo i payoff \"fuori diagonale\" per acquisire un vantaggio positivo nelle interazioni AB dirette. Troviamo che questo in realt\u00e0 non \u00e8 pi\u00f9 generale del modello con profitti pari a zero per le interazioni AB. Considereremo quindi le estensioni a tre tecnologie, identificando le situazioni in cui due tecnologie dominanti coesistenti possono o meno voler aumentare la loro reciproca compatibilit\u00e0 a fronte di una nuova, terza tecnologia. Due tecnologie. Un rilassamento naturale del modello a due tecnologie consiste nell'introdurre profitti positivi -LRB- piccoli -RRB- per l'interazione AB; cio\u00e8, la comunicazione intertecnologica produce un valore minore per entrambi gli agenti. Possiamo modellare questo utilizzando una variabile xAB che rappresenta il profitto ottenuto da un agente con la tecnologia A quando il suo vicino ha la tecnologia B, e analogamente, una variabile xBA che rappresenta il profitto ottenuto da un agente con B quando il suo vicino ha A. Qui consideriamo la caso speciale in cui questi elementi ``fuori diagonale'' sono simmetrici, cio\u00e8 xAB = xBA = x. Assumiamo anche che x < q < 1 -- q. Mostriamo innanzitutto che il gioco con voci fuori diagonale \u00e8 equivalente a un gioco senza queste voci, con un semplice riscalamento di q e r. Si noti che se ridimensioniamo tutti i guadagni mediante una costante additiva o moltiplicativa, il comportamento del gioco non viene influenzato. Dato un gioco con voci fuori diagonale parametrizzate da q, r e x, considera di sottrarre x da tutti i profitti e di aumentare di un fattore pari a 1 / -LRB- 1 -- 2x -RRB-. Come si pu\u00f2 vedere esaminando la Tabella 1, i guadagni risultanti sono esattamente quelli di un gioco senza voci fuori diagonale,parametrizzato da q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- e r ' = r / -LRB- 1 -- 2x -RRB-. Pertanto l'aggiunta di voci simmetriche fuori diagonale non espande la classe di giochi considerati. La tabella 1 rappresenta i profitti nel gioco di coordinamento in termini di questi parametri. Tuttavia, possiamo ancora chiederci in che modo l\u2019aggiunta di un ingresso fuori diagonale potrebbe influenzare l\u2019esito di un particolare gioco. Come mostra l\u2019esempio seguente, una maggiore compatibilit\u00e0 tra due tecnologie pu\u00f2 consentire a una tecnologia che inizialmente non era epidemica di diventarlo. ESEMPIO 6.1. Consideriamo il gioco del contagio giocato su un grafico a linee spesse -LRB- vedere Sezione 3 -RRB- con r = 5/32 e q = 3/8. In questo caso A non \u00e8 epidemico, come si pu\u00f2 vedere esaminando la Figura 1, poich\u00e9 2r < q e q + r > 1/2. Tuttavia, se inseriamo pagamenti simmetrici fuori diagonale x = 1/4, abbiamo un nuovo gioco, equivalente a un gioco parametrizzato da r ' = 5/16 e q ' = 1/4. Poich\u00e9 q ' < 1/2 e q ' < 2r ', A \u00e8 epidemico in questo gioco, e quindi anche nel gioco con compatibilit\u00e0 limitata. Mostriamo ora che in generale, se A \u00e8 la tecnologia superiore -LRB- cio\u00e8 q < 1/2 -RRB-, l'aggiunta di un termine di compatibilit\u00e0 x pu\u00f2 solo aiutare A a diffondersi. TEOREMA 6.2. Sia G un gioco senza compatibilit\u00e0, parametrizzato da r e q su una particolare rete. Sia G ' lo stesso gioco, ma con l'aggiunta di un termine di compatibilit\u00e0 simmetrica x. Se A \u00e8 epidemico per G, allora A \u00e8 epidemico per G\u2019. PROVA. Mostreremo che qualsiasi struttura bloccante in G ' \u00e8 anche una struttura bloccante in G. Per il nostro teorema di caratterizzazione, Teorema 4.6, ci\u00f2 implica il risultato desiderato. Abbiamo che G ' equivale a un gioco senza compatibilit\u00e0 parametrizzato da q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- e r ' = r / -LRB- 1 -- 2x -RRB-. Consideriamo una struttura bloccante -LRB- SB, SAB -RRB- per G '. Quindi pi\u00f9 di due tecnologie. Data la struttura complessa inerente ai giochi di contagio con due tecnologie, la comprensione dei giochi di contagio con tre o pi\u00f9 tecnologie \u00e8 ampiamente aperta. Qui indichiamo alcune delle problematiche tecniche che si presentano con molteplici tecnologie, attraverso una serie di primi risultati. L'assetto di base che studiamo \u00e8 quello in cui due tecnologie dominanti B e C coesistono inizialmente, e una terza tecnologia A, superiore ad entrambe, viene inizialmente introdotta in un insieme finito di nodi. Presentiamo innanzitutto un teorema che afferma che per ogni \u0394 pari, esiste un gioco di contagio su un grafo regolare \u0394 in cui le due tecnologie dominanti B e C possono trovare utile aumentare la loro compatibilit\u00e0 in modo da evitare di essere spazzate via dal nuova tecnologia superiore A. In particolare, consideriamo una situazione in cui inizialmente due tecnologie B e C con compatibilit\u00e0 zero si trovano in uno stato stabile. Per stato stabile intendiamo che nessuna perturbazione finita degli stati attuali pu\u00f2 portare a un\u2019epidemia sia per B che per C. Abbiamo anche una tecnologia A che \u00e8 superiore sia a B che a C,e possono diventare epidemici costringendo un singolo nodo a scegliere A. Tuttavia, aumentando la loro compatibilit\u00e0, B e C possono mantenere la loro stabilit\u00e0 e resistere a un\u2019epidemia da A. Sia qA denotare i profitti di due nodi adiacenti che scelgono entrambi la tecnologia A, e definire qB e qC in modo analogo. Assumeremo qA > qB > qC. Assumiamo inoltre che r, il costo della selezione di tecnologie aggiuntive, sia sufficientemente elevato da garantire che i nodi non adottino mai pi\u00f9 di una tecnologia. Infine, consideriamo un parametro di compatibilit\u00e0 qBC che rappresenta i vantaggi per due nodi adiacenti quando uno seleziona B e l'altro seleziona C. Pertanto il nostro gioco di contagio \u00e8 ora descritto da cinque parametri -LRB- G, qA, qB, qC, qBC -RRB -. PROVA. -LRB- Schizzo. -RRB- Dato \u0394, definisci G iniziando con una griglia infinita e collegando ciascun nodo al suo \u0394 pi\u00f9 vicino - 2 vicini che si trovano nella stessa riga. Lo stato iniziale s assegna la strategia B alle righe pari e la strategia C alle righe dispari. La prima, la terza e la quarta affermazione del teorema possono essere verificate controllando le corrispondenti disuguaglianze. La seconda affermazione deriva dalla prima e dall'osservazione che le file alternate contengono qualsiasi epidemia plausibile che cresca verticalmente. Il teorema precedente mostra che due tecnologie possono entrambe essere in grado di sopravvivere all\u2019introduzione di una nuova tecnologia aumentando il loro livello di compatibilit\u00e0 reciproca. Come ci si potrebbe aspettare, Tabella 1: I guadagni nel gioco di coordinazione. La voce -LRB- x, y -RRB- nella riga i, colonna j indica che il giocatore della riga ottiene un profitto di x e il giocatore della colonna ottiene un profitto di y quando il giocatore della riga gioca la strategia i e il giocatore della colonna gioca la strategia j. ci sono casi in cui una maggiore compatibilit\u00e0 tra due tecnologie aiuta una tecnologia a scapito dell'altra. Sorprendentemente, per\u00f2, ci sono anche casi in cui la compatibilit\u00e0 \u00e8 di fatto dannosa per entrambe le parti; l'esempio successivo considera una configurazione iniziale fissa con le tecnologie A, B e C che \u00e8 all'equilibrio quando qBC = 0. Tuttavia, se questo termine di compatibilit\u00e0 viene aumentato sufficientemente, l'equilibrio viene perso e A diventa epidemica. ESEMPIO 6.4. Consideriamo l'unione di un grafico a griglia bidimensionale infinito con nodi u -LRB- x, y -RRB- e un grafico a linee infinite con nodi v -LRB- y -RRB-. Aggiungi un bordo tra u -LRB- 1, y -RRB- e v -LRB- y -RRB- per ogni y. Per questa rete, consideriamo la configurazione iniziale in cui tutti i nodi v -LRB- y -RRB- selezionano A, e il nodo u -LRB- x, y -RRB- seleziona B se x < 0 e seleziona C altrimenti. Definiamo ora i parametri di questo gioco come segue. Si verifica facilmente che per questi valori la configurazione iniziale sopra riportata \u00e8 un equilibrio. Supponiamo ora per\u00f2 di aumentare il termine di coordinazione, ponendo qBC = 0,9. Questo non \u00e8 un equilibrio, poich\u00e9 ogni nodo della forma u -LRB- 0, y -RRB- ha ora un incentivo a passare da C -LRB- generando un payoff di 3,9 -RRB- a B -LRB- generando cos\u00ec un payoff di 3,95 -RRB-. Tuttavia,una volta che questi nodi hanno adottato B, la risposta migliore per ciascun nodo della forma u -LRB- 1, y -RRB- \u00e8 A -LRB- A genera un profitto di 4 mentre B genera solo un profitto di 3,95 -RRB- . Da qui non \u00e8 difficile dimostrare che A si diffonde direttamente attraverso l'intera rete.", "keyphrases": ["processo diffuso", "modello diffuso della teoria dei giochi", "strategia incompatibile", "bilingue", "limite compat", "interoperabile", "propriet\u00e0 non convesse", "carattere", "teorema di morri", "soglia di contagio", "gioco del contagio", "funzione potente"]}
{"file_name": "I-34", "text": "Risolvere conflitti e incoerenze nelle organizzazioni virtuali regolate da norme ABSTRACT Le organizzazioni virtuali governate da norme definiscono, governano e facilitano la condivisione coordinata delle risorse e la risoluzione dei problemi nelle societ\u00e0 di agenti. Con un resoconto esplicito delle norme, \u00e8 possibile raggiungere l\u2019apertura nelle organizzazioni virtuali: nuovi componenti, progettati da varie parti, possono essere integrati senza soluzione di continuit\u00e0. Ci concentriamo su organizzazioni virtuali realizzate come sistemi multi-agente, in cui agenti umani e software interagiscono per raggiungere obiettivi individuali e globali. Tuttavia, qualsiasi spiegazione realistica delle norme dovrebbe affrontare la loro natura dinamica: le norme cambieranno man mano che gli agenti interagiscono tra loro e con il loro ambiente. A causa della natura mutevole delle norme o a causa delle norme derivanti da diverse organizzazioni virtuali, ci saranno situazioni in cui un'azione \u00e8 contemporaneamente consentita e vietata, cio\u00e8 sorge un conflitto. Allo stesso modo, ci saranno situazioni in cui un'azione \u00e8 sia obbligata che vietata, ovvero si verifica un'incoerenza. Introduciamo un approccio, basato sull'unificazione del primo ordine, per individuare e risolvere tali conflitti e incoerenze. Nella soluzione proposta, annotiamo una norma con l'insieme di valori che le sue variabili non dovrebbero avere per evitare un conflitto o un'incoerenza con un'altra norma. Il nostro approccio si adatta perfettamente alle interrelazioni dipendenti dal dominio tra le azioni e ai conflitti/incoerenze indiretti che queste possono causare. Pi\u00f9 in generale, possiamo acquisire un'utile nozione di delega inter-agente -LRB- e inter-ruolo -RRB- di azioni e norme ad esse associate, e usarla per affrontare conflitti/incoerenze causati dalla delega dell'azione. Illustriamo il nostro approccio con un esempio di e-Science in cui gli agenti supportano i servizi Grid. 1. INTRODUZIONE Le organizzazioni virtuali -LRB- VO -RRB- facilitano la condivisione coordinata delle risorse e la risoluzione dei problemi coinvolgendo varie parti geograficamente remote -LSB- 9 -RSB-. Le VO definiscono e regolano le interazioni -LRB- facilitando cos\u00ec il coordinamento -RRB- tra software e/o agenti umani che comunicano per raggiungere obiettivi individuali e globali -LSB- 16 -RSB-. Le VO sono realizzate come sistemi multi-agente e una caratteristica pi\u00f9 desiderabile di tali sistemi \u00e8 l'apertura grazie alla quale i nuovi componenti progettati da altre parti vengono integrati senza soluzione di continuit\u00e0. Le norme regolano il comportamento osservabile di agenti software egoistici ed eterogenei, progettati da varie parti che potrebbero non fidarsi completamente l'una dell'altra -LSB- 3, 24 -RSB-. Tuttavia, le VO regolamentate da norme possono incontrare problemi quando le norme assegnate ai loro agenti sono in conflitto -LRB- cio\u00e8, un'azione \u00e8 contemporaneamente proibita e consentita -RRB- o incoerenti -LRB- cio\u00e8, un'azione \u00e8 contemporaneamente vietata e obbligata -RRB- . Proponiamo un mezzo per rilevare e risolvere automaticamente conflitti e incoerenze nelle VO regolamentate dalle norme. Usiamo l'unificazione dei termini del primo ordine -LSB- 8 -RSB- per scoprire se e come le norme si sovrappongono nella loro influenza -LRB- cio\u00e8,gli agenti e i valori dei parametri nelle azioni degli agenti che le norme possono influenzare -RRB-. Ci\u00f2 consente una soluzione a grana fine in cui l\u2019influenza di norme contrastanti o incoerenti viene ridotta per particolari insiemi di valori. Ad esempio, le norme `` all'agente x \u00e8 consentito inviare un'offerta -LRB- ag1, 20 -RRB- '' e `` all'agente ag2 \u00e8 vietato inviare un'offerta -LRB- y, z -RRB- '' -LRB- dove x, y, z sono variabili e ag1, ag2, 20 sono costanti -RRB- sono in conflitto perch\u00e9 i loro agenti, azioni e termini -LRB- all'interno delle azioni -RRB- si unificano. Risolviamo il conflitto annotando le norme con insiemi di valori che le loro variabili non possono avere, limitando cos\u00ec la loro influenza. Nel nostro esempio, il conflitto viene evitato se richiediamo che la variabile y non possa essere ag1 e che z non possa essere 20. Nella sezione successiva forniremo una definizione minimalista per le VO regolate dalle norme. Nella sezione 3 definiamo formalmente i conflitti tra norme e spieghiamo come vengono rilevati e risolti. Nella sezione 4 descriviamo come i meccanismi della sezione precedente possono essere adattati per rilevare e risolvere le incongruenze normative. Nella sezione 5 descriviamo come le nostre norme ridotte vengono utilizzate nelle societ\u00e0 di agenti consapevoli delle norme. Nella sezione 6 spieghiamo come i nostri macchinari possono essere utilizzati per rilevare e risolvere conflitti/incoerenze indiretti, cio\u00e8 quelli causati attraverso le relazioni tra le azioni; estendiamo e adattiamo i macchinari per accogliere la delega delle norme. Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.", "keyphrases": ["organo virtuale", "sistema multiagente", "norma-regola vo", "agente", "conflitto di norme", "il conflitto vieta", "la norma \u00e8 inconsistente", "agente esterno", "agente del governatore"]}
{"file_name": "J-31", "text": "Calcolare la strategia ottimale a cui impegnarsi \u2217 ABSTRACT Nei sistemi multiagente, le impostazioni strategiche vengono spesso analizzate partendo dal presupposto che i giocatori scelgano le loro strategie simultaneamente. Tuttavia, questo modello non \u00e8 sempre realistico. In molti contesti, un giocatore \u00e8 in grado di impegnarsi in una strategia prima che l'altro giocatore prenda una decisione. Tali modelli sono sinonimi di leadership, impegno o modelli Stackelberg e il gioco ottimale in tali modelli \u00e8 spesso significativamente diverso dal gioco ottimale nel modello in cui le strategie vengono selezionate simultaneamente. Il recente aumento di interesse per le soluzioni informatiche della teoria dei giochi ha finora ignorato i modelli di leadership -LRB- con l'eccezione dell'interesse per la progettazione di meccanismi, dove il progettista \u00e8 implicitamente in una posizione di leadership -RRB-. In questo articolo, studiamo come calcolare le strategie ottimali per impegnarsi sia nell'impegno per le strategie pure che nell'impegno per le strategie miste, sia nei giochi in forma normale che in quelli bayesiani. Forniamo sia risultati positivi -LRB- algoritmi efficienti -RRB- sia risultati negativi -LRB- risultati di durezza NP -RRB-. 1. INTRODUZIONE Nei sistemi multiagente con agenti autointeressati -LRB- inclusa la maggior parte dei contesti economici -RRB-, l'azione ottimale che un agente intraprende dipende dalle azioni intraprese dagli altri agenti. Per analizzare come dovrebbe comportarsi un agente in tali contesti, \u00e8 necessario applicare gli strumenti della teoria dei giochi. Tipicamente, quando un\u2019impostazione strategica \u00e8 modellata nel quadro della teoria dei giochi, si presuppone che i giocatori scelgano le loro strategie simultaneamente. Ci\u00f2 \u00e8 particolarmente vero quando l'ambientazione \u00e8 modellata come un gioco in forma normale, che specifica solo l'utilit\u00e0 di ciascun agente in funzione del vettore di strategie che gli agenti scelgono, e non fornisce alcuna informazione sull'ordine in cui gli agenti effettuano le loro decisioni e ci\u00f2 che gli agenti osservano riguardo alle decisioni precedenti di altri agenti. Dato che il gioco \u00e8 modellato in forma normale, viene tipicamente analizzato utilizzando il concetto di equilibrio di Nash. Un equilibrio di Nash specifica una strategia per ciascun giocatore, in modo tale che nessun giocatore abbia un incentivo a deviare individualmente da questo profilo di strategie. -LRB- In genere, le strategie possono essere miste, ovvero distribuzioni di probabilit\u00e0 sulle strategie originali -LRB- pure -RRB-. -RRB- Una -LRB- strategia mista -RRB- L'equilibrio di Nash \u00e8 garantito nei giochi finiti -LSB- 18 -RSB-, ma un problema \u00e8 che potrebbero esserci pi\u00f9 equilibri di Nash. Ci\u00f2 porta al problema della selezione dell\u2019equilibrio: come un agente pu\u00f2 sapere quale strategia giocare se non sa quale equilibrio deve essere giocato. Quando l'ambientazione \u00e8 modellata come un gioco in forma estesa, \u00e8 possibile specificare che alcuni giocatori ricevano alcune informazioni sulle azioni intraprese da altri nelle fasi precedenti del gioco prima di decidere la loro azione. Tuttavia, in generale, i giocatori non sanno tutto quello che \u00e8 successo all'inizio del gioco. A causa di ci\u00f2,questi giochi vengono tipicamente ancora analizzati utilizzando un concetto di equilibrio, in cui si specifica una strategia mista per ciascun giocatore e si richiede che la strategia di ciascun giocatore sia la migliore risposta alle strategie degli altri. -LRB- In genere viene ora imposto un vincolo aggiuntivo sulle strategie per garantire che i giocatori non giochino in un modo irrazionale rispetto alle informazioni che hanno ricevuto finora. Ci\u00f2 porta a perfezionamenti dell'equilibrio di Nash come il sottogioco perfetto e l'equilibrio sequenziale. -RRB- Tuttavia, in molti contesti del mondo reale, le strategie non vengono selezionate in modo cos\u00ec simultaneo. Spesso, un giocatore -LRB- il leader -RRB- \u00e8 in grado di impegnarsi in una strategia prima di un altro giocatore -LRB- il follower -RRB-. Ci\u00f2 pu\u00f2 essere dovuto a una serie di motivi. Ad esempio, uno dei giocatori potrebbe arrivare al sito in cui si svolger\u00e0 la partita prima di un altro agente -LRB-; ad esempio, in contesti economici, un giocatore potrebbe entrare prima in un mercato e impegnarsi in un modo di fare affari -RRB -. Tale potere di impegno ha un profondo impatto sul modo in cui il gioco dovrebbe essere giocato. Ad esempio, il leader potrebbe trovarsi meglio a giocare una strategia che \u00e8 dominata nella rappresentazione in forma normale del gioco. In generale, se \u00e8 possibile impegnarsi in strategie miste, allora -LRB- sotto presupposti minori -RRB- non fa mai male, e spesso aiuta, impegnarsi in una strategia -LSB- 26 -RSB-. Essere costretti a impegnarsi in una strategia pura a volte aiuta, a volte fa male -LRB-, ad esempio, impegnarsi in una strategia pura in sasso-carta-forbici prima che la decisione dell'altro giocatore si tradurr\u00e0 naturalmente in una perdita -RRB-. In questo articolo assumeremo che l'impegno sia sempre forzato; in caso contrario, il giocatore che ha la scelta se impegnarsi pu\u00f2 semplicemente confrontare il risultato dell'impegno con il risultato del non impegno -LRB- con movimento simultaneo -RRB-. I modelli di leadership sono particolarmente importanti in contesti con pi\u00f9 agenti software egoisti. Una volta finalizzato il codice per un agente -LRB- o per un team di agenti -RRB- e l'agente viene distribuito, l'agente si impegna a riprodurre la strategia -LRB- possibilmente randomizzata -RRB- prescritta dal codice. Infine, esiste anche una situazione di leadership implicita nel campo della progettazione dei meccanismi, in cui un giocatore -LRB- e il progettista -RRB- scelgono le regole del gioco a cui poi giocano gli altri giocatori. In effetti, il progettista del meccanismo pu\u00f2 trarre vantaggio dall\u2019impegnarsi in una scelta che, se le restanti azioni degli agenti -RRB- fossero fissate, sarebbe subottimale. Tuttavia, il calcolo della strategia ottimale da adottare in una situazione di leadership \u00e8 stato ignorato. Teoricamente, le situazioni di leadership possono essere semplicemente pensate come un gioco in forma estesa in cui un giocatore sceglie prima una strategia -LRB- per il gioco originale -RRB-. Il numero di strategie in questo gioco in forma estesa, tuttavia, pu\u00f2 essere estremamente elevato. Ad esempio, se il leader \u00e8 in grado di impegnarsi in una strategia mista nel gioco originale,quindi ciascuna delle strategie miste -LRB- del continuum -RRB- costituisce una strategia pura nella rappresentazione in forma estesa della situazione di leadership. -LRB- Notiamo che un impegno per una distribuzione non \u00e8 la stessa cosa di una distribuzione sugli impegni. -RRB- Inoltre, se il gioco originale \u00e8 esso stesso un gioco in forma estesa, il numero di strategie nella rappresentazione in forma estesa della situazione di leadership -LRB- che \u00e8 un diverso gioco in forma estesa -RRB- diventa ancora maggiore. Per questo motivo, di solito non \u00e8 computazionalmente fattibile trasformare semplicemente il gioco originale nella rappresentazione in forma estesa della situazione di leadership; dobbiamo invece analizzare il gioco nella sua rappresentazione originale. In questo articolo studiamo come calcolare la strategia ottimale da seguire, sia nei giochi in forma normale -LRB- Sezione 2 -RRB- che nei giochi bayesiani, che sono un caso speciale di giochi in forma estesa -LRB- Sezione 3 -RRB -. 4. CONCLUSIONI E RICERCHE FUTURE Nei sistemi multiagente, le impostazioni strategiche vengono spesso analizzate partendo dal presupposto che i giocatori scelgano le loro strategie simultaneamente. Ci\u00f2 richiede una certa nozione di equilibrio -LRB- Equilibrio di Nash e i suoi perfezionamenti -RRB-, e spesso porta al problema della selezione dell'equilibrio: non \u00e8 chiaro a ogni singolo giocatore in base a quale equilibrio dovrebbe giocare. Tuttavia, questo modello non \u00e8 sempre realistico. In molti contesti, un giocatore \u00e8 in grado di impegnarsi in una strategia prima che l'altro giocatore prenda una decisione. Ad esempio, un agente pu\u00f2 arrivare prima dell'altro al sito -LRB- reale o virtuale -RRB- del gioco, oppure, nel caso specifico degli agenti software, il codice di un agente pu\u00f2 essere completato e impegnato prima di quello di un altro agente. Tali modelli sono sinonimi di leadership, impegno o modelli Stackelberg e il gioco ottimale in tali modelli \u00e8 spesso significativamente diverso dal gioco ottimale nel modello in cui le strategie vengono selezionate simultaneamente. Nello specifico, se l\u2019impegno verso strategie miste \u00e8 possibile, allora l\u2019impegno -LRB- ottimale -RRB- non danneggia mai il leader, e spesso aiuta. Il recente aumento di interesse per le soluzioni informatiche della teoria dei giochi ha finora ignorato i modelli di leadership -LRB- con l'eccezione dell'interesse per la progettazione di meccanismi, dove il progettista \u00e8 implicitamente in una posizione di leadership -RRB-. In questo articolo, abbiamo studiato come calcolare le strategie ottimali per impegnarsi sia nell'impegno per le strategie pure che nell'impegno per le strategie miste, sia nei giochi in forma normale che in quelli bayesiani. Per i giochi in forma normale, abbiamo dimostrato che la strategia pura ottimale a cui impegnarsi pu\u00f2 essere trovata in modo efficiente per qualsiasi numero di giocatori. Una strategia mista ottimale a cui impegnarsi in un gioco in forma normale pu\u00f2 essere trovata in modo efficiente per due giocatori utilizzando la programmazione lineare -LRB- e non pi\u00f9 efficiente di cos\u00ec, nel senso che qualsiasi programma lineare con un vincolo di probabilit\u00e0 pu\u00f2 essere codificato come tale problema -RRB-.-LRB- Questa \u00e8 una generalizzazione della computabilit\u00e0 in tempo polinomiale delle strategie minimax nei giochi in forma normale. -RRB- Il problema diventa NP-difficile per tre giocatori -LRB- o pi\u00f9 -RRB-. Nei giochi bayesiani, il problema di trovare una strategia pura ottimale a cui impegnarsi \u00e8 NP-difficile anche nei giochi a due giocatori in cui il follower ha un solo tipo, sebbene i giochi a due giocatori in cui il leader abbia un solo tipo possano essere risolti in modo efficiente. Il problema di trovare una strategia mista ottimale a cui impegnarsi in un gioco bayesiano \u00e8 NP-difficile anche nei giochi a due giocatori in cui il leader ha un solo tipo, sebbene i giochi a due giocatori in cui il follower abbia un solo tipo possano essere risolti in modo efficiente utilizzando una generalizzazione dell'approccio di programmazione lineare per giochi in forma normale. Le due tabelle seguenti riassumono questi risultati. Risultati per l\u2019impegno verso strategie miste. -LRB- Con pi\u00f9 di 2 giocatori, il `` follower '' \u00e8 l'ultimo giocatore a impegnarsi, il `` leader '' \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.il \"leader\" \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo Come gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.il \"leader\" \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo Come gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.", "keyphrases": ["strategie ottimali", "sistema multiag", "modo simultaneo", "modello di Stackelberg", "modello di leadership", "pura strategia", "mescolare le strategie", "gioco in forma normale", "gioco bayesiano", "equilibrio di Nash", "np-difficile"]}
{"file_name": "H-13", "text": "L'influenza delle caratteristiche delle didascalie sui modelli di click-through nella ricerca web ABSTRACT I motori di ricerca web presentano elenchi di didascalie, comprendenti titolo, snippet e URL, per aiutare gli utenti a decidere quali risultati di ricerca visitare. Comprendere l'influenza delle caratteristiche di queste didascalie sul comportamento di ricerca sul Web pu\u00f2 aiutare a convalidare algoritmi e linee guida per la loro generazione migliorata. In questo documento sviluppiamo una metodologia per utilizzare i registri dei clic da un motore di ricerca commerciale per studiare il comportamento degli utenti quando interagiscono con le didascalie dei risultati di ricerca. I risultati del nostro studio suggeriscono che caratteristiche relativamente semplici delle didascalie, come la presenza di tutti i termini di ricerca, la leggibilit\u00e0 dello snippet e la lunghezza dell'URL mostrato nella didascalia, possono influenzare in modo significativo il comportamento di ricerca sul Web degli utenti. 1. INTRODUZIONE I principali motori di ricerca Web commerciali presentano tutti i loro risultati pi\u00f9 o meno allo stesso modo. Ogni risultato della ricerca \u00e8 descritto da una breve didascalia, comprendente l'URL della pagina Web associata, un titolo e un breve riepilogo -LRB- o `` snippet '' -RRB- che descrive il contenuto della pagina. Spesso lo snippet viene estratto dalla pagina Web stessa, ma pu\u00f2 anche essere preso da fonti esterne, come i riepiloghi generati dagli esseri umani presenti nelle directory Web. La Figura 1 mostra una tipica ricerca sul Web, con didascalie per i primi tre risultati. Mentre le tre didascalie condividono lo stesso Lo snippet della terza didascalia \u00e8 lungo quasi il doppio di quello della prima, mentre lo snippet manca completamente dalla seconda didascalia. Il titolo della terza didascalia contiene tutti i termini della query in ordine, mentre i titoli della prima e della seconda didascalia contengono solo due dei tre termini. Uno dei termini della query viene ripetuto nella prima didascalia. Tutti i termini della query vengono visualizzati nell'URL della terza didascalia, mentre nessuno appare nell'URL della prima didascalia. Sebbene queste differenze possano sembrare minori, potrebbero anche avere un impatto sostanziale sul comportamento degli utenti. Una delle motivazioni principali per fornire una didascalia \u00e8 aiutare l'utente a determinare la pertinenza della pagina associata senza dover effettivamente fare clic per raggiungere il risultato. Nel caso di una query di navigazione, in particolare quando la destinazione \u00e8 ben nota, il solo URL pu\u00f2 essere sufficiente per identificare la pagina desiderata. Ma nel caso di una richiesta informativa, il titolo e lo snippet potrebbero essere necessari per guidare l'utente nella selezione di una pagina per ulteriori approfondimenti, e l'utente potrebbe giudicare la pertinenza di una pagina solo sulla base della didascalia. Quando questo giudizio \u00e8 corretto, pu\u00f2 accelerare il processo di ricerca consentendo all'utente di evitare materiale indesiderato. Quando fallisce, l'utente pu\u00f2 perdere tempo facendo clic su un risultato inappropriato e scansionando una pagina che contiene poco o nulla di interessante. Ancora peggio, l'utente potrebbe essere indotto in errore a saltare una pagina che contiene le informazioni desiderate. Tutti e tre i risultati nella Figura 1 sono rilevanti, con alcune limitazioni. Il primo risultato si collega ai principali Yahoo Kids! home page,ma poi \u00e8 necessario seguire un collegamento in un menu per trovare la pagina principale dei giochi. Nonostante le apparenze, il secondo risultato si collega a una collezione sorprendentemente ampia di giochi online, principalmente con temi ambientali. Sfortunatamente, queste caratteristiche della pagina non si riflettono interamente nelle didascalie. In questo articolo, esaminiamo l'influenza delle funzionalit\u00e0 dei sottotitoli sul comportamento di ricerca sul Web dell'utente, utilizzando i clic estratti dai registri dei motori di ricerca come principale strumento di indagine. Figura 1: Primi tre risultati per la query: giochi online per bambini. Comprendere questa influenza pu\u00f2 aiutare a convalidare algoritmi e linee guida per una migliore generazione dei risultati. didascalie stesse. Inoltre, queste caratteristiche possono svolgere un ruolo nel processo di deduzione di giudizi di rilevanza dal comportamento dell'utente -LSB- 1 -RSB-. Comprendendo meglio la loro influenza, si possono ottenere giudizi migliori. Diversi algoritmi di generazione dei sottotitoli potrebbero selezionare snippet di diversa lunghezza da diverse aree di una pagina. Gli snippet possono essere generati in modo indipendente dalla query, fornendo un riepilogo della pagina nel suo insieme, o in modo dipendente dalla query, fornendo un riepilogo di come la pagina si collega ai termini della query. La scelta corretta dello snippet pu\u00f2 dipendere da aspetti sia della query che della pagina dei risultati. Per i collegamenti che reindirizzano, potrebbe essere possibile visualizzare URL alternativi. Inoltre, per le pagine elencate nelle directory Web modificate dall'uomo come l'Open Directory Project, potrebbe essere possibile visualizzare titoli e frammenti alternativi derivati \u200b\u200bda questi elenchi. Quando questi frammenti, titoli e URL alternativi sono disponibili, la selezione di una combinazione appropriata per la visualizzazione pu\u00f2 essere guidata dalle loro caratteristiche. Uno snippet da una directory Web pu\u00f2 consistere di frasi complete ed essere meno frammentario di uno snippet estratto. Un titolo estratto dal corpo pu\u00f2 fornire una maggiore copertura dei termini della query. Il lavoro riportato in questo articolo \u00e8 stato intrapreso nel context del motore di ricerca Windows Live. Gli esperimenti riportati nelle sezioni successive si basano sui registri delle query di Windows Live, sulle pagine dei risultati e sui giudizi di pertinenza raccolti come parte della ricerca in corso sulle prestazioni dei motori di ricerca -LSB- 1, 2 -RSB-. Tuttavia, data la somiglianza dei formati dei sottotitoli tra i principali motori di ricerca Web, riteniamo che i risultati siano applicabili a questi altri motori. La query in ` www.dmoz.org figura 1 produce risultati con rilevanza simile sugli altri principali motori di ricerca. Questa e altre query producono didascalie che presentano variazioni simili. Inoltre, riteniamo che la nostra metodologia possa essere generalizzata ad altre applicazioni di ricerca quando saranno disponibili dati sufficienti sui clic. 2. LAVORO CORRELATO Sebbene i motori di ricerca Web commerciali abbiano seguito approcci simili alla visualizzazione dei sottotitoli sin dalla loro genesi, \u00e8 stata pubblicata relativamente poca ricerca sui metodi per generare questi sottotitoli e valutare il loro impatto sul comportamento dell'utente.La maggior parte delle ricerche sulla visualizzazione dei risultati Web hanno proposto modifiche sostanziali all'interfaccia, piuttosto che affrontare i dettagli delle interfacce esistenti. 2.1 Visualizzazione dei risultati web Varadarajan e Hristidis -LSB- 16 -RSB- sono tra i pochi che hanno tentato di migliorare direttamente gli snippet generati dai sistemi di ricerca commerciali, senza introdurre ulteriori modifiche all'interfaccia. Hanno generato frammenti da alberi di grafici di documenti e hanno confrontato sperimentalmente questi frammenti con quelli generati per gli stessi documenti dal sistema di ricerca desktop di Google e dal sistema di ricerca desktop MSN. Hanno valutato il loro metodo chiedendo agli utenti di confrontare frammenti provenienti da varie fonti. 6. CONCLUSIONI Le inversioni click-through costituiscono uno strumento appropriato per valutare l'influenza delle caratteristiche delle didascalie. Utilizzando le inversioni di click-through, abbiamo dimostrato che funzionalit\u00e0 di didascalie relativamente semplici possono influenzare in modo significativo il comportamento degli utenti. A nostra conoscenza, questa \u00e8 la prima metodologia validata per valutare la qualit\u00e0 dei sottotitoli Web attraverso feedback implicito. Speriamo anche di raggiungere direttamente l'obiettivo di prevedere la pertinenza dai clic e da altre informazioni presenti nei log dei motori di ricerca.", "keyphrases": ["modello di clic", "funzione didascalia", "comportamento di ricerca sul web", "fattore umano", "estrarre il riassunto", "frammento", "registro delle query", "queri riformulazione", "parola significativa", "clic inverso", "corrispondenza dei termini della query"]}
{"file_name": "I-5", "text": "Verso un'allocazione auto-organizzata delle risorse basata su agenti in un ambiente multi-server ABSTRACT Le applicazioni distribuite richiedono tecniche distribuite per un'allocazione efficiente delle risorse. Queste tecniche devono tenere conto dell\u2019eterogeneit\u00e0 e della potenziale inaffidabilit\u00e0 delle risorse e dei consumatori di risorse in ambienti distribuiti. In questo articolo proponiamo un algoritmo distribuito che risolve il problema dell'allocazione delle risorse nei sistemi multiagente distribuiti. La nostra soluzione si basa sull'auto-organizzazione degli agenti, che non richiede alcun facilitatore o livello di gestione. L\u2019allocazione delle risorse nel sistema \u00e8 un effetto puramente emergente. Presentiamo i risultati del meccanismo di allocazione delle risorse proposto nell'ambiente multi-server statico e dinamico simulato. 1. INTRODUZIONE In questo senso ogni agente \u00e8 un consumatore di risorse che acquisisce una certa quantit\u00e0 di risorse per l'esecuzione dei suoi compiti. \u00c8 difficile per un meccanismo centrale di allocazione delle risorse raccogliere e gestire le informazioni su tutte le risorse condivise e sui consumatori di risorse per eseguire efficacemente l\u2019allocazione delle risorse. Pertanto, sono necessarie soluzioni distribuite del problema dell\u2019allocazione delle risorse. I ricercatori hanno riconosciuto questi requisiti -LSB- 10 -RSB- e hanno proposto tecniche per l'allocazione distribuita delle risorse. Un tipo promettente di tali approcci distribuiti si basa su modelli di mercato economico -LSB-4 -RSB-, ispirati ai principi dei mercati azionari reali. Anche se questi approcci sono distribuiti, di solito richiedono un facilitatore per la determinazione dei prezzi, la scoperta delle risorse e l\u2019invio dei lavori alle risorse -LSB- 5, 9 -RSB-. Un altro problema principalmente irrisolto di questi approcci \u00e8 la regolazione fine di prezzo e tempo, vincoli di budget per consentire un'allocazione efficiente delle risorse in sistemi grandi e dinamici -LSB- 22 -RSB-. In questo articolo proponiamo una soluzione distribuita del problema dell'allocazione delle risorse basata sull'auto-organizzazione dei consumatori di risorse in un sistema con risorse limitate. Nel nostro approccio, gli agenti assegnano dinamicamente le attivit\u00e0 ai server che forniscono una quantit\u00e0 limitata di risorse. Nel nostro approccio, gli agenti selezionano autonomamente la piattaforma di esecuzione per l'attivit\u00e0 anzich\u00e9 chiedere a un broker di risorse di eseguire l'allocazione. Tutto il controllo necessario per il nostro algoritmo \u00e8 distribuito tra gli agenti nel sistema. Ottimizzano il processo di allocazione delle risorse continuamente nel corso della loro vita in base ai cambiamenti nella disponibilit\u00e0 delle risorse condivise imparando dalle decisioni di allocazione passate. Le uniche informazioni disponibili a tutti gli agenti sono il carico delle risorse e le informazioni sull'esito positivo dell'allocazione derivanti dalle allocazioni di risorse precedenti. Ulteriori informazioni sul carico delle risorse sui server non vengono diffuse. Il meccanismo proposto non richiede un'autorit\u00e0 di controllo centrale, un livello di gestione delle risorse o l'introduzione di comunicazioni aggiuntive tra gli agenti per decidere quale attivit\u00e0 \u00e8 assegnata su quale server.Dimostriamo che questo meccanismo esegue bene sistemi dinamici con un gran numero di compiti e pu\u00f2 essere facilmente adattato a varie dimensioni di sistema. Inoltre, le prestazioni complessive del sistema non vengono influenzate nel caso in cui gli agenti o i server si guastino o diventino non disponibili. L'approccio proposto fornisce un modo semplice per implementare l'allocazione distribuita delle risorse e tiene conto delle tendenze del sistema multi-agente verso l'autonomia, l'eterogeneit\u00e0 e l'inaffidabilit\u00e0 delle risorse e degli agenti. Questa tecnica proposta pu\u00f2 essere facilmente integrata da tecniche per accodare o rifiutare le richieste di allocazione delle risorse degli agenti -LSB- 11 -RSB-. Tali capacit\u00e0 di autogestione degli agenti software consentono un'allocazione affidabile delle risorse anche in un ambiente con fornitori di risorse inaffidabili. Ci\u00f2 pu\u00f2 essere ottenuto mediante le reciproche interazioni tra agenti applicando tecniche tratte dalla teoria dei sistemi complessi. L'autorganizzazione di tutti gli agenti porta ad un'autorganizzazione dei 2. LAVORI CORRELATI L'allocazione delle risorse \u00e8 un problema importante nel campo dell'informatica. In generale, l'allocazione delle risorse \u00e8 un meccanismo o una politica per la gestione efficiente ed efficace dell'accesso a una risorsa o a un insieme di risorse limitate da parte dei suoi consumatori. Nel caso pi\u00f9 semplice, i consumatori di risorse chiedono a un intermediario centrale o a un dispatcher le risorse disponibili a cui verranno allocate il consumatore di risorse. Il broker di solito ha piena conoscenza di tutte le risorse di sistema. In questi approcci, il consumatore di risorse non pu\u00f2 influenzare il processo decisionale di allocazione. Il bilanciamento del carico -LSB- 3 -RSB- \u00e8 un caso speciale del problema di allocazione delle risorse che utilizza un broker che cerca di essere equo con tutte le risorse bilanciando equamente il carico del sistema tra tutti i fornitori di risorse. Questo meccanismo funziona meglio in un sistema omogeneo. Una semplice tecnica distribuita per la gestione delle risorse \u00e8 la pianificazione della capacit\u00e0 rifiutando o accodando gli agenti in entrata per evitare il sovraccarico delle risorse -LSB- 11 -RSB-. Dal punto di vista del proprietario della risorsa, questa tecnica \u00e8 importante per prevenire il sovraccarico della risorsa ma non \u00e8 sufficiente per un'allocazione efficace delle risorse. Questa tecnica pu\u00f2 solo fornire un buon complemento ai meccanismi di allocazione distribuita delle risorse. Questi coordinatori di solito devono avere una conoscenza globale dello stato di tutte le risorse del sistema. Un esempio di algoritmo di allocazione dinamica delle risorse \u00e8 il progetto Cactus -LSB- 1 -RSB- per l'allocazione di lavori computazionalmente molto costosi. Il valore delle soluzioni distribuite per il problema dell'allocazione delle risorse \u00e8 stato riconosciuto dalla ricerca -LSB- 10 -RSB-. Ispirandosi ai principi dei mercati azionari, sono stati sviluppati modelli di mercato economico per lo scambio di risorse per la regolazione della domanda e dell'offerta nella rete. Gli utenti cercano di acquistare le risorse a basso costo necessarie per eseguire il lavoro mentre i fornitori cercano di ottenere il massimo profitto possibile e di utilizzare le risorse disponibili a piena capacit\u00e0.Una raccolta di diverse tecniche di allocazione delle risorse distribuite basate su modelli di mercato \u00e8 presentata in Clearwater -LSB- 10 -RSB-. Buyya et al. ha sviluppato un quadro di allocazione delle risorse basato sulla regolamentazione della domanda e dell'offerta -LSB- 4 -RSB- per Nimrod-G -LSB- 6 -RSB- con particolare attenzione alle scadenze lavorative e ai vincoli di budget. Il modello di allocazione delle risorse basato su agenti -LRB- ARAM -RRB- per le griglie \u00e8 progettato per pianificare lavori computazionalmente costosi utilizzando agenti. Lo svantaggio di questo modello \u00e8 l'uso estensivo dello scambio di messaggi tra agenti per il monitoraggio periodico e lo scambio di informazioni all'interno della struttura gerarchica. Le sottoattivit\u00e0 di un lavoro migrano attraverso la rete finch\u00e9 non trovano una risorsa che soddisfa i vincoli di prezzo. L'itinerario di migrazione del lavoro \u00e8 determinato dalle risorse nel collegarli in diverse topologie -LSB- 17 -RSB-. Il meccanismo proposto in questo documento elimina la necessit\u00e0 di uno scambio periodico di informazioni sui carichi delle risorse e non necessita di una topologia di connessione tra le risorse. Negli ultimi anni \u00e8 stato pubblicato un lavoro considerevole sulle tecniche di allocazione decentralizzata delle risorse utilizzando la teoria dei giochi. \u00c8 un problema decisionale mal definito che presuppone e modella il ragionamento induttivo. In questo gioco decisionale ripetitivo, un numero dispari di agenti deve scegliere tra due risorse sulla base delle informazioni sui successi passati cercando di allocarsi nella risorsa con la minoranza. Galstyan et al. -LSB- 14 -RSB- ha studiato una variazione con pi\u00f9 di due risorse, modificando le capacit\u00e0 delle risorse e le informazioni provenienti dagli agenti vicini. Hanno dimostrato che gli agenti possono adattarsi efficacemente alle mutevoli capacit\u00e0 in questo ambiente utilizzando una serie di semplici tabelle di ricerca -LRB- strategie -RRB- per agente. Un'altra tecnica distribuita utilizzata per risolvere il problema dell'allocazione delle risorse \u00e8 basata sull'apprendimento per rinforzo -LSB- 18 -RSB-. Similmente al nostro approccio, un insieme di agenti competono per un numero limitato di risorse basandosi solo su precedenti esperienze individuali. In questo documento, l'obiettivo del sistema \u00e8 massimizzare la produttivit\u00e0 del sistema garantendo al tempo stesso l'equit\u00e0 delle risorse, misurata come tempo di elaborazione medio per unit\u00e0 di lavoro. Un approccio di allocazione delle risorse per reti di sensori basato su tecniche di auto-organizzazione e apprendimento per rinforzo \u00e8 presentato in -LSB- 16 -RSB- con focus principale sull'ottimizzazione del consumo energetico dei nodi della rete. Noi -LSB- 19 -RSB- abbiamo proposto un approccio di bilanciamento del carico auto-organizzato per un singolo server con particolare attenzione all'ottimizzazione dei costi di comunicazione degli agenti mobili. Un agente mobile rifiuter\u00e0 una migrazione a un server dell'agente remoto, se prevede che il server di destinazione sia gi\u00e0 sovraccaricato da altri agenti o attivit\u00e0 del server. Gli agenti prendono le loro decisioni in base alle previsioni di utilizzo del server. In questo documento viene presentata una soluzione per un ambiente multiserver senza considerare i costi di comunicazione o di migrazione. 6.CONCLUSIONI E LAVORO FUTURO In questo articolo \u00e8 stata presentata una tecnica di allocazione distribuita delle risorse auto-organizzata per sistemi multi-agente. Consentiamo agli agenti di selezionare autonomamente la piattaforma di esecuzione per le proprie attivit\u00e0 prima di ogni esecuzione in fase di esecuzione. Nel nostro approccio gli agenti competono per un'allocazione in uno dei Figura 5: Risultati dell'esperimento 2 in un ambiente server dinamico con una media di oltre 100 ripetizioni. risorsa condivisa disponibile. Gli agenti percepiscono l'ambiente del loro server e adottano la loro azione per competere in modo pi\u00f9 efficiente nel nuovo ambiente creato. Questo processo \u00e8 adattivo e ha un forte feedback poich\u00e9 le decisioni di allocazione influenzano indirettamente le decisioni di altri agenti. L\u2019allocazione delle risorse \u00e8 un effetto puramente emergente. Il nostro meccanismo dimostra che l\u2019allocazione delle risorse pu\u00f2 essere effettuata attraverso la concorrenza effettiva di agenti individuali e autonomi. Non necessitano n\u00e9 di coordinamento n\u00e9 di informazioni da parte di un'autorit\u00e0 superiore, n\u00e9 \u00e8 necessaria un'ulteriore comunicazione diretta tra gli agenti. Questo meccanismo \u00e8 stato ispirato dal ragionamento induttivo e dai principi di razionalit\u00e0 limitata che consentono agli agenti di adattare le loro strategie per competere efficacemente in un ambiente dinamico. Nel caso in cui un server diventi indisponibile, gli agenti possono adattarsi rapidamente a questa nuova situazione esplorando nuove risorse o rimanere sul server principale se un'allocazione non \u00e8 possibile. Soprattutto in ambienti dinamici e scalabili come i sistemi a griglia, \u00e8 necessario un meccanismo robusto e distribuito per l\u2019allocazione delle risorse. Il nostro approccio auto-organizzato all'allocazione delle risorse \u00e8 stato valutato con una serie di esperimenti di simulazione in un ambiente dinamico di agenti e risorse del server. I risultati presentati per questo nuovo approccio per l'ottimizzazione della migrazione strategica sono molto promettenti e giustificano ulteriori indagini in un ambiente di sistema multi-agente reale. Si tratta di una politica distribuita, scalabile e di facile comprensione per la regolamentazione della domanda e dell\u2019offerta di risorse. Tutto il controllo \u00e8 implementato negli agenti. Un semplice meccanismo decisionale basato sulle diverse convinzioni dell\u2019agente crea un comportamento emergente che porta ad un\u2019efficace allocazione delle risorse. Questo approccio pu\u00f2 essere facilmente esteso o supportato da meccanismi di bilanciamento/accodamento delle risorse forniti dalle risorse. Il nostro approccio si adatta ai cambiamenti dell\u2019ambiente ma non \u00e8 evolutivo. Non vi \u00e8 alcuna scoperta di nuove strategie da parte degli agenti. indagato in futuro. Nel prossimo futuro esamineremo se un adattamento automatico del tasso di decadimento delle informazioni storiche \u00e8 possibile nel nostro algoritmo e pu\u00f2 migliorare le prestazioni di allocazione delle risorse. Un gran numero di risorse condivise richiede informazioni storiche pi\u00f9 vecchie per evitare un'esplorazione delle risorse troppo frequente. Al contrario, un ambiente dinamico con capacit\u00e0 variabili richiede informazioni pi\u00f9 aggiornate per fare previsioni pi\u00f9 affidabili.Siamo consapevoli della lunga fase di apprendimento in ambienti con un gran numero di risorse condivise conosciute da ciascun agente. Nel caso in cui gli agenti richiedano pi\u00f9 risorse di quelle fornite da tutti i server, tutti gli agenti esploreranno in modo casuale tutti i server conosciuti. Questo processo di acquisizione delle informazioni sul carico delle risorse su tutti i server pu\u00f2 richiedere molto tempo nel caso in cui non vengano fornite risorse condivise sufficienti per tutte le attivit\u00e0. In questa situazione, \u00e8 difficile per un agente raccogliere in modo efficiente informazioni cronologiche su tutti i server remoti. Questo problema necessita di ulteriori indagini in futuro.", "keyphrases": ["sistema multiagente", "agente", "allocazione delle risorse", "algoritmo di distribuzione", "attivit\u00e0 di allocazione dinamica", "rete di server", "utilit\u00e0 del server", "adattare il processo", "competere", "predittore"]}
{"file_name": "J-14", "text": "Calcolo del buon equilibrio di Nash nei giochi grafici * ABSTRACT Questo articolo affronta il problema della selezione dell'equilibrio giusto nei giochi grafici. Il nostro approccio si basa sulla struttura dei dati chiamata politica di migliore risposta, proposta da Kearns et al. -LSB- 13 -RSB- come modo per rappresentare tutti gli equilibri di Nash di un gioco grafico. In -LSB- 9 -RSB-, \u00e8 stato dimostrato che la politica di risposta migliore ha dimensione polinomiale purch\u00e9 il grafico sottostante sia un percorso. In questo articolo mostriamo che se il grafico sottostante \u00e8 un albero a gradi limitati e la politica di risposta migliore ha dimensione polinomiale, allora esiste un algoritmo efficiente che costruisce un equilibrio di Nash che garantisce determinati guadagni a tutti i partecipanti. Un\u2019altra soluzione interessante \u00e8 l\u2019equilibrio di Nash che massimizza il benessere sociale. Mostriamo che, mentre calcolare esattamente quest'ultimo \u00e8 irrealizzabile -LRB- dimostriamo che la risoluzione di questo problema pu\u00f2 coinvolgere numeri algebrici di grado arbitrariamente alto -RRB-, esiste un FPTAS per trovare un tale equilibrio purch\u00e9 la politica di risposta migliore abbia dimensione polinomiale. Questi due algoritmi possono essere combinati per produrre equilibri di Nash che soddisfano vari criteri di equit\u00e0. 1. INTRODUZIONE Questa \u00e8 l'intuizione alla base dei giochi grafici, introdotti da Kearns, Littman e Singh in -LSB- 13 -RSB- come uno schema di rappresentazione compatto per giochi con molti giocatori. In un gioco grafico con n giocatori, ciascun giocatore \u00e8 associato a un vertice di un grafo sottostante G, e i guadagni di ciascun giocatore dipendono dalla sua azione cos\u00ec come dalle azioni dei suoi vicini nel grafico. Se il grado massimo di G \u00e8 \u0394, e ciascun giocatore ha a disposizione due azioni, allora il gioco pu\u00f2 essere rappresentato utilizzando n2\u0394+1 numeri. Al contrario, abbiamo bisogno di n2n numeri per rappresentare un gioco generale con 2 azioni e n-giocatori, il che \u00e8 pratico solo per piccoli valori di n. Per i giochi grafici con \u0394 costante, la dimensione del gioco \u00e8 lineare in n. Uno dei problemi pi\u00f9 naturali per un gioco grafico \u00e8 quello di trovare un equilibrio di Nash, la cui esistenza deriva dal celebre teorema di Nash -LRB- poich\u00e9 i giochi grafici sono solo un caso speciale di giochi con n-giocatori -RRB-. Il primo tentativo di affrontare questo problema \u00e8 stato fatto in -LSB- 13 -RSB-, dove gli autori considerano giochi grafici con due azioni per giocatore in cui il grafico sottostante \u00e8 un albero di gradi limitato. Propongono un algoritmo generico per trovare equilibri di Nash che pu\u00f2 essere specializzato in due modi: un algoritmo in tempo esponenziale per trovare un equilibrio di Nash -LRB- esatto -RRB-, e uno schema di approssimazione temporale completamente polinomiale -LRB- FPTAS -RRB- per trovare un\u2019approssimazione all\u2019equilibrio di Nash. Per ogni e > 0 questo algoritmo produce un equilibrio e-Nash, che \u00e8 un profilo strategico in cui nessun giocatore pu\u00f2 migliorare il proprio payoff di pi\u00f9 di e modificando unilateralmente la propria strategia. Sebbene gli equilibri di e-Nash siano spesso pi\u00f9 facili da calcolare rispetto agli equilibri di Nash esatti, questo concetto di soluzione presenta diversi inconvenienti. Primo,i giocatori potrebbero essere sensibili a una piccola perdita di payoff, quindi il profilo strategico che \u00e8 un equilibrio e-Nash non sar\u00e0 stabile. In secondo luogo, i profili strategici che si avvicinano agli equilibri di Nash possono essere molto migliori rispetto alle propriet\u00e0 in considerazione rispetto agli equilibri di Nash esatti. Pertanto, l'approssimazione -LRB- al valore -RRB- della migliore soluzione che corrisponde a un equilibrio di e-Nash potrebbe non essere indicativa di ci\u00f2 che pu\u00f2 essere ottenuto con un esatto equilibrio di Nash. Ci\u00f2 \u00e8 particolarmente importante se lo scopo della soluzione approssimata \u00e8 fornire un buon punto di riferimento per un sistema di agenti egoisti, poich\u00e9 il punto di riferimento implicito in un equilibrio e-Nash potrebbe non essere realistico. Per queste ragioni, in questo articolo ci concentreremo sul problema del calcolo degli equilibri di Nash esatti. Basandosi sulle idee di -LSB- 14 -RSB-, Elkind et al. -LSB- 9 -RSB- ha mostrato come trovare un equilibrio di Nash -LRB- esatto -RRB- in tempo polinomiale quando il sottostante. Al contrario, trovare un equilibrio di Nash in un grafico generale limitato ai gradi sembra essere computazionalmente intrattabile: \u00e8 stato mostrato -LRB- vedere -LSB- 5, 12, 7 -RSB- -RRB- da completare per la classe di complessit\u00e0 PPAD. -LSB- 9 -RSB- estende questo risultato di durezza al caso in cui il grafico sottostante ha una larghezza di percorso limitata. Un gioco grafico potrebbe non avere un unico equilibrio di Nash, anzi potrebbe averne esponenzialmente molti. Inoltre, alcuni equilibri di Nash sono pi\u00f9 desiderabili di altri. Piuttosto che avere un algoritmo che trovi semplicemente qualche equilibrio di Nash, vorremmo avere algoritmi per trovare equilibri di Nash con varie propriet\u00e0 socialmente desiderabili, come massimizzare il profitto complessivo o distribuire equamente il profitto. Una propriet\u00e0 utile della struttura dati di -LSB- 13 -RSB- \u00e8 che rappresenta simultaneamente l'insieme di tutti gli equilibri di Nash del gioco sottostante. Se questa rappresentazione ha dimensione polinomiale -LRB- come nel caso dei cammini, come mostrato in -LSB- 9 -RSB- -RRB-, si pu\u00f2 sperare di estrarre da essa un equilibrio di Nash con le propriet\u00e0 desiderate. Infatti, in -LSB- 13 -RSB- gli autori affermano che ci\u00f2 \u00e8 effettivamente possibile se si \u00e8 interessati a trovare un equilibrio -LRB- approssimato -RRB- a-Nash. L\u2019obiettivo di questo articolo \u00e8 estendere questo concetto agli equilibri di Nash esatti. 1.1 I nostri risultati In questo articolo studiamo giochi grafici a 2 azioni con n giocatori su alberi a gradi limitati per i quali la struttura dati di -LSB- 13 -RSB- ha dimensione poly -LRB- n -RRB-. Ci concentreremo sul problema di trovare equilibri di Nash esatti con determinate propriet\u00e0 socialmente desiderabili. In particolare, mostriamo come trovare un equilibrio di Nash che -LRB- quasi -RRB- massimizza il benessere sociale, ovvero la somma dei payoff dei giocatori, e mostriamo come trovare un equilibrio di Nash che -LRB- quasi -RRB - Soddisfa i limiti di pagamento prescritti per tutti i giocatori. I giochi grafici su alberi a gradi limitati hanno una struttura algebrica semplice. Una caratteristica interessante, che deriva da -LSB- 13 -RSB-,\u00e8 che ognuno di questi giochi ha un equilibrio di Nash in cui la strategia di ogni giocatore \u00e8 un numero razionale. La sezione 3 studia la struttura algebrica degli equilibri di Nash che massimizzano il benessere sociale. Mostriamo -LRB- Teoremi 1 e 2 -RRB- che, sorprendentemente, l'insieme degli equilibri di Nash che massimizzano il benessere sociale \u00e8 pi\u00f9 complesso. Sembra essere una caratteristica nuova del context che consideriamo qui, che un equilibrio di Nash ottimale sia difficile da rappresentare, in una situazione in cui \u00e8 facile trovare e rappresentare un equilibrio di Nash. Poich\u00e9 l\u2019equilibrio di Nash che massimizza il benessere sociale pu\u00f2 essere difficile da rappresentare in modo efficiente, dobbiamo accontentarci di un\u2019approssimazione. Tuttavia, la differenza cruciale tra il nostro approccio e quello degli articoli precedenti -LSB- 13, 16, 19 -RSB- \u00e8 che richiediamo al nostro algoritmo di produrre un equilibrio di Nash esatto, sebbene non necessariamente quello ottimale rispetto ai nostri criteri. Nella Sezione 4 descriviamo un algoritmo che soddisfa questo requisito. In particolare, proponiamo un algoritmo che per ogni e > 0 trova un equilibrio di Nash il cui payoff totale \u00e8 compreso tra a e ottimale. Un risultato pi\u00f9 correlato a pre1A in un context diverso \u00e8 stato ottenuto da Datta -LSB- 8 -RSB-, il quale mostra che i giochi a 2 azioni con n giocatori sono universali, nel senso che qualsiasi variet\u00e0 algebrica reale pu\u00f2 essere rappresentata come l'insieme di Nash totalmente misti equilibri di tali giochi. Mostriamo -LRB- Sezione 4.1 -RRB- che, sotto alcune restrizioni sulle matrici dei payoff, l'algoritmo pu\u00f2 essere trasformato in un algoritmo -LRB- veramente -RRB- in tempo polinomiale che produce un equilibrio di Nash il cui payoff totale \u00e8 compreso tra 1 \u2212 e fattore da quello ottimale. Nella Sezione 5, consideriamo il problema di trovare un equilibrio di Nash in cui il payoff atteso di ciascun giocatore Vi supera una soglia prescritta Ti. Usando l'idea della Sezione 4 diamo -LRB- Teorema 5 -RRB- uno schema di approssimazione temporale completamente polinomiale per questo problema. Il tempo di esecuzione dell'algoritmo \u00e8 limitato da un polinomio in n, Pmax ed E. Se l'istanza ha un equilibrio di Nash che soddisfa le soglie prescritte, allora l'algoritmo costruisce un equilibrio di Nash in cui il profitto atteso di ciascun giocatore Vi \u00e8 almeno Ti \u2212 E. Nella Sezione 6, introduciamo altri criteri naturali per selezionare un equilibrio di Nash ``buon'' e mostriamo che gli algoritmi descritti nelle due sezioni precedenti possono essere utilizzati come elementi costitutivi per trovare equilibri di Nash che soddisfano questi criteri. In particolare, nella Sezione 6.1 mostriamo come trovare un equilibrio di Nash che approssimi il massimo benessere sociale, garantendo al tempo stesso che ogni payoff individuale sia vicino ad una soglia prescritta. Nella Sezione 6.2 mostriamo come trovare un equilibrio di Nash in cui -LRB- quasi -RRB- massimizza il payoff individuale minimo. Infine, nella Sezione 6.3 mostriamo come trovare un equilibrio di Nash in cui i payoff individuali dei giocatori sono vicini tra loro. 1.2 Lavori correlati Il nostro schema di approssimazione -LRB- Teorema 3 e Teorema 4 -RRB- mostra un contrasto tra i giochi che studiamo e i giochi n-azione a due giocatori, per i quali i problemi corrispondenti sono solitamente intrattabili. Per i giochi n-azione a due giocatori, il problema di trovare equilibri di Nash con propriet\u00e0 speciali \u00e8 tipicamente NP-difficile. In particolare, questo \u00e8 il caso degli equilibri di Nash che massimizzano il benessere sociale -LSB- 11, 6 -RSB-. Inoltre, \u00e8 probabile che sia difficile anche solo approssimare tali equilibri. In particolare, Chen, Deng e Teng -LSB- 4 -RSB- mostrano che esiste un e, polinomio inverso in n, per il quale il calcolo di un equilibrio e-Nash in giochi a 2 giocatori con n azioni per giocatore \u00e8 PPAD-completo. Lipton e Markakis -LSB- 15 -RSB- studiano le propriet\u00e0 algebriche degli equilibri di Nash e sottolineano che gli algoritmi standard di eliminazione dei quantificatori possono essere utilizzati per risolverli. Si noti che questi algoritmi non sono tempo polinomiale in generale. I giochi che studiamo in questo articolo hanno equilibri di Nash calcolabili in tempo polinomiale in cui tutte le strategie miste sono numeri razionali, ma un equilibrio di Nash ottimale pu\u00f2 necessariamente includere strategie miste con alto grado algebrico. Qualsiasi equilibrio di Nash \u00e8 un EC ma in generale non vale il contrario. In contrasto con gli equilibri di Nash, equilibri correlati possono essere trovati per giochi grafici di basso grado -LRB- cos\u00ec come per altre classi di giochi multiplayer rappresentati in modo conciso -RRB- in tempo polinomiale -LSB- 17 -RSB-. Ma per i giochi grafici \u00e8 NP-difficile trovare un equilibrio correlato che massimizzi il payoff totale -LSB- 18 -RSB-. Tuttavia, i risultati della durezza NP si applicano a giochi pi\u00f9 generali di quello qui considerato, in particolare i grafici non sono alberi. Da -LSB- 2 -RSB- \u00e8 anche noto che esistono giochi a 2 giocatori e 2 azioni per i quali il payoff totale atteso del miglior equilibrio correlato \u00e8 maggiore del miglior equilibrio di Nash, e discuteremo ulteriormente questo problema nella Sezione 7. 7. CONCLUSIONI Abbiamo studiato il problema della selezione dell'equilibrio in giochi grafici su alberi a gradi limitati. Abbiamo considerato diversi criteri per selezionare un equilibrio di Nash, come massimizzare il benessere sociale, garantire un limite inferiore al payoff atteso di ciascun giocatore, ecc. In primo luogo, ci siamo concentrati sulla complessit\u00e0 algebrica di un equilibrio di Nash che massimizza il benessere sociale, e ha dimostrato forti risultati negativi per quel problema. Vale a dire, abbiamo dimostrato che anche per i giochi grafici su percorsi, qualsiasi numero algebrico \u03b1 E -LSB- 0, 1 -RSB- pu\u00f2 essere l'unica strategia disponibile per alcuni giocatori in tutti gli equilibri di Nash che massimizzano il benessere sociale. Ci\u00f2 \u00e8 in netto contrasto con il fatto che i giochi grafici sugli alberi possiedono sempre un equilibrio di Nash in cui tutte le strategie dei giocatori sono numeri razionali. Abbiamo quindi fornito algoritmi di approssimazione per selezionare equilibri di Nash con propriet\u00e0 speciali. Sebbene il problema di trovare equilibri di Nash approssimativi per varie classi di giochi abbia ricevuto molta attenzione negli ultimi anni,la maggior parte del lavoro esistente mira a trovare equilibri E-Nash che soddisfino -LRB- o siano E-vicini a soddisfare -RRB- determinate propriet\u00e0. Il nostro approccio \u00e8 diverso in quanto insistiamo nel fornire un esatto equilibrio di Nash, che sia E-vicino al soddisfacimento di un dato requisito. Come sostenuto nell\u2019introduzione, ci sono diverse ragioni per preferire una soluzione che costituisca un esatto equilibrio di Nash. Mentre dimostriamo i nostri risultati per i giochi su un percorso, essi possono essere generalizzati a qualsiasi albero per il quale le politiche di risposta ottima hanno rappresentazioni compatte come unioni di rettangoli. Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.", "keyphrases": ["gioco grafico", "equilibrio di Nash", "schema approssimativo", "Algoritmo esponenziale-tempo", "approssimativo", "varie propriet\u00e0 socialmente desiderate", "ricompensa complessiva", "distribuire gli utili", "benessere sociale", "gioco grafico a vincita integrale g", "grave inconveniente", "profilo strategico", "grafico legato ai gradi"]}
{"file_name": "I-22", "text": "Modellazione realistica del carico cognitivo per migliorare i modelli mentali condivisi nella collaborazione uomo-agente ABSTRACT I membri del team umano spesso sviluppano aspettative condivise per prevedere i bisogni reciproci e coordinare i loro comportamenti. In questo articolo il concetto di \"Mappa delle credenze condivise\" viene proposto come base per lo sviluppo di aspettative realistiche condivise all'interno di un team di coppie di agenti umani -LRB- HAP -RRB-. La creazione di mappe di credenze condivise si basa sulla condivisione di informazioni tra agenti, la cui efficacia dipende in larga misura dai carichi di elaborazione degli agenti e dai carichi cognitivi istantanei dei loro partner umani. Investighiamo modelli di carico cognitivo basati su HMM per facilitare i membri del team a \"condividere le informazioni giuste con la parte giusta al momento giusto\". Il concetto di mappa delle credenze condivise e i modelli di carico cognitivo/elaborativo sono stati implementati in un'architettura di agenti cognitivi: SMMall. Sono stati condotti una serie di esperimenti per valutare il concetto, i modelli e il loro impatto sull'evoluzione dei modelli mentali condivisi dei team HAP. 1. INTRODUZIONE Il lavoro di squadra multiagente centrato sull'uomo ha quindi attirato crescenti attenzioni nel campo dei sistemi multiagente -LSB- 2, 10, 4 -RSB-. Umani e autonomi In breve, gli esseri umani e gli agenti possono collaborare per ottenere prestazioni migliori, dato che possono stabilire una certa consapevolezza reciproca per coordinare le loro attivit\u00e0 di iniziativa mista. Tuttavia, il fondamento della collaborazione uomo-agente continua a essere messo in discussione a causa di modelli non realistici di consapevolezza reciproca dello stato delle cose. In particolare, pochi ricercatori guardano oltre per valutare i principi di modellazione dei costrutti mentali condivisi tra un essere umano e il suo agente assistente. Inoltre, le relazioni uomo-agente possono andare oltre i partner e raggiungere i team. Pertanto, esiste una chiara richiesta di indagini per ampliare e approfondire la nostra comprensione sui principi della modellazione mentale condivisa tra i membri di un team misto uomo-agente. Esistono linee di ricerca sul lavoro di squadra multi-agente, sia teoricamente che empiricamente. Ad esempio, Joint Intention -LSB- 3 -RSB- e SharedPlans -LSB- 5 -RSB- sono due framework teorici per specificare le collaborazioni degli agenti. Uno degli svantaggi \u00e8 che, sebbene entrambi abbiano una profonda radice filosofica e cognitiva, non consentono di modellare i membri del team umano. Studi cognitivi suggeriscono che i team che hanno modelli mentali condivisi dovrebbero avere aspettative comuni sul compito e sul team, che consentono loro di prevedere il comportamento e le esigenze di risorse dei membri del team in modo pi\u00f9 accurato -LSB- 14, 6 -RSB-. Cannon-Bowers et al. -LSB- 14 -RSB- sostengono esplicitamente che i membri del team dovrebbero possedere modelli compatibili che conducano a \u201caspettative\u201d comuni. Siamo d'accordo su questo e crediamo che la creazione di aspettative condivise tra i membri del team umano e degli agenti sia un passo fondamentale per far avanzare la ricerca sul lavoro di squadra incentrata sull'uomo.Va notato che il concetto di aspettativa condivisa pu\u00f2 includere in generale l\u2019assegnazione dei ruoli e le sue dinamiche, gli schemi e i progressi del lavoro di squadra, i modelli e le intenzioni di comunicazione, ecc. Mentre l\u2019obiettivo a lungo termine della nostra ricerca \u00e8 capire come le strutture cognitive condivise possono 6. CONCLUSIONE L'attenzione della recente ricerca sul lavoro di squadra incentrato sull'uomo richiede fortemente la progettazione di sistemi di agenti come ausili cognitivi in \u200b\u200bgrado di modellare e sfruttare i partner umani. capacit\u00e0 cognitive per offrire aiuto in modo non intrusivo. In questo articolo, abbiamo studiato diversi fattori che circondano il difficile problema dell\u2019evoluzione di modelli mentali condivisi di team composti da coppie di agenti umani. Il contributo principale di questa ricerca include -LRB-1 -RRB- Sono stati proposti modelli di carico basati su HMM per un agente per stimare il carico cognitivo del suo partner umano e i carichi di elaborazione di altri compagni di squadra HAP; -LRB- 2 -RRB- Il concetto di mappa delle credenze condivise \u00e8 stato introdotto e implementato. Permette ai membri del gruppo di rappresentare e ragionare efficacemente su modelli mentali condivisi; -LRB- 3 -RRB- Sono stati condotti esperimenti per valutare i modelli di carico cognitivo/elaborativo basati su HMM e gli impatti della comunicazione multilaterale sull'evoluzione delle SMM del team. Nel corso degli esperimenti \u00e8 stata dimostrata anche l\u2019utilit\u00e0 delle mappe delle credenze condivise.", "keyphrases": ["condividere la mappa delle credenze", "lavoro di squadra multiag", "eurista", "motivo", "risoluzione dei problemi", "collaborare", "lavoro di squadra", "aspettarsi", "schema del lavoro di squadra", "eseguire il team di agenti umani", "Teoria del carico cognitivo", "prestazione umana", "allocazione delle risorse", "compito eseguire", "condivisione di informazioni", "comune multipartitico"]}
{"file_name": "J-23", "text": "Rapporti di frugalit\u00e0 e meccanismi di Truthful migliorati per la copertura dei vertici * Nelle aste a sistema fisso, ci sono diversi team di agenti che si sovrappongono e un compito che pu\u00f2 essere completato da uno qualsiasi di questi team. L'obiettivo del banditore \u00e8 assumere una squadra e pagare il meno possibile. Esempi di questa impostazione includono le aste del percorso pi\u00f9 breve e le aste della copertura dei vertici. Recentemente, Karlin, Kempe e Tamir hanno introdotto una nuova definizione del rapporto di offrugalit\u00e0 per questo problema. Informalmente, il \"rapporto di frugalit\u00e0\" \u00e8 il rapporto tra il pagamento totale di un meccanismo e il limite di pagamento desiderato. Il rapporto cattura la misura in cui il meccanismo paga pi\u00f9 del dovuto, rispetto al costo equo percepito in un'asta veritiera. In questo articolo proponiamo una nuova asta polinomiale veritiera per il problema della copertura dei vertici e ne limitiamo il rapporto di frugalit\u00e0. Mostriamo che la qualit\u00e0 della soluzione \u00e8 con un fattore costante di ottimale e il rapporto di frugalit\u00e0 \u00e8 all'interno di un fattore costante del miglior limite possibile del caso peggiore; questa \u00e8 la prima asta per questo problema ad avere queste propriet\u00e0. Inoltre, mostriamo come trasformare qualsiasi asta veritiera in un'asta frugale preservando il rapporto di approssimazione. Inoltre, consideriamo due modifiche naturali della definizione di Karlin et al., e analizziamo le propriet\u00e0 dei limiti di pagamento risultanti, come monotonicit\u00e0, durezza computazionale e robustezza rispetto alla regola di risoluzione del sorteggio. Studiamo le relazioni tra i diversi limiti di pagamento, sia per sistemi di insiemi generali che per aste di insiemi specifici, come aste di percorso e aste di vertex-cover. Utilizziamo queste nuove definizioni nella dimostrazione del nostro risultato principale per le aste di copertura dei vertici tramite una tecnica di bootstrap, che potrebbe essere di interesse indipendente. 1. INTRODUZIONE In un'asta a sistema fisso c'\u00e8 un unico acquirente e molti venditori che possono fornire vari servizi. Si presuppone che i requisiti dell'acquirente possano essere soddisfatti da vari sottoinsiemi di venditori; questi sottoinsiemi sono detti insiemi ammissibili. Assumiamo che ogni venditore abbia un costo per fornire i propri servizi, ma presenti un'offerta possibilmente pi\u00f9 alta al banditore. Sulla base di queste offerte, il banditore seleziona un possibile sottoinsieme di venditori ed effettua i pagamenti ai venditori in questo sottoinsieme. Ogni venditore selezionato gode di un profitto dal pagamento meno i costi. I venditori vogliono massimizzare il profitto, mentre l\u2019acquirente vuole ridurre al minimo l\u2019importo da pagare. Un obiettivo naturale in questo context \u00e8 progettare un'asta veritiera, in cui i venditori abbiano un incentivo a offrire il loro costo reale. Ci\u00f2 pu\u00f2 essere ottenuto pagando a ciascun venditore selezionato un premio superiore alla sua offerta in modo tale che il venditore non abbia alcun incentivo a fare offerte superiori. Una questione interessante nella progettazione del meccanismo \u00e8 quanto il banditore dovr\u00e0 pagare in eccesso per garantire offerte veritiere. Nel context delle aste dei percorsi questo argomento \u00e8 stato affrontato per la prima volta da Archer e Tardos -LSB- 1 -RSB-.Essi definiscono il rapporto di frugalit\u00e0 di un meccanismo come il rapporto tra il suo pagamento totale e il costo del percorso pi\u00f9 economico disgiunto dal percorso selezionato dal meccanismo. Essi mostrano che, per un\u2019ampia classe di meccanismi veritieri per questo problema, il rapporto di frugalit\u00e0 \u00e8 grande quanto il numero di archi nel percorso pi\u00f9 breve. Talwar -LSB- 21 -RSB- estende questa definizione di rapporto di frugalit\u00e0 ai sistemi di insiemi generali e studia il rapporto di frugalit\u00e0 del meccanismo VCG classico -LSB- 22, 4, 14 -RSB- per molti sistemi di insiemi specifici, come lo spanning minimo alberi e coperture. Sebbene la definizione del rapporto di frugalit\u00e0 proposta da -LSB- 1 -RSB- sia ben motivata e sia stata determinante nello studio dei meccanismi veritieri per i sistemi di insiemi, non \u00e8 completamente soddisfacente. Consideriamo, ad esempio, il grafico della Figura 1 con i costi CAB = CBC = Figura 1: Il grafico del diamante Questo grafico \u00e8 2-connesso e il pagamento VCG al percorso vincente ABCD \u00e8 limitato. Tuttavia, il grafico non contiene alcun percorso A-D disgiunto da ABCD, e quindi il rapporto di frugalit\u00e0 di VCG su questo grafico rimane indefinito. Allo stesso tempo, non esiste alcun monopolio, cio\u00e8 non esiste un venditore che compaia in tutti gli insiemi possibili. Per affrontare questo problema, Karlin et al. -LSB- 16 -RSB- suggeriscono un punto di riferimento migliore, definito per qualsiasi sistema di set privo di monopolio. Sulla base di questa nuova definizione, gli autori costruiscono nuovi meccanismi per il problema del percorso pi\u00f9 breve e mostrano che il pagamento in eccesso di questi meccanismi rientra in un fattore costante di ottimale. 1.1 I nostri risultati Aste di copertura dei vertici Proponiamo un'asta polinomiale veritiera per la copertura dei vertici che produce una soluzione il cui costo rientra in un fattore 2 rispetto a quello ottimale e il cui rapporto di frugalit\u00e0 \u00e8 al massimo 2\u0394, dove \u0394 \u00e8 il grado massimo del grafico -LRB- Teorema 4 -RRB-. Completiamo questo risultato dimostrando -LRB- Teorema 5 -RRB- che per ogni \u0394 e n, ci sono grafici di massimo grado \u0394 e dimensione \u0398 -LRB- n -RRB- per i quali qualsiasi meccanismo veritiero ha un rapporto di frugalit\u00e0 almeno \u0394 / 2. Ci\u00f2 significa che la qualit\u00e0 della soluzione della nostra asta \u00e8 ottimale per un fattore 2 e il rapporto di frugalit\u00e0 rientra per un fattore 4 rispetto al miglior limite possibile per gli input nel caso peggiore. Per quanto ne sappiamo, questa \u00e8 la prima asta per questo problema che gode di queste propriet\u00e0. Inoltre, mostriamo come trasformare qualsiasi meccanismo veritiero per il problema della copertura dei vertici in uno frugale preservando il rapporto di approssimazione. Rapporti di frugalit\u00e0 I nostri risultati sulla copertura dei vertici suggeriscono naturalmente due modifiche della definizione di \u03bd in -LSB- 16 -RSB-. Queste modifiche possono essere apportate indipendentemente l'una dall'altra, risultando in quattro diversi limiti di pagamento TUmax, TUmin, NTUmax e NTUmin, dove NTUmin \u00e8 uguale al limite di pagamento originale \u03bd di in -LSB- 16 -RSB-. Mentre il nostro risultato principale sulle aste di copertura dei vertici -LRB- Teorema 4 -RRB- \u00e8 rispetto a NTUmin = \u03bd, utilizziamo le nuove definizioni confrontando prima il pagamento del nostro meccanismo con un limite pi\u00f9 debole NTUmax,e quindi eseguire il bootstrap da questo risultato per ottenere il limite desiderato. Ispirati da questa applicazione, ci imbarchiamo in un ulteriore studio di questi limiti di pagamento. I nostri risultati qui sono i seguenti: 1. Osserviamo -LRB- Proposizione 1 -RRB- che i quattro limiti di pagamento obbediscono sempre a un ordine particolare che \u00e8 indipendente dalla scelta del sistema di insiemi e del vettore di costo, vale a dire TUmin < NTUmin < NTUmax < TUmax. Forniamo esempi -LRB- Proposizione 5 e Corollari 1 e 2 -RRB- che mostrano che per il problema della copertura dei vertici qualsiasi due limiti consecutivi possono differire di un fattore n \u2212 2, dove n \u00e8 il numero di agenti. Mostriamo poi -LRB- Teorema 2 -RRB- che questa separazione \u00e8 quasi la migliore possibile per sistemi di insiemi generali dimostrando che per qualsiasi sistema di insiemi TUmax/TUmin < n. Al contrario, dimostriamo -LRB- Teorema 3 -RRB- che per aste di percorso TUmax/TUmin < 2. Forniamo esempi -LRB- Proposizioni 2, 3 e 4 -RRB- che mostrano che questo limite \u00e8 stretto. Consideriamo questo come un argomento a favore dello studio delle aste di vertexcover, poich\u00e9 sembrano essere pi\u00f9 rappresentative del problema generale di selezione del team rispetto alle aste di percorso ampiamente studiate. 2. Questa osservazione suggerisce che i quattro limiti di pagamento dovrebbero essere studiati in un quadro unitario; inoltre, ci porta a ritenere che la tecnica di bootstrap del Teorema 4 possa avere altre applicazioni. 3. Valutiamo i limiti di pagamento qui introdotti rispetto a una lista di controllo delle caratteristiche desiderabili. Questo pu\u00f2 essere visto come un argomento a favore dell'utilizzo di limiti NTUmax e TUmax pi\u00f9 deboli ma computabili in modo efficiente. Lavori correlati Le aste delle coperture dei vertici sono state studiate in passato da Talwar -LSB- 21 -RSB- e Calinescu -LSB- 5 -RSB-. Entrambi questi documenti si basano sulla definizione del rapporto di frugalit\u00e0 utilizzata in -LSB- 1 -RSB- ; come accennato in precedenza, ci\u00f2 significa che i loro risultati si applicano solo ai grafi bipartiti. Talwar -LSB- 21 -RSB- mostra che il rapporto di frugalit\u00e0 di VCG \u00e8 al massimo \u0394. Tuttavia, poich\u00e9 trovare la copertura del vertice pi\u00f9 economica \u00e8 un problema NP-difficile, il meccanismo VCG \u00e8 computazionalmente irrealizzabile. Il primo articolo -LRB- e, per quanto ne sappiamo, unico -RRB- per indagare i meccanismi veritieri in tempo polinomiale per la copertura dei vertici \u00e8 -LSB- 5 -RSB-. Questo articolo studia un'asta basata sull'algoritmo di allocazione greedy, che ha un rapporto di approssimazione di log n. Mentre l'obiettivo principale di -LSB- 5 -RSB- \u00e8 il problema pi\u00f9 generale della copertura dell'insieme, i risultati di -LSB- 5 -RSB- implicano un rapporto di frugalit\u00e0 di 2\u03942 per la copertura dei vertici. 2. PRELIMINARI Nella maggior parte di questo articolo si parler\u00e0 di aste per sistemi di insiemi. Nelle aste a sistema fisso, ogni elemento e del set terrestre \u00e8 di propriet\u00e0 di un agente indipendente e ha associato un costo non negativo ce. L'obiettivo del centro \u00e8 selezionare -LRB- acquistare -RRB- un insieme fattibile. Ogni elemento e nell'insieme selezionato comporta un costo di ce. Gli elementi non selezionati non comportano alcun costo. L'asta si svolge come segue: tutti gli elementi del terreno fanno le loro offerte,il centro seleziona un insieme fattibile in base alle offerte ed effettua i pagamenti agli agenti. Formalmente un'asta \u00e8 definita da una regola di allocazione A : R '' _, F e da una regola di pagamento P : R '' _, R ''. La regola di allocazione prende come input un vettore di offerte e decide quale degli insiemi in F deve essere selezionato. La regola di pagamento prende anche come input un vettore di offerte e decide quanto pagare a ciascun agente. I requisiti standard sono la razionalit\u00e0 individuale, ovvero il pagamento a ciascun agente dovrebbe essere almeno pari al costo sostenuto -LRB- 0 per gli agenti non nell'insieme selezionato e ce per gli agenti nell'insieme selezionato -RRB- e compatibilit\u00e0 degli incentivi, o veridicit\u00e0, cio\u00e8 la strategia dominante di ciascun agente \u00e8 quella di puntare il suo vero costo. Una regola di allocazione \u00e8 monotona se un agente non pu\u00f2 aumentare le sue possibilit\u00e0 di essere selezionato aumentando la sua offerta. Data una regola di allocazione monotona A e un vettore di offerta b, l'offerta soglia te di un agente e EA -LRB- b -RRB- \u00e8 l'offerta pi\u00f9 alta di questo agente che vince comunque l'asta, dato che le offerte degli altri partecipanti rimangono le Stesso. \u00c8 noto -LRB- vedi ad es. -LSB- 19, 13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.", "keyphrases": ["rapporto frugale", "tecnica di bootstrap", "asta di copertura del vertice", "trasferimento util", "procedere al pagamento vincolato", "regola di allocazione monotona", "bottaio", "tempo polinomiale", "non monotono"]}
{"file_name": "I-11", "text": "Caratterizzazione e previsione degli agenti in tempo reale ABSTRACT Ragionare sugli agenti che osserviamo nel mondo \u00e8 impegnativo. Le nostre informazioni disponibili sono spesso limitate all'osservazione del comportamento esterno dell'agente nel passato e nel presente. Per comprendere queste azioni, dobbiamo dedurre lo stato interno dell'agente, che comprende non solo elementi razionali -LRB- come intenzioni e progetti -RRB-, ma anche emotivi -LRB- come la paura -RRB-. Inoltre, spesso vogliamo prevedere le azioni future dell'agente, che sono vincolate non solo da queste caratteristiche interiori, ma anche dalle dinamiche dell'interazione dell'agente con il suo ambiente. BEE -LRB- Behavior Evolution and Extrapolation -RRB- utilizza un modello ambientale basato su agenti pi\u00f9 veloce del tempo reale per caratterizzare lo stato interno degli agenti mediante evoluzione rispetto al comportamento osservato e quindi prevedere il loro comportamento futuro, tenendo conto delle dinamiche di la loro interazione con l\u2019ambiente. 1. INTRODUZIONE Il ragionamento sugli agenti che osserviamo nel mondo deve integrare due livelli disparati. Le nostre osservazioni sono spesso limitate al comportamento esterno dell'agente, che spesso pu\u00f2 essere riassunto numericamente come una traiettoria nello spazio-tempo -LRB- forse punteggiata da azioni provenienti da un vocabolario abbastanza limitato -RRB-. Tuttavia, questo comportamento \u00e8 guidato dallo stato interno dell'agente, che -LRB- nel caso di un essere umano -RRB- pu\u00f2 coinvolgere concetti psicologici e cognitivi di alto livello come intenzioni ed emozioni. Una sfida centrale in molti domini applicativi \u00e8 il ragionamento a partire dalle osservazioni esterne del comportamento degli agenti fino alla stima del loro stato interno. Tale ragionamento \u00e8 motivato dal desiderio di prevedere il comportamento dell'agente. Questo problema \u00e8 stato tradizionalmente affrontato sotto la rubrica \"riconoscimento del piano\" o \"inferenza del piano\". ''Molti problemi realistici si discostano da queste condizioni. \u2022 L'aumento del numero di agenti porta a un'esplosione combinatoria che pu\u00f2 sommergere l'analisi convenzionale. \u2022 Le dinamiche ambientali possono frustrare le intenzioni degli agenti. \u2022 Gli agenti spesso cercano di nascondere le loro intenzioni -LRB- e perfino la loro presenza -RRB-, piuttosto che condividere intenzionalmente le informazioni. \u2022 Lo stato emotivo di un agente pu\u00f2 essere importante almeno quanto il suo stato razionale nel determinare il suo comportamento. BEE -LRB- Behavioral Evolution and Extrapolation -RRB- \u00e8 un nuovo approccio per riconoscere lo stato razionale ed emotivo di pi\u00f9 agenti interagenti basandosi esclusivamente sul loro comportamento, senza ricorrere a comunicazioni intenzionali da parte loro. Si ispira alle tecniche utilizzate per prevedere il comportamento dei sistemi dinamici non lineari, in cui una rappresentazione del sistema \u00e8 continuamente adattata al suo comportamento passato recente. Per i sistemi dinamici non lineari, la rappresentazione \u00e8 un'equazione matematica in forma chiusa. In BEE, \u00e8 un insieme di parametri che governano il comportamento degli agenti software che rappresentano gli individui analizzati.L'attuale versione di BEE caratterizza e prevede il comportamento degli agenti che rappresentano i soldati impegnati nel combattimento urbano -LSB- 8 -RSB-. La sezione 2 esamina il lavoro precedente pertinente. La sezione 3 descrive l'architettura di BEE. La sezione 4 riporta i risultati degli esperimenti con il sistema. La sezione 5 conclude. 2. IL LAVORO PRECEDENTE BEE \u00e8 paragonabile alla ricerca precedente sul riconoscimento del piano AI -LRB- -RRB-, sui modelli Markov nascosti e sui sistemi dinamici non lineari -LRB- previsione della traiettoria -RRB-. 2.1 Il riconoscimento del piano nell'intelligenza artificiale La teoria degli agenti descrive comunemente lo stato cognitivo di un agente in termini di credenze, desideri e intenzioni -LRB- il cosiddetto modello \"BDI\" -LSB- 5, 20 -RSB- -RRB- . Le credenze di un agente sono proposizioni sullo stato del mondo che considera vero, in base alle sue percezioni. I suoi desideri sono proposizioni sul mondo che vorrebbe fosse vero. I desideri non sono necessariamente coerenti tra loro: un agente potrebbe desiderare di essere ricco e di non lavorare allo stesso tempo. Le intenzioni, o obiettivi, di un agente sono un sottoinsieme dei suoi desideri che ha selezionato, in base alle sue convinzioni, per guidare le sue azioni future. A differenza dei desideri, gli obiettivi devono essere coerenti tra loro -LRB- o almeno ritenuti coerenti dall'agente -RRB-. Gli obiettivi di un agente guidano le sue azioni. Pertanto si dovrebbe essere in grado di imparare qualcosa sugli obiettivi di un agente osservando le sue azioni passate, e la conoscenza degli obiettivi dell'agente a sua volta consente di trarre conclusioni su ci\u00f2 che l'agente potrebbe fare in futuro. Questo processo di ragionamento dalle azioni di un agente ai suoi obiettivi \u00e8 noto come \"riconoscimento del piano\" o \"inferenza del piano\". Il riconoscimento del piano \u00e8 raramente perseguito fine a se stesso. Di solito supporta una funzione di livello superiore. Ad esempio, nelle interfacce uomo-computer, il riconoscimento del piano di un utente pu\u00f2 consentire al sistema di fornire informazioni e opzioni pi\u00f9 appropriate per l'azione dell'utente. In un sistema di tutoraggio, dedurre il piano dello studente \u00e8 il primo passo per identificare i piani difettosi e fornire soluzioni adeguate. In molti casi, la funzione di livello superiore prevede le probabili azioni future dell'entit\u00e0 di cui si deduce il piano. Ci concentriamo sul riconoscimento del piano a supporto della previsione. Il piano di un agente \u00e8 un input necessario per prevedere il suo comportamento futuro, ma difficilmente sufficiente. Devono essere prese in considerazione almeno altre due influenze, una interna e una esterna. L'influenza esterna \u00e8 la dinamica dell'ambiente, che pu\u00f2 includere altri agenti. Le dinamiche del mondo reale impongono vincoli significativi. \u2022 L'ambiente pu\u00f2 interferire con i desideri dell'agente -LSB- 4, 10 -RSB-. \u2022 La maggior parte delle interazioni tra agenti, e tra agenti e il mondo, sono non lineari. Un'analisi razionale degli obiettivi di un agente pu\u00f2 permetterci di prevedere cosa tenter\u00e0, ma qualsiasi piano non banale con diversi passaggi dipender\u00e0 sensibilmente in ogni passaggio dalla reazione dell'ambiente.e la nostra previsione deve tenere conto anche di questa reazione. La simulazione effettiva dei futuri \u00e8 un modo -LRB- l'unico che conosciamo ora -RRB- per affrontare l'impatto delle dinamiche ambientali sulle azioni di un agente. Anche gli agenti umani sono soggetti a un'influenza interna. Lo stato emotivo dell'agente pu\u00f2 modulare il suo processo decisionale e il suo focus di attenzione -LRB- e quindi la sua percezione dell'ambiente -RRB-. In casi estremi, l'emozione pu\u00f2 portare un agente a scegliere azioni che dal punto di vista di un'analisi logica possono apparire irrazionali. Il lavoro attuale sul riconoscimento del piano per la previsione si concentra sul piano razionale e non tiene conto n\u00e9 delle influenze ambientali esterne n\u00e9 dei pregiudizi emotivi interni. BEE integra tutti e tre gli elementi nelle sue previsioni. 2.2 Modelli Markov nascosti BEE \u00e8 superficialmente simile ai modelli Markov nascosti -LRB- HMM -LSB- 19 -RSB- -RRB-. BEE offre due importanti vantaggi rispetto a HMM. Innanzitutto, le variabili nascoste di un singolo agente non soddisfano la propriet\u00e0 di Markov. Cio\u00e8, i loro valori at + 1 dipendono non solo dai loro valori at, ma anche dalle variabili nascoste di altri agenti. Si potrebbe evitare questa limitazione costruendo un singolo HMM sullo spazio degli stati congiunto di tutti gli agenti, ma questo approccio \u00e8 combinatoriamente proibitivo. BEE combina l'efficienza di modellare in modo indipendente i singoli agenti con la realt\u00e0 di tenere conto delle interazioni tra di loro. In secondo luogo, i modelli di Markov presuppongono che le probabilit\u00e0 di transizione siano stazionarie. Il processo evolutivo di BEE aggiorna continuamente le personalit\u00e0 degli agenti sulla base di osservazioni reali e quindi tiene conto automaticamente dei cambiamenti nelle personalit\u00e0 degli agenti. 2.3 Fitting di sistemi non lineari in tempo reale Molti sistemi di interesse possono essere descritti da un vettore di numeri reali che cambia in funzione del tempo. Le dimensioni del vettore definiscono lo spazio degli stati del sistema. La previsione a lungo termine di un tale sistema \u00e8 impossibile. Tuttavia, \u00e8 spesso utile anticipare il comportamento del sistema a breve distanza nel futuro. Questo processo si ripete costantemente, fornendo all'utente una visione anticipata limitata. Questo approccio \u00e8 robusto e ampiamente applicato, ma richiede sistemi che possano essere descritti in modo efficiente con equazioni matematiche. BEE estende questo approccio ai comportamenti degli agenti, adattandoli al comportamento osservato utilizzando un algoritmo genetico. 5. CONCLUSIONI In molti ambiti, \u00e8 importante ragionare a partire dal comportamento osservato di un'entit\u00e0 fino a una stima del suo stato interno, e quindi estrapolare tale stima per prevedere il comportamento futuro dell'entit\u00e0. BEE esegue questo compito utilizzando una simulazione pi\u00f9 veloce del tempo reale degli agenti sciamanti, coordinata tramite feromoni digitali. Questa simulazione integra la conoscenza delle aree di minaccia, un'analisi cognitiva delle credenze, dei desideri e delle intenzioni dell'agente, un modello della disposizione e dello stato emotivo dell'agente,e la dinamica delle interazioni con l'ambiente. Facendo evolvere gli agenti in questo ricco ambiente, possiamo adattare il loro stato interno al comportamento osservato. Nei wargame realistici, il sistema rileva con successo le emozioni deliberatamente riprodotte e fa previsioni ragionevoli sui comportamenti futuri delle entit\u00e0. BEE pu\u00f2 modellare solo variabili di stato interne che influiscono sul comportamento esterno dell'agente. Non pu\u00f2 adattarsi a variabili che l'agente non manifesta esternamente, poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto del comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto tra il comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto tra il comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.", "keyphrases": ["motivo dell'agente", "comportamento esterno", "stato interno", "prevedere il comportamento dell'agente", "comportamento evoluto ed estrapol", "sistema dinamico non lineare", "obiettivo dell'agente", "emot", "sapore di feromoni", "disporre", "comportamento futuro"]}
{"file_name": "J-9", "text": "Calcolo in un mercato dell'informazione distribuita \u2217 ABSTRACT Secondo la teoria economica \u2013 supportata da prove empiriche e di laboratorio \u2013 il prezzo di equilibrio di un titolo finanziario riflette tutte le informazioni riguardanti il \u200b\u200bvalore del titolo. Indaghiamo il processo computazionale nel percorso verso l'equilibrio, dove le informazioni distribuite tra i trader vengono rivelate passo dopo passo nel tempo e incorporate nel prezzo di mercato. Sviluppiamo un modello semplificato di un mercato dell'informazione, insieme a strategie di trading, al fine di formalizzare le propriet\u00e0 computazionali del processo. Mostriamo che i titoli i cui profitti non possono essere espressi come funzioni di soglia ponderate dei bit di input distribuiti non sono garantiti per convergere al corretto equilibrio previsto dalla teoria economica. D'altro canto, \u00e8 garantita la convergenza dei titoli i cui payoff sono funzioni di soglia, per tutte le distribuzioni di probabilit\u00e0 precedenti. Inoltre, questi valori di soglia convergono al massimo in n round, dove n \u00e8 il numero di bit di informazione distribuita. Dimostriamo anche un limite inferiore, mostrando un tipo di sicurezza a soglia che richiede almeno n/2 round per convergere nel caso peggiore. \u2217 Questo lavoro \u00e8 stato supportato dalla DoD University Research Initiative -LRB- URI -RRB- amministrata dall'Office of Naval Research con la sovvenzione N00014-01-1-0795. \u2020 Supportato in parte dalla sovvenzione ONR N00014-01-0795 e dalle sovvenzioni NSF CCR-0105337, CCR-TC-0208972, ANI-0207399 e ITR-0219018. \u2021 Questo lavoro \u00e8 stato condotto presso NEC Laboratories America, Princeton, NJ. 1. INTRODUZIONE La forma forte dell'ipotesi dei mercati efficienti afferma che i prezzi di mercato incorporano quasi istantaneamente tutte le informazioni disponibili a tutti i trader. Di conseguenza, i prezzi di mercato codificano le migliori previsioni sui risultati futuri sulla base di tutte le informazioni, anche se tali informazioni sono distribuite attraverso molte fonti. Il processo di incorporazione delle informazioni \u00e8, nella sua essenza, un calcolo distribuito. Ogni trader inizia con le proprie informazioni. Man mano che vengono effettuate le negoziazioni, le informazioni sintetiche vengono rivelate attraverso i prezzi di mercato. I trader apprendono o deducono quali informazioni gli altri potrebbero avere osservando i prezzi, quindi aggiornano le proprie convinzioni in base alle loro osservazioni. Nel tempo, se il processo funziona come pubblicizzato, tutte le informazioni vengono rivelate e tutti i trader convergono allo stesso stato informativo. A questo punto, il mercato si trova in quello che viene chiamato equilibrio di aspettative razionali -LSB- 11, 16, 19 -RSB-. Tutte le informazioni a disposizione di tutti i trader si riflettono ora nei prezzi correnti e non sono auspicabili ulteriori operazioni finch\u00e9 non saranno disponibili nuove informazioni. Sebbene la maggior parte dei mercati non siano progettati con l'aggregazione delle informazioni come motivazione primaria, ad esempio i derivati. In questo articolo, indaghiamo la natura del processo computazionale mediante il quale le informazioni distribuite vengono rivelate e combinate nel tempo nei prezzi nei mercati dell'informazione. A tal fine, nella sezione 3,proponiamo un modello di mercato dell'informazione che sia trattabile per l'analisi teorica e, a nostro avviso, catturi gran parte dell'importante essenza dei mercati dell'informazione reali. Dimostriamo che solo i titoli booleani i cui payoff possono essere espressi come funzioni di soglia dei bit di informazione distribuiti in input sono garantiti per convergere come previsto dalla teoria delle aspettative razionali. I titoli booleani con pagamenti pi\u00f9 complessi potrebbero non convergere in alcune distribuzioni precedenti. Forniamo inoltre limiti superiori e inferiori sul tempo di convergenza per questi titoli soglia. Mostriamo che, per tutte le distribuzioni precedenti, il prezzo di un titolo soglia converge al prezzo di equilibrio delle aspettative razionali al massimo in n cicli, dove n \u00e8 il numero di bit di informazione distribuita. Mostriamo che questo limite nel caso peggiore \u00e8 ristretto entro un fattore due, illustrando una situazione in cui un titolo soglia richiede n/2 round per convergere.", "keyphrases": ["teoria economica", "empir e laboratori evid", "prezzo di equilibrio", "finanza sicura", "valore di sicurezza", "processo di calcolo", "percorso verso l\u2019equilibrio", "commerciante", "prezzo di mercato", "modello semplificato", "strategie commerciali", "comput property del processo", "sicuro", "saldare", "funzione di soglia", "distribuzione probabile", "girare", "numero di bit", "distribuire informare", "limite inferiore", "caso peggiore", "informare il mercato"]}
{"file_name": "H-19", "text": "Analisi delle traiettorie delle caratteristiche per il rilevamento degli eventi ABSTRACT Consideriamo il problema dell'analisi delle traiettorie delle parole sia nel dominio del tempo che in quello della frequenza, con l'obiettivo specifico di identificare parole importanti e meno riportate, periodiche e aperiodiche. Un insieme di parole con andamento identico pu\u00f2 essere raggruppato per ricostruire un evento in modo del tutto automatico. La frequenza del documento di ciascuna parola nel tempo viene trattata come una serie temporale, in cui ogni elemento \u00e8 il punteggio frequenza del documento - frequenza inversa del documento -LRB- DFIDF -RRB- in un determinato momento. In questo articolo, abbiamo 1 -RRB- applicato per la prima volta l'analisi spettrale per classificare le caratteristiche di diversi eventi: importanti e meno segnalati, periodici e aperiodici; 2 -RRB- hanno modellato caratteristiche aperiodiche con densit\u00e0 gaussiana e caratteristiche periodiche con densit\u00e0 di miscele gaussiane, e successivamente hanno rilevato l'esplosione di ciascuna caratteristica mediante l'approccio gaussiano troncato; 3 -RRB- ha proposto un algoritmo di rilevamento di eventi greedy non supervisionato per rilevare eventi sia aperiodici che periodici. Tutti i metodi sopra indicati possono essere applicati ai dati delle serie temporali in generale. Abbiamo valutato approfonditamente i nostri metodi sul Reuters News Corpus -LSB- 3 -RSB- a 1 anno e abbiamo dimostrato che erano in grado di scoprire eventi aperiodici e periodici significativi. 1. INTRODUZIONE Ci sono pi\u00f9 di 4.000 fonti di notizie online nel mondo. Monitorarli manualmente tutti per eventi importanti \u00e8 diventato difficile o praticamente impossibile. In effetti, la comunit\u00e0 di rilevamento e tracciamento degli argomenti -LRB- TDT -RRB- ha cercato per molti anni di trovare una soluzione pratica per aiutare le persone a monitorare le notizie in modo efficace. le soluzioni proposte per il rilevamento degli eventi -LSB- 20, 5, 17, 4, 21, 7, 14, 10 -RSB- sono troppo semplicistiche -LRB- basate sulla somiglianza del coseno -LSB- 5 -RSB- -RRB- o poco pratiche a causa alla necessit\u00e0 di mettere a punto un gran numero di parametri -LSB- 9 -RSB-. Pertanto in questo articolo esamineremo le notizie e le tendenze in evidenza dal punto di vista dell'analisi di un segnale verbale in serie temporale. Lavori precedenti come -LSB- 9 -RSB- hanno tentato di ricostruire un evento con le sue caratteristiche rappresentative. Tuttavia, in molti compiti di rilevamento predittivo di eventi -LRB-, ovvero rilevamento retrospettivo di eventi -RRB-, esiste un vasto insieme di caratteristiche potenziali solo per un insieme fisso di osservazioni -LRB-, ovvero i burst evidenti -RRB-. Di queste funzionalit\u00e0, spesso ci si aspetta che solo un piccolo numero sia utile. In particolare, studiamo il nuovo problema dell'analisi delle traiettorie delle caratteristiche per il rilevamento degli eventi, prendendo in prestito una tecnica ben nota dall'elaborazione del segnale: identificare le correlazioni distribuzionali tra tutte le caratteristiche mediante l'analisi spettrale. Per valutare il nostro metodo, proponiamo successivamente un algoritmo di rilevamento di eventi non supervisionati per i flussi di notizie. Figura 1: Correlazione delle caratteristiche -LRB- DFIDF: tempo -RRB- tra a -RRB- Pasqua e aprile b -RRB- Non certificato e terminato. Come esempio illustrativo, si consideri la correlazione tra le parole Pasqua e aprile dal Reuters Corpus '.Dal grafico del loro DFIDF normalizzato nella Figura 1 -LRB- a -RRB-, osserviamo la forte sovrapposizione tra le due parole intorno a 04/1997, il che significa che probabilmente appartengono entrambe allo stesso evento durante quel periodo -LRB- festa di Pasqua -RRB-. In questo esempio, l'evento nascosto della festa di Pasqua \u00e8 un tipico evento aperiodico importante su dati di 1 anno. Un altro esempio \u00e8 fornito dalla Figura 1 -LRB- b -RRB-, dove entrambe le parole Unaudited e Ended `Reuters Corpus sono il set di dati predefinito per tutti gli esempi. mostrano un comportamento simile per periodi di 3 mesi. Queste due parole in realt\u00e0 hanno origine dallo stesso evento periodico, i rapporti sulle perdite di reddito netto, che vengono pubblicati trimestralmente dalle societ\u00e0 quotate in borsa. Altre osservazioni tratte dalla Figura 1 sono: 1 -RRB- il periodo di burst di aprile \u00e8 molto pi\u00f9 lungo di Pasqua, il che suggerisce che aprile possa esistere in altri eventi durante lo stesso periodo; 2 -RRB- Unaudited ha un valore DFIDF medio pi\u00f9 alto rispetto a Ended, il che indica che Unaudited \u00e8 pi\u00f9 rappresentativo dell'evento sottostante. Questi due esempi non sono che la punta dell\u2019iceberg tra tutte le tendenze e le correlazioni di parole nascoste in un flusso di notizie come Reuters. Se un gran numero di essi potesse essere scoperto, ci\u00f2 potrebbe aiutare in modo significativo le attivit\u00e0 TDT. In particolare, indica l'importanza dell'estrazione di caratteristiche di correlazione per il rilevamento di eventi corrispondenti. Per riassumere, postuliamo che: 1 -RRB- Un evento \u00e8 descritto dalle sue caratteristiche rappresentative. Sulla base di queste osservazioni, possiamo estrarre caratteristiche rappresentative dato un evento o rilevare un evento da un elenco di caratteristiche altamente correlate. In questo articolo ci concentreremo su quest'ultimo, ovvero su come le caratteristiche correlate possono essere scoperte per formare un evento in modo non supervisionato. 1.1 Contributi Questo articolo contiene tre contributi principali: 9 Per quanto ne sappiamo, il nostro approccio \u00e8 il primo a categorizzare le caratteristiche delle parole per eventi eterogenei. 9 Proponiamo un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per modellare e rilevare i burst di caratteristiche. 9 Creiamo un algoritmo di rilevamento degli eventi non supervisionati per rilevare eventi sia aperiodici che periodici. Il nostro algoritmo \u00e8 stato valutato su un flusso di notizie reale per dimostrarne l'efficacia. 2. LAVORO CORRELATO Inoltre, la maggior parte della ricerca TDT finora si \u00e8 occupata di raggruppare/classificare documenti in tipi di argomenti, identificare nuove frasi -LSB- 6 -RSB- per nuovi eventi, ecc., senza molta attenzione all'analisi della traiettoria delle parole rispetto al tempo. Swan e Allan -LSB- 18 -RSB- hanno tentato per la prima volta di utilizzare termini co-ricorrenti per costruire un evento. Tuttavia, hanno considerato solo entit\u00e0 nominate e coppie di frasi nominali, senza considerare la loro periodicit\u00e0. Al contrario, il nostro articolo considera tutto quanto sopra. Recentemente, c'\u00e8 stato un notevole interesse nel modellare un evento nei flussi di text come una \"esplosione di attivit\u00e0\" incorporando informazioni temporali. Tuttavia, nessuno dei lavori esistenti identificava specificamente le caratteristiche degli eventi, ad eccezione di Fung et al. -LSB-9 -RSB-,che ha raggruppato le caratteristiche del seno per identificare vari eventi esplosivi. Il nostro lavoro differisce da -LSB- 9 -RSB- in diversi modi: 1 -RRB- analizziamo ogni singola caratteristica, non solo le caratteristiche bursty; 2 -RRB- classifichiamo le caratteristiche lungo due dimensioni categoriche -LRB- periodicit\u00e0 e potenza -RRB-, ottenendo complessivamente cinque tipi di caratteristiche primarie; 3 -RRB- non limitiamo ogni caratteristica ad appartenere esclusivamente ad un solo evento. Le tecniche di analisi spettrale sono state precedentemente utilizzate da Vlachos et al. -LSB- 19 -RSB- per identificare periodicit\u00e0 e burst dai log delle query. Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.", "keyphrases": ["rilevamento eventi", "traiettorie delle parole", "evento di un periodo", "evento del periodo", "segnale verbale", "analisi spettrale", "rilevamento dell'argomento", "traccia dell'argomento", "flusso di text", "nuovo flusso", "serie temporale"]}
{"file_name": "H-3", "text": "Utilizzo dei contesti di query nel recupero delle informazioni ABSTRACT La query dell'utente \u00e8 un elemento che specifica un bisogno di informazione, ma non \u00e8 l'unico. Gli studi in letteratura hanno rilevato molti fattori contestuali che influenzano fortemente l'interpretazione di una query. Studi recenti hanno cercato di considerare gli interessi dell'utente creando un profilo utente. Tuttavia, un singolo profilo per un utente potrebbe non essere sufficiente per una variet\u00e0 di query dell'utente. In questo studio, proponiamo di utilizzare contesti specifici della query anzich\u00e9 quelli incentrati sull'utente, incluso il context attorno alla query e il context all'interno della query. Il primo specifica l'ambiente di una query come il dominio di interesse, mentre il secondo si riferisce alle parole di context all'interno della query, il che \u00e8 particolarmente utile per la selezione delle relazioni tra termini rilevanti. In questo articolo, entrambi i tipi di context sono integrati in un modello IR basato sulla modellazione del linguaggio. I nostri esperimenti su diverse raccolte TREC mostrano che ciascuno dei fattori di context apporta miglioramenti significativi nell'efficacia del recupero. 1. INTRODUZIONE Le interrogazioni, soprattutto quelle brevi, non forniscono una specificazione completa del bisogno informativo. Molti termini rilevanti possono essere assenti dalle query e i termini inclusi potrebbero essere ambigui. Questi problemi sono stati affrontati in un gran numero di studi precedenti. In questi studi, tuttavia, si \u00e8 generalmente assunto che la query sia l'unico elemento disponibile riguardo al bisogno informativo dell'utente. In realt\u00e0, la query \u00e8 sempre formulata in un context di ricerca. Questi fattori includono, tra molti altri, il dominio di interesse dell'utente, la conoscenza, le preferenze, ecc.. Tutti questi elementi specificano le 8. CONCLUSIONI Gli approcci IR tradizionali solitamente considerano la query come l'unico elemento disponibile per il bisogno di informazioni dell'utente. Molti studi precedenti hanno studiato l\u2019integrazione di alcuni fattori contestuali nei modelli IR, tipicamente incorporando un profilo utente. Analogamente ad alcuni studi precedenti, proponiamo di modellare i domini degli argomenti anzich\u00e9 l'utente. Le precedenti indagini sul context si sono concentrate sui fattori relativi alla query. In questo articolo abbiamo dimostrato che anche i fattori all'interno della query sono importanti: aiutano a selezionare le relazioni tra termini appropriati da applicare nell'espansione della query. Abbiamo integrato i fattori contestuali di cui sopra, insieme al modello di feedback, in un unico modello linguistico. I nostri risultati sperimentali confermano fortemente il vantaggio dell\u2019utilizzo di contesti in IR. Questo lavoro mostra anche che il quadro di modellazione del linguaggio \u00e8 appropriato per integrare molti fattori contestuali. Questo lavoro pu\u00f2 essere ulteriormente migliorato su diversi aspetti, inclusi altri metodi per estrarre relazioni tra termini, per integrare pi\u00f9 parole di context nelle condizioni e per identificare domini di query. Sarebbe interessante anche testare il metodo sulla ricerca Web utilizzando la cronologia delle ricerche degli utenti.", "keyphrases": ["profilo utente", "context specifico della query", "incentrato sull'utente", "dominio di interesse", "fattore di context", "parola sens disambigua", "informare il bisogno", "context di ricerca", "conoscenza del dominio", "util della conoscenza generale", "problema dell'ambiguit\u00e0 della conoscenza", "indipendente dal context", "il context informa", "modello di dominio", "soluzione radic", "ricerca persona su google"]}
{"file_name": "C-28", "text": "PackageBLAST: un servizio di griglia multi-policy adattivo per il confronto di sequenze biologiche * ABSTRACT In questo articolo, proponiamo un quadro di allocazione delle attivit\u00e0 adattivo per eseguire ricerche BLAST in un ambiente di griglia rispetto a segmenti di database di sequenze. Il framework, chiamato PackageBLAST, fornisce un'infrastruttura per scegliere o incorporare strategie di allocazione delle attivit\u00e0. Inoltre, proponiamo un meccanismo per calcolare il peso di esecuzione dei nodi della griglia, adattando la politica di allocazione scelta all'attuale potenza computazionale dei nodi. I nostri risultati presentano accelerazioni molto buone e mostrano anche che nessuna singola strategia di allocazione \u00e8 in grado di raggiungere i tempi di esecuzione pi\u00f9 bassi per tutti gli scenari. 1. INTRODUZIONE SW -LSB- 14 -RSB- \u00e8 un algoritmo esatto che trova il miglior allineamento locale tra due sequenze di dimensione n nel tempo e nello spazio quadratici. Per questo motivo sono state proposte euristiche come BLAST -LSB- 3 -RSB- per ridurre i tempi di esecuzione. CLS. Supportato dall'ACM. La pianificazione delle risorse \u00e8 uno dei componenti pi\u00f9 importanti di un sistema a griglia. La scelta delle migliori risorse per una particolare applicazione \u00e8 chiamata task allocation, che \u00e8 un problema NP-Completo. Le applicazioni di rete di solito non hanno velocit\u00e0 di comunicazione elevate e molte di esse seguono il modello master/slave -LSB- 13 -RSB-. Per programmare le applicazioni master/slave sono state proposte molte politiche di allocazione delle attivit\u00e0 come Self Scheduling -LSB- 15 -RSB- e FAC2 -LSB- 8 -RSB-. La scelta della migliore politica di allocazione dipende dal modello di accesso dell'applicazione e dall'ambiente in cui viene eseguita -LSB- 13 -RSB-. In questo articolo proponiamo PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST in griglie composte da database genetici segmentati. PackageBLAST viene eseguito su Globus 3 -LSB- 4 -RSB- e, per ora, fornisce cinque politiche di allocazione. Inoltre, proponiamo un meccanismo adattivo per assegnare pesi ai nodi della griglia, tenendo conto del loro attuale carico di lavoro. Per quanto ne sappiamo, questo \u00e8 il primo servizio grid che esegue BLAST con policy di attivit\u00e0 multiple con un database segmentato in una piattaforma eterogenea non dedicata. Questo documento \u00e8 organizzato come segue. La Sezione 2 presenta il problema del confronto di sequenze e l'algoritmo BLAST. La sezione 3 descrive le politiche di assegnazione per le reti. La sezione 4 discute il lavoro correlato. La sezione 5 presenta la progettazione di PackageBLAST. I risultati sperimentali sono discussi nella sezione 6. La sezione 7 conclude il documento. 4. LAVORI CORRELATI Innanzitutto, il database genetico viene segmentato. Quindi, le query vengono distribuite uniformemente tra i nodi. Se il nodo non dispone di un frammento di database, viene creata una copia locale. Viene proposto un metodo che associa frammenti di dati ai nodi, cercando di minimizzare il numero di copie. BLAST + + -LSB- 10 -RSB- raggruppa pi\u00f9 sequenze per ridurre il numero di accessi al database. Viene utilizzato un approccio master/slave che assegna le query agli slave secondo la politica fissa -LRB- sezione 3.3 -RRB-. Ogni lavoratore esegue BLAST++ in modo indipendente e, infine,i risultati vengono raccolti e combinati dal master. GridBlast -LSB- 9 -RSB- \u00e8 un'applicazione di rete master/slave che utilizza Globus 2. Distribuisce sequenze tra i nodi della rete utilizzando due politiche di allocazione: FCFS e minmax. Tuttavia, per utilizzare minmax, \u00e8 necessario conoscere il tempo di esecuzione totale di ciascun task BLAST. Dopo aver deciso quali sequenze verranno confrontate da ciascun nodo, GridBlast invia le sequenze, i file eseguibili e l'intero database al nodo prescelto. Al termine della ricerca, i risultati vengono compattati e inviati al master. Grid Blast Toolkit -LRB- GBTK -RRB- -LSB- 12 -RSB- \u00e8 un portale web per eseguire ricerche BLAST in Globus 3. Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Quindi verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Quindi verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Successivamente verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.", "keyphrases": ["confronto di sequenze biologiche", "adeguare il servizio di rete multipolitica", "allocazione dei compiti", "ricerca esplosiva", "packageblast", "bioinformatica", "calcolo della griglia", "biologia informatica", "progetto genoma", "segmentare i database genet", "piattaforma eterogenea non dedicata", "ambiente della griglia", "p.s", "il peso del pacco si adatta all'autoprogrammazione"]}
{"file_name": "C-14", "text": "Strategia di implementazione dei sensori per il rilevamento dei target ABSTRACT Per monitorare l'attraversamento del traffico di una regione, i sensori possono essere implementati per eseguire il rilevamento collaborativo dei target. Una rete di sensori di questo tipo raggiunge un certo livello di prestazioni di rilevamento con un costo di implementazione associato. Questo documento affronta questo problema proponendo l'esposizione del percorso come misura della bont\u00e0 di una distribuzione e presenta un approccio per la distribuzione sequenziale in fasi. Dimostra che il costo di implementazione pu\u00f2 essere ridotto al minimo per ottenere le prestazioni di rilevamento desiderate scegliendo opportunamente il numero di sensori distribuiti in ogni fase. 1. INTRODUZIONE Tale rete pu\u00f2 essere utilizzata per monitorare l'ambiente, rilevare, classificare e localizzare eventi specifici e tracciare obiettivi su una regione specifica. L'implementazione delle reti di sensori varia a seconda dell'applicazione considerata. Pu\u00f2 essere predeterminato quando l'ambiente \u00e8 sufficientemente conosciuto e sotto controllo, nel qual caso i sensori possono essere posizionati strategicamente a mano. Questo articolo analizza le strategie di implementazione per le reti di sensori che eseguono il rilevamento dei bersagli in una regione di interesse. Poich\u00e9 le osservazioni locali effettuate dai sensori dipendono dalla loro posizione, le prestazioni dell'algoritmo di rilevamento dipendono dall'implementazione. Una possibile misura della bont\u00e0 dell'implementazione per il rilevamento del bersaglio \u00e8 chiamata esposizione del percorso. \u00c8 una misura della probabilit\u00e0 di rilevare un bersaglio che attraversa la regione utilizzando un determinato percorso. Maggiore \u00e8 l'esposizione del percorso, migliore \u00e8 la distribuzione. L'insieme dei percorsi da considerare pu\u00f2 essere vincolato dall'ambiente. Ad esempio, se si prevede che l'obiettivo segua una strada, \u00e8 necessario considerare solo i percorsi costituiti dalle strade. In questo studio si presuppone che l'implementazione sia casuale, il che corrisponde a molte applicazioni pratiche in cui la regione da monitorare non \u00e8 accessibile per il posizionamento preciso dei sensori. L'obiettivo di questo documento \u00e8 determinare il numero di sensori da implementare per effettuare il rilevamento del bersaglio in una regione di interesse. I compromessi risiedono tra le prestazioni della rete, il costo dei sensori distribuiti e il costo di distribuzione dei sensori. Questo documento \u00e8 organizzato come segue. Nella sezione 2 viene proposta una definizione di esposizione del percorso e viene sviluppato un metodo per valutare l'esposizione di un determinato percorso. Nella sezione 3 viene formulato il problema della distribuzione casuale e vengono presentate diverse soluzioni. Il documento si conclude con la sezione 7. 7. CONCLUSIONE Questo documento affronta il problema dell'implementazione del sensore in una regione da monitorare per l'intrusione del bersaglio. Viene proposto e analizzato un meccanismo per la collaborazione dei sensori per eseguire il rilevamento del bersaglio per valutare l'esposizione dei percorsi attraverso la regione. L'esposizione minima viene utilizzata come misura della bont\u00e0 dello spiegamento, con l'obiettivo di massimizzare l'esposizione del percorso meno esposto nella regione. Nel caso in cui i sensori siano posizionati casualmente in una regione da monitorare,viene sviluppato un meccanismo per l'implementazione sequenziale in fasi. La strategia consiste nell'utilizzare un numero limitato di sensori alla volta fino al raggiungimento dell'esposizione minima desiderata. La funzione di costo utilizzata in questo studio dipende dal numero di sensori distribuiti in ogni fase e dal costo di ciascuna distribuzione. Attraverso la simulazione, la distribuzione dell'esposizione minima ottenuta mediante l'implementazione casuale \u00e8 stata valutata per un numero variabile di sensori distribuiti. Questi risultati sono stati utilizzati per valutare il costo di implementazione per un numero variabile di sensori distribuiti in ciascuna fase. Abbiamo scoperto che il numero ottimale di sensori distribuiti in ogni fase varia in base al costo relativo assegnato alla distribuzione e ai sensori. I risultati di questo studio possono essere estesi a regioni pi\u00f9 grandi con parametri target diversi. La soluzione proposta in questo documento pu\u00f2 anche essere migliorata considerando l'implementazione di un numero variabile di sensori in ogni fase e questo problema a variabili multiple richiede ulteriori indagini.", "keyphrases": ["rilevamento del bersaglio", "rete di sensori", "esposizione del percorso", "numero di sensore", "distribuzione sequenziale", "esposizione minima", "posizionamento casuale del sensore", "campo del sensore", "bersaglio decai"]}
{"file_name": "C-6", "text": "Progettazione e implementazione di un sistema di gestione dei contenuti distribuiti ABSTRACT La convergenza dei progressi nelle tecnologie di archiviazione, codifica e rete ci ha portato in un ambiente in cui enormi quantit\u00e0 di contenuti multimediali continui vengono regolarmente archiviati e scambiati tra dispositivi abilitati alla rete. Tenere traccia di -LRB- o gestire -RRB- tali contenuti rimane impegnativo a causa dell'enorme volume di dati. La memorizzazione di media continui \"dal vivo\" -LRB- come contenuti TV o radio -RRB- aumenta la complessit\u00e0 in quanto questo contenuto non ha un inizio o una fine ben definiti ed \u00e8 quindi complicato da gestire. L'archiviazione in rete consente al contenuto che viene logicamente visualizzato come parte della stessa raccolta di essere effettivamente distribuito su una rete, rendendo il compito di gestione dei contenuti quasi impossibile da gestire senza un sistema di gestione dei contenuti. In questo articolo presentiamo la progettazione e l'implementazione del sistema di gestione dei contenuti Spectrum, che gestisce efficacemente i contenuti multimediali in questo ambiente. Spectrum ha un'architettura modulare che ne consente l'applicazione sia a scenari standalone che a vari scenari di rete. Un aspetto unico di Spectrum \u00e8 che richiede uno o pi\u00f9 criteri di conservazione -LRB- da applicare a ogni contenuto archiviato nel sistema. Ci\u00f2 significa che non esistono politiche di sfratto. Il contenuto a cui non \u00e8 pi\u00f9 applicato un criterio di conservazione viene semplicemente rimosso dal sistema. \u00c8 possibile applicare facilmente diverse politiche di conservazione allo stesso contenuto facilitando cos\u00ec naturalmente la condivisione senza duplicazioni. Questo approccio consente inoltre a Spectrum di applicare facilmente ai contenuti policy basate sul tempo, che rappresentano gli elementi fondamentali necessari per gestire l'archiviazione di contenuti multimediali live continui. Non solo descriviamo i dettagli dell'architettura Spectrum ma forniamo anche casi d'uso tipici. 1. INTRODUZIONE La manipolazione e la gestione dei contenuti \u00e8 ed \u00e8 sempre stata una delle funzioni primarie di un computer. Le applicazioni informatiche iniziali includono formattatori di text e compilatori di programmi. Il contenuto era inizialmente gestito dall'interazione esplicita dell'utente attraverso l'uso di file e filesystem. Con l\u2019avanzare della tecnologia, sia i tipi di contenuti che il modo in cui le persone desiderano utilizzarli sono notevolmente cambiati. Nuovi tipi di contenuto, come i flussi multimediali continui, sono diventati comuni grazie alla convergenza dei progressi nelle tecnologie di archiviazione, codifica e rete. Un altro esempio \u00e8 la combinazione della codifica e della tecnologia di rete a banda larga. Questa combinazione ha consentito agli utenti di accedere e condividere contenuti multimediali sia su reti locali che remote, con la rete stessa che funge da enorme archivio di dati. La proliferazione di contenuti di alta qualit\u00e0 resa possibile da questi progressi nella tecnologia di archiviazione, codifica e rete crea la necessit\u00e0 di nuovi modi per manipolare e gestire i dati.Il focus del nostro lavoro \u00e8 sull'archiviazione di contenuti multimediali ricchi e in particolare sull'archiviazione di contenuti multimediali continui in forme preconfezionate o \"live\". \u2022 Sebbene sia vero per tutti i tipi di contenuto, l'archiviazione di contenuti multimediali continui \u00e8 particolarmente problematica. Innanzitutto, i contenuti multimediali continui sono ancora molto impegnativi in \u200b\u200btermini di risorse di archiviazione, il che significa che un approccio senza policy all'archiviazione non funzioner\u00e0 per tutti tranne che per i sistemi pi\u00f9 piccoli. In secondo luogo, la memorizzazione di contenuti \"dal vivo\" come TV o radio \u00e8 intrinsecamente problematica poich\u00e9 questi segnali sono flussi continui senza punti finali. Ci\u00f2 significa che prima ancora che si possa pensare a gestire tali contenuti \u00e8 necessario astrarli in qualcosa che possa essere manipolato e gestito. . Quando si ha a che fare con supporti continui archiviati, \u00e8 necessario gestire tali contenuti sia a livello granulare che aggregato. Ad esempio, un singolo utente PVR che desidera conservare solo i momenti salienti di un particolare evento sportivo non dovrebbe essere tenuto a dover memorizzare il contenuto relativo all'evento completo. . Come indicato sopra, provare a tenere traccia dei contenuti su un sistema autonomo senza un sistema di gestione dei contenuti \u00e8 molto difficile. Tuttavia, quando i dispositivi di archiviazione effettivi sono distribuiti su una rete, il compito di tenere traccia dei contenuti \u00e8 quasi impossibile. Questo scenario \u00e8 sempre pi\u00f9 comune nei sistemi di distribuzione di contenuti basati sulla rete ed \u00e8 probabile che diventi importante anche negli scenari di reti domestiche. Sembrerebbe chiaro quindi che sia necessario un sistema di gestione dei contenuti in grado di gestire in modo efficiente contenuti multimediali, sfruttando allo stesso tempo la capacit\u00e0 di rete dei dispositivi di archiviazione. Questo sistema dovrebbe consentire un'archiviazione efficiente e l'accesso ai contenuti su dispositivi di archiviazione di rete eterogenei in base alle preferenze dell'utente. Il sistema di gestione dei contenuti dovrebbe tradurre le preferenze dell'utente in appropriate politiche di archiviazione di basso livello e dovrebbe consentire che tali preferenze siano espresse ad un livello fine di granularit\u00e0 -LRB- senza richiederlo in generale -RRB-. Il sistema di gestione dei contenuti dovrebbe consentire all'utente di manipolare e ragionare su -LRB- ovvero modificare la politica di archiviazione associata a -RRB- l'archiviazione di parti -LRB- di contenuti multimediali continui -RRB-. Affrontare questo problema di gestione dei contenuti distribuiti \u00e8 difficile a causa del numero di requisiti imposti al sistema. Per esempio :. Il sistema di gestione dei contenuti deve operare su un gran numero di sistemi eterogenei. In alcuni casi il sistema potrebbe gestire il contenuto archiviato su un file system locale, mentre in altri il contenuto potrebbe essere archiviato su un dispositivo di archiviazione di rete separato. Il gestore dei contenuti potrebbe essere responsabile dell'implementazione delle policy utilizzate per fare riferimento al contenuto oppure tale ruolo potrebbe essere delegato a un computer separato. Affinch\u00e9 il sistema di gestione dei contenuti possa fornire un'interfaccia uniforme sono necessari un'interfaccia del programma applicativo -LRB- API -RRB- e i protocolli di rete associati. .Il sistema di gestione dei contenuti dovrebbe essere flessibile ed essere in grado di gestire requisiti diversi per le politiche di gestione dei contenuti. Queste politiche riflettono quali contenuti dovrebbero essere ottenuti, quando dovrebbero essere recuperati, per quanto tempo dovrebbero essere conservati e in quali circostanze dovrebbero essere scartati. Ci\u00f2 significa che il sistema di gestione dei contenuti dovrebbe consentire a pi\u00f9 applicazioni di fare riferimento ai contenuti con un ricco insieme di policy e che tutti dovrebbero funzionare insieme senza problemi. . Il sistema di gestione dei contenuti deve essere in grado di monitorare i riferimenti ai contenuti e utilizzare tali informazioni per collocare i contenuti nella giusta posizione nella rete per un accesso efficiente alle applicazioni. . Il sistema di gestione dei contenuti deve gestire l'interazione tra la popolazione implicita ed esplicita di contenuti ai margini della rete. . Il sistema di contenuti deve essere in grado di gestire in modo efficiente grandi insiemi di contenuti, inclusi flussi continui. Deve essere in grado di impacchettare questo contenuto in modo tale che sia conveniente per gli utenti accedervi. Per affrontare questi problemi abbiamo progettato e implementato l'architettura del sistema di gestione dei contenuti Spectrum. Consente a pi\u00f9 applicazioni di fare riferimento al contenuto utilizzando policy diverse. Si noti che l'architettura Spectrum presuppone l'esistenza di una rete di distribuzione dei contenuti -LRB- CDN -RRB- che pu\u00f2 facilitare la distribuzione efficiente dei contenuti -LRB-, ad esempio l'architettura PRISM CDN -LSB- 2 -RSB- -RRB-. La sezione 2 descrive l'architettura del nostro sistema di gestione dei contenuti. Nella Sezione 3 descriviamo sia la nostra implementazione dell'architettura Spectrum sia esempi del suo utilizzo. 4. LAVORI CORRELATI Diversi autori hanno affrontato il problema della gestione dei contenuti nelle reti distribuite. Gran parte del lavoro si concentra sull\u2019aspetto della gestione delle politiche. Ad esempio in -LSB- 5 -RSB- viene considerato il problema di servire contenuti multimediali tramite server distribuiti. Il contenuto viene distribuito tra le risorse del server in proporzione alla domanda dell'utente utilizzando un protocollo di diffusione della domanda. Le prestazioni del sistema vengono valutate tramite simulazione. In -LSB- 1 -RSB- il contenuto \u00e8 distribuito tra le sottocache. La base di conoscenza di Cache consente l'impiego di policy sofisticate. La simulazione viene utilizzata per confrontare lo schema proposto con algoritmi di sostituzione noti. Il nostro lavoro differisce in quanto prendiamo in considerazione qualcosa di pi\u00f9 degli aspetti di gestione politica del problema. Dopo aver considerato attentamente le funzionalit\u00e0 richieste per implementare la gestione dei contenuti nell'ambiente di rete, abbiamo suddiviso il sistema in tre semplici funzioni, vale a dire Content manager, Policy manager e Storage manager. Ci\u00f2 ci ha permesso di implementare e sperimentare facilmente un sistema prototipo. Altro lavoro correlato riguarda i cosiddetti sistemi di raccomandazione TV che vengono utilizzati nei PVR per selezionare automaticamente i contenuti per gli utenti, ad esempio -LSB- 6 -RSB-. Infine, nei fornitori di ambienti CDN commerciali -LRB- ad esCisco e Netapp -RRB- hanno sviluppato e implementato prodotti e strumenti per la gestione dei contenuti. 5. CONCLUSIONE E LAVORO FUTURO In questo articolo abbiamo presentato la progettazione e l'implementazione dell'architettura di gestione dei contenuti Spectrum. Spectrum consente di applicare policy di archiviazione a grandi volumi di contenuti per facilitare un'archiviazione efficiente. Nello specifico, il sistema consente di applicare policy diverse allo stesso contenuto senza replica. Spectrum pu\u00f2 anche applicare politiche \"time-aware\" che gestiscono efficacemente l'archiviazione di contenuti multimediali continui. Infine, il design modulare dell'architettura Spectrum consente realizzazioni sia stand-alone che distribuite in modo che il sistema possa essere implementato in una variet\u00e0 di applicazioni. Ci sono una serie di questioni aperte che richiederanno lavoro futuro. Alcuni di questi problemi includono: \u2022 Prevediamo che Spectrum sia in grado di gestire i contenuti su sistemi che vanno dai grandi CDN fino agli apparecchi pi\u00f9 piccoli come TiVO -LSB-8 -RSB-. Affinch\u00e9 questi sistemi pi\u00f9 piccoli possano supportare Spectrum, richiederanno una rete e un'API esterna. Quando l'API sar\u00e0 disponibile, dovremo capire come inserirla nell'architettura Spectrum. \u2022 Spectrum nomina i contenuti in base all'URL, ma non abbiamo intenzionalmente definito il formato degli URL Spectrum, il modo in cui vengono ricondotti al nome effettivo del contenuto o il modo in cui i nomi e gli URL devono essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.come si associano al nome effettivo del contenuto o come i nomi e gli URL dovrebbero essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.come si associano al nome effettivo del contenuto o come i nomi e gli URL dovrebbero essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.", "keyphrases": ["sistema di gestione dei contenuti dello spettro", "archiviazione multimediale continua", "scenario della rete domestica", "interfaccia del programma applicativo", "rete di distribuzione dei contenuti", "localizzazione uniforme delle risorse", "gestione politica", "abilitazione rete DVR", "sistema di database ad alte prestazioni", "gestione dello spettro di livello carrier"]}
{"file_name": "H-11", "text": "Progettazione ottimale laplaciana per il recupero di immagini ABSTRACT Il feedback sulla pertinenza \u00e8 una tecnica potente per migliorare le prestazioni del recupero di immagini basato sui contenuti -LRB- CBIR -RRB-. Sollecita i giudizi di pertinenza dell'utente sulle immagini recuperate restituite dai sistemi CBIR. L'etichettatura dell'utente viene quindi utilizzata per apprendere un classificatore per distinguere tra immagini rilevanti e irrilevanti. Tuttavia, le immagini restituite pi\u00f9 frequentemente potrebbero non essere quelle pi\u00f9 informative. La sfida \u00e8 quindi determinare quali immagini senza etichetta sarebbero le pi\u00f9 informative -LRB-, ovvero migliorerebbero maggiormente il classificatore -RRB- se fossero etichettate e utilizzate come campioni di addestramento. In questo articolo, proponiamo un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design -LRB- LOD -RRB-, per il recupero di immagini con feedback di pertinenza. Il nostro algoritmo si basa su un modello di regressione che minimizza l'errore minimo quadrato sulle immagini misurate -LRB- o etichettate -RRB- e contemporaneamente preserva la struttura geometrica locale dello spazio dell'immagine. Nello specifico, assumiamo che se due immagini sono sufficientemente vicine l'una all'altra, anche le loro misurazioni -LRB- o le etichette -RRB- sono vicine. Costruendo un grafo del vicino pi\u00f9 vicino, la struttura geometrica dello spazio dell'immagine pu\u00f2 essere descritta dal grafo laplaciano. Discuteremo di come i risultati del campo del disegno sperimentale ottimale possano essere utilizzati per guidare la nostra selezione di un sottoinsieme di immagini, che ci fornisce la maggior quantit\u00e0 di informazioni. I risultati sperimentali sul database Corel suggeriscono che l'approccio proposto raggiunge una maggiore precisione nel recupero delle immagini con feedback di pertinenza. 1. INTRODUZIONE In molte attivit\u00e0 di machine learning e di recupero delle informazioni, non mancano i dati senza etichetta, ma le etichette sono costose. La sfida \u00e8 quindi quella di determinare quali campioni senza etichetta sarebbero i pi\u00f9 informativi -LRB-, vale a dire, migliorerebbero maggiormente il classificatore -RRB- se fossero etichettati e utilizzati come campioni di addestramento. Questo problema \u00e8 tipicamente chiamato apprendimento attivo -LSB- 4 -RSB-. Molte applicazioni del mondo reale possono essere inserite in un framework di apprendimento attivo. In particolare, consideriamo il problema del feedback di pertinenza guidato dal Content-Based Image Retrieval -LRB- CBIR -RRB- -LSB- 13 -RSB-. Il recupero di immagini basato sui contenuti ha attirato notevoli interessi nell'ultimo decennio -LSB- 13 -RSB-. Ci\u00f2 \u00e8 motivato dalla rapida crescita dei database di immagini digitali che, a loro volta, richiedono schemi di ricerca efficienti. Invece di descrivere un'immagine utilizzando il text, in questi sistemi una query di immagine viene descritta utilizzando una o pi\u00f9 immagini di esempio. Le caratteristiche visive di basso livello -LRB- colore, trama, forma, ecc. -RRB- vengono estratte automaticamente per rappresentare le immagini. Per ridurre il divario semantico, nel CBIR -LSB- 12 -RSB- viene introdotto il feedback sulla pertinenza. In molti degli attuali sistemi CBIR basati sul feedback di pertinenza, all'utente \u00e8 richiesto di fornire i propri giudizi di pertinenza sulle immagini principali restituite dal sistema.Le immagini etichettate vengono quindi utilizzate per addestrare un classificatore a separare le immagini che corrispondono al concetto di query da quelle che non lo fanno. Tuttavia, in generale le immagini restituite pi\u00f9 frequentemente potrebbero non essere quelle pi\u00f9 informative. Nel peggiore dei casi, tutte le immagini migliori etichettate dall'utente potrebbero essere positive e quindi le tecniche di classificazione standard non possono essere applicate a causa della mancanza di esempi negativi. A differenza dei problemi di classificazione standard in cui i campioni etichettati vengono preassegnati, nel recupero delle immagini con feedback di pertinenza il sistema pu\u00f2 selezionare attivamente le immagini da etichettare. Pertanto l'apprendimento attivo pu\u00f2 essere introdotto naturalmente nel recupero delle immagini. Nonostante molte tecniche di apprendimento attivo esistenti, Support Vector Machine -LRB- SVM -RRB- apprendimento attivo -LSB- 14 -RSB- e apprendimento attivo basato sulla regressione -LSB- 1 -RSB- hanno ricevuto il maggior interesse. Lo svantaggio principale dell\u2019apprendimento attivo SVM \u00e8 che il limite stimato potrebbe non essere sufficientemente accurato. Inoltre, potrebbe non essere applicato all'inizio del recupero quando non sono presenti immagini etichettate. Alcuni altri algoritmi di apprendimento attivo basati su SVM possono essere trovati in -LSB- 7 -RSB-, -LSB- 9 -RSB-. In statistica, il problema della selezione dei campioni da etichettare viene generalmente definito disegno sperimentale. Il campione x viene definito esperimento e la sua etichetta y viene definita misurazione. Lo studio del disegno sperimentale ottimale -LRB- OED -RRB- -LSB- 1 -RSB- riguarda la progettazione di esperimenti che dovrebbero minimizzare le varianze di un modello parametrizzato. L'intento di un disegno sperimentale ottimale \u00e8 solitamente quello di massimizzare la fiducia in un dato modello, minimizzare le varianze dei parametri per l'identificazione del sistema o minimizzare la varianza dell'output del modello. Gli approcci classici alla progettazione sperimentale includono A-Optimal Design, D-Optimal Design ed E-Optimal Design. Tutti questi approcci si basano su un modello di regressione dei minimi quadrati. Rispetto agli algoritmi di apprendimento attivo basati su SVM, gli approcci di progettazione sperimentale sono molto pi\u00f9 efficienti nel calcolo. Tuttavia, questo tipo di approccio prende in considerazione solo i dati misurati -LRB- o etichettati -RRB- nella loro funzione obiettivo, mentre i dati non misurati -LRB- o non etichettati -RRB- vengono ignorati. Approfittando dei recenti progressi sulla progettazione sperimentale ottimale e sull'apprendimento semi-supervisionato, in questo articolo proponiamo un nuovo algoritmo di apprendimento attivo per il recupero di immagini, chiamato Laplacian Optimal Design -LRB- LOD -RRB-. A differenza dei tradizionali metodi di progettazione sperimentale le cui funzioni di perdita sono definite solo sui punti misurati, la funzione di perdita del nostro algoritmo LOD proposto \u00e8 definita sia sui punti misurati che su quelli non misurati. Nello specifico, introduciamo un regolarizzatore che preserva la localit\u00e0 nella funzione di perdita standard basata sull'errore quadrato. La nuova funzione di perdita mira a trovare un classificatore che sia localmente il pi\u00f9 agevole possibile. In altre parole, se due punti sono sufficientemente vicini tra loro nello spazio di input, ci si aspetta che condividano la stessa etichetta.Una volta definita la funzione di perdita, possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sul disegno sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.confrontiamo il nostro algoritmo con gli algoritmi pi\u00f9 avanzati e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sul disegno sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.confrontiamo il nostro algoritmo con gli algoritmi pi\u00f9 avanzati e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.", "keyphrases": ["feedback rilevante", "l'immagine rappresenta", "recupero immagini contentbas", "apprendimento attivo", "modello di regresso dei minimi quadrati", "progettazione ottimale dell'esperimento", "immagine ritorno in alto", "tasso preciso", "struttura geometrica intrinseca", "riconoscimento del patten", "etichetta"]}
{"file_name": "J-27", "text": "Imparare dalle preferenze rivelate ABSTRACT Una sequenza di prezzi e domande \u00e8 razionalizzabile se esiste una funzione di utilit\u00e0 concava, continua e monotona tale che le domande massimizzano la funzione di utilit\u00e0 sull'insieme di bilancio corrispondente al prezzo. Afriat -LSB- 1 -RSB- presentava le condizioni necessarie e sufficienti affinch\u00e9 una sequenza finita fosse razionalizzabile. Varian -LSB- 20 -RSB- e successivamente Blundell et al. -LSB- 3, 4 -RSB- ha continuato questa linea di lavoro studiando metodi non parametrici per prevedere la domanda. I loro risultati caratterizzano essenzialmente l'apprendimento di classi degenerate di funzioni di domanda e quindi non riescono a fornire un grado generale di fiducia nelle previsioni. Il presente articolo integra questa linea di ricerca introducendo un modello statistico e una misura di complessit\u00e0 attraverso i quali siamo in grado di studiare l\u2019apprendibilit\u00e0 di classi di funzioni di domanda e ricavare un grado di fiducia nelle previsioni. I nostri risultati mostrano che la classe di tutte le funzioni di domanda ha una complessit\u00e0 illimitata e quindi non \u00e8 apprendibile, ma che esistono classi interessanti e potenzialmente utili che possono essere apprese da campioni finiti. Presentiamo anche un algoritmo di apprendimento che \u00e8 un adattamento di una nuova dimostrazione del teorema di Afriat dovuta a Teo e Vohra -LSB- 17 -RSB-. 1. INTRODUZIONE La relazione di preferenza \u00e8 quindi il fattore chiave per comprendere il comportamento dei consumatori. Uno dei presupposti comuni in questa teoria \u00e8 che la relazione di preferenza sia rappresentata da una funzione di utilit\u00e0 e che gli agenti si sforzino di massimizzare la propria utilit\u00e0 dato un vincolo di bilancio. Questo modello di comportamento \u00e8 l\u2019essenza della domanda e dell\u2019offerta, degli equilibri generali e di altri aspetti della teoria del consumatore. Inoltre, come spiegato nella sezione 2, le osservazioni di base sul comportamento della domanda di mercato suggeriscono che le funzioni di utilit\u00e0 sono monotone e concave. Questo ci porta alla domanda, sollevata per la prima volta da Samuelson -LSB- 18 -RSB-, fino a che punto questa teoria \u00e8 confutabile? Date le osservazioni del prezzo e della domanda, in quali circostanze possiamo concludere che i dati sono coerenti con il comportamento di un agente che massimizza l\u2019utilit\u00e0 dotato di una funzione di utilit\u00e0 concava monotona e soggetto a un vincolo di bilancio? Samuelson ha posto una condizione necessaria ma insufficiente sulla preferenza sottostante nota come assioma debole della preferenza rivelata. Uzawa -LSB- 16 -RSB- e Mas-Colell -LSB- 10, 11 -RSB- hanno introdotto la nozione di reddito-Lipschitz e hanno dimostrato che le funzioni di domanda con questa propriet\u00e0 sono razionalizzabili. Queste propriet\u00e0 non richiedono alcuna ipotesi parametrica e sono tecnicamente confutabili, ma presuppongono la conoscenza dell\u2019intera funzione di domanda e fanno molto affidamento sulle propriet\u00e0 differenziali delle funzioni di domanda. Pertanto, per confutare la teoria \u00e8 necessaria una quantit\u00e0 infinita di informazioni. Spesso accade che, oltre alle osservazioni sulla domanda, ci siano informazioni aggiuntive sul sistema ed \u00e8 sensato fare ipotesi parametriche, vale a dire:stipulare una forma funzionale di utilit\u00e0. La coerenza con la massimizzazione dell\u2019utilit\u00e0 dipenderebbe quindi dalla fissazione dei parametri della funzione di utilit\u00e0 in modo che siano coerenti con le osservazioni e con un insieme di equazioni chiamate equazioni di Slutski. Se tali parametri esistono, concludiamo che la forma di utilit\u00e0 stipulata \u00e8 coerente con le osservazioni. Questo approccio \u00e8 utile quando vi \u00e8 motivo di fare tali stipulazioni, poich\u00e9 fornisce una funzione di utilit\u00e0 esplicita che pu\u00f2 essere utilizzata per fare previsioni precise sulla domanda per prezzi non osservati. Lo svantaggio di questo approccio \u00e8 che i dati della vita reale spesso non sono coerenti con le forme funzionali convenienti. Inoltre, se le osservazioni sono incoerenti, non \u00e8 chiaro se si tratti di una confutazione della forma funzionale stipulata o della massimizzazione dell'utilit\u00e0. Si chiede quando si potr\u00e0 determinare che un insieme finito di osservazioni \u00e8 coerente con la massimizzazione dell'utilit\u00e0 senza fare ipotesi parametriche? Egli mostra che la razionalizzabilit\u00e0 di un insieme finito di osservazioni equivale all'assioma forte della preferenza rivelata. Richter -LSB- 15 -RSB- mostra che l'assioma forte della preferenza rivelata equivale alla razionalizzabilit\u00e0 mediante una funzione di utilit\u00e0 monotona strettamente concava. Afriat -LSB- 1 -RSB- fornisce un altro insieme di condizioni di razionalizzabilit\u00e0 che le osservazioni devono soddisfare. Varian -LSB- 20 -RSB- introduce l'assioma generalizzato della preferenza rivelata -LRB- GARP -RRB-, una forma equivalente della condizione di coerenza di Afriat che \u00e8 pi\u00f9 facile da verificare computazionalmente. Afriat -LSB- 1 -RSB- ha dimostrato il suo teorema costruendo esplicita una funzione di utilit\u00e0 che testimonia la consistenza. Varian -LSB- 20 -RSB- ha compiuto un ulteriore passo avanti passando dalla coerenza alla previsione. L'algoritmo di previsione di Varian esclude sostanzialmente i bundle che si rivelano inferiori ai bundle osservati e trova un bundle dall'insieme rimanente che, insieme alle osservazioni, \u00e8 coerente con GARP. Inoltre, introduce la \"metrica monetaria\" di Samuelson come una funzione di utilit\u00e0 canonica e fornisce funzioni di utilit\u00e0 dell'inviluppo superiore e inferiore per la metrica monetaria. Knoblauch -LSB- 9 -RSB- mostra che questi inviluppi possono essere calcolati in modo efficiente. Un approccio diverso \u00e8 presentato da Blundell et al. -LSB- 3, 4 -RSB-. Questi documenti introducono un modello in cui un agente osserva i prezzi e le curve di Engel per questi prezzi. Ci\u00f2 fornisce un miglioramento rispetto ai limiti originali di Varian, sebbene l'idea di base sia ancora quella di escludere richieste che si rivelano inferiori. Questo modello \u00e8 in un certo senso un ibrido tra gli approcci di Mas-Colell e di Afriat. Il primo richiede informazioni complete per tutti i prezzi, il secondo per un numero finito di prezzi. D'altra parte l'approccio adottato da Blundell et al. richiede informazioni complete solo su un numero finito di traiettorie dei prezzi. Differenti segmenti della popolazione affrontano gli stessi prezzi con budget differenti, e per quanto i dati aggregati possano testimoniare sulle preferenze individuali,mostrare come la domanda varia con il budget. Applicando metodi statistici non parametrici, ricostruiscono una traiettoria dalle richieste osservate di diversi segmenti e la utilizzano per ottenere limiti pi\u00f9 stretti. Entrambi questi metodi fornirebbero molto probabilmente una buona previsione per una funzione di domanda fissa dopo un numero sufficiente di osservazioni, presupponendo che siano distribuiti in modo ragionevole. Tuttavia, questi metodi non considerano la complessit\u00e0 delle funzioni di domanda e non utilizzano alcun modello probabilistico delle osservazioni. Pertanto, non sono in grado di fornire alcuna stima del numero di osservazioni sufficienti per una buona previsione o del grado di fiducia in tale previsione. In questo articolo esaminiamo la fattibilit\u00e0 della previsione della domanda con un alto grado di confidenza utilizzando le condizioni di Afriat. Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.", "keyphrases": ["imparare da rivelare preferire", "problema complesso", "previsione", "probabile approssimazione corretta", "funzione util monoton concav", "funzione della domanda", "razionalizzare", "insieme finito di osservaz", "incom-lipschitz", "dimensioni della frantumazione del grasso"]}
{"file_name": "C-18", "text": "Un'analisi iniziale e una presentazione del malware che presenta un comportamento simile a uno sciame ABSTRACT \u00c8 stato osservato che Slammer, attualmente il worm informatico pi\u00f9 veloce mai registrato nella storia, infetta il 90% di tutti gli host Internet vulnerabili entro 10 minuti. Sebbene l'azione principale intrapresa dal worm Slammer sia una replica relativamente poco sofisticata di se stesso, si diffonde comunque cos\u00ec rapidamente che la risposta umana \u00e8 risultata inefficace. La maggior parte delle strategie di contromisure proposte si basano principalmente su algoritmi di rilevamento e limitazione della velocit\u00e0. Tuttavia, tali strategie vengono progettate e sviluppate per contenere efficacemente i worm i cui comportamenti sono simili a quelli di Slammer. Nel nostro lavoro avanziamo l\u2019ipotesi che i vermi della prossima generazione saranno radicalmente diversi e che potenzialmente tali tecniche si riveleranno inefficaci. Nello specifico, si propone di studiare una nuova generazione di worm denominata ''Swarm Worms'', il cui comportamento si basa sul concetto di ''intelligenza emergente''. L\u2019intelligenza emergente \u00e8 il comportamento dei sistemi, molto simile ai sistemi biologici come le formiche o le api, dove semplici interazioni locali di membri autonomi, con semplici azioni primitive, danno origine a comportamenti globali complessi e intelligenti. In questo manoscritto introdurremo i principi di base dietro l'idea di ''Swarm Worms'', nonch\u00e9 la struttura di base richiesta per essere considerato uno ''Swarm Worms''. Inoltre, presenteremo i risultati preliminari sulle velocit\u00e0 di propagazione di uno di questi vermi sciami, chiamato verme ZachiK. Mostreremo che ZachiK \u00e8 in grado di propagarsi a una velocit\u00e0 2 ordini di grandezza pi\u00f9 veloce rispetto a vermi simili senza capacit\u00e0 di sciame. 1. INTRODUZIONE E LAVORO PRECEDENTE Nelle prime ore del mattino -LRB- 05:30 GMT -RRB- del 25 gennaio 2003 il worm informatico pi\u00f9 veloce mai registrato nella storia inizi\u00f2 a diffondersi su Internet. Dopo Slammer, i ricercatori hanno esplorato i comportamenti dei worm a rapida diffusione e hanno progettato strategie di contromisure basate principalmente sul rilevamento della velocit\u00e0 e su algoritmi di limitazione. Ad esempio, Zou et al., -LSB- 2 -RSB-, hanno proposto uno schema in cui viene utilizzato un filtro di Kalman per rilevare la propagazione precoce di un verme. Cio\u00e8, vengono progettati e sviluppati sistemi per contenere efficacemente worm i cui comportamenti sono simili a quelli di Slammer. Nel lavoro qui descritto, avanziamo l'ipotesi che i worm della prossima generazione saranno diversi e pertanto tali tecniche potrebbero presentare alcune limitazioni significative. Nello specifico, si propone di studiare una nuova generazione di worm denominata ''Swarm Worms'', il cui comportamento si basa sul concetto di ''intelligenza emergente''. Il concetto di intelligenza emergente \u00e8 stato studiato per la prima volta in associazione con i sistemi biologici. In tali studi, i primi ricercatori hanno scoperto una variet\u00e0 di comportamenti interessanti di insetti o animali in natura. Uno stormo di uccelli solca il cielo. Generalmente,questo tipo di movimento aggregato \u00e8 stato chiamato \"comportamento dello sciame\". Biologi e scienziati informatici nel campo dell'intelligenza artificiale hanno studiato tali sciami biologici e hanno tentato di creare modelli che spieghino come gli elementi di uno sciame interagiscono, raggiungono obiettivi ed evolvono. I concetti di base che sono stati sviluppati negli ultimi dieci anni per spiegare gli \u201csciami e il comportamento degli sciami\u201d comprendono quattro componenti fondamentali. Questi sono: 1. Semplicit\u00e0 di logica e di azioni: uno sciame \u00e8 composto da N agenti la cui intelligenza \u00e8 limitata. Gli agenti dello sciame utilizzano semplici regole locali per governare le loro azioni. Alcuni modelli chiamano queste azioni o comportamenti primitivi; 2. Meccanismi di comunicazione locale: gli agenti interagiscono con gli altri membri dello sciame tramite semplici meccanismi di comunicazione \"locale\". Ad esempio, un uccello in uno stormo rileva la posizione dell'uccello adiacente e applica una semplice regola di evitamento e di inseguimento. 3.4. \u201cIntelligenza emergente\u201d: il comportamento aggregato di agenti autonomi risulta in comportamenti \u201cintelligenti\u201d complessi; compresa l\u2019autorganizzazione\u2019\u2019. Per comprendere appieno il comportamento di tali sciami \u00e8 necessario costruire un modello che spieghi il comportamento di quelli che chiameremo vermi generici. Questo modello, che estende il lavoro di Weaver -LSB- 5 -RSB- \u00e8 presentato qui nella sezione 2. Inoltre, intendiamo estendere detto modello in modo tale che spieghi chiaramente i comportamenti di questa nuova classe di vermi potenzialmente pericolosi chiamati vermi dello sciame. I vermi sciame si comportano in modo molto simile agli sciami biologici e mostrano un alto grado di apprendimento, comunicazione e intelligenza distribuita. Tali vermi dello sciame sono potenzialmente pi\u00f9 dannosi delle loro controparti generiche simili. Nello specifico, \u00e8 stato creato il primo esempio, a nostra conoscenza, di un simile worm di apprendimento, chiamato ZachiK. ZachiK \u00e8 un semplice worm sciame che cracka password e incorpora diverse strategie di apprendimento e condivisione delle informazioni. Uno sciame di worm di questo tipo \u00e8 stato implementato sia in una rete locale di trenta host -LRB- 30 -RRB-, sia simulato in una topologia da 10.000 nodi. I risultati preliminari hanno mostrato che tali worm sono in grado di compromettere gli ospiti a velocit\u00e0 fino a due ordini di grandezza pi\u00f9 veloci rispetto alla loro controparte generica. Il resto di questo manoscritto \u00e8 strutturato come segue. Nella sezione 2 viene presentato un modello astratto sia dei vermi generici che dei vermi sciame. Questo modello viene utilizzato nella sezione 2.6 per descrivere la prima istanza di un verme sciame, ZachiK. Nella sezione 4 vengono presentati i risultati preliminari ottenuti sia tramite misurazioni empiriche che tramite simulazione. Infine, nella sezione 5 vengono presentate le nostre conclusioni e approfondimenti sul lavoro futuro. 5. SOMMARIO E LAVORO FUTURO In questo manoscritto abbiamo presentato un modello astratto, simile per alcuni aspetti a quello di Weaver -LSB- 5 -RSB-, che aiuta a spiegare la natura generica dei vermi.Il modello presentato nella sezione 2 \u00e8 stato esteso per incorporare una nuova classe di vermi potenzialmente pericolosi chiamati Swarm Worms. I vermi sciame si comportano in modo molto simile agli sciami biologici e mostrano un alto grado di apprendimento, comunicazione e intelligenza distribuita. Tali vermi dello sciame sono potenzialmente pi\u00f9 dannosi delle loro controparti generiche. Inoltre, per quanto ne sappiamo, \u00e8 stato creato il primo esempio di un simile worm di apprendimento, chiamato ZachiK. ZachiK \u00e8 un semplice worm sciame che cracka password e incorpora diverse strategie di apprendimento e condivisione delle informazioni. Uno sciame di worm di questo tipo \u00e8 stato implementato sia in una rete locale di trenta host -LRB- 30 -RRB-, sia simulato in una topologia da 10.000 nodi. I risultati preliminari hanno mostrato che tali worm sono in grado di compromettere gli host a una velocit\u00e0 fino a 2 ordini di grandezza pi\u00f9 veloce rispetto alla loro controparte generica, pur mantenendo capacit\u00e0 invisibili. Questo lavoro apre una nuova area di problemi interessanti. Alcuni dei problemi pi\u00f9 interessanti e urgenti da considerare sono i seguenti: \u2022 \u00c8 possibile applicare alcuni dei concetti di apprendimento sviluppati negli ultimi dieci anni nelle aree dell'intelligenza di sciame, dei sistemi di agenti e del controllo distribuito alla progettazione di sofisticati sistemi di sciame? vermi in modo tale da provocare un vero comportamento emergente? \u2022 Le attuali tecniche in fase di sviluppo nella progettazione di sistemi di rilevamento e contromisura delle intrusioni e di sistemi sopravvissuti sono efficaci contro questa nuova classe di worm? ; e \u2022 Quali tecniche, se ce ne sono, possono essere sviluppate per creare difese contro gli sciami di vermi?si pu\u00f2 sviluppare per creare difese contro gli sciami di vermi?si pu\u00f2 sviluppare per creare difese contro gli sciami di vermi?", "keyphrases": ["malwar", "verme dello sciame", "intelligenza emergente", "verme sbattitore", "meccanico comune locale", "zachik", "metodo prng", "elenco target pre-generazione", "distribuire l'intelligenza", "rilevamento delle intrusioni", "sistema di contromisure"]}
{"file_name": "J-26", "text": "Agenzia combinatoria ABSTRACT Molte ricerche recenti riguardano sistemi, come Internet, i cui componenti sono posseduti e gestiti da parti diverse, ciascuno con il proprio obiettivo \"egoistico\". Il campo dell'Algorithmic Mechanism Design gestisce la questione delle informazioni private detenute dalle diverse parti in tali contesti computazionali. Questo articolo affronta un problema complementare in tali contesti: la gestione delle \"azioni nascoste\" eseguite dalle diverse parti. Il nostro modello \u00e8 una variante combinatoria del classico problema dell\u2019agente principale della teoria economica. Nel nostro context un principale deve motivare una squadra di agenti strategici a compiere sforzi costosi per suo conto, ma le loro azioni gli sono nascoste. La nostra attenzione si concentra sui casi in cui combinazioni complesse degli sforzi degli agenti influenzano il risultato. Il principale motiva gli agenti offrendo loro una serie di contratti, che insieme pongono gli agenti in un punto di equilibrio del gioco indotto. Presentiamo modelli formali per questo context, suggeriamo e intraprendiamo un'analisi di alcune questioni fondamentali, ma lasciamo aperte molte domande. 1. INTRODUZIONE 1.1 Context Una delle caratteristiche pi\u00f9 sorprendenti delle moderne reti di computer - in particolare Internet - \u00e8 che le diverse parti di esse sono possedute e gestite da diversi individui, aziende e organizzazioni. L'analisi e la progettazione di protocolli per questo ambiente devono quindi naturalmente tenere conto dei diversi interessi economici \"egoistici\" dei diversi partecipanti. In particolare, il campo della progettazione di meccanismi algoritmici -LSB- 6 -RSB- utilizza incentivi adeguati per \"estrarre\" le informazioni private dai partecipanti. Questo articolo affronta la mancanza di conoscenza complementare, quella delle azioni nascoste. In molti casi i comportamenti effettivi \u2013 le azioni \u2013 dei diversi partecipanti sono \u201cnascosti\u201d agli altri e influenzano il risultato finale solo indirettamente. Come possiamo garantire che la giusta combinazione di allocazioni venga effettivamente effettuata dai diversi server? Una classe di esempi correlati riguarda le questioni di sicurezza: ciascun \"collegamento\" in un sistema complesso pu\u00f2 esercitare diversi livelli di sforzo per proteggere alcune propriet\u00e0 di sicurezza desiderate del sistema. Come possiamo garantire che il livello desiderato di 5. ASPETTI ALGORITMICI La nostra analisi in tutto il documento fa luce sugli aspetti algoritmici del calcolo del miglior contratto. In questa sezione enunciamo queste implicazioni -LRB- per le dimostrazioni vedi -LSB- 2 -RSB- -RRB-. Consideriamo dapprima il modello generale in cui la funzione tecnologica \u00e8 data da una funzione monotona arbitraria t -LRB- con valori razionali -RRB-, e poi consideriamo il caso di tecnologie strutturate date da una rappresentazione in rete della funzione booleana sottostante. 5.1 Tecnologie ad azione binaria con risultato binario Qui assumiamo che ci venga data una tecnologia e un valore v come input, e il nostro output dovrebbe essere il contratto ottimale, cio\u00e8l'insieme S* degli agenti da contrarre e il contratto pi per ciascun i ES*. Nel caso generale, la funzione di successo t ha dimensione esponenziale in n, il numero di agenti, e dovremo occuparci di questo. Nel caso speciale delle tecnologie anonime, la descrizione di t \u00e8 costituita solo dagli n+1 numeri t0,..., tn, e in questo caso la nostra analisi nella sezione 3 \u00e8 completamente sufficiente per calcolare il contratto ottimo. \u2022 L'orbita della tecnologia sia nei casi di agenzia che in quelli non strategici. \u2022 Un contratto ottimo per ogni dato valore v, sia per l'agenzia che per i casi non strategici. \u2022 Il prezzo dell'irresponsabilit\u00e0 POU -LRB- t, ~ c -RRB-. PROVA. Dimostriamo le affermazioni per il caso non anonimo, la prova per il caso anonimo \u00e8 simile. Mostriamo innanzitutto come costruire l'orbita della tecnologia -LRB-, in entrambi i casi si applica la stessa procedura -RRB-. Per costruire l'orbita troviamo tutti i punti di transizione e gli insiemi che si trovano nell'orbita. Il contratto vuoto \u00e8 sempre ottimo per v = 0. Supponiamo di aver calcolato i contratti ottimi e i punti di transizione fino ad un punto di transizione v per il quale S \u00e8 un contratto ottimo con la pi\u00f9 alta probabilit\u00e0 di successo. Mostriamo come calcolare il successivo punto di transizione e il successivo contratto ottimale. Per il Lemma 3 il prossimo contratto sull'orbita -LRB- per valori pi\u00f9 alti -RRB- ha una probabilit\u00e0 di successo maggiore -LRB- non ci sono due insiemi con la stessa probabilit\u00e0 di successo sull'orbita -RRB-. Calcoliamo il prossimo contratto ottimo mediante la seguente procedura. Esaminiamo tutti gli insiemi T tali che t -LRB- T -RRB- > t -LRB- S -RRB-, e calcoliamo il valore per il quale il principale \u00e8 indifferente tra la contrazione con T e la contrazione con S. Il valore di indifferenza minima \u00e8 il successivo punto di transizione e il contratto che ha il valore minimo di indifferenza \u00e8 il successivo contratto ottimale. La linearit\u00e0 dell'utilit\u00e0 nel valore e la monotonicit\u00e0 della probabilit\u00e0 di successo dei contratti ottimali garantiscono il funzionamento di quanto sopra. Chiaramente il calcolo precedente \u00e8 polinomiale nella dimensione dell'input. Una volta ottenuta l'orbita, \u00e8 chiaro che \u00e8 possibile calcolare un contratto ottimo per ogni dato valore v. Troviamo il punto di transizione pi\u00f9 grande che non \u00e8 maggiore del valore v, e il contratto ottimale in v \u00e8 l'insieme con la maggiore probabilit\u00e0 di successo in questo punto di transizione. Infine, poich\u00e9 possiamo calcolare l\u2019orbita della tecnologia sia nel caso dell\u2019agenzia che in quello non strategico in tempo polinomiale, possiamo trovare il prezzo dell\u2019irresponsabilit\u00e0 in tempo polinomiale. Per il Lemma 1 il prezzo di irresponsabilit\u00e0 POU -LRB- t -RRB- si ottiene in un punto di transizione, quindi dobbiamo solo esaminare tutti i punti di transizione e trovare quello con il massimo rapporto di benessere sociale. Una questione pi\u00f9 interessante \u00e8 se, data la funzione t come una scatola nera, possiamo calcolare il contratto ottimale nel tempo che \u00e8 polinomiale in n. Possiamo dimostrare che, in generale, non \u00e8 cos\u00ec: TEOREMA 5.Data come input una scatola nera per una funzione di successo t -LRB- quando i costi sono identici -RRB-, e un valore v, il numero di query necessarie, nel caso peggiore, per trovare il contratto ottimo \u00e8 esponenziale in n . PROVA. Consideriamo la seguente famiglia di tecnologie. Per alcuni piccoli e > 0 e k = -LSB- n/2 -RSB- definiamo la probabilit\u00e0 di successo per un dato insieme T come segue. Se l'algoritmo interroga al massimo -LRB- n -RRB- -- 2 insiemi fin/2 -RSB- di dimensione k, allora non pu\u00f2 sempre determinare il contratto ottimale -LRB- come uno qualsiasi degli insiemi che non ha interrogato circa potrebbe essere quello ottimale -RRB-. Concludiamo che -LRB- n -RRB- -- 1 query fin/2 -RSB- sono necessarie per determinare il contratto ottimo, e questo \u00e8 esponenziale in n. 5.2 Tecnologie strutturate In questa sezione considereremo la rappresentazione naturale delle reti read-once per la funzione booleana sottostante. Quindi il problema che affronteremo sar\u00e0: Il problema del contratto ottimo per reti Read Once: Input: Una rete read-once G = -LRB- V, E -RRB-, con due vertici specifici s, t ; valori razionali - ye, \u03b4e per ciascun giocatore e \u2208 E -LRB- e ce = 1 -RRB-, e un valore razionale v. Output: un insieme S di agenti che dovrebbero essere contrattati in un contratto ottimo. Sia t -LRB- E -RRB- la probabilit\u00e0 di successo quando ciascun arco riesce con probabilit\u00e0 \u03b4e. Notiamo innanzitutto che anche calcolare il valore t -LRB- E -RRB- \u00e8 un problema difficile: \u00e8 chiamato problema di affidabilit\u00e0 della rete ed \u00e8 noto essere #P \u2212 difficile -LSB- 8 -RSB-. Solo un piccolo sforzo riveler\u00e0 che il nostro problema non \u00e8 pi\u00f9 semplice: TEOREMA 6. Il problema del contratto ottimo per reti Read Once \u00e8 #P - difficile -LRB- per riduzioni di Turing -RRB-. PROVA. Mostreremo che un algoritmo per questo problema pu\u00f2 essere utilizzato per risolvere il problema dell'affidabilit\u00e0 della rete. Data un'istanza di un problema di affidabilit\u00e0 della rete < G, -LCB- -LRB- e -RCB- eEE > -LRB- dove -LRB- e denota la probabilit\u00e0 di successo di e -RRB-, definiamo un'istanza del contratto ottimo problema come segue: definire innanzitutto un nuovo grafo G ' che si ottiene '' And '' ing G con un nuovo giocatore x, con - yx molto vicino a 21 e \u03b4x = 1 \u2212 - yx. Una volta trovato tale valore, scegliamo - yx st c 1 -- 2\u03b3x \u00e8 maggiore di quel valore -RRB-. Indichiamo \u03b2x = 1 \u2212 2-yx. Il valore critico di v dove il giocatore x inserisce il contratto ottimale di G ', pu\u00f2 essere trovato utilizzando la ricerca binaria sull'algoritmo che presumibilmente trova il contratto ottimale per qualsiasi rete e qualsiasi valore. Si noti che a questo valore critico v, il principale \u00e8 indifferente tra l'insieme E ed E \u222a -LCB- x -RCB-. quindi, se riusciamo sempre a trovare il contratto ottimo, siamo anche in grado di calcolare il valore di t -LRB- E -RRB-. In conclusione, calcolare il contratto ottimo in generale \u00e8 difficile. Questi risultati suggeriscono due direzioni naturali di ricerca. La prima strada \u00e8 studiare famiglie di tecnologie i cui contratti ottimi possono essere calcolati in tempo polinomiale.La seconda strada \u00e8 esplorare algoritmi di approssimazione per il problema del contratto ottimo. Un possibile candidato per la prima direzione \u00e8 la famiglia delle reti serie-parallele, per le quali il problema dell'affidabilit\u00e0 della rete -LRB- che calcola il valore di t -RRB- \u00e8 polinomiale.", "keyphrases": ["insieme ottimale di contratto", "classico principale-agente", "qualit\u00e0 del servizio", "agenzia combinatoria", "equilibrio di Nash", "azione contrattuale", "orbita k", "tecnologia anonima", "rete serie-parallela", "prezzo di non conto"]}
{"file_name": "J-11", "text": "Reti commerciali con agenti di fissazione dei prezzi SOMMARIO In un'ampia gamma di mercati, i singoli acquirenti e venditori spesso commerciano attraverso intermediari, che determinano i prezzi sulla base di considerazioni strategiche. In genere, non tutti gli acquirenti e i venditori hanno accesso agli stessi intermediari e commerciano a prezzi corrispondentemente diversi che riflettono la loro relativa quantit\u00e0 di potere sul mercato. Modelliamo questo fenomeno utilizzando un gioco in cui acquirenti, venditori e trader intraprendono scambi commerciali su un grafico che rappresenta l'accesso che ciascun acquirente e venditore ha ai trader. In questo modello, i trader fissano i prezzi in modo strategico, quindi acquirenti e venditori reagiscono ai prezzi che vengono loro offerti. Mostriamo che il gioco risultante ha sempre un equilibrio di Nash perfetto nel sottogioco e che tutti gli equilibri portano a un'allocazione dei beni efficiente -LRB- cio\u00e8 socialmente ottimale -RRB-. Estendiamo questi risultati a un tipo pi\u00f9 generale di mercato di abbinamento, come quello che si trova nell'abbinamento tra candidati e datori di lavoro. Infine, consideriamo come i profitti ottenuti dai trader dipendano dal grafico sottostante: grossomodo, un trader pu\u00f2 ottenere un profitto positivo se e solo se ha una connessione \"essenziale\" nella struttura della rete, fornendo cos\u00ec un grafico- base teorica per quantificare l\u2019entit\u00e0 della concorrenza tra gli operatori. Il nostro lavoro differisce dai recenti studi su come il prezzo \u00e8 influenzato dalla struttura della rete attraverso il nostro modello di fissazione dei prezzi come attivit\u00e0 strategica svolta da un sottoinsieme di agenti nel sistema, piuttosto che studiare i prezzi fissati tramite equilibrio competitivo o da un meccanismo veritiero. 1. INTRODUZIONE In una serie di contesti in cui i mercati mediano le interazioni tra acquirenti e venditori, si osservano diverse propriet\u00e0 ricorrenti: i singoli acquirenti e venditori spesso commerciano attraverso intermediari, non tutti gli acquirenti e i venditori hanno accesso agli stessi intermediari, e non tutti gli acquirenti e i venditori i venditori commerciano allo stesso prezzo. Un esempio di questo context \u00e8 il commercio di prodotti agricoli nei paesi in via di sviluppo. Date le reti di trasporto inadeguate e l\u2019accesso limitato al capitale da parte degli agricoltori poveri, molti agricoltori non hanno alternative al commercio con intermediari in mercati locali inefficienti. Un paese in via di sviluppo pu\u00f2 avere molti di questi mercati parzialmente sovrapposti esistenti accanto a mercati moderni ed efficienti -LSB- 2 -RSB-. I mercati finanziari forniscono un esempio diverso di un context con queste caratteristiche generali. In questi mercati gran parte del commercio tra acquirenti e venditori \u00e8 intermediato da una variet\u00e0 di agenti che vanno dai broker ai market maker fino ai sistemi di commercio elettronico. Per molti asset non esiste un mercato unico; lo scambio di un singolo asset pu\u00f2 avvenire simultaneamente sul pavimento di una borsa, su reti incrociate, su scambi elettronici e nei mercati di altri paesi. Alcuni acquirenti e venditori hanno accesso a molte o a tutte queste sedi di negoziazione; altri hanno accesso solo a uno o alcuni di essi. Il prezzo al quale l'asset viene scambiato pu\u00f2 variare a seconda delle sedi di negoziazione.In realt\u00e0, non esiste un \u201cprezzo\u201d poich\u00e9 diversi commercianti pagano o ricevono prezzi diversi. In molti contesti esiste anche un divario tra il prezzo che un acquirente paga per un asset, il prezzo ask, e il prezzo che un venditore riceve per l\u2019asset, il prezzo bid. Gli spread, definiti come la differenza tra i prezzi bid e ask, differiscono significativamente tra questi mercati, anche se lo stesso asset viene scambiato nei due mercati. In questo articolo, sviluppiamo un quadro in cui tali fenomeni emergono da un modello di commercio basato sulla teoria dei giochi, con acquirenti, venditori e trader che interagiscono su una rete. I confini della rete collegano i trader agli acquirenti e ai venditori e rappresentano quindi l\u2019accesso che i diversi partecipanti al mercato hanno tra loro. I trader fungono da intermediari in un gioco di negoziazione in due fasi: scelgono strategicamente le offerte e chiedono i prezzi da offrire ai venditori e agli acquirenti a cui sono collegati; i venditori e gli acquirenti reagiscono quindi ai prezzi che devono affrontare. Pertanto, la rete codifica il potere relativo nelle posizioni strutturali dei partecipanti al mercato, compresi i livelli impliciti di concorrenza tra i trader. Mostriamo che questo gioco ha sempre un equilibrio di Nash perfetto nei sottogiochi e che tutti gli equilibri portano a un'allocazione dei beni efficiente -LRB- cio\u00e8 socialmente ottimale -RRB-. Analizzeremo anche come i profitti dei trader dipendano dalla struttura della rete, essenzialmente caratterizzando in termini di teoria dei grafi come il profitto di un trader sia determinato dalla quantit\u00e0 di concorrenza che sperimenta con altri trader. Sviluppando un modello di rete che include esplicitamente i trader come agenti di fissazione dei prezzi, in un sistema insieme ad acquirenti e venditori, siamo in grado di catturare la formazione dei prezzi in un context di rete come un processo strategico portato avanti dagli intermediari, piuttosto che come il risultato di un meccanismo controllato centralmente o esogeno. Il modello base: beni indistinguibili. Il nostro obiettivo nel formulare il modello \u00e8 esprimere il processo di fissazione dei prezzi in mercati come quelli discussi sopra, dove i partecipanti non hanno tutti un accesso uniforme gli uni agli altri. Ci viene dato un insieme B di acquirenti, un insieme S di venditori e un insieme T di commercianti. C'\u00e8 un grafico G non orientato che indica chi \u00e8 in grado di commerciare con chi. Ci\u00f2 riflette i vincoli che tutte le transazioni acquirente-venditore passano attraverso i trader come intermediari. Nella versione pi\u00f9 elementare del modello, consideriamo beni identici, di cui inizialmente ciascun venditore ne detiene una copia. Acquirenti e venditori hanno ciascuno un valore per una copia del bene e presumiamo che questi valori siano di conoscenza comune. Successivamente generalizzeremo questo concetto a un context in cui i beni sono distinguibili, gli acquirenti possono valutare beni diversi in modo diverso e potenzialmente anche i venditori possono valutare diversamente le transazioni con acquirenti diversi. Avere valutazioni diverse degli acquirenti cattura impostazioni come gli acquisti di case; l'aggiunta di diverse valutazioni del venditore e l'acquisizione di mercati corrispondenti, ad esempio,venditori come candidati a un posto di lavoro e acquirenti come datori di lavoro, entrambi preoccupati di chi finisce con quale \"buono\" -LRB- e con i commercianti che agiscono come servizi che mediano la ricerca di lavoro -RRB-. Quindi, partendo dal modello base, esiste un'unica tipologia di bene; il bene si presenta in unit\u00e0 indivisibili; e ciascun venditore detiene inizialmente una unit\u00e0 del bene. Tutti e tre i tipi di agenti valutano il denaro allo stesso tasso; e ciascun i EBUS valuta inoltre una copia del bene pari a \u03b8i unit\u00e0 di moneta. Nessun agente desidera pi\u00f9 di una copia del bene, quindi le copie aggiuntive vengono valutate pari a 0. Ciascun agente ha una dotazione iniziale di moneta che \u00e8 maggiore di qualsiasi valutazione individuale \u03b8i ; l'effetto di ci\u00f2 \u00e8 quello di garantire che qualsiasi acquirente che si ritrova senza una copia del bene venga escluso dal mercato a causa della sua valutazione e della posizione nella rete, non per mancanza di fondi. Immaginiamo che ogni bene venduto scorra lungo una sequenza di due bordi: da un venditore a un commerciante, e poi dal commerciante a un acquirente. Il modo particolare in cui fluiscono le merci \u00e8 determinato dal gioco seguente. Innanzitutto, ciascun trader offre un prezzo bid a ciascun venditore a cui \u00e8 connesso e un prezzo ask a ciascun acquirente a cui \u00e8 connesso. Venditori e acquirenti scelgono quindi tra le offerte presentate loro dai commercianti. Se pi\u00f9 trader propongono lo stesso prezzo a un venditore o a un acquirente, non esiste una risposta migliore e rigorosa per il venditore o l\u2019acquirente. Infine, ciascun commerciante acquista una copia del bene da ciascun venditore che accetta la sua offerta e vende una copia del bene a ciascun acquirente che accetta la sua offerta. Se un particolare commerciante scopre che pi\u00f9 acquirenti che venditori accettano le sue offerte, allora si \u00e8 impegnato a fornire pi\u00f9 copie del bene di quelle che ha ricevuto, e diremo che ci\u00f2 si traduce in una grossa penalit\u00e0 per il commerciante per inadempienza; l'effetto di ci\u00f2 \u00e8 che in equilibrio nessun trader sceglier\u00e0 prezzi bid e ask che si tradurranno in un default. Pi\u00f9 precisamente, una strategia per ciascun trader t \u00e8 la specificazione di un prezzo bid 3ti per ciascun venditore i a cui t \u00e8 connesso, e di un prezzo ask \u03b1tj per ciascun acquirente j a cui t \u00e8 connesso. -LRB- Possiamo anche gestire un modello in cui un trader pu\u00f2 scegliere di non fare un'offerta ad alcuni dei suoi venditori o acquirenti adiacenti. -RRB- Ogni venditore o acquirente sceglie poi al massimo un margine incidente, indicando il commerciante con il quale effettuer\u00e0 la transazione, al prezzo indicato. -LRB- La scelta di un singolo vantaggio riflette il fatto che i venditori -LRB- a -RRB- inizialmente hanno ciascuno solo una copia del bene, e gli acquirenti -LRB- b -RRB- vogliono ciascuno solo una copia del bene. -RRB- I guadagni sono i seguenti: Per ogni venditore i, il profitto derivante dalla selezione del trader t \u00e8 3ti, mentre il profitto derivante dalla selezione di nessun trader \u00e8 \u03b8i. -LRB- Nel primo caso il venditore riceve 3ti unit\u00e0 di moneta, mentre nel secondo conserva la sua copia del bene, che valuta \u03b8i. -RRB- Per ciascun acquirente j, il profitto derivante dalla selezione del trader t \u00e8 \u03b8j -- \u03b1tj, mentre il profitto derivante dalla selezione di nessun trader \u00e8 0.-LRB- Nel primo caso, l'acquirente riceve il bene ma cede \u03b1tj unit\u00e0 di moneta. -RRB- Per ogni trader t, con le offerte accettate dai venditori i1,..., is e dagli acquirenti j1,..., jb, il profitto \u00e8 Pr \u03b1tjr -- Pr 3tir, meno una penalit\u00e0 \u03c0 se b > s. La penalit\u00e0 viene scelta in modo tale che un trader non la incorrer\u00e0 mai in equilibrio, e quindi generalmente non ci preoccuperemo della sanzione. Questo definisce gli elementi base del gioco. Il concetto di equilibrio che usiamo \u00e8 l\u2019equilibrio di Nash perfetto del sottogioco. Qualche esempio. Per aiutare a riflettere sul modello, descriviamo ora tre esempi illustrativi, rappresentati nella Figura 1. Tutti i venditori negli esempi avranno valutazioni per il bene pari a 0; all'interno del suo cerchio viene disegnata la valutazione di ciascun acquirente; e il prezzo bid o ask su ciascun bordo viene disegnato sopra il bordo. Nella Figura 1 -LRB- a -RRB-, mostriamo come un'asta standard al secondo prezzo nasca naturalmente dal nostro modello. Supponiamo che le valutazioni dell'acquirente dall'alto verso il basso siano w > x > y > z. I prezzi bid e ask mostrati sono coerenti con un equilibrio in cui i1 e j1 accettano le offerte del trader t1, e nessun altro acquirente accetta l'offerta del trader adiacente: quindi, il trader t1 riceve il bene con un prezzo bid pari a x, e produce w -- x vendendo il bene all'acquirente j1 in cambio di w. In questo modo, possiamo considerare questo caso particolare come un'asta per un singolo bene in cui i commercianti agiscono come \"delegati\" per i loro acquirenti adiacenti. L'acquirente con la valutazione pi\u00f9 alta del bene finisce con esso e il surplus viene diviso tra il venditore e il commerciante associato. Si noti che \u00e8 possibile costruire un'asta di k unit\u00e0 con f > k acquirenti altrettanto facilmente, costruendo un grafico bipartito completo su k venditori e f commercianti, e quindi collegando ciascun commerciante a un singolo acquirente distinto. Nella Figura 1 -LRB- b -RRB-, mostriamo come i nodi con diverse posizioni nella topologia di rete possono ottenere profitti diversi, anche quando tutti Figura 1: -LRB- a -RRB- Un'asta, mediata dai trader, in cui il se lo aggiudica l'acquirente con la valutazione pi\u00f9 alta del bene. -LRB- b -RRB- Una rete in cui il venditore e l'acquirente intermedio beneficiano della concorrenza perfetta tra i trader, mentre gli altri venditori e acquirenti non hanno alcun potere a causa della loro posizione nella rete. -LRB- c -RRB- Una forma di concorrenza perfetta implicita: tutti gli spread bid/ask saranno pari a zero in equilibrio, anche se nessun trader \u201ccompete\u201d direttamente con nessun altro trader per la stessa coppia acquirente-venditore. le valutazioni degli acquirenti sono le stesse numericamente. Nello specifico, il venditore i2 e l'acquirente j2 occupano posizioni di potere, perch\u00e9 i due trader sono in competizione per i loro affari; d\u2019altro canto, gli altri venditori e acquirenti sono in posizioni deboli, perch\u00e9 ciascuno di loro ha una sola opzione. E in effetti, in ogni equilibrio, esiste un numero reale x E -LSB- 0, 1 -RSB- tale che entrambi i trader offrono prezzi bid e ask rispettivamente da x a i2 e j2,mentre offrono offerte pari a 0 e chiedono 1 agli altri venditori e acquirenti. Pertanto, questo esempio illustra alcuni ingredienti cruciali che identificheremo tra breve a un livello pi\u00f9 generale. Nello specifico, i2 e j2 sperimentano i vantaggi della concorrenza perfetta, in quanto i due trader portano gli spread bid-ask a 0 nel competere per la propria attivit\u00e0. D'altra parte, gli altri venditori e acquirenti sperimentano gli svantaggi del monopolio: ricevono un profitto pari a 0 poich\u00e9 hanno una sola opzione per lo scambio e il trader corrispondente ottiene tutto il profitto. Si noti inoltre come questo comportamento naturale emerga dal fatto che i trader sono in grado di offrire prezzi diversi a diversi agenti, cogliendo il fatto che non esiste un \"prezzo\" fisso nei tipi di mercati che motivano il modello, ma piuttosto diversi prezzi. prezzi che riflettono il potere relativo dei diversi agenti coinvolti. L'esempio precedente mostra forse il modo pi\u00f9 naturale in cui il profitto di un trader su una particolare transazione pu\u00f2 scendere a 0: quando c'\u00e8 un altro trader che pu\u00f2 replicare esattamente la sua funzione. -LRB- In questo esempio, due trader avevano ciascuno la possibilit\u00e0 di spostare una copia del bene da i2 a j2. -RRB- Ma come mostreranno i nostri risultati successivi, i trader realizzano profitti pari a zero pi\u00f9 in generale per ragioni globali e legate alla teoria dei grafici. L'esempio in Figura 1 -LRB- c -RRB- d\u00e0 una prima indicazione di ci\u00f2: si pu\u00f2 mostrare che per ogni equilibrio, c'\u00e8 ay E -LSB- 0, 1 -RSB- tale che ogni prezzo bid e ask \u00e8 uguale a y. In altre parole, tutti i trader realizzano profitti pari a zero, indipendentemente dal fatto che una copia del bene li attraversi o meno, eppure non esistono due trader che abbiano in comune un percorso venditore-acquirente. Gli spread di prezzo sono stati portati a zero da un vincolo globale imposto dal ciclo lungo a tutti gli agenti; questo \u00e8 un esempio di concorrenza perfetta implicita determinata dalla topologia della rete. Estendere il modello ai beni distinguibili. Estendiamo il modello base ad un context con beni distinguibili, come segue. Una strategia per un trader ora consiste nell'offrire un'offerta a ciascun venditore che specifica sia un prezzo che un acquirente, e nell'offrire una domanda a ciascun acquirente che specifica sia un prezzo che un venditore. -LRB- Possiamo anche gestire un modello in cui un trader offre rispettivamente offerte -LRB-, chiede -RRB- sotto forma di vettori, specificando essenzialmente un `` menu '' con un prezzo allegato a ciascun acquirente -LRB- risp. venditore -RRB-. -RRB- Ciascun acquirente e venditore seleziona un'offerta da un commerciante adiacente e i profitti per tutti gli agenti vengono determinati come prima. Qui i venditori sono candidati al lavoro, gli acquirenti sono datori di lavoro e i commercianti sono gli agenti che mediano il mercato del lavoro. Naturalmente, se si specificano valutazioni a coppie per gli acquirenti ma solo valutazioni singole per i venditori, modelliamo un ambiente in cui gli acquirenti possono distinguere tra i beni, ma ai venditori non interessa a chi vendono - questo -LRB- pi\u00f9 o meno -RRB- cattura contesti come i mercati immobiliari. I nostri risultati. Per precisarli,introduciamo la seguente notazione. -LRB- I venditori che non compaiono in nessuna tripla conservano la loro copia del bene. -RRB- Diciamo che il valore dell'allocazione \u00e8 pari a Pe \u2208 M \u03b8jeie -- \u03b8ieje. Sia \u03b8 \u2217 il valore massimo di qualsiasi allocazione M fattibile data la rete. Mostriamo che ogni istanza del nostro gioco ha un equilibrio e che in ognuno di questi equilibri l'allocazione ha valore \u03b8 \u2217 -- in altre parole, raggiunge il miglior valore possibile. Pertanto, gli equilibri in questo modello sono sempre efficienti, nel senso che il mercato consente all\u2019insieme \u201cgiusto\u201d di persone di ottenere il bene, soggetto ai vincoli della rete. Stabiliamo l'esistenza e l'efficienza degli equilibri costruendo un programma lineare per catturare il flusso di beni attraverso la rete; il duale di questo programma lineare contiene informazioni sufficienti per estrarre i prezzi di equilibrio. Per definizione del gioco, il valore dell\u2019allocazione di equilibrio \u00e8 suddiviso come profitto per gli agenti, ed \u00e8 interessante chiedersi come viene distribuito questo valore, in particolare quanto profitto un trader \u00e8 in grado di realizzare in base alla sua posizione. nella rete. Troviamo che, sebbene tutti gli equilibri abbiano lo stesso valore, il profitto di un dato trader pu\u00f2 variare a seconda degli equilibri. Otteniamo anche risultati per la somma di tutti i profitti del trader. Lavoro correlato. L\u2019approccio di base standard per analizzare l\u2019interazione tra acquirenti e venditori \u00e8 il modello Walrasiano in cui acquirenti e venditori anonimi scambiano un bene a un prezzo di equilibrio del mercato unico. Questa forma ridotta di commercio, costruita sull\u2019idealizzazione di un prezzo di mercato, \u00e8 un modello potente che ha portato a molte intuizioni. Ma non \u00e8 un buon modello da utilizzare per esaminare da dove provengono i prezzi o esattamente come acquirenti e venditori commerciano tra loro. La difficolt\u00e0 \u00e8 che nel modello Walrasiano non esiste un agente che fissa il prezzo e gli agenti non commerciano tra loro. Non esiste infatti un mercato, nel senso quotidiano del termine, nel modello walrasiano. Cio\u00e8, non esiste un luogo fisico o virtuale in cui acquirenti e venditori interagiscono per commerciare e fissare i prezzi. Pertanto, in questo modello semplice, tutti gli acquirenti e i venditori sono uniformi e commerciano allo stesso prezzo, e non vi \u00e8 alcun ruolo per gli intermediari. Esistono diverse letterature in economia e finanza che esaminano il modo in cui vengono fissati i prezzi piuttosto che limitarsi a determinare i prezzi di equilibrio. La letteratura sulla concorrenza imperfetta \u00e8 forse la pi\u00f9 antica di queste. Qui un monopolista, o un gruppo di oligopolisti, sceglie i prezzi per massimizzare i propri profitti -LRB- vedere -LSB- 14 -RSB- per la trattazione standard da manuale di questi mercati -RRB-. Un monopolista utilizza la sua conoscenza della domanda di mercato per scegliere un prezzo o un insieme di prezzi se discrimina. Gli oliogopolisti giocano a un gioco in cui i loro guadagni dipendono dalla domanda del mercato e dalle azioni dei loro concorrenti. In questa letteratura ci sono agenti che fissano i prezzi, ma viene mantenuta la finzione di un mercato unico. Nella letteratura sulla ricerca dell\u2019equilibrio,le imprese stabiliscono i prezzi e i consumatori li cercano -LRB- vedi -LSB- 3 -RSB- -RRB-. I consumatori finiscono per pagare prezzi diversi, ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.analizzare i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.analizzare i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.", "keyphrases": ["Teoria dei giochi algoritmici", "mercato", "rete commerciale", "interagiscono tra acquirente e venditore", "initi dotare di soldi", "prezzo di offerta", "concorrenza perfetta", "beneficio", "importo massimo e minimo", "economia e finanza", "comportamento strategico del trader", "gioco complementare", "monopolio"]}
{"file_name": "I-15", "text": "Ricerca e condivisione di informazioni in reti dinamiche su larga scala ABSTRACT Trovare gli agenti giusti in una rete vasta e dinamica per fornire le risorse necessarie in modo tempestivo \u00e8 un problema di vecchia data. Questo articolo presenta un metodo per la ricerca e la condivisione di informazioni che combina indici di routing con metodi basati su token. Il metodo proposto consente agli agenti di effettuare ricerche in modo efficace acquisendo gli interessi dei loro vicini, pubblicizzando le loro capacit\u00e0 di fornitura di informazioni e mantenendo indici per l'instradamento delle query, in modo integrato. Nello specifico, l'articolo dimostra attraverso esperimenti sulle prestazioni come reti statiche e dinamiche di agenti possono essere \"messe a punto\" per rispondere alle domande in modo efficace mentre raccolgono prove degli interessi e delle capacit\u00e0 di fornitura di informazioni di altri, senza alterare la topologia o imporre una struttura di sovrapposizione alla rete. di conoscenti. 1. INTRODUZIONE reti di agenti associati. D'altra parte, c'\u00e8 molta ricerca sulle reti di ricerca semantica peer to peer e sui social network -LSB- 1,5,6,8,9,10,16,18,19 -RSB- molti dei quali si occupano di ottimizzazione una rete di pari per un\u2019efficace ricerca e condivisione delle informazioni. Lo fanno principalmente imponendo strutture di sovrapposizione logiche e semantiche. Tuttavia, per quanto ne sappiamo, non esiste alcun lavoro che dimostri l'efficacia di un processo di ottimizzazione graduale in reti dinamiche su larga scala che studi l'impatto delle informazioni raccolte dagli agenti man mano che sempre pi\u00f9 query vengono emesse e servite in sessioni simultanee nel sistema. rete. La questione principale in questo articolo riguarda la \"messa a punto\" di una rete di agenti, ciascuno con una competenza specifica, per la ricerca e la condivisione di informazioni efficiente ed efficace, senza alterare la topologia o imporre una struttura di sovrapposizione tramite clustering, introduzione di indici di scelta rapida o ri- cablaggio. Il `tuning' \u00e8 il compito di condividere e raccogliere la conoscenza necessaria affinch\u00e9 gli agenti possano propagare le richieste ai giusti conoscenti, minimizzando lo sforzo di ricerca, aumentando l'efficienza e il beneficio del sistema. Nello specifico, questo articolo propone un metodo per la ricerca e la condivisione di informazioni in reti dinamiche e su larga scala, che combina indici di instradamento con metodi basati su token per la condivisione di informazioni in sistemi multi-agente su larga scala. Questo documento \u00e8 strutturato come segue: la sezione 2 presenta il lavoro correlato e motiva il metodo proposto. La sezione 3 espone il problema e la sezione 4 presenta in dettaglio le singole tecniche e il metodo complessivamente proposto. La sezione 5 presenta l'impostazione sperimentale e i risultati, mentre la sezione 6 conclude il documento, delineando il lavoro futuro. 2. LAVORI CORRELATI La fornitura e la condivisione di informazioni possono essere considerate un processo decisionale markoviano decentralizzato e parzialmente osservabile -LSB- 3,4,11,14 -RSB-. Nel caso generale, il controllo decentralizzato di sistemi dinamici su larga scala di agenti cooperativi \u00e8 un problema difficile. Le soluzioni ottime possono essere approssimate solo mediante euristiche,attraverso allentamenti del problema originario o soluzioni centralizzate. Tuttavia, in un sistema dinamico su larga scala con controllo decentralizzato \u00e8 molto difficile per gli agenti possedere accurate visioni parziali dell\u2019ambiente, ed \u00e8 ancora pi\u00f9 difficile per gli agenti possedere una visione globale dell\u2019ambiente. Inoltre, le osservazioni degli agenti non possono essere considerate indipendenti, poich\u00e9 le azioni di un agente possono influenzare le osservazioni degli altri: ad esempio, quando un agente si unisce/lascia il sistema, ci\u00f2 pu\u00f2 influenzare la valutazione da parte di altri agenti delle capacit\u00e0 di fornitura di informazioni dei vicini. . Considerando attivit\u00e0 e osservazioni indipendenti, gli autori di -LSB- 4 -RSB- propongono una soluzione teorica della decisione che tratta l'azione standard e lo scambio di informazioni come scelte esplicite che il decisore deve fare. Approssimano la soluzione utilizzando un algoritmo miope. Il loro lavoro differisce da quello riportato qui nei seguenti aspetti: in primo luogo, mira a ottimizzare la comunicazione, mentre l'obiettivo qui \u00e8 sintonizzare la rete per un'efficace condivisione delle informazioni, riducendo la comunicazione e aumentando i benefici del sistema. In terzo luogo, ritengono che le transizioni e le osservazioni fatte dagli agenti siano indipendenti, il che, come gi\u00e0 discusso, non \u00e8 vero nel caso generale. Infine, in contrasto con il loro approccio in cui gli agenti trasmettono messaggi, qui gli agenti decidono non solo quando comunicare, ma anche a chi inviare un messaggio. Gli approcci basati su token sono promettenti per ampliare efficacemente il coordinamento e quindi la fornitura e la condivisione di informazioni su sistemi su larga scala. In -LSB- 11 -RSB- gli autori forniscono un quadro matematico per l'instradamento dei token, fornendo anche un'approssimazione per risolvere il problema originale in caso di attivit\u00e0 di agenti indipendenti. Il metodo proposto richiede un elevato volume di calcoli che gli autori mirano a ridurre restringendo la sua applicazione a gruppi logici statici di agenti associati. In accordo con questo approccio, in -LSB- 12,13,14 -RSB-, la condivisione delle informazioni \u00e8 considerata solo per le reti statiche e l'autotuning delle reti non \u00e8 dimostrato. Come verr\u00e0 mostrato nella sezione 5, i nostri esperimenti mostrano che sebbene questi approcci possano gestire la condivisione delle informazioni in reti dinamiche, richiedono una maggiore quantit\u00e0 di messaggi rispetto all'approccio qui proposto e non possono sintonizzare la rete per una condivisione efficiente delle informazioni. La comunicazione proattiva \u00e8 stata proposta in -LSB- 17 -RSB- come risultato di una determinazione teorica delle decisioni dinamiche delle strategie di comunicazione. Questo approccio si basa sulla specificazione degli agenti come \"fornitori\" e \"necessari\": ci\u00f2 avviene mediante un precalcolo basato su un piano delle esigenze di informazione e delle capacit\u00e0 di fornitura degli agenti. Tuttavia, questo approccio non pu\u00f2 adattarsi a reti grandi e dinamiche, poich\u00e9 sarebbe altamente inefficiente per ciascun agente calcolare e determinare le sue potenziali esigenze e capacit\u00e0 di fornitura di informazioni data la sua potenziale interazione con centinaia di altri agenti.Considerando il recupero delle informazioni nei sistemi peer-to-peer dalla prospettiva di un sistema multi-agente, l'approccio proposto in -LSB- 18 -RSB- si basa su un modello linguistico di raccolta dei documenti degli agenti. Sfruttando i modelli di altri agenti nella rete, gli agenti costruiscono la loro visione della rete che viene utilizzata per prendere decisioni di routing. Inizialmente, gli agenti costruiscono le loro opinioni utilizzando i modelli dei loro vicini. Quindi, il sistema si riorganizza formando cluster di agenti con contenuti simili. I cluster vengono sfruttati durante il recupero delle informazioni utilizzando un approccio kNN e uno schema di ricerca a gradiente. Sebbene questo lavoro miri a mettere a punto una rete per un'efficiente fornitura di informazioni -LRB- attraverso la riorganizzazione -RRB-, non dimostra l'efficacia dell'approccio rispetto a questo problema. Inoltre, sebbene durante la riorganizzazione e il recupero misurino la somiglianza del contenuto tra gli agenti, \u00e8 necessario un approccio pi\u00f9 dettagliato che consenta agli agenti di misurare le somiglianze degli elementi informativi o delle sottoraccolte di elementi informativi. Basandosi sul loro lavoro sui sistemi peer-to-peer, H.Zhand e V.Lesser in -LSB- 19 -RSB- studiano sessioni di ricerca simultanee. Considerando la ricerca sui sistemi semantici peer-to-peer1, la maggior parte degli approcci sfrutta quello che pu\u00f2 essere genericamente definito un \"indice di instradamento\". Una questione importante riguardante la ricerca di informazioni \u00e8 \"quali informazioni devono essere condivise tra pari, quando e quali aggiustamenti devono essere apportati affinch\u00e9 le domande vengano indirizzate a fonti di informazione affidabili nel modo pi\u00f9 efficace ed efficiente\". RICORDARE I peer -LSB- 10 -RSB- raccolgono informazioni riguardanti le domande a cui \u00e8 stato risposto con successo da altri peer, in modo da selezionare successivamente i peer a cui inoltrare le richieste: questo \u00e8 un approccio di apprendimento lento che non comporta la pubblicit\u00e0 della fornitura di informazioni tra pari abilit\u00e0. Ci\u00f2 si traduce in un processo di ottimizzazione in cui il richiamo complessivo aumenta nel tempo, mentre il numero di messaggi per query rimane pi\u00f9 o meno lo stesso. Qui, gli agenti pubblicizzano attivamente le loro capacit\u00e0 di fornire informazioni in base agli interessi valutati dei loro pari: ci\u00f2 si traduce in un numero molto inferiore di messaggi per query rispetto a quelli riportati in REMINDIN'. In -LSB- 5,6 -RSB- i peer, utilizzando un'ontologia comune, pubblicizzano la propria esperienza, che viene sfruttata per la formazione di una rete semantica sovrapposta: le query vengono propagate in questa rete a seconda della loro somiglianza con l'esperienza dei peer. Secondo il nostro approccio, gli agenti pubblicizzano selettivamente le loro capacit\u00e0 di fornire informazioni su argomenti specifici ai loro vicini con interessi informativi simili -LRB- e solo a questi -RRB-. Tuttavia, ci\u00f2 avviene con il passare del tempo e mentre gli agenti ricevono richieste dai loro peer. Generano un sovraccarico sostanziale in ambienti altamente dinamici, dove i nodi si uniscono/escono dal sistema. 248 La Sesta Intl.. Conf. congiunta.gli agenti pubblicizzano le loro capacit\u00e0 di fornire informazioni tenendo conto degli interessi dei loro vicini. Dato il successo di questo metodo, studieremo come l'aggiunta di percorsi logici e la graduale evoluzione della topologia della rete possano aumentare ulteriormente l'efficacia del metodo proposto. 6. CONCLUSIONI Questo articolo presenta un metodo per l'elaborazione di query semantiche in grandi reti di agenti che combina indici di routing con metodi di condivisione delle informazioni. Il metodo presentato consente agli agenti di tenere traccia degli interessi dei conoscenti, di pubblicizzare le proprie capacit\u00e0 di fornitura di informazioni a coloro che hanno un alto interesse nei loro confronti e di mantenere indici per instradare le query a quegli agenti che hanno le capacit\u00e0 di fornitura di informazioni richieste. Nello specifico, l'articolo dimostra attraverso estesi esperimenti sulle prestazioni: -LRB- a -RRB- Come le reti di agenti possono essere \"sintonizzate\" in modo da fornire le informazioni richieste in modo efficace, aumentando i benefici e l'efficienza del sistema. -LRB- b -RRB- Come diversi tipi di conoscenza locale -LRB- numero, archivi di informazioni locali, percentuale, interessi e capacit\u00e0 di fornitura di informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.gli interessi e la capacit\u00e0 di fornire informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.gli interessi e la capacit\u00e0 di fornire informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.", "keyphrases": ["informare, cercare e condividere", "rete sociale", "agente Cooper", "rete di ricerca peer to peer", "sistema peer-to-peer", "rete dinamica e su larga scala", "processo decentralizzato con osservazione parziale markov decis", "controllo decentrato", "algoritmo miope", "approccio knn", "schema di ricerca del gradiente"]}
{"file_name": "J-1", "text": "Meccanismi generalizzati di riduzione del commercio ABSTRACT Quando si progetta un meccanismo ci sono diverse propriet\u00e0 desiderabili da mantenere come la compatibilit\u00e0 degli incentivi -LRB- IC -RRB-, la razionalit\u00e0 individuale -LRB- IR -RRB- e l'equilibrio di bilancio -LRB- BB -RRB-. \u00c8 noto -LSB- 15 -RSB- che \u00e8 impossibile che un meccanismo massimizzi il benessere sociale pur essendo IR, IC e BB. Ci sono stati diversi tentativi di aggirare -LSB- 15 -RSB- scambiando welfare con BB, ad esempio in ambiti quali aste a doppia faccia -LSB- 13 -RSB-, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura problemi -LSB- 2, 4 -RSB-. In questo articolo forniamo una procedura chiamata Riduzione Commerciale Generalizzata -LRB- GTR -RRB- per giocatori a valore singolo, che dato un meccanismo IR e IC, produce un meccanismo che \u00e8 IR, IC e BB con una perdita di welfare. Abbiamo limitato il benessere raggiunto dalla nostra procedura per un'ampia gamma di domini. In particolare, i nostri risultati migliorano le soluzioni esistenti per problemi quali mercati a doppia faccia con beni omogenei, mercati distribuiti e diversi tipi di catene di approvvigionamento. Inoltre, la nostra soluzione fornisce meccanismi di bilanciamento del bilancio per diversi problemi aperti come aste combinatorie a doppia faccia e mercati distribuiti con vantaggi di trasporto strategici. 1. INTRODUZIONE Quando si progetta un meccanismo ci sono diverse propriet\u00e0 chiave che \u00e8 opportuno mantenere. In molti meccanismi la funzione obiettivo che il progettista di un meccanismo tenta di massimizzare \u00e8 il benessere sociale, ovvero il beneficio totale per la societ\u00e0. Tuttavia, \u00e8 ben noto da -LSB- 15 -RSB- che qualsiasi meccanismo che massimizza il benessere sociale pur mantenendo la razionalit\u00e0 individuale e la compatibilit\u00e0 degli incentivi va necessariamente in deficit, cio\u00e8 non \u00e8 in pareggio di bilancio. Per mantenere la propriet\u00e0 BB in un meccanismo IR e IC \u00e8 necessario scendere a compromessi sull\u2019ottimalit\u00e0 del benessere sociale. 1.1 Lavori correlati e soluzioni specifiche Ci sono stati diversi tentativi di progettare meccanismi di pareggio di bilancio per domini particolari2. Nel problema dei mercati distribuiti -LRB- e nei problemi strettamente correlati -RRB- le merci vengono trasportate tra localit\u00e0 geografiche sostenendo costi costanti per il trasporto. -LSB- 16, 9, 3 -RSB- presentano meccanismi che si avvicinano al benessere sociale ottenendo un meccanismo IR, IC e BB. Per i problemi della catena di fornitura -LSB- 2, 4 -RSB- limita la perdita di benessere sociale che \u00e8 necessario infliggere al meccanismo per ottenere la combinazione desiderata di IR, IC e BB. Nonostante i lavori discussi sopra, la questione di come progettare un meccanismo generale che raggiunga IR, IC e BB indipendentemente dal dominio del problema rimane aperta. Inoltre, ci sono diversi ambiti in cui la questione di come progettare un meccanismo IR, IC e BB che approssimi il benessere sociale rimane un problema aperto. Per esempio,nell\u2019importante ambito delle aste combinatorie a doppia faccia non esiste alcun risultato noto che limiti la perdita di benessere sociale necessaria per raggiungere il pareggio di bilancio. Un altro esempio interessante \u00e8 la domanda aperta lasciata da -LSB- 3 -RSB-: come si pu\u00f2 limitare la perdita di benessere sociale necessaria per raggiungere l\u2019equilibrio di bilancio in un mercato distribuito IR e IC in cui i margini dei trasporti sono strategici. Naturalmente una risposta al mercato distribuito dei BB con margini strategici ha vaste implicazioni pratiche, ad esempio per le reti di trasporto. 1.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato concorrenza interna ed esterna.Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.", "keyphrases": ["riduzione commerciale", "saldo di bilancio", "concorso stagisti", "concorrenza esterna", "effici", "potere del giocatore", "riduzione del commercio generale", "gtr", "ottimo", "ineguale nel benessere", "giocatore multi-mente", "meccanismo del pareggio di bilancio", "buono omogeneo", "mercato della distribuzione spaziale"]}
{"file_name": "H-24", "text": "Investigare il comportamento di interrogazione e navigazione degli utenti avanzati dei motori di ricerca ABSTRACT Un modo per aiutare tutti gli utenti dei motori di ricerca Web commerciali ad avere pi\u00f9 successo nelle loro ricerche \u00e8 capire meglio cosa stanno facendo gli utenti con maggiore esperienza di ricerca e utilizzare questa conoscenza a vantaggio di tutti . In questo articolo studiamo i log di interazione degli utenti avanzati dei motori di ricerca -LRB- e di quelli non cos\u00ec avanzati -RRB- per comprendere meglio come questi gruppi di utenti effettuano le ricerche. I risultati mostrano che ci sono differenze marcate nelle query, nei clic sui risultati, nella navigazione post-query e nel successo della ricerca degli utenti che classifichiamo come avanzati -LRB- in base al loro utilizzo degli operatori di query -RRB-, rispetto a quelli classificati come non- Avanzate. I nostri risultati hanno implicazioni su come gli utenti avanzati dovrebbero essere supportati durante le loro ricerche e su come le loro interazioni potrebbero essere utilizzate per aiutare gli utenti di tutti i livelli di esperienza a trovare informazioni pi\u00f9 pertinenti e ad apprendere strategie di ricerca migliorate. 1. INTRODUZIONE La formulazione di affermazioni di query che catturino sia gli aspetti salienti dei bisogni informativi sia che siano significativi per i sistemi di Information Retrieval -LRB- IR -RRB- pone una sfida per molti ricercatori -LSB- 3 -RSB-. Queste tecniche possono essere utili per migliorare la precisione dei risultati, tuttavia, oltre alle analisi logaritmiche -LRB- ad esempio, -LSB- 15 -RSB- -LSB- 27 -RSB- -RRB-, sono state generalmente trascurate dalla comunit\u00e0 di ricerca nei tentativi per migliorare la qualit\u00e0 dei risultati di ricerca. La ricerca IR si \u00e8 generalmente concentrata su modi alternativi con cui gli utenti possono specificare le proprie esigenze piuttosto che aumentare l'adozione della sintassi avanzata. La ricerca sulle tecniche pratiche per integrare la tecnologia di ricerca esistente e supportare gli utenti si \u00e8 intensificata negli ultimi anni -LRB- ad es. -LSB- 18 -RSB- -LSB- 34 -RSB- -RRB-. Tuttavia, \u00e8 difficile implementare tali tecniche su larga scala con latenze tollerabili. Le query tipiche inviate ai motori di ricerca Web assumono la forma di una serie di token separati da spazi. Generalmente \u00e8 presente un operatore booleano AND implicito tra i token che limita i risultati della ricerca ai documenti contenenti tutti i termini di query. De Lima e Pedersen -LSB- 7 -RSB- hanno studiato l'effetto dell'analisi, del riconoscimento delle frasi e dell'espansione sulle query di ricerca sul Web. Hanno dimostrato che il riconoscimento automatico delle frasi nelle query pu\u00f2 migliorare la precisione dei risultati nella ricerca sul Web. Tuttavia, il valore della sintassi avanzata per i ricercatori tipici \u00e8 stato generalmente limitato, poich\u00e9 la maggior parte degli utenti non conosce la sintassi avanzata o non capisce come usarla -LSB- 15 -RSB-. In questo articolo esploriamo l'uso degli operatori di query in maggiore dettaglio e proponiamo applicazioni alternative che non richiedono a tutti gli utenti di utilizzare esplicitamente la sintassi avanzata. Ipotizziamo che gli utenti che utilizzano la sintassi avanzata delle query dimostrino un grado di esperienza nella ricerca che la maggior parte della popolazione degli utenti non possiede; un'affermazione supportata dalla ricerca precedente -LSB- 13 -RSB-.Lo studio del comportamento di questi utenti avanzati dei motori di ricerca pu\u00f2 fornire importanti informazioni sulla ricerca e sulla navigazione dei risultati da cui altri potrebbero trarre vantaggio. Utilizzando i log raccolti da un gran numero di utenti consenzienti, esaminiamo le differenze tra il comportamento di ricerca di coloro che utilizzano la sintassi avanzata e quelli che non lo fanno, e le differenze nelle informazioni a cui vengono indirizzati gli utenti. Siamo interessati a rispondere a tre domande di ricerca: -LRB- i -RRB- Esiste una relazione tra l'uso della sintassi avanzata e altre caratteristiche di una ricerca? -LRB- ii -RRB- Esiste una relazione tra l'uso della sintassi avanzata e i comportamenti di navigazione post-query? -LRB- iii -RRB- Esiste una relazione tra l'uso della sintassi avanzata e le misure del successo della ricerca? Attraverso uno studio e un\u2019analisi sperimentale, offriamo potenziali risposte per ciascuna di queste domande. Una relazione tra l'uso della sintassi avanzata e una qualsiasi di queste funzionalit\u00e0 potrebbe supportare la progettazione di sistemi su misura per gli utenti avanzati dei motori di ricerca o utilizzare le interazioni degli utenti avanzati per aiutare gli utenti non avanzati ad avere pi\u00f9 successo nelle loro ricerche. Descriviamo il lavoro correlato nella Sezione 2, i dati che abbiamo utilizzato in questo studio basato sui log nella Sezione 3, le caratteristiche di ricerca su cui focalizziamo la nostra analisi nella Sezione 4 e i risultati di questa analisi nella Sezione 5. 2. LAVORO CORRELATO Fattori come la mancanza di conoscenza del dominio, la scarsa comprensione della raccolta di documenti oggetto della ricerca e un bisogno di informazioni poco sviluppato possono influenzare la qualit\u00e0 delle query che gli utenti inviano ai sistemi IR -LRB- -LSB- 24 -RSB-, -LSB- 28 -RSB- -RRB-. Sono state condotte numerose ricerche sui diversi modi per aiutare gli utenti a specificare le proprie esigenze di informazione in modo pi\u00f9 efficace. Belkin et al. -LSB- 4 -RSB- ha sperimentato fornendo spazio aggiuntivo affinch\u00e9 gli utenti potessero digitare una descrizione pi\u00f9 dettagliata delle loro esigenze di informazione. Un approccio simile \u00e8 stato tentato da Kelly et al. -LSB- 18 -RSB-, che ha utilizzato moduli di chiarimento per ottenere dagli utenti informazioni aggiuntive sul context di ricerca. Questi approcci si sono dimostrati efficaci nei sistemi di recupero della corrispondenza migliore in cui query pi\u00f9 lunghe generalmente portano a risultati di ricerca pi\u00f9 pertinenti -LSB- 4 -RSB-. Tuttavia, nella ricerca sul Web, dove molti sistemi sono basati su un modello di recupero booleano esteso, query pi\u00f9 lunghe possono effettivamente compromettere le prestazioni di recupero, portando al recupero di un numero limitato di risultati potenzialmente irrilevanti. Non \u00e8 semplicemente sufficiente richiedere maggiori informazioni agli utenti; queste informazioni devono essere di migliore qualit\u00e0. Il feedback sulla pertinenza -LRB- RF -RRB- -LSB- 22 -RSB- e l'espansione interattiva delle query -LSB- 9 -RSB- sono tecniche popolari che sono state utilizzate per migliorare la qualit\u00e0 delle informazioni che gli utenti forniscono ai sistemi IR in merito alle loro esigenze informative . Nel caso di RF, l'utente presenta al sistema esempi di informazioni rilevanti che vengono poi utilizzati per formulare una query migliorata o recuperare una nuova serie di documenti.Si \u00e8 dimostrato difficile convincere gli utenti a utilizzare RF nel dominio Web a causa della difficolt\u00e0 nel trasmettere il significato e i vantaggi della RF agli utenti tipici -LSB- 17 -RSB-. I suggerimenti sulle query offerti in base ai log delle query hanno il potenziale per migliorare le prestazioni di recupero con un carico utente limitato. Questo approccio si limita alla riesecuzione delle query pi\u00f9 frequenti e gli utenti spesso ignorano i suggerimenti presentati loro -LSB- 1 -RSB-. Inoltre, entrambe queste tecniche non aiutano gli utenti a imparare a produrre query pi\u00f9 efficaci. La maggior parte dei motori di ricerca commerciali fornisce una sintassi di query avanzata che consente agli utenti di specificare le proprie esigenze di informazioni in modo pi\u00f9 dettagliato. Gli operatori booleani -LRB- AND, OR e NOT -RRB- possono unire termini e frasi e modificatori come ``site:'' e ``link:'' possono essere utilizzati per limitare lo spazio di ricerca. Le query create con queste tecniche possono essere potenti. L'analisi basata sui log delle interazioni degli utenti con i motori di ricerca Excite e AltaVista ha dimostrato che solo il 10-20% delle query conteneva una sintassi avanzata -LSB- 14 -RSB- -LSB- 25 -RSB-. Questa analisi pu\u00f2 essere un modo utile per acquisire caratteristiche degli utenti che interagiscono con i sistemi IR. La ricerca sulla modellazione degli utenti -LSB- 6 -RSB- e sulla personalizzazione -LSB- 30 -RSB- ha dimostrato che la raccolta di pi\u00f9 informazioni sugli utenti pu\u00f2 migliorare l'efficacia delle ricerche, ma richiede pi\u00f9 informazioni sugli utenti di quelle generalmente disponibili solo dai log di interazione. A meno che non sia abbinato a una tecnica qualitativa, come un questionario post-sessione -LSB- 23 -RSB-, pu\u00f2 essere difficile associare le interazioni alle caratteristiche dell'utente. Nel nostro studio ipotizziamo che, data la difficolt\u00e0 nell'individuare funzionalit\u00e0 di ricerca avanzata all'interno della tipica interfaccia di ricerca e i potenziali problemi nella comprensione della sintassi, quegli utenti che utilizzano regolarmente la sintassi avanzata rappresentano una classe distinta di ricercatori che mostreranno altri comportamenti di ricerca comuni . Altri studi sui comportamenti di ricerca dei ricercatori avanzati hanno tentato di comprendere meglio la conoscenza strategica che hanno acquisito. Tuttavia, possono fornire informazioni preziose sui comportamenti degli utenti con competenze di dominio, sistema o ricerca che superano quelle dell'utente medio. In particolare, il comportamento delle query \u00e8 stato ampiamente studiato per comprendere meglio gli utenti -LSB- 31 -RSB- e supportare altri utenti -LSB- 16 -RSB-. In questo articolo studiamo altre caratteristiche di ricerca degli utenti della sintassi avanzata nel tentativo di determinare se c'\u00e8 qualcosa di diverso nel modo in cui questi utenti dei motori di ricerca effettuano le ricerche e se le loro ricerche possono essere utilizzate a vantaggio di coloro che non utilizzano le funzionalit\u00e0 avanzate dei motori di ricerca. Per fare ci\u00f2 utilizziamo i log di interazione raccolti da un ampio insieme di utenti consenzienti per un periodo prolungato. Nella sezione successiva descriviamo i dati che utilizziamo per studiare il comportamento degli utenti che utilizzano la sintassi avanzata, rispetto a quelli che non utilizzano questa sintassi. Atti SIGIR 2007 Sessione 11:Comportamento dell'interazione come query, clic sui risultati, navigazione post-query e successo della ricerca. La classificazione approssimativa degli utenti basata su una sola caratteristica facilmente estraibile dal flusso di query produce risultati notevoli sul comportamento di interazione degli utenti che non utilizzano la sintassi e di quelli che la usano. Come abbiamo suggerito, i sistemi di ricerca possono sfruttare le interazioni di questi utenti per migliorare il posizionamento dei documenti, consigliare le pagine o persino formare gli utenti.", "keyphrases": ["motore di ricerca", "queri", "informazioni rilevanti", "strategie di ricerca", "latente tollerante", "sintassi avanzata", "comportamento di navigazione", "comportamento di ricerca", "successo della ricerca", "feedback rilevante", "rilev"]}
{"file_name": "J-8", "text": "Forte equilibrio nei giochi di connessione con condivisione dei costi * ABSTRACT In questo lavoro studiamo giochi di connessione con condivisione dei costi, in cui ogni giocatore ha una sorgente e un pozzo che vorrebbe connettere, e il costo dei bordi \u00e8 condiviso equamente -LRB- giochi di connessione equi - RRB- o in modo arbitrario -LRB- giochi di connessione generali -RRB-. Studiamo le topologie dei grafi che garantiscono l'esistenza di un equilibrio forte -LRB- dove nessuna coalizione pu\u00f2 migliorare il costo di ciascuno dei suoi membri -RRB- indipendentemente dai costi specifici sui bordi. I nostri principali risultati di esistenza sono i seguenti: -LRB- 1 -RRB- Per una singola sorgente e pozzo mostriamo che esiste sempre un equilibrio forte -LRB- sia per giochi di connessione equi che generali -RRB-. -LRB- 2 -RRB- Per una singola sorgente con pi\u00f9 sink mostriamo che per un grafo parallelo in serie esiste sempre un equilibrio forte -LRB- sia per giochi di connessione equi che generali -RRB-. -LRB- 3 -RRB- Per multi source e sink mostriamo che un grafo parallelo di estensione ammette sempre un equilibrio forte nei giochi di connessione equi. Per quanto riguarda la qualit\u00e0 dell'equilibrio forte mostriamo che in ogni gioco di connessione equa il costo di un equilibrio forte \u00e8 \u0398 -LRB- log n -RRB- dalla soluzione ottima, dove n \u00e8 il numero di giocatori. -LRB- Questo dovrebbe essere contrapposto al prezzo \u03a9 -LRB- n -RRB- dell'anarchia per la stessa impostazione. -RRB- Per i giochi a connessione generale a sorgente singola e i giochi a connessione equa a singolo sink a sorgente singola, mostriamo che un equilibrio forte \u00e8 sempre una soluzione ottima. * Ricerca finanziata in parte da un finanziamento della Israel Science Foundation, della Binational Science Foundation -LRB- BSF -RRB-, della GermanIsraeli Foundation -LRB- GIF -RRB-, della Lady Davis Fellowship, di un premio della facolt\u00e0 IBM e del programma IST della Comunit\u00e0 Europea, nell'ambito della Rete di Eccellenza PASCAL, IST-2002-506778. Questa pubblicazione riflette solo le opinioni degli autori. 1. INTRODUZIONE La teoria dei giochi computazionali ha introdotto la questione degli incentivi in \u200b\u200bmolti dei classici problemi di ottimizzazione combinatoria. Consideriamo i classici problemi di routing e trasporto come i problemi multicast o multi-commodity, che molte volte vengono visti come segue. Ci viene fornito un grafico con i costi edge e le richieste di connettivit\u00e0 tra i nodi e il nostro obiettivo \u00e8 trovare una soluzione a costo minimo. Il punto di vista della teoria dei giochi presupporrebbe che ogni domanda individuale sia controllata da un giocatore che ottimizza la propria utilit\u00e0, e il risultato risultante potrebbe essere lontano dalla soluzione ottimale. Quando si considerano gli incentivi individuali \u00e8 necessario discutere il concetto di soluzione appropriata. Gran parte della ricerca sulla teoria computazionale dei giochi si \u00e8 concentrata sul classico equilibrio di Nash come concetto di soluzione primaria. In effetti l\u2019equilibrio di Nash presenta molti vantaggi e, cosa pi\u00f9 importante, esiste sempre -LRB- nelle strategie miste -RRB-. Tuttavia, il concetto di soluzione dell\u2019equilibrio di Nash \u00e8 resistente solo a deviazioni unilaterali, mentre in realt\u00e0 i giocatori potrebbero essere in grado di coordinare le proprie azioni.Un equilibrio forte -LSB- 4 -RSB- \u00e8 uno stato dal quale nessuna coalizione -LRB- di qualsiasi dimensione -RRB- pu\u00f2 discostarsi e migliorare l\u2019utilit\u00e0 di ogni membro della coalizione -LRB- riducendo al contempo l\u2019utilit\u00e0 dei giocatori esterni all\u2019equilibrio. coalizione -RRB-. Questa resilienza alle deviazioni delle coalizioni degli attori \u00e8 molto attraente, e si pu\u00f2 sperare che, una volta raggiunto un equilibrio forte, esso abbia molte probabilit\u00e0 di mantenersi. Da un punto di vista della teoria dei giochi computazionale, un ulteriore vantaggio di un equilibrio forte \u00e8 che ha il potenziale di ridurre la distanza tra la soluzione ottimale e la soluzione ottenuta come risultato di un comportamento egoistico. Il prezzo forte dell\u2019anarchia -LRB- SPoA -RRB-, introdotto in -LSB- 1 -RSB-, \u00e8 il rapporto tra il costo del peggior equilibrio forte e il costo di una soluzione ottima. Ovviamente, SPoA ha significato solo nei casi in cui esiste un equilibrio forte. Uno dei principali svantaggi dell\u2019equilibrio forte \u00e8 che la maggior parte dei giochi non ammette alcun equilibrio forte. Anche semplici giochi classici come il dilemma del prigioniero non possiedono alcun equilibrio forte -LRB- che \u00e8 anche un esempio di gioco di congestione che non possiede equilibri forti -RRB-. Questo fatto sfortunato ha ridotto la concentrazione nell'equilibrio forte, nonostante le sue propriet\u00e0 altamente attrattive. In questo lavoro ci concentreremo sui giochi di connessione a ripartizione dei costi, introdotti da -LSB- 3, 2 -RSB-. In un gioco del genere, esiste un grafico diretto sottostante con costi marginali e i singoli utenti hanno richieste di connettivit\u00e0 -LRB- tra una sorgente e un sink -RRB-. Consideriamo due modelli. Il modello di connessione a costo equo -LSB- 2 -RSB- consente a ciascun giocatore di selezionare un percorso dalla sorgente al sink2. In questo gioco il costo di un bordo \u00e8 diviso equamente tra tutti i giocatori che hanno selezionato il bordo, e il costo del giocatore \u00e8 la somma dei suoi costi sui bordi che ha selezionato. Il gioco di connessione generale -LSB- 3 -RSB- consente a ciascun giocatore di offrire prezzi per i bordi. In questo gioco un vantaggio viene acquistato se la somma delle offerte copre almeno il suo costo, e il costo del giocatore \u00e8 la somma delle sue offerte sui margini acquistati -LRB- in entrambi i giochi assumiamo che il giocatore debba garantire il connettivit\u00e0 tra la sua sorgente e il suo sink -RRB-. In questo lavoro ci concentriamo su due questioni importanti. Il primo \u00e8 identificare in quali condizioni \u00e8 garantita l'esistenza di un equilibrio forte, e il secondo \u00e8 la qualit\u00e0 degli equilibri forti. Per quanto riguarda l'esistenza, identifichiamo famiglie di topologie di grafi che possiedono un forte equilibrio per qualsiasi assegnazione di costi degli archi. Si pu\u00f2 vedere questa separazione tra la topologia del grafo e i costi dei margini, come una separazione tra l\u2019infrastruttura sottostante e i costi che i giocatori osservano per acquistare i margini. Mentre ci si aspetta che l\u2019infrastruttura sia stabile per lunghi periodi di tempo, i costi osservati dagli attori possono essere facilmente modificati in brevi periodi di tempo. I nostri risultati sono i seguenti.Per il caso della singola merce -LRB- tutti i giocatori hanno la stessa sorgente e lo stesso pozzo -RRB-, esiste un forte equilibrio in qualsiasi grafico -LRB- sia per i giochi equi che per i giochi di connessione generale -RRB-. Inoltre, l'equilibrio forte \u00e8 anche s mentre \u00e8 noto che qualsiasi gioco di congestione ammette almeno un equilibrio di Nash nelle strategie pure -LSB- 16 -RSB-. 2Lo schema di equa condivisione dei costi \u00e8 interessante anche dal punto di vista della progettazione del meccanismo, in quanto \u00e8 un meccanismo di condivisione dei costi a prova di strategia -LSB- 14 -RSB-. la soluzione ottimale -LRB- vale a dire, i giocatori condividono un percorso pi\u00f9 breve dalla sorgente comune al pozzo comune -RRB-. Per il caso di una singola sorgente e pi\u00f9 pozzi -LRB-, ad esempio, in un albero multicast -RRB-, mostriamo che in un gioco di connessioni eque c'\u00e8 un forte equilibrio se il grafo sottostante \u00e8 un grafo parallelo in serie, e mostriamo un esempio di grafo parallelo non serie che non ha un equilibrio forte. Per il caso di multi-commodity -LRB- multi sorgenti e pozzi -RRB-, mostriamo che in un gioco di connessione equa se il grafico \u00e8 un grafo parallelo di estensione allora c'\u00e8 sempre un equilibrio forte, e mostriamo un esempio di una serie grafico parallelo che non ha un equilibrio forte. Per quanto ne sappiamo, siamo i primi a fornire una caratterizzazione topologica dell\u2019esistenza dell\u2019equilibrio nei giochi di rete multi-commodity e single-source. Per ogni gioco di connessione equa mostriamo che se esiste un equilibrio forte \u00e8 al massimo un fattore di \u0398 -LRB- log n -RRB- dalla soluzione ottima, dove n \u00e8 il numero di giocatori. Questo dovrebbe essere contrapposto al limite \u0398 -LRB- n -RRB- che esiste per il prezzo dell\u2019anarchia -LSB- 2 -RSB-. Per i giochi di connessione generale a sorgente singola, mostriamo che qualsiasi grafo parallelo in serie possiede un equilibrio forte e mostriamo un esempio di un grafico che non ha un equilibrio forte. In questo caso dimostriamo anche che qualsiasi equilibrio forte \u00e8 ottimo. Lavoro correlato Le caratterizzazioni topologiche per i giochi di rete di un singolo bene sono state recentemente fornite per varie propriet\u00e0 di equilibrio, tra cui l'esistenza dell'equilibrio -LSB- 12, 7, 8 -RSB-, l'unicit\u00e0 dell'equilibrio -LSB- 10 -RSB- e l'efficienza dell'equilibrio -LSB- 17 , 11 -RSB-. L'esistenza di un equilibrio di Nash puro nei giochi di congestione della rete di un singolo bene con costi o pesi specifici del giocatore \u00e8 stata studiata in -LSB- 12 -RSB-. L'esistenza di un equilibrio forte \u00e8 stata studiata sia in -LRB- con utilit\u00e0 decrescente, ad esempio nei giochi di congestione -RRB- di routing, sia in quelli con utilit\u00e0 crescente -LRB-, ad esempio nei giochi di congestione con equa condivisione dei costi. -LSB-7, 8 -RSB- hanno fornito una caratterizzazione topologica completa dell'esistenza di un SE nei giochi di congestione che riducono l'utilit\u00e0 di un singolo bene, e hanno dimostrato che un SE esiste sempre se e solo se il grafico sottostante \u00e8 parallelo all'estensione. -LSB- 19 -RSB- hanno dimostrato che nei giochi di congestione che aumentano l'utilit\u00e0 di un singolo bene, la caratterizzazione topologica \u00e8 essenzialmente equivalente ai collegamenti paralleli. Inoltre,hanno dimostrato che questi risultati valgono anche per gli equilibri forti correlati -LRB- in contrasto con l'impostazione decrescente, dove gli equilibri forti correlati potrebbero non esistere affatto -RRB-. Sebbene i giochi a condivisione dei costi equi che studiamo siano giochi di congestione della rete che aumentano l'utilit\u00e0, ne ricaviamo una caratterizzazione diversa rispetto a -LSB- 19 -RSB- a causa delle diverse ipotesi riguardanti le azioni dei giocatori.3 4. GIOCHI DI CONNESSIONE GENERALE In questa sezione, ricavare i nostri risultati per i giochi di connessione generali. 4.1 Esistenza di un equilibrio forte Iniziamo con una caratterizzazione dell'esistenza di un equilibrio forte nei giochi di connessione generale simmetrici. Similmente al Teorema 3.1 -LRB- utilizzando una dimostrazione simile -RRB- stabiliamo, TEOREMA 4.1. In ogni gioco simmetrico di connessione equa esiste un equilibrio forte. Anche se ogni gioco di connessione generale a sorgente singola possiede un equilibrio di Nash puro -LSB- 3 -RSB-, non ammette necessariamente un equilibrio forte.11 Il gioco di connessione equa ha ispirato questo esempio. TEOREMA 4.2. Esiste un gioco di connessione generale a sorgente unica che non ammette alcun equilibrio forte. PROVA. Consideriamo un gioco di connessione generale a fonte singola con 3 giocatori sul grafico illustrato nella Figura 4. Abbiamo dimostrato che nessuno dei NE \u00e8 SE, e quindi il gioco non possiede alcun SE. Successivamente mostriamo che per la classe dei grafi paralleli in serie, esiste sempre un equilibrio forte nel caso di una singola sorgente. PROVA. Sia \u039b un gioco di connessione generale a sorgente singola su un SPG G = -LRB- V, E -RRB- con sorgente se sink t. Consideriamo innanzitutto il seguente ordine parziale tra i giocatori. Per i giocatori i e j, abbiamo che i \u2192 j se esiste un percorso diretto da ti a tj. L'algoritmo COMPUTE-SE, considera i giocatori in ordine crescente, a partire dal giocatore 1. Ogni giocatore i acquister\u00e0 interamente un sottoinsieme degli archi, e ogni giocatore j > i considerer\u00e0 il costo di quelli -LRB- acquistati -RRB- bordi come zero. Quando COMPUTE-SE considera il giocatore j, il costo dei bordi che i giocatori da 1 a j \u2212 1 hanno acquistato \u00e8 fissato a zero, e il giocatore j acquista interamente un percorso minimo Qj da s a tj. Vale a dire, per ogni arco e G Qj \\ Ui < jQi abbiamo pj -LRB- e -RRB- = ce altrimenti pj -LRB- e -RRB- = 0. Mostriamo poi che l'algoritmo COMPUTESE calcola un SE. Supponiamo per assurdo che il profilo p non sia un SE. Allora esiste una coalizione che pu\u00f2 migliorare i costi di tutti i suoi attori attraverso una deviazione. Sia \u0393 una tale coalizione di dimensione minima e sia il giocatore i = max -LCB- j G \u0393 -RCB-. Per un giocatore j G \u0393 siano \u00af Qj e \u00af pj rispettivamente il percorso e il pagamento del giocatore j dopo la deviazione. Sia Q ' un percorso dal pozzo del giocatore i, cio\u00e8 ti, al pozzo di G, iet Allora Q = \u00af Qi UQ ' \u00e8 un percorso dalla sorgente s al pozzo t. Per ogni giocatore j < i, sia yj il vertice di intersezione di Q e tj -LRB- per il Lemma 2.1 \u00e8 garantito che esista -RRB-. Sia y il vertice pi\u00f9 lontano sul cammino Q tale che y = yj per qualche j < i.Il percorso dalla sorgente s al nodo y \u00e8 stato interamente pagato dai giocatori j < i in p -LRB- prima della deviazione -RRB-. I casi che consideriamo sono due. caso a : Dopo la deviazione il giocatore i non paga per i bordi in U j \u2208 \u0393 \\ -LCB- i -RCB- \u00af Qj. Prima della deviazione della coalizione \u0393, il percorso da s a y veniva interamente pagato dai giocatori j < i. Successivamente mostriamo che nessun giocatore k > i paga per alcun vantaggio su qualsiasi percorso da s a ti. Consideriamo un giocatore k > i e sia Q0k = Qk U Q00k, dove Q00k \u00e8 un percorso che collega tk a t. Sia yk il vertice di intersezione di Q0k e ti. Poich\u00e9 esiste un percorso da s a yk che \u00e8 stato interamente pagato dai giocatori j < k prima della deviazione, in particolare il percorso Qis, yk, il giocatore k non pagher\u00e0 alcun vantaggio su qualsiasi percorso che collega s e yk. Pertanto il giocatore i paga interamente per tutti gli archi sul percorso \u00af Qiy, ti, cio\u00e8, \u00af pi -LRB- e -RRB- = ce per tutti gli archi e E \u00af Qiy, ti. Consideriamo ora l'algoritmo COMPUTESE nel passo in cui il giocatore i seleziona il percorso pi\u00f9 breve dalla sorgente s al suo sink ti e determina il suo pagamento pi. A questo punto il giocatore i potrebbe acquistare il percorso \u00af Qiy, ti, poich\u00e9 il percorso da s a y \u00e8 gi\u00e0 stato pagato dai giocatori j < i. Quindi, ci -LRB- \u00af p -RRB- > ci -LRB- p -RRB-. Ci\u00f2 contraddice il fatto che il giocatore i ha migliorato il proprio costo e quindi non tutti i giocatori in \u0393 riducono il proprio costo. Ci\u00f2 implica che p \u00e8 un equilibrio forte. 4.2 Forte prezzo dell'anarchia Mentre per ogni gioco con connessione generale a sorgente singola, vale che PoS = 1 -LSB- 3 -RSB-, il prezzo dell'anarchia pu\u00f2 essere grande quanto n, anche per due archi paralleli. Qui mostriamo che qualsiasi equilibrio forte nei giochi di connessione generale a fonte singola produce il costo ottimale. PROVA. Sia p = -LRB- p1,..., pn -RRB- un equilibrio forte, e sia T \u2217 l'albero di Steiner di costo minimo su tutti i giocatori, radicato nella sorgente -LRB- singola -RRB- s. Sia Te \u2217 il sottoalbero di T \u2217 disconnesso da s quando viene rimosso l'arco e. Sia \u0393 -LRB- Te -RRB- l'insieme dei giocatori che hanno pozzi in Te. Per un insieme di archi E, sia c -LRB- E -RRB- = Ee \u2208 E ce. Assumiamo per assurdo che c -LRB- p -RRB- > c -LRB- T \u2217 -RRB-. Mostreremo che esiste un sottoalbero T0 di T \u2217, che connette un sottoinsieme di giocatori \u0393 C _ N, e un nuovo insieme di pagamenti \u00af p, tale che per ogni i E \u0393, ci -LRB- \u00af p - RRB- < ci -LRB- p -RRB-. Ci\u00f2 contraddir\u00e0 l\u2019ipotesi che p sia un equilibrio forte. Per prima cosa mostriamo come trovare un sottoalbero T0 di T \u2217, tale che per ogni arco e, i pagamenti dei giocatori con sink in Te \u2217 siano maggiori del costo di Te \u2217 U -LCB- e -RCB-. Per costruire T0, definire un arco e come cattivo se il costo di Te \u2217 U -LCB- e -RCB- \u00e8 almeno il pagamento dei giocatori con pozzi in Te \u2217, cio\u00e8 c -LRB- Te \u2217 U -LCB - e -RCB- -RRB- > P -LRB- Te \u2217 -RRB-. Sia B l\u2019insieme degli archi difettosi. Pertanto, in T0 per ogni arco e, abbiamo che c -LRB- Te0 U -LCB- e -RCB- -RRB- < P -LRB- T0e -RRB-.Ci\u00f2 che resta \u00e8 trovare i pagamenti p \u00af per i giocatori in \u0393 -LRB- T0 -RRB- tali che compreranno l'albero T0 e ogni giocatore in \u0393 -LRB- T0 -RRB- abbasser\u00e0 il suo costo, cio\u00e8 ci -LRB- p -RRB- > ci -LRB- \u00af p -RRB- per i E \u0393 -LRB- T0 -RRB-. -LRB- Ricordiamo che i pagamenti hanno la restrizione che il giocatore i pu\u00f2 pagare solo per i bordi sul percorso da s a ti. -RRB- Definiremo ora i pagamenti della coalizione \u00af p. Siano ci -LRB- \u00af p, T0 e \u2208 Te \u00af pi -LRB- e -RRB- i pagamenti del giocatore i per il sottoalbero T0e. Consideriamo il seguente processo bottom up che definisce \u00af p. Assegniamo i pagamenti del bordo e in T0, dopo aver assegnato i pagamenti a tutti i bordi in T0e. Possiamo quindi aggiornare i pagamenti p \u00af dei giocatori i E \u0393 -LRB- T0e -RRB-, impostando dove abbiamo utilizzato il fatto che E e -RRB-.", "keyphrases": ["gioco di connessione a condivisione dei costi", "numero di giocatori", "sorgente e lavandino unici", "lavello multiplo a sorgente singola", "multifonte e lavandino", "costo del bordo", "gioco di connessione equa", "gioco di connessione gener", "topologia del grafico", "forte equilibrio", "coalizione", "costo specifico", "estende il grafico parallelo", "soluzione ottim"]}
{"file_name": "C-3", "text": "Applicazioni auto-adattative sulla griglia Abstract Le griglie sono intrinsecamente eterogenee e dinamiche. Un problema importante nel grid computing \u00e8 la selezione delle risorse, ovvero la ricerca di un insieme di risorse appropriato per l'applicazione. Un altro problema \u00e8 l\u2019adattamento alle mutevoli caratteristiche dell\u2019ambiente della rete. Le soluzioni esistenti a questi due problemi richiedono che sia noto un modello di prestazioni per un'applicazione. Tuttavia, costruire tali modelli \u00e8 un compito complesso. In questo articolo analizziamo un approccio che non richiede modelli prestazionali. Avviamo un'applicazione su qualsiasi insieme di risorse. Durante l'esecuzione dell'applicazione, raccogliamo periodicamente le statistiche sull'esecuzione dell'applicazione e deduciamo i requisiti dell'applicazione da queste statistiche. Quindi, adattiamo il set di risorse per adattarlo meglio alle esigenze dell'applicazione. Questo approccio ci consente di evitare colli di bottiglia nelle prestazioni, come collegamenti WAN sovraccarichi o processori molto lenti, e quindi pu\u00f2 produrre miglioramenti significativi delle prestazioni. Valutiamo il nostro approccio in una serie di scenari tipici della Grid. 1. Introduzione Negli ultimi anni il grid computing \u00e8 diventato una reale alternativa al tradizionale calcolo parallelo. Una griglia fornisce molta potenza computazionale e quindi offre la possibilit\u00e0 di risolvere problemi molto grandi, soprattutto se le applicazioni possono essere eseguite su pi\u00f9 siti contemporaneamente -LRB-7; 15; 20 -RRB-. Tuttavia, anche la complessit\u00e0 degli ambienti Grid \u00e8 molte volte maggiore di quella delle tradizionali macchine parallele come cluster e supercomputer. Un problema importante \u00e8 la selezione delle risorse, ovvero la selezione di un insieme di nodi di calcolo in modo tale che l'applicazione raggiunga buone prestazioni. In un ambiente di griglia questo problema \u00e8 ancora pi\u00f9 difficile, a causa dell'eterogeneit\u00e0 delle risorse: i nodi di calcolo hanno vari Un altro problema importante \u00e8 che le prestazioni e la disponibilit\u00e0 delle risorse di griglia variano nel tempo: i collegamenti di rete o i nodi di calcolo potrebbero sovraccaricarsi, o i nodi di calcolo potrebbero non essere pi\u00f9 disponibili a causa di arresti anomali o perch\u00e9 sono stati richiesti da un'applicazione con priorit\u00e0 pi\u00f9 elevata. Inoltre, potrebbero diventare disponibili risorse nuove e migliori. Per mantenere un livello di prestazioni ragionevole, l'applicazione deve quindi adattarsi alle mutevoli condizioni. Il problema dell'adattamento pu\u00f2 essere ridotto al problema della selezione delle risorse: la fase di selezione delle risorse pu\u00f2 essere ripetuta durante l'esecuzione dell'applicazione, sia a intervalli regolari, sia quando viene rilevato un problema di prestazioni, o quando diventano disponibili nuove risorse. Questo approccio \u00e8 stato adottato da numerosi sistemi -LRB-5; 14; 18 -RRB-. Per la selezione delle risorse, il tempo di esecuzione dell'applicazione viene stimato per alcuni set di risorse e viene selezionato per l'esecuzione il set che fornisce il tempo di esecuzione pi\u00f9 breve. Per prevedere il tempo di esecuzione dell'applicazione su un determinato insieme di risorse, tuttavia, \u00e8 necessaria la conoscenza dell'applicazione. Tipicamente, viene utilizzato un modello di prestazione analitica,ma costruire un modello del genere \u00e8 intrinsecamente difficile e richiede competenze che i programmatori di applicazioni potrebbero non avere. In questo articolo introduciamo e valutiamo un approccio alternativo all'adattamento delle applicazioni e alla selezione delle risorse che non necessita di un modello di prestazioni. Avviamo un'applicazione su qualsiasi insieme di risorse. Durante l'esecuzione dell'applicazione, raccogliamo periodicamente informazioni sui tempi di comunicazione e sui tempi di inattivit\u00e0 dei processori. Utilizziamo queste statistiche per stimare automaticamente i requisiti di risorse dell'applicazione. Successivamente, adattiamo il set di risorse su cui \u00e8 in esecuzione l'applicazione aggiungendo o rimuovendo nodi di calcolo o anche interi cluster. I processori vengono aggiunti o eliminati per rimanere entro le soglie, adattandosi cos\u00ec automaticamente all'ambiente in evoluzione. Uno dei principali vantaggi del nostro approccio \u00e8 che migliora le prestazioni delle applicazioni in molte situazioni diverse tipiche del grid computing. Gestisce tutti i seguenti casi: Il nostro lavoro presuppone che l'applicazione sia malleabile e possa essere eseguita -LRB- in modo efficiente -RRB- su pi\u00f9 siti di una griglia -LRB- ovvero utilizzando la co-allocazione -LRB- 15 -RRB- -RRB- . latenze di zona. Abbiamo applicato le nostre idee alle applicazioni \u201cdivide et impera\u201d che soddisfano questi requisiti. Il divide et impera ha dimostrato di essere un paradigma attraente per la programmazione delle applicazioni grid -LRB-4; 20 -RRB-. Riteniamo che il nostro approccio possa essere esteso ad altre classi di applicazioni con i presupposti dati. Abbiamo implementato la nostra strategia in Satin, che \u00e8 un framework incentrato su Java per la scrittura di applicazioni divide et impera abilitate alla griglia -LRB- 20 -RRB-. Il resto di questo documento \u00e8 strutturato come segue. Nella Sezione 2 spieghiamo quali ipotesi stiamo facendo riguardo alle applicazioni e alle risorse di rete. Nella Sezione 3 presentiamo la nostra strategia di selezione e adattamento delle risorse. Nella Sezione 4 descriviamo la sua implementazione nel framework Satin. Nella Sezione 5, valutiamo il nostro approccio in una serie di scenari di rete. Nella Sezione 6 confrontiamo il nostro approccio con il lavoro correlato. Infine, nella Sezione 7, concludiamo e descriviamo il lavoro futuro. 2. Context e ipotesi In questa sezione descriviamo le nostre ipotesi sulle applicazioni e sulle loro risorse. Assumiamo il seguente modello di risorsa. Le applicazioni vengono eseguite su pi\u00f9 siti contemporaneamente, dove i siti sono cluster o supercomputer. I processori appartenenti a un sito sono collegati tramite una LAN veloce con bassa latenza e larghezza di banda elevata. I diversi siti sono collegati da una WAN. La comunicazione tra i siti soffre di latenze elevate. Abbiamo studiato il problema dell'adattamento nel context delle applicazioni divide et impera. Riteniamo tuttavia che la nostra metodologia possa essere utilizzata anche per altri tipi di applicazioni. In questa sezione riassumiamo le ipotesi sulle applicazioni che sono importanti per il nostro approccio. La prima ipotesi che facciamo \u00e8 che l'applicazione sia malleabile, cio\u00e8\u00e8 in grado di gestire i processori che si uniscono e lasciano il calcolo in corso. In -LRB- 23 -RRB-, abbiamo mostrato come le applicazioni divide et impera possono essere rese tolleranti ai guasti e malleabili. I processori possono essere aggiunti o rimossi in qualsiasi momento del calcolo con un sovraccarico minimo. Il secondo presupposto \u00e8 che l'applicazione possa essere eseguita in modo efficiente su processori con velocit\u00e0 diverse. Ci\u00f2 pu\u00f2 essere ottenuto utilizzando una strategia di bilanciamento del carico dinamico, come il furto di lavoro utilizzato dalle applicazioni divide et impera -LRB- 19 -RRB-. Inoltre, le applicazioni master-worker utilizzano tipicamente strategie di bilanciamento del carico dinamico -LRB- ad esempio, MW - un framework per scrivere applicazioni master-worker abilitate alla griglia -LRB- 12 -RRB- -RRB-. Riteniamo che sia un presupposto ragionevole per un'applicazione di rete, poich\u00e9 le applicazioni per le quali il processore pi\u00f9 lento diventa un collo di bottiglia non saranno in grado di utilizzare in modo efficiente le risorse di rete. Infine, l'applicazione dovrebbe essere insensibile alle latenze su vasta area, in modo che possa funzionare in modo efficiente su una griglia su vasta area -LRB-16; 17 -RRB-. 6. Lavori correlati Numerosi progetti Grid affrontano la questione della selezione e dell'adattamento delle risorse. In GrADS -LRB- 18 -RRB- e ASSIST -LRB- 1 -RRB-, la selezione e l'adattamento delle risorse richiedono un modello di prestazioni che consenta di prevedere i tempi di esecuzione dell'applicazione. Nella fase di selezione delle risorse, viene esaminato un numero di possibili set di risorse e viene selezionato il set di risorse con il tempo di esecuzione previsto pi\u00f9 breve. Se durante il calcolo viene rilevato un degrado delle prestazioni, la fase di selezione delle risorse viene ripetuta. GrADS utilizza il rapporto tra i tempi di esecuzione previsti -LRB- di determinate fasi dell'applicazione -RRB- e i tempi di esecuzione reali come indicatore delle prestazioni dell'applicazione. ASSIST utilizza il numero di iterazioni per unit\u00e0 di tempo -LRB- per applicazioni iterative -RRB- o il numero di attivit\u00e0 per unit\u00e0 di tempo -LRB- per normali applicazioni master-worker -RRB- come indicatore di prestazione. La principale differenza tra questi approcci e il nostro approccio \u00e8 l\u2019uso di modelli di performance. Il vantaggio principale \u00e8 che una volta noto il modello di prestazione, il sistema \u00e8 in grado di prendere decisioni di migrazione pi\u00f9 accurate rispetto al nostro approccio. Tuttavia, anche se le prestazioni non presentano alcun adattamento con adattamento, \u00e8 noto il problema di trovare un set di risorse ottimale -LRB- ovvero il set di risorse con il tempo di esecuzione minimo - RRB- \u00e8 NP-completo. All\u2019aumentare del numero di risorse di rete disponibili, l\u2019accuratezza di questo approccio diminuisce, poich\u00e9 il sottoinsieme dei possibili insiemi di risorse che possono essere esaminati in un tempo ragionevole diventa pi\u00f9 piccolo. Un altro svantaggio di questi sistemi \u00e8 che il rilevamento del degrado delle prestazioni \u00e8 adatto solo per applicazioni iterative o regolari. Cactus -LRB- 2 -RRB- e GridWay -LRB- 14 -RRB- non utilizzano modelli prestazionali. Tuttavia, questi framework sono adatti solo per applicazioni sequenziali -LRB- GridWay -RRB- o per sito singolo -LRB- Cactus -RRB-.In tal caso, il problema della selezione delle risorse si riduce alla selezione della macchina o del cluster pi\u00f9 veloce. La velocit\u00e0 di clock del processore, il carico medio e il numero di processori in un cluster -LRB- Cactus -RRB- vengono utilizzati per classificare le risorse e viene selezionata la risorsa con il rango pi\u00f9 alto. L'applicazione viene migrata se viene rilevato un peggioramento delle prestazioni o se vengono scoperte risorse migliori. Sia Cactus che GridWay utilizzano il numero di iterazioni per unit\u00e0 di tempo come indicatore di prestazione. Il limite principale di questa metodologia \u00e8 che \u00e8 adatta solo per applicazioni sequenziali o su un singolo sito. Inoltre, la selezione delle risorse in base alla velocit\u00e0 di clock non \u00e8 sempre accurata. Infine, il rilevamento del degrado delle prestazioni \u00e8 adatto solo per applicazioni iterative e non pu\u00f2 essere utilizzato per calcoli irregolari come problemi di ricerca e ottimizzazione. Il problema della selezione delle risorse \u00e8 stato studiato anche dal progetto AppLeS -LRB- 5 -RRB-. Nell'ambito di questo progetto sono state studiate una serie di applicazioni e sono stati creati modelli prestazionali per tali applicazioni. Sulla base di tale modello viene costruito un agente di pianificazione che utilizza il modello di prestazioni per selezionare il miglior set di risorse e la migliore pianificazione delle applicazioni su questo set. Gli agenti di pianificazione AppLeS vengono scritti caso per caso e non possono essere riutilizzati per un'altra applicazione. Sono stati inoltre sviluppati due modelli riutilizzabili per classi specifiche di applicazioni, ovvero le applicazioni master-worker -LRB- modello AMWAT -RRB- e le applicazioni di parametrizzazione -LRB- modello APST -RRB-. \u00c8 iniziato il crash di 2 cluster su 9 aggiungendo nodi 96 nodi raggiunti In -LRB- 13 -RRB- viene studiato il problema della schedulazione delle applicazioni master-worker. Pertanto il problema si riduce a trovare il giusto numero di lavoratori. L'approccio qui \u00e8 simile al nostro in quanto non viene utilizzato alcun modello di prestazione. Invece, il sistema tenta di dedurre i requisiti dell'applicazione in fase di esecuzione e regola il numero di lavoratori per avvicinarsi al numero ideale. 7. Conclusioni e lavoro futuro In questo articolo, abbiamo studiato il problema della selezione e dell'adattamento delle risorse negli ambienti grid. Gli approcci esistenti a questi problemi presuppongono in genere l'esistenza di un modello di prestazioni che consenta di prevedere i tempi di esecuzione delle applicazioni su vari insiemi di risorse. Tuttavia, la creazione di modelli prestazionali \u00e8 intrinsecamente difficile e richiede la conoscenza dell'applicazione. Proponiamo un approccio che non richiede una conoscenza approfondita dell'applicazione. Avviamo l'applicazione su un insieme arbitrario di risorse e ne monitoriamo le prestazioni. Il monitoraggio delle prestazioni ci consente di conoscere determinati requisiti dell'applicazione come il numero di processori necessari all'applicazione o i requisiti di larghezza di banda dell'applicazione. Utilizziamo questa conoscenza per affinare gradualmente il set di risorse rimuovendo i nodi inadeguati o aggiungendo nuovi nodi se necessario. Questo approccio non si traduce nell'insieme di risorse ottimale, ma in un insieme di risorse ragionevole, ad esun set esente da vari colli di bottiglia prestazionali come connessioni di rete lente o processori sovraccarichi. Il nostro approccio consente inoltre all'applicazione di adattarsi alle mutevoli condizioni della rete. Se l\u2019efficienza media ponderata scende al di sotto di un certo livello, il coordinatore dell\u2019adattamento inizia a rimuovere i nodi \u201cpeggiori\u201d. Se l'efficienza media ponderata supera un certo livello, vengono aggiunti nuovi nodi. L'applicazione si adatta in modo completamente automatico alle mutevoli condizioni. Il lavoro futuro comporter\u00e0 l\u2019estensione della nostra strategia di adattamento per sostenere la migrazione opportunistica. Ci\u00f2, tuttavia, richiede pianificatori di rete con funzionalit\u00e0 pi\u00f9 sofisticate di quelle attualmente esistenti. Sono inoltre necessarie ulteriori ricerche per ridurre il sovraccarico del benchmarking. Un'altra linea di ricerca che desideriamo indagare \u00e8 l'utilizzo del controllo del feedback per affinare la strategia di adattamento durante l'esecuzione dell'applicazione. Infine, l'implementazione centralizzata del coordinatore dell'adattamento potrebbe diventare un collo di bottiglia per le applicazioni che funzionano su un numero molto elevato di nodi -LRB- centinaia o migliaia -RRB-.", "keyphrases": ["calcolo della griglia", "selezione delle risorse", "ambiente della griglia", "calcolo parallelo", "ambiente parallelo omogeneo", "eterogeneo di risorse", "rete locale a larghezza di banda elevata", "rete WAN con larghezza di banda inferiore", "collegamento di rete", "tempo comune", "tempo di inattivit\u00e0 del processore", "grado di parallelo", "sovraccarico delle risorse", "divide et impera"]}
{"file_name": "I-19", "text": "Fare offerte ottimali in aste simultanee al secondo prezzo di beni perfettamente sostituibili ABSTRACT Deriviamo strategie di offerta ottimali per un agente offerente globale che partecipa a pi\u00f9 aste simultanee al secondo prezzo con sostituti perfetti. Consideriamo innanzitutto un modello in cui tutti gli altri offerenti sono locali e partecipano a un'unica asta. In questo caso, dimostriamo che, assumendo la libera disposizione, l'offerente globale dovrebbe sempre fare offerte diverse da zero in tutte le aste disponibili, indipendentemente dalla distribuzione della valutazione degli offerenti locali. Inoltre, per distribuzioni di valutazione non decrescenti, dimostriamo che il problema di trovare le offerte ottimali si riduce a due dimensioni. Questi risultati valgono sia nel caso in cui il numero di offerenti locali sia noto sia quando questo numero \u00e8 determinato da una distribuzione di Poisson. Questa analisi si estende ai mercati online dove, in genere, le aste si svolgono sia contemporaneamente che in sequenza. Inoltre, combinando risultati analitici e di simulazione, dimostriamo che risultati simili valgono nel caso di diversi offerenti globali, a condizione che il mercato sia composto sia da offerenti globali che locali. Infine, affronteremo l\u2019efficienza del mercato complessivo e dimostreremo che le informazioni sul numero di offerenti locali sono un fattore determinante per il modo in cui un offerente globale influisce sull\u2019efficienza. 1. INTRODUZIONE Il recente aumento di interesse per le aste online ha portato a un numero crescente di aste che offrono prodotti molto simili o simili. Solo su eBay, ad esempio, ci sono spesso centinaia o talvolta addirittura migliaia di aste simultanee in tutto il mondo che vendono tali articoli sostituibili1. In questo context, \u00e8 essenziale sviluppare strategie di offerta che gli agenti autonomi possano utilizzare per operare in modo efficace in un ampio numero di aste. Come mostreremo, tuttavia, questa analisi \u00e8 rilevante anche per un context pi\u00f9 ampio in cui le aste vengono condotte in sequenza, oltre che contemporaneamente. Al contrario, qui consideriamo strategie di offerta per mercati con pi\u00f9 aste simultanee e sostituti perfetti. In particolare, la nostra attenzione \u00e8 rivolta alle aste Vickrey o alle aste con offerta sigillata di secondo prezzo. Tuttavia, i nostri risultati si generalizzano alle impostazioni con aste inglesi poich\u00e9 queste sono strategicamente equivalenti alle aste di secondo prezzo. In questo context, siamo in grado di caratterizzare, per la prima volta, la strategia di massimizzazione dell'utilit\u00e0 di un offerente per fare offerte simultaneamente in un numero qualsiasi di aste di questo tipo e per qualsiasi tipo di distribuzione della valutazione degli offerenti. Pi\u00f9 in dettaglio, consideriamo innanzitutto un mercato in cui un singolo offerente, chiamato offerente globale, pu\u00f2 fare offerte in un numero qualsiasi di aste, mentre si presuppone che gli altri offerenti, chiamati offerenti locali, facciano offerte solo in una singola asta. In questo caso, otteniamo i seguenti risultati: \u2022 Mentre nel caso di un'asta singola al secondo prezzo, la migliore strategia di un offerente \u00e8 offrire il suo valore reale, la migliore strategia per un offerente globale \u00e8 fare un'offerta inferiore a tale valore. \u2022 Siamo in grado di dimostrare che,anche se un offerente globale richiede un solo articolo, l'utilit\u00e0 attesa viene massimizzata partecipando a tutte le aste in cui viene venduto l'articolo desiderato. \u2022 Trovare l'offerta ottimale per ciascuna asta pu\u00f2 essere un compito arduo se si considerano tutte le possibili combinazioni. 2. LAVORI CORRELATI La ricerca nel campo delle aste simultanee pu\u00f2 essere segmentata lungo due grandi linee. Tali analisi vengono tipicamente utilizzate quando il formato d'asta utilizzato nelle aste concorrenti \u00e8 lo stesso -LRB-, ad esempio ci sono aste M Vickrey o aste M primo prezzo -RRB-. Questo articolo adotta il primo approccio nello studio di un mercato di M aste Vickrey simultanee poich\u00e9 questo approccio produce strategie di offerta dimostrabilmente ottimali. Il loro lavoro analizza un mercato composto da coppie di pari valutazione che vogliono fare un'offerta per un com\u00f2. Pertanto, lo spazio di offerta della coppia pu\u00f2 contenere al massimo due offerte poich\u00e9 marito e moglie possono partecipare al massimo a due aste distribuite geograficamente contemporaneamente. Derivano un equilibrio di Nash a strategia mista per il caso speciale in cui il numero di acquirenti \u00e8 elevato. La nostra analisi differisce dalla loro in quanto studiamo aste concorrenti in cui gli offerenti hanno valutazioni diverse e l'offerente globale pu\u00f2 fare offerte in tutte le aste contemporaneamente -LRB-, cosa del tutto possibile con agenti autonomi -RRB-. Successivamente -LSB- 7 -RSB- ha studiato il caso di aste simultanee con beni complementari. Analizzano il caso degli offerenti sia locali che globali e caratterizzano le offerte degli acquirenti e la conseguente efficienza del mercato. L'impostazione prevista in -LSB- 7 -RSB- viene ulteriormente estesa al caso di valori comuni in -LSB- 9 -RSB-. Tuttavia, nessuno di questi lavori si estende facilmente al caso dei beni sostituibili che consideriamo. Per questo caso speciale viene derivato lo spazio delle strategie di equilibrio misto simmetriche, ma ancora una volta il nostro risultato \u00e8 pi\u00f9 generale. Infine, -LSB- 11 -RSB- considera il caso delle aste inglesi concorrenti, in cui sviluppa algoritmi di offerta per acquirenti con diverse attitudini al rischio. Tuttavia, impone che le offerte siano le stesse in tutte le aste, cosa che come mostriamo in questo documento non sempre \u00e8 ottimale. 7. CONCLUSIONI In questo articolo deriviamo strategie di massimizzazione dell'utilit\u00e0 per fare offerte in aste multiple e simultanee al secondo prezzo. Analizziamo innanzitutto il caso in cui un singolo offerente globale fa un'offerta in tutte le aste, mentre tutti gli altri offerenti sono locali e fanno un'offerta in un'unica asta. Per questa impostazione, troviamo il risultato controintuitivo secondo cui \u00e8 ottimale piazzare offerte diverse da zero in tutte le aste che vendono l'oggetto desiderato, anche quando un offerente richiede solo un singolo oggetto e non trae alcun vantaggio aggiuntivo dall'averne di pi\u00f9. Pertanto, un potenziale acquirente pu\u00f2 ottenere notevoli vantaggi partecipando a pi\u00f9 aste e impiegando una strategia di offerta ottimale. Per un certo numero di distribuzioni di valutazione comuni, mostriamo analiticamente che il problema di trovare offerte ottimali si riduce a due dimensioni.Ci\u00f2 semplifica notevolmente il problema di ottimizzazione originale e pu\u00f2 quindi essere utilizzato nella pratica per calcolare le offerte ottimali per qualsiasi numero di aste. Inoltre, indaghiamo un context con pi\u00f9 offerenti globali combinando soluzioni analitiche con un approccio di simulazione. Troviamo che la strategia di un offerente globale non si stabilizza quando sul mercato sono presenti solo offerenti globali, ma converge solo quando sono presenti anche offerenti locali. Sosteniamo, tuttavia, che \u00e8 probabile che i mercati del mondo reale contengano offerenti sia locali che globali. I risultati convergenti sono quindi molto simili all'impostazione con un singolo offerente globale e scopriamo che un offerente trae vantaggio presentando offerte ottimali in pi\u00f9 aste. Per i contesti pi\u00f9 complessi con pi\u00f9 offerenti globali, la simulazione pu\u00f2 quindi essere utilizzata per trovare queste offerte per casi specifici. Infine, confrontiamo l'efficienza di un mercato con pi\u00f9 aste simultanee con e senza un offerente globale. Mostriamo che, se l'offerente pu\u00f2 prevedere con precisione il numero di offerenti locali in ciascuna asta, l'efficienza aumenta leggermente. Al contrario, se c\u2019\u00e8 molta incertezza, l\u2019efficienza diminuisce significativamente all\u2019aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale si aggiudichi pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.l'efficienza diminuisce significativamente all'aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale vinca pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.l'efficienza diminuisce significativamente all'aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale vinca pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.", "keyphrases": ["strategie di offerta ottimali", "agente di offerta globale", "asta simultanea al secondo prezzo", "distribuzione del valore non decrescente", "mercato online", "sistema multiag", "efficienza del mercato", "sostituto perfetto", "asta vickrei", "scienze sociali e del comportamento", "strategia di massima utilit\u00e0"]}
{"file_name": "J-2", "text": "Ridistribuzione ottimale dei pagamenti VCG nel caso peggiore nelle aste di articoli eterogenei con domanda unitaria. ABSTRACT Molti problemi importanti nei sistemi multiagente implicano l'allocazione di pi\u00f9 risorse tra gli agenti. Per i problemi di allocazione delle risorse, il noto meccanismo VCG soddisfa un elenco di propriet\u00e0 desiderate, tra cui l\u2019efficienza, la resistenza alla strategia, la razionalit\u00e0 individuale e la propriet\u00e0 di non deficit. Tuttavia, VCG generalmente non ha un budget equilibrato. Con VCG, gli agenti pagano i pagamenti VCG, il che riduce il benessere sociale. Per compensare la perdita di benessere sociale dovuta ai pagamenti di VCG, sono stati introdotti meccanismi di ridistribuzione di VCG. Questi meccanismi mirano a ridistribuire quanto pi\u00f9 possibile i pagamenti VCG agli agenti, pur mantenendo le propriet\u00e0 desiderate del meccanismo VCG sopra menzionate. Continuiamo la ricerca di meccanismi di ridistribuzione VCG ottimali nel caso peggiore: meccanismi che massimizzano la frazione del pagamento totale di VCG ridistribuito nel caso peggiore. In precedenza, un meccanismo di ridistribuzione VCG ottimale nel caso peggiore -LRB- indicato da WCO -RRB- era caratterizzato per aste multi-unit\u00e0 con valori marginali non crescenti -LSB- 7 -RSB-. Successivamente, il WCO \u00e8 stato generalizzato a contesti che coinvolgono elementi eterogenei -LSB-4 -RSB-, dando vita al meccanismo HETERO. -LSB- 4 -RSB- ha ipotizzato che HETERO sia fattibile e ottimale nel caso peggiore per aste di articoli eterogenei con domanda unitaria. In questo articolo proponiamo un modo pi\u00f9 naturale per generalizzare il meccanismo del WCO. Dimostriamo che il nostro meccanismo generalizzato, sebbene rappresentato diversamente, in realt\u00e0 coincide con HETERO. Sulla base di questa nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore nelle aste di articoli eterogenei con domanda unitaria. Infine, congetturiamo che HETERO rimanga fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 1. INTRODUZIONE 1.1 Meccanismi di ridistribuzione VCG Molti problemi importanti nei sistemi multiagente coinvolgono l'allocazione di pi\u00f9 risorse tra gli agenti. Per i problemi di allocazione delle risorse, il noto meccanismo VCG soddisfa il seguente elenco di propriet\u00e0 desiderate: \u2022 Efficienza: l'allocazione massimizza la valutazione totale degli agenti -LRB- senza considerare i pagamenti -RRB-. \u2022 A prova di strategia: per qualsiasi agente, riferire in modo veritiero \u00e8 una strategia dominante, indipendentemente dalla tipologia degli altri agenti. \u2022 -LRB- Razionalit\u00e0 individuale ex post -RRB- : l'utilit\u00e0 finale di ogni agente -LRB- dopo aver dedotto il suo pagamento -RRB- \u00e8 sempre non negativa. \u2022 Non-deficit: il pagamento totale da parte degli agenti non \u00e8 negativo. Tuttavia, VCG generalmente non ha un budget equilibrato. Con VCG, gli agenti pagano i pagamenti VCG, il che riduce il benessere sociale. Per compensare la perdita di benessere sociale dovuta ai pagamenti di VCG, sono stati introdotti meccanismi di ridistribuzione di VCG. Questi meccanismi continuano ad allocare le risorse utilizzando VCG. Oltre al VCG,questi meccanismi cercano di ridistribuire quanto pi\u00f9 possibile i pagamenti VCG agli agenti. Richiediamo che la redistribuzione di un agente sia indipendente dal suo tipo. Ci\u00f2 \u00e8 sufficiente per mantenere la resistenza alla strategia e l'efficienza -LRB- un agente non ha alcun controllo sulla propria redistribuzione -RRB-. Per i domini collegati in modo uniforme -LRB- comprese le aste multiunit\u00e0 con valori marginali non crescenti e le aste di articoli eterogenei con domanda unitaria -RRB-, il requisito di cui sopra \u00e8 necessario anche per mantenere la resistenza e l'efficienza della strategia -LSB- 8 -RSB-. Un meccanismo di ridistribuzione del VCG \u00e8 fattibile se mantiene tutte le propriet\u00e0 desiderate del meccanismo VCG. Chiediamo cio\u00e8 anche che il processo di redistribuzione mantenga la razionalit\u00e0 individuale e la propriet\u00e0 di non deficit. Sia n il numero di agenti. Poich\u00e9 tutti i meccanismi di ridistribuzione VCG iniziano con l'allocazione secondo il meccanismo VCG, un meccanismo di ridistribuzione VCG \u00e8 caratterizzato dal suo schema di ridistribuzione r ~ = -LRB- r1, r2,..., rn -RRB-. Nel meccanismo di ridistribuzione VCG ~ r, la ridistribuzione dell'agente i \u00e8 uguale a ri -LRB- 01,..., 0i \u2212 1, 0i +1,..., 0n -RRB-, dove 0j \u00e8 il tipo dell'agente j. -LRB- Non dobbiamo distinguere tra il vero tipo di un agente e il tipo riportato, poich\u00e9 tutti i meccanismi di ridistribuzione VCG sono a prova di strategia. -RRB- Un meccanismo di ridistribuzione VCG anonimo \u00e8 caratterizzato da un'unica funzione r. Sotto il meccanismo di ridistribuzione -LRB- anonimo -RRB- VCG r, la ridistribuzione dell'agente i \u00e8 uguale a r -LRB- 0 \u2212 i -RRB-, dove 0 \u2212 i \u00e8 il multiset dei tipi di agenti diversi da i. Usiamo \u03b8 ~ per denotare il profilo del tipo. Sia V CG -LRB- ~ \u03b8 -RRB- il totale. Organizziamo i risultati esistenti in base alle loro impostazioni. Pagamento VCG per questo tipo di profilo. Un meccanismo di ridistribuzione delle VCG soddisfa la propriet\u00e0 di non-deficit se la redistribuzione totale non supera mai il pagamento totale delle VCG. Un meccanismo di ridistribuzione VCG r \u00e8 -LRB- ex post -RRB- individualmente razionale se l'utilit\u00e0 finale di ogni agente \u00e8 sempre non negativa. Dopo la ridistribuzione, l'utilit\u00e0 dell'agente i \u00e8 esattamente la sua ridistribuzione r -LRB- \u03b8 \u2212 i -RRB-. Vogliamo trovare meccanismi di ridistribuzione dei VCG che massimizzino la frazione del pagamento totale dei VCG ridistribuiti nel caso peggiore. Questo problema di progettazione del meccanismo \u00e8 equivalente al seguente modello di ottimizzazione funzionale: In questo articolo, caratterizzeremo analiticamente un meccanismo di ridistribuzione VCG ottimale nel caso peggiore per aste di articoli eterogenei con domanda unitaria.1 Concludiamo questa sottosezione con un esempio di meccanismo di ridistribuzione VCG in l'impostazione pi\u00f9 semplice delle aste per singolo articolo. In un'asta per un singolo oggetto, il tipo di agente \u00e8 un numero reale non negativo che rappresenta la sua utilit\u00e0 per aggiudicarsi l'oggetto. Nelle aste a singolo oggetto, il meccanismo di ridistribuzione Bailey-Cavallo VCG -LSB- 2, 3 -RSB- funziona come segue: \u2022 Assegna l'oggetto secondo VCG: L'Agente 1 si aggiudica l'oggetto e paga \u03b82. Gli altri agenti non vincono nulla e non pagano.\u2022 Ogni agente riceve una ridistribuzione pari a n1 volte il secondo altro tipo pi\u00f9 alto: l'agente 1 e 2 ricevono ciascuno n1 \u03b83. Gli altri agenti ricevono ciascuno n1\u03b82. Il meccanismo di cui sopra ovviamente mantiene la resistenza alla strategia e l'efficienza -LRB- la redistribuzione di un agente non dipende dal suo tipo -RRB-. Mantiene anche la razionalit\u00e0 individuale perch\u00e9 tutte le ridistribuzioni sono non negative. La ridistribuzione totale \u00e8 pari a 2n \u03b83 + il meccanismo di cui sopra mantiene la propriet\u00e0 di non deficit. Infine, la ridistribuzione totale 2 n aste di articoli, la frazione di ridistribuzione nel caso peggiore di questo meccanismo di esempio \u00e8 n \u2212 2 n 1.2 Ricerca precedente sui meccanismi di ridistribuzione VCG ottimale nel caso peggiore In questa sottosezione, esaminiamo i risultati esistenti sul VCG ottimale nel caso peggiore meccanismi di redistribuzione. Ridistribuzione ottimale nel caso peggiore nelle aste a pi\u00f9 unit\u00e0 con domanda unitaria -LSB- 7, 12 -RSB-: Nelle aste a pi\u00f9 unit\u00e0 con domanda unitaria, gli articoli in vendita sono identici. Ogni agente desidera al massimo una copia dell'articolo. -LRB- Le aste a articolo singolo sono casi speciali di aste multi-unit\u00e0 con domanda unitaria. -RRB- Sia m il numero di elementi. In questo articolo considereremo solo i casi in cui m \u2264 n \u2212 2.2. Qui il tipo di un agente \u00e8 un numero reale non negativo che rappresenta la sua valutazione per aver vinto una copia dell'oggetto. -LSB- 7 -RSB- ha inoltre caratterizzato un meccanismo di ridistribuzione VCG per aste multiunit\u00e0 con domanda unitaria, chiamato meccanismo WCO.3 La frazione di ridistribuzione nel caso peggiore di WCO \u00e8 esattamente \u03b1 \u2217. Cio\u00e8, \u00e8 ottimale nel caso peggiore. Il WCO \u00e8 stato ottenuto ottimizzando all'interno della famiglia dei meccanismi di ridistribuzione lineare del VCG. Un meccanismo di ridistribuzione lineare del VCG r assume la forma seguente: Qui, ci sono costanti. -LRB- Consideriamo solo i ci che corrispondono a possibili meccanismi di ridistribuzione del VCG. -RRB- -LSB- \u03b8 \u2212 i -RSB- j \u00e8 il j-esimo tipo pi\u00f9 alto tra \u03b8 \u2212 i. Il meccanismo lineare r \u00e8 caratterizzato dai valori di ci. I valori ottimali ci sono i seguenti: La caratterizzazione di WCO segue quindi: Ridistribuzione ottimale nel caso peggiore nelle aste multi-unit\u00e0 con valori marginali non crescenti -LSB- 7 -RSB-: Aste multi-unit\u00e0 con non2 -LSB- 7 -RSB - ha dimostrato che per aste multi-unit\u00e0 con domanda unitaria, quando m = n \u2212 1, la frazione di ridistribuzione nel caso peggiore -LRB- di qualsiasi meccanismo di ridistribuzione VCG fattibile -RRB- \u00e8 al massimo 0. Poich\u00e9 l'impostazione studiata in questo articolo \u00e8 pi\u00f9 generali -LRB- aste per articoli eterogenei con domanda unitaria -RRB-, abbiamo anche che la frazione di ridistribuzione nel caso peggiore \u00e8 al massimo 0 quando m = n \u2212 1. Poich\u00e9 le aste per articoli eterogenei con x unit\u00e0 sono casi speciali di aste eterogenee -item con x + 1 unit\u00e0, abbiamo che per la nostra impostazione la frazione di ridistribuzione nel caso peggiore \u00e8 al massimo 0 quando m \u2265 n \u2212 1. Cio\u00e8, non ridistribuire nulla \u00e8 ottimale nel caso peggiore quando m \u2265 n \u2212 1. Inoltre, per l'obiettivo di -LSB- 12 -RSB- , il meccanismo ottimo coincide con il WCO solo quando viene applicato il vincolo di razionalit\u00e0 individuale.i valori marginali crescenti sono pi\u00f9 generali delle aste multi-unit\u00e0 con domanda unitaria. In questo context pi\u00f9 generale, gli articoli sono ancora identici, ma un agente pu\u00f2 richiedere pi\u00f9 di una copia dell'articolo. La valutazione di un agente per aver vinto la prima copia dell'oggetto \u00e8 chiamata valore iniziale/primo marginale. Allo stesso modo, la valutazione aggiuntiva di un agente per aver vinto l'i-esima copia dell'oggetto \u00e8 chiamata il suo i-esimo valore marginale. Il tipo di un agente contiene m numeri reali non negativi -LRB- i-esimo valore marginale per i = 1,..., m -RRB-. In questo context, si presuppone inoltre che i valori marginali non siano crescenti. Come discusso in precedenza, in questo context pi\u00f9 generale, la frazione di ridistribuzione nel caso peggiore di qualsiasi meccanismo di ridistribuzione VCG \u00e8 ancora delimitata sopra da \u03b1 *. -LSB- 7 -RSB- ha generalizzato la WCO a questo context e ha dimostrato che la sua frazione di ridistribuzione nel caso peggiore rimane la stessa. Pertanto, WCO -LRB- dopo la generalizzazione -RRB- \u00e8 anche ottimale nel caso peggiore per aste multi-unit\u00e0 con valori marginali non crescenti. La definizione originale di WCO non si generalizza direttamente alle aste multi-unit\u00e0 con valori marginali non crescenti. Quando si tratta di aste multi-unit\u00e0 con valori marginali non crescenti, il tipo di un agente non \u00e8 pi\u00f9 un valore singolo, il che significa che non esiste qualcosa come \"il j-esimo tipo pi\u00f9 alto tra \u03b8_i\". Abusiamo della notazione non differenziando gli agenti e i loro tipi. Ad esempio, \u03b8_i \u00e8 equivalente all'insieme di agenti diversi da i. Sia S un insieme di agenti. io - 1 -RRB-. Qui, U -LRB- S, j -RRB- \u00e8 il nuovo insieme di agenti, dopo aver rimosso da S l'agente con il j-esimo valore marginale iniziale pi\u00f9 alto in S. La forma generale di WCO \u00e8 la seguente: Caso peggiore Ottimale Ridistribuzione nelle aste di articoli eterogenei con domanda unitaria -LSB- 4 -RSB-: Nelle aste di articoli eterogenei con domanda unitaria, gli articoli in vendita sono diversi. Ogni agente richiede al massimo un articolo. Qui, il tipo di un agente \u00e8 costituito da m numeri reali non negativi -LRB- la sua valutazione per l'elemento vincente i per i = 1,...,m -RRB-. Le aste di articoli eterogenei con domanda unitaria sono l'obiettivo principale di questo documento. Poich\u00e9 le aste per articoli eterogenei con domanda unitaria sono pi\u00f9 generali delle aste multi-unit\u00e0 con domanda unitaria, \u03b1 * \u00e8 ancora un limite superiore alla frazione di ridistribuzione nel caso peggiore. -LSB- 4 -RSB- ha proposto il meccanismo HETERO, generalizzando il WCO. Gli autori hanno ipotizzato che HETERO sia fattibile e abbia una frazione di ridistribuzione nel caso peggiore pari ad \u03b1 *. Cio\u00e8, gli autori hanno ipotizzato che HETERO sia il caso peggiore ottimale in questo context. Il contributo principale di questo articolo \u00e8 una dimostrazione di questa congettura. Ridistribuzione nelle aste combinatorie con sostituti lordi -LSB- 6 -RSB-: La condizione dei sostituti lordi \u00e8 stata proposta per la prima volta in -LSB- 9 -RSB-. Come la domanda unitaria, la condizione dei sostituti lordi \u00e8 una condizione sul tipo di agente -LRB- e non dipende dal meccanismo in discussione -RRB-. In parole,Il tipo di agente soddisfa la condizione di sostituzione lorda se la sua domanda per un bene non diminuisce quando i prezzi degli altri beni aumentano. Sia le aste multi-unit\u00e0 con valori marginali non crescenti che le aste per articoli eterogenei con domanda unitaria sono casi particolari di aste combinatorie con sostituti lordi -LSB- 5, 9 -RSB-. Gli autori non hanno trovato un meccanismo ottimale nel caso peggiore per questa impostazione. Alla fine di questo articolo, congettureremo che HETERO sia ottimale per le aste combinatorie con sostituti lordi. Infine, Naroditskiy et al. -LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.-LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.-LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.", "keyphrases": ["progettazione meccanica", "vickrei-clark-grove", "ridistribuire il pagamento", "meccanica efficiente", "a prova di strategia", "meccanismo di razione individuale", "meccan", "meccanismo di ridistribuzione lineare vcg", "trasformarsi in un programma lineare", "carattere analista", "meccanico ottimale nel caso peggiore"]}
{"file_name": "H-4", "text": "Verso valutazioni di gestione delle informazioni personali basate su attivit\u00e0 ABSTRACT La gestione delle informazioni personali -LRB- PIM -RRB- \u00e8 un'area di ricerca in rapida crescita che riguarda il modo in cui le persone archiviano, gestiscono e ritrovano le informazioni. Una caratteristica della ricerca PIM \u00e8 che molti sistemi sono stati progettati per aiutare gli utenti a gestire e ritrovare le informazioni, ma pochissimi sono stati valutati. Ci\u00f2 \u00e8 stato notato da diversi studiosi e spiegato con le difficolt\u00e0 legate all\u2019esecuzione delle valutazioni PIM. Le difficolt\u00e0 includono che le persone ritrovano le informazioni all'interno di raccolte personali uniche; i ricercatori sanno poco sui compiti che inducono le persone a ritrovare le informazioni; e numerose questioni relative alla privacy riguardanti le informazioni personali. In questo documento miriamo a facilitare le valutazioni PIM affrontando ciascuna di queste difficolt\u00e0. Nella prima parte presentiamo uno studio diario sui compiti di ricerca delle informazioni. Lo studio esamina il tipo di attivit\u00e0 che richiedono agli utenti di ritrovare informazioni e produce una tassonomia delle attivit\u00e0 di ricerca per messaggi di posta elettronica e pagine web. Nella seconda parte, proponiamo una metodologia di valutazione basata sui compiti basata sui nostri risultati ed esaminiamo la fattibilit\u00e0 dell'approccio utilizzando due diversi metodi di creazione dei compiti. 1. INTRODUZIONE La gestione delle informazioni personali -LRB- PIM -RRB- \u00e8 un'area di ricerca in rapida crescita che riguarda il modo in cui le persone archiviano, gestiscono e ritrovano le informazioni. I sistemi PIM - i metodi e le procedure con cui le persone gestiscono, classificano e recuperano le informazioni quotidianamente -LSB- 18 -RSB- - stanno diventando sempre pi\u00f9 popolari. Tuttavia la valutazione di questi sistemi PIM \u00e8 problematica. Una delle principali difficolt\u00e0 \u00e8 causata dalla natura personale del PIM. Le persone raccolgono informazioni come conseguenza naturale del completamento di altre attivit\u00e0. Ci\u00f2 significa che le raccolte che le persone generano sono uniche solo per loro e le informazioni all'interno di una raccolta sono intrinsecamente legate alle esperienze personali del proprietario. Poich\u00e9 le raccolte personali sono uniche, non \u00e8 possibile creare attivit\u00e0 di valutazione applicabili a tutti i partecipanti a una valutazione. In secondo luogo, le raccolte personali possono contenere informazioni che i partecipanti si sentono a disagio nel condividere nell\u2019ambito di una valutazione. La natura precisa di queste informazioni (quali informazioni gli individui preferiscono mantenere private) varia da individuo a individuo, rendendo difficile basare le attivit\u00e0 di ricerca sui contenuti delle singole raccolte. Pertanto, gli sperimentatori devono affrontare una serie di sfide per condurre valutazioni PIM realistiche ma controllate. Recentemente, tuttavia, i ricercatori hanno iniziato a concentrarsi sui modi per affrontare il problema della valutazione PIM. Capra -LSB- 6 -RSB- identifica anche la necessit\u00e0 di valutazioni di laboratorio PIM controllate per integrare altre tecniche di valutazione, ponendo un'enfasi specifica sulla necessit\u00e0 di comprendere il comportamento del PIM a livello di attivit\u00e0. In questo articolo, tentiamo di affrontare le difficolt\u00e0 coinvolte nel facilitare le valutazioni PIM controllate di laboratorio.Nella prima parte di questo articolo presentiamo uno studio diario sui compiti di ricerca delle informazioni. Lo studio esamina il tipo di attivit\u00e0 che richiedono agli utenti di ritrovare informazioni e produce una tassonomia delle attivit\u00e0 di ricerca per messaggi di posta elettronica e pagine web. Esaminiamo anche le caratteristiche dei compiti che rendono difficile il ritrovamento. Nella seconda parte, proponiamo una metodologia di valutazione basata sui compiti basata sui nostri risultati ed esaminiamo la fattibilit\u00e0 dell'approccio utilizzando diversi metodi di creazione dei compiti. Pertanto, questo articolo offre due contributi al campo: una maggiore comprensione del comportamento del PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini. 2. LAVORI CORRELATI Sono disponibili diversi approcci per studiare il PIM. Gli approcci naturalistici studiano i partecipanti che si comportano in modo naturale, completando i propri compiti man mano che si verificano, all'interno di ambienti familiari. Questi approcci consentono ai ricercatori di superare molte delle difficolt\u00e0 causate dalla natura personale del PIM. Poich\u00e9 i compiti svolti sono \"reali\" e non simulati, i partecipanti possono utilizzare le proprie esperienze, conoscenze precedenti e raccolte di informazioni per completare i compiti. Sia i metodi etnografici che quelli sul campo richiedono la presenza di uno sperimentatore per valutare come viene eseguita la PIM, il che solleva una serie di questioni. In primo luogo, la valutazione in questo modo \u00e8 costosa; impiegare lunghi periodi di tempo per studiare un piccolo numero di partecipanti e questi piccoli campioni potrebbero non essere rappresentativi del comportamento di popolazioni pi\u00f9 grandi. In secondo luogo, poich\u00e9 i partecipanti non possono essere osservati continuamente, gli sperimentatori devono scegliere quando osservare e ci\u00f2 pu\u00f2 influenzare i risultati. Una strategia alternativa alla conduzione di valutazioni naturalistiche consiste nell'utilizzare l'analisi dei file di registro. Questo approccio fa uso di software di registrazione che cattura un ampio campione di attivit\u00e0 degli utenti nel context dell'uso naturale di un sistema. Ci\u00f2 rivela la necessit\u00e0 di integrare gli studi naturalistici con esperimenti controllati in cui lo sperimentatore pu\u00f2 mettere in relazione il comportamento dei partecipanti allo studio con obiettivi associati a compiti di ricerca noti. Una difficolt\u00e0 nell'esecuzione di questo tipo di valutazione \u00e8 l'approvvigionamento delle collezioni da valutare. Kelly -LSB- 16 -RSB- propone l'introduzione di una raccolta di test condivisa che fornirebbe set di dati, attivit\u00e0 e metriche condivisibili e riutilizzabili per coloro che sono interessati a condurre ricerche PIM. Tuttavia, una raccolta condivisa non sarebbe adatta per gli studi sugli utenti perch\u00e9 non sarebbe possibile incorporare gli aspetti personali del PIM utilizzando una raccolta comune e non familiare. Un approccio alternativo consiste nel chiedere agli utenti di fornire le proprie raccolte di informazioni per simulare ambienti familiari all'interno del laboratorio. Questo approccio \u00e8 stato applicato per studiare il ritrovamento di fotografie personali -LSB- 11 -RSB-, messaggi di posta elettronica -LSB- 20 -RSB- e segnalibri web -LSB- 21 -RSB-. L'utilit\u00e0 di questo approccio dipende dalla facilit\u00e0 con cui \u00e8 possibile trasferire la raccolta o ottenere l'accesso remoto.Un'altra soluzione \u00e8 utilizzare l'intero Web come raccolta quando si studia la ricerca di pagine Web -LSB- 4 -RSB-. Questo potrebbe essere appropriato per studiare la ricerca di pagine web perch\u00e9 studi precedenti hanno dimostrato che le persone spesso utilizzano i motori di ricerca web per questo scopo -LSB- 5 -RSB-. Una seconda difficolt\u00e0 nello svolgimento degli studi di laboratorio PIM \u00e8 la creazione di compiti da far eseguire ai partecipanti che possono essere risolti effettuando una ricerca in una raccolta condivisa o personale. I compiti si riferiscono all'attivit\u00e0 che determina la necessit\u00e0 di informazioni -LSB- 14 -RSB- e sono riconosciuti come importanti nel determinare il comportamento dell'utente -LSB- 26 -RSB-. \u00c8 stato svolto un ampio lavoro per comprendere la natura dei compiti e come il tipo di compito influenza il comportamento degli utenti nella ricerca di informazioni. Ad esempio, i compiti sono stati classificati in termini di complessit\u00e0 crescente -LSB- 3 -RSB- e si ritiene che la complessit\u00e0 dei compiti influenzi il modo in cui gli utenti percepiscono le loro esigenze di informazione -LSB- 25 -RSB- e il modo in cui cercano di trovare informazioni -LSB- 3 -RSB-. Altri lavori precedenti hanno fornito metodologie che consentono la simulazione di compiti durante lo studio del comportamento di ricerca di informazioni -LSB- 2 -RSB-. Tuttavia, si sa poco sui tipi di attivit\u00e0 che inducono le persone a cercare nei propri archivi personali o a ritrovare informazioni gi\u00e0 viste in precedenza. Di conseguenza, \u00e8 difficile ideare situazioni di attivit\u00e0 lavorative simulate per il PIM. L'eccezione \u00e8 lo studio sulla gestione delle fotografie personali, dove il lavoro di Rodden sulla categorizzazione dei compiti di ricerca di fotografie personali ha facilitato la creazione di situazioni di compiti di lavoro simulati -LSB- 22 -RSB-. Sono stati forniti altri suggerimenti su come classificare le attivit\u00e0 PIM. Sebbene queste siano propriet\u00e0 interessanti che possono influenzare il modo in cui un compito verr\u00e0 eseguito, non danno agli sperimentatori spazio sufficiente per ideare i compiti. Le raccolte personali sono uno dei motivi per cui la creazione di attivit\u00e0 \u00e8 cos\u00ec difficile. La tassonomia delle attivit\u00e0 fotografiche di Rodden fornisce una soluzione in questo caso perch\u00e9 consente di classificare le attivit\u00e0 su misura per le collezioni private. I sistemi possono quindi essere confrontati tra tipi di attivit\u00e0 per diversi utenti -LSB- 11 -RSB-. Sfortunatamente non esiste una tassonomia equivalente per altri tipi di oggetti informativi. Inoltre, altri tipi di oggetti sono pi\u00f9 sensibili alla privacy rispetto alle fotografie; \u00e8 improbabile che i partecipanti siano cos\u00ec contenti di consentire ai ricercatori di sfogliare le loro raccolte di posta elettronica per creare attivit\u00e0 come facevano con le fotografie in -LSB- 11 -RSB-. Ci\u00f2 presenta un problema serio: come possono i ricercatori ideare compiti che corrispondano a collezioni private senza comprendere il tipo di compiti che le persone svolgono o senza mettere a repentaglio la privacy dei partecipanti allo studio? Sono stati proposti alcuni metodi. Ad esempio, -LSB- 20 -RSB- ha studiato la ricerca e-mail chiedendo ai partecipanti di ritrovare le e-mail che erano state inviate a ogni membro di un dipartimento; consentendo di utilizzare gli stessi compiti per tutti i partecipanti allo studio.Questo approccio ha garantito che i problemi di privacy fossero evitati e che i partecipanti potessero utilizzare le cose che ricordavano per completare le attivit\u00e0. Tuttavia, i sistemi sono stati testati utilizzando solo un tipo di attivit\u00e0: ai partecipanti \u00e8 stato chiesto di trovare singole e-mail, ciascuna delle quali condivideva propriet\u00e0 comuni. Nella sezione 4 mostriamo che le persone eseguono una gamma pi\u00f9 ampia di attivit\u00e0 di ritrovamento della posta elettronica rispetto a questa. In -LSB- 4 -RSB-, attivit\u00e0 di ricerca generiche venivano create artificialmente eseguendo valutazioni su due sessioni. Nella prima sessione, ai partecipanti \u00e8 stato chiesto di completare attivit\u00e0 lavorative che prevedevano la ricerca di alcune informazioni sconosciute. Nella seconda sessione, i partecipanti hanno completato nuovamente gli stessi compiti, il che naturalmente ha comportato qualche comportamento di ricerca. I limiti di questa tecnica sono che non consente ai partecipanti di sfruttare alcuna connessione personale con le informazioni perch\u00e9 le informazioni che stanno cercando potrebbero non corrispondere a nessun altro aspetto della loro vita. La nostra revisione degli approcci di valutazione motiva la necessit\u00e0 di esperimenti di laboratorio controllati che consentano di testare aspetti strettamente definiti di sistemi o interfacce. Sfortunatamente, \u00e8 stato anche dimostrato che ci sono difficolt\u00e0 nell'esecuzione di questo tipo di valutazione: \u00e8 difficile reperire collezioni e ideare compiti che corrispondano a collezioni private, proteggendo allo stesso tempo la privacy dei partecipanti allo studio. Nella sezione seguente presentiamo uno studio diario sulle attivit\u00e0 di ricerca per e-mail e pagine web. Il risultato \u00e8 una classificazione dei compiti simile a quella ideata da Rodden per le fotografie personali -LSB- 22 -RSB-. Nella sezione 5 ci baseremo su questo lavoro esaminando i metodi per creare attivit\u00e0 che non compromettano la privacy dei partecipanti e discuteremo come il nostro lavoro pu\u00f2 facilitare le valutazioni degli utenti PIM basate sulle attivit\u00e0. Mostriamo che raccogliendo attivit\u00e0 utilizzando diari elettronici, non solo possiamo conoscere le attivit\u00e0 che inducono le persone a ritrovare informazioni personali, ma possiamo conoscere il contenuto delle raccolte private senza compromettere la privacy dei partecipanti. Questa conoscenza pu\u00f2 quindi essere utilizzata per costruire attivit\u00e0 da utilizzare nelle valutazioni PIM. 6. CONCLUSIONI Il presente documento si \u00e8 concentrato sul superamento delle difficolt\u00e0 legate all'esecuzione delle valutazioni PIM. La natura personale del PIM significa che \u00e8 difficile costruire esperimenti equilibrati perch\u00e9 ciascuno dei partecipanti ha le proprie collezioni uniche che si autogenerano completando altre attivit\u00e0. Abbiamo suggerito che per incorporare gli aspetti personali del PIM nelle valutazioni, le prestazioni dei sistemi o degli utenti dovrebbero essere esaminate quando gli utenti completano le attivit\u00e0 sulle proprie raccolte. Questo approccio presenta di per s\u00e9 dei problemi perch\u00e9 la creazione di attivit\u00e0 per le raccolte personali \u00e8 difficile: i ricercatori non sanno molto sui tipi di attivit\u00e0 di ricerca che le persone eseguono e non sanno quali informazioni si trovano all'interno delle singole raccolte personali.In questo documento abbiamo descritto le modalit\u00e0 per superare queste sfide per facilitare le valutazioni degli utenti PIM basate sulle attivit\u00e0. Nella prima parte dell'articolo abbiamo eseguito uno studio diario che ha esaminato le attivit\u00e0 che inducono le persone a ritrovare messaggi di posta elettronica e pagine web. I dati raccolti includevano un'ampia gamma di attivit\u00e0 lavorative e non lavorative e, sulla base dei dati, abbiamo creato una tassonomia delle attivit\u00e0 di ricerca sul web e tramite posta elettronica. Abbiamo scoperto che le persone eseguono tre tipi principali di attivit\u00e0 di ricerca: attivit\u00e0 che richiedono informazioni specifiche all'interno di una singola risorsa, attivit\u00e0 che richiedono una singola risorsa completa e attivit\u00e0 che richiedono il recupero di informazioni da pi\u00f9 risorse. Nella seconda parte del lavoro abbiamo discusso il significato della tassonomia rispetto alla valutazione PIM. Abbiamo dimostrato che \u00e8 possibile condurre esperimenti equilibrati confrontando le prestazioni del sistema o dell'utente nelle categorie di attivit\u00e0 all'interno della tassonomia. Abbiamo anche suggerito due metodi per creare attivit\u00e0 che possono essere completate sulle collezioni personali. Questi metodi non compromettono la privacy dei partecipanti allo studio. Abbiamo esaminato le tecniche suggerite, in primo luogo simulando una situazione sperimentale: ai partecipanti \u00e8 stato chiesto di ripetere i propri compiti mentre li registravano, e in secondo luogo, nel context di una valutazione completa. L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.", "keyphrases": ["persona informa il manag", "misura", "sperimentare", "fattore umano", "ritrovare le informazioni", "problema privato", "tassonomi", "raccogliere individualmente", "messaggio di posta elettronica", "approccio naturalista", "studi di laboratorio"]}
{"file_name": "I-12", "text": "Condividere esperienze per apprendere le caratteristiche degli utenti in ambienti dinamici con dati sparsi ABSTRACT Questo articolo indaga il problema della stima del valore dei parametri probabilistici necessari per il processo decisionale in ambienti in cui un agente, operante all'interno di un sistema multi-agente, non ha informazioni a priori su la struttura della distribuzione dei valori dei parametri. L'agente deve essere in grado di produrre stime anche quando ha effettuato solo un numero limitato di osservazioni dirette, e quindi deve essere in grado di operare con dati sparsi. L'articolo descrive un meccanismo che consente all'agente di migliorare significativamente la propria stima integrando le sue osservazioni dirette con quelle ottenute da altri agenti con cui si sta coordinando. Per evitare distorsioni indesiderate in ambienti relativamente eterogenei e allo stesso tempo utilizzare in modo efficace i dati rilevanti per migliorare le proprie stime, il meccanismo valuta i contributi delle osservazioni di altri agenti sulla base di una stima in tempo reale del livello di somiglianza tra ciascuno di questi agenti e se stesso. Il modulo di \u201cautonomia di coordinamento\u201d di un sistema di gestione del coordinamento ha fornito un context empirico per la valutazione. Le valutazioni basate sulla simulazione hanno dimostrato che il meccanismo proposto supera le stime basate esclusivamente sulle osservazioni di un agente, nonch\u00e9 le stime basate su un aggregato non ponderato delle osservazioni di tutti gli altri agenti. 1. INTRODUZIONE Per molti scenari del mondo reale, gli agenti autonomi devono operare in ambienti dinamici e incerti in cui hanno solo informazioni incomplete sui risultati delle loro azioni e sulle caratteristiche di altri agenti o persone con cui devono cooperare o collaborare. In tali ambienti, gli agenti possono trarre vantaggio dalla condivisione delle informazioni raccolte, mettendo in comune le loro esperienze individuali per migliorare le loro stime dei parametri sconosciuti richiesti per ragionare sulle azioni in condizioni di incertezza. Questo articolo affronta il problema dell'apprendimento della distribuzione dei valori di un parametro probabilistico che rappresenta una caratteristica di una persona che sta interagendo con un agente informatico. La caratteristica da apprendere \u00e8 -LRB- o \u00e8 chiaramente correlata a -RRB-, un fattore importante nel processo decisionale dell'agente.1 L'impostazione di base che consideriamo \u00e8 quella in cui un agente accumula osservazioni su una specifica caratteristica dell'utente e le utilizza produrre una stima tempestiva di una misura che dipende dalla distribuzione di quella caratteristica. In genere, gli agenti devono prendere decisioni in tempo reale, contemporaneamente all\u2019esecuzione delle attivit\u00e0 e nel mezzo di una grande incertezza. Nel resto di questo articolo utilizzeremo il termine \u201cveloce\u201d per riferirci a tali ambienti. In ambienti frenetici, la raccolta di informazioni pu\u00f2 essere limitata e non \u00e8 possibile apprendere offline o attendere la raccolta di grandi quantit\u00e0 di dati prima di prendere decisioni. Cos\u00ec,L'obiettivo dei metodi di stima presentati in questo documento \u00e8 quello di minimizzare l'errore medio nel tempo, piuttosto che determinare un valore accurato al termine di un lungo periodo di interazione. Cio\u00e8, si prevede che l'agente collabori con l'utente per un tempo limitato e tenti di ridurre al minimo l'errore complessivo nelle sue stime. In tali ambienti, i dati acquisiti individualmente da un agente -LRB- e le sue osservazioni -RRB- sono troppo scarsi perch\u00e9 possa ottenere buone stime nell'intervallo di tempo richiesto. Data l\u2019assenza di vincoli strutturali dell\u2019ambiente, gli approcci che dipendono da distribuzioni strutturate possono comportare un bias di stima significativamente elevato. Consideriamo questo problema nel context di un sistema distribuito multi-agente in cui gli agenti informatici supportano persone che svolgono compiti complessi in un ambiente dinamico. Il fatto che gli agenti facciano parte di un ambiente multi-agente, in cui anche altri agenti possono raccogliere dati per stimare una caratteristica simile dei loro utenti, offre la possibilit\u00e0 ad un agente di aumentare le proprie osservazioni con quelle di altri agenti, migliorando cos\u00ec l\u2019accuratezza del suo processo di apprendimento. Inoltre, negli ambienti che consideriamo, gli agenti solitamente accumulano dati a una velocit\u00e0 relativamente simile. Tuttavia, la misura in cui le osservazioni di altri agenti saranno utili a un dato agente dipende dalla misura in cui le distribuzioni delle caratteristiche dei loro utenti sono correlate con quella dell'utente di questo agente. Non vi \u00e8 alcuna garanzia che la distribuzione di due agenti diversi sia altamente correlata positivamente, per non parlare che siano gli stessi. Pertanto, per utilizzare un approccio di condivisione dei dati, un meccanismo di apprendimento deve essere in grado di identificare efficacemente il livello di correlazione tra i dati raccolti da diversi agenti e di pesare i dati condivisi in base al livello di correlazione. La progettazione di un modulo di autonomia di coordinamento -LRB- CA -RRB- all'interno di un sistema di gestione del coordinamento -LRB- come parte del progetto DARPA Coordinators -LSB- 18 -RSB- -RRB-, in cui gli agenti supportano un'attivit\u00e0 di pianificazione distribuita, ha fornito la motivazione iniziale e un context concettuale per questo lavoro. Tuttavia, i meccanismi stessi sono generali e possono essere applicati non solo ad altri ambiti dal ritmo frenetico, ma anche in altri contesti multi-agente in cui gli agenti raccolgono dati che si sovrappongono in una certa misura, a velocit\u00e0 approssimativamente simili, e in cui l\u2019ambiente impone i vincoli di assenza di struttura, limitazione e utilizzo anticipato definiti sopra -LRB-, ad esempio l'esplorazione di pianeti remoti -RRB-. In particolare, le nostre tecniche sarebbero utili in qualsiasi context in cui un gruppo di agenti intraprende un compito in un nuovo ambiente, in cui ciascun agente ottiene osservazioni a una velocit\u00e0 simile dei parametri individuali di cui ha bisogno per il proprio processo decisionale. In questo articolo presentiamo un meccanismo utilizzato per apprendere le caratteristiche chiave dell'utente in ambienti frenetici.Il meccanismo fornisce stime relativamente accurate in tempi brevi, integrando le osservazioni dirette di un singolo agente con osservazioni ottenute da altri agenti con i quali si sta coordinando. In particolare, ci concentreremo sui problemi connessi alla stima del costo di interruzione di una persona e alla stima della probabilit\u00e0 che quella persona disponga delle informazioni richieste dal sistema. Il meccanismo \u00e8 stato testato con successo utilizzando un sistema che simula l'ambiente di un coordinatore. La sezione successiva dell'articolo descrive il problema della stima dei parametri relativi all'utente in domini a ritmo serrato. La sezione 3 fornisce una panoramica dei metodi che abbiamo sviluppato. L'implementazione, il context empirico e i risultati sono forniti nelle Sezioni 4 e 5. Un confronto con i metodi correlati \u00e8 fornito nella Sezione 6 e le conclusioni nella sezione 7. 6. LAVORI CORRELATI Oltre alla letteratura sulla gestione delle interruzioni esaminata nella Sezione 2, molti altri le aree di lavoro precedente sono rilevanti per il meccanismo di condivisione selettiva descritto in questo documento. Il filtraggio collaborativo, che fa previsioni -LRB- filtraggio -RRB- sugli interessi di un utente -LSB- 7 -RSB-, funziona in modo simile alla condivisione selettiva. Tuttavia, i sistemi di filtraggio collaborativo mostrano scarse prestazioni quando non ci sono informazioni sufficienti sugli utenti e quando non ci sono informazioni sufficienti su un nuovo utente il cui gusto il sistema tenta di prevedere -LSB- 7 -RSB-. La condivisione selettiva si basa sulla capacit\u00e0 di trovare somiglianze tra parti specifiche della funzione di distribuzione di probabilit\u00e0 associata a una caratteristica di utenti diversi. Questa capacit\u00e0 \u00e8 strettamente correlata al clustering e alla classificazione, un'area ampiamente studiata nell'apprendimento automatico. Date considerazioni di spazio, la nostra revisione di quest\u2019area \u00e8 limitata ad alcuni approcci rappresentativi per il clustering. Di particolare importanza \u00e8 che l'AC deve trovare somiglianze tra funzioni, definite su un intervallo continuo, senza attributi predefiniti distinti. Un'ulteriore difficolt\u00e0 \u00e8 definire la misura della distanza. Molte tecniche di clustering sono state utilizzate nel data mining -LSB- 2 -RSB-, con particolare attenzione agli aggiornamenti incrementali del clustering, a causa delle dimensioni molto grandi dei database -LSB- 3 -RSB-. Tuttavia, l\u2019applicabilit\u00e0 di questi a domini dal ritmo frenetico \u00e8 piuttosto limitata perch\u00e9 si basano su un ampio insieme di dati esistenti. Il metodo pi\u00f9 rilevante per i nostri scopi \u00e8 l'indice di entropia relativa di Kullback-Leibler utilizzato nella teoria della probabilit\u00e0 e nella teoria dell'informazione -LSB- 12 -RSB-. Tuttavia, il metodo funzioner\u00e0 male negli scenari in cui le funzioni si alternano tra diversi livelli mantenendo la struttura e i momenti \"generali\". 208 La Sesta Intl.. Conf. congiunta. su agenti autonomi e sistemi multi-agente -LRB- AAMAS 07 -RRB-, mentre il nostro approccio basato su Wilcoxon dar\u00e0 loro il punteggio pi\u00f9 alto in termini di somiglianza. Sebbene il test di Wilcoxon sia una procedura statistica ampiamente utilizzata -LSB- 22,14 -RSB-, viene solitamente utilizzato per confrontare due insiemi di dati a variabile singola. A nostra conoscenza, non \u00e8 stato ancora fatto alcun tentativo per estendere le sue propriet\u00e0 come infrastruttura per determinare con chi e in che misura le informazioni dovrebbero essere condivise, come presentato in questo documento. In queste applicazioni viene utilizzato principalmente come strumento di identificazione e criterio di classificazione.", "keyphrases": ["parametro probabilistico", "agente", "informare condividere", "decidere", "ambiente frenetico", "sistema di distribuzione multi-agente", "impara la meccanica", "seleziona-condividi", "stima dei parametri"]}
{"file_name": "I-1", "text": "Interruzione delle attivit\u00e0 negli agenti BDI ABSTRACT Gli agenti intelligenti destinati a funzionare in ambienti dinamici devono essere in grado di gestire con garbo attivit\u00e0 e piani non riusciti. Inoltre, tali agenti dovrebbero essere in grado di prendere decisioni razionali su una linea d'azione appropriata, che pu\u00f2 includere l'interruzione di un compito o di un piano, sia come risultato delle proprie deliberazioni, sia potenzialmente su richiesta di un altro agente. In questo articolo esaminiamo l'incorporazione degli aborti in un'architettura in stile BDI. Discutiamo alcune condizioni in cui \u00e8 appropriato interrompere un'attivit\u00e0 o un piano e come determinare le conseguenze di tale decisione. Integriamo ogni piano con un metodo di interruzione opzionale, analogo al metodo di fallimento presente in alcuni linguaggi di programmazione degli agenti. Forniamo una semantica operativa per il ciclo di esecuzione in presenza di interruzioni nel linguaggio dell'agente astratto CAN, che ci consente di specificare un modello di esecuzione basato su BDI senza limitare la nostra attenzione a un particolare sistema di agenti -LRB- come JACK, Jadex, Jason, o SPARK -RRB-. Una sfida tecnica chiave che affrontiamo \u00e8 la presenza di thread di esecuzione paralleli e di attivit\u00e0 secondarie, che richiedono all'agente di garantire che i metodi di interruzione per ciascun piano siano eseguiti in una sequenza appropriata. 1. INTRODUZIONE Gli agenti intelligenti generalmente lavorano in ambienti complessi e dinamici, come il controllo del traffico aereo o la navigazione robotica, in cui il successo di una particolare azione o piano non pu\u00f2 essere garantito -LSB- 13 -RSB-. Di conseguenza, gestire il fallimento \u00e8 fondamentale per la programmazione dell'agente ed \u00e8 un elemento importante delle caratteristiche dell'agente come robustezza, flessibilit\u00e0 e persistenza -LSB- 21 -RSB-. Nelle architetture degli agenti ispirate al modello Credenza-Desiderio-Intenzione -LRB- BDI -RRB- -LSB- 16 -RSB-, queste propriet\u00e0 sono spesso caratterizzate dalle interazioni tra credenze, obiettivi e piani -LSB- 2 -RSB-. 1 In generale, un agente che desidera realizzare una particolare serie di compiti perseguir\u00e0 una serie di piani contemporaneamente. Quando si verificano fallimenti, la scelta dei piani verr\u00e0 rivista. Ci\u00f2 pu\u00f2 comportare la ricerca di piani alternativi per un compito particolare, la riprogrammazione delle attivit\u00e0 per rispettare meglio i vincoli delle risorse, l'eliminazione di alcune attivit\u00e0 o qualche altra decisione che aumenter\u00e0 la probabilit\u00e0 di successo -LSB- 12, 14 -RSB-. Data questa necessit\u00e0 di deliberazione su attivit\u00e0 o piani falliti, la deliberazione del fallimento \u00e8 comunemente incorporata nel ciclo di esecuzione dell'agente. Oltre ad affrontare il fallimento, una capacit\u00e0 importante di un agente intelligente \u00e8 quella di essere in grado di interrompere un particolare compito o piano. L'interruzione di un'attivit\u00e0 o di un piano \u00e8 distinta dal suo fallimento. Al contrario, abortire non dice nulla sulla capacit\u00e0 di eseguire; elimina semplicemente la necessit\u00e0. Il fallimento si propaga dal basso verso l\u2019alto, mentre l\u2019aborto si propaga dall\u2019alto verso il basso. La possibilit\u00e0 di eseguire contemporaneamente sottopiani introduce diverse complessit\u00e0 in termini di interruzione e fallimento. Per abortire,ci\u00f2 significa che potrebbe essere necessario interrompere pi\u00f9 sottopiani simultanei poich\u00e9 l'interruzione viene propagata verso il basso. In caso di errore, significa che potrebbe essere necessario interrompere i piani di pari livello paralleli poich\u00e9 l'errore si propaga. \u00c8 stato svolto un notevole lavoro sui fallimenti dei piani -LRB- come il rilevamento e la risoluzione dei conflitti di risorse -LSB- 20, 10 -RSB- -RRB- e la maggior parte dei sistemi ad agenti incorporano alcune nozioni di gestione dei fallimenti. Tuttavia, c\u2019\u00e8 stato relativamente poco lavoro sullo sviluppo di tecniche di interruzione oltre al semplice abbandono dei piani e dei compiti attualmente previsti, che non riguardano la pulizia necessaria. Di conseguenza, la maggior parte dei sistemi ad agenti sono piuttosto limitati nel trattare la situazione in cui un ramo di un costrutto parallelo 1Uno pu\u00f2 considerare sia i compiti da svolgere sia gli obiettivi per raggiungere un certo stato del mondo. Un compito pu\u00f2 essere considerato un obiettivo volto a raggiungere lo stato di \"il compito \u00e8 stato eseguito\", e un obiettivo pu\u00f2 essere considerato un compito volto a realizzare quello stato del mondo. Adottiamo quest'ultima visione e usiamo \"compito\" per riferirci anche agli obiettivi. fallisce -LRB- gli approcci comuni includono lasciare che l'altro ramo venga eseguito fino al completamento senza ostacoli o abbandonarlo completamente -RRB-. In questo articolo discutiamo in dettaglio l'integrazione dei metodi di pulizia di interruzione nel ciclo di esecuzione dell'agente, fornendo un approccio unificato agli errori e alle interruzioni. Una caratteristica chiave del nostro approccio basato sulle procedure \u00e8 che consentiamo a ciascun piano di eseguire un codice particolare in caso di fallimento e interruzione. Ci\u00f2 consente a un piano di tentare di garantire uno stato stabile e noto e possibilmente di recuperare alcune risorse e di ripulirlo in altro modo prima di uscire. Di conseguenza, una sfida tecnica centrale \u00e8 gestire l\u2019esecuzione ordinata del codice di pulizia appropriato. Mostriamo come gli aborti possono essere introdotti senza problemi in un'architettura in stile BDI e per la prima volta forniamo una semantica operativa per l'aborto nel linguaggio dell'agente astratto CAN -LSB- 23, 17 -RSB-. Il nostro focus \u00e8 su un singolo agente, complementare al lavoro correlato che considera la gestione delle eccezioni per sistemi singoli e multiagente -LRB- ad esempio, -LSB- 22, 5, 6 -RSB- -RRB-. Questo documento \u00e8 organizzato come segue. Nella Sezione 2 forniamo un esempio delle conseguenze dell'interruzione di un'attivit\u00e0 e nella Sezione 3 discutiamo alcune circostanze in cui dovrebbero verificarsi interruzioni e le procedure appropriate di rappresentazione e invocazione. Nella Sezione 4 mostriamo come possiamo usare CAN per specificare formalmente il comportamento di un piano interrotto. La sezione 5 discute il lavoro correlato e nella sezione 6 presentiamo le nostre conclusioni e il lavoro futuro. 5. LAVORI CORRELATI Il fallimento del piano viene gestito nella versione estesa di AgentSpeak presente nel sistema Jason -LSB- 6 -RSB-. I piani di \"pulizia\" falliti vengono attivati \u200b\u200bdagli eventi di eliminazione degli obiettivi --! G. In un piano di eliminazione dell'obiettivo, il programmatore pu\u00f2 specificare qualsiasi azione di \"annullamento\" e se tentare nuovamente l'obiettivo. Se non viene fornito alcun piano di eliminazione degli obiettivi,Il comportamento predefinito di Jason \u00e8 quello di non ritentare l'obiettivo. La gestione degli errori viene applicata solo ai piani attivati \u200b\u200bdall'aggiunta di un risultato o di un obiettivo di test; in particolare, non vengono registrati gli eventi di eliminazione degli obiettivi in \u200b\u200bcaso di fallimento di un piano di eliminazione degli obiettivi. L'implementazione di H \u00a8 ubner et al. -LSB- 6 -RSB- richiede le azioni interne di Jason. Un requisito per implementare il nostro approccio \u00e8 una capacit\u00e0 riflessiva nell'implementazione dell'agente BDI. Tutti e tre consentono metodi di meta livello che sono stimolati da meta eventi come l'adozione di un obiettivo o il fallimento del piano e offrono capacit\u00e0 introspettive sugli stati di obiettivo e intenzione. Tali strutture di meta livello sono richieste anche dall'approccio di Unruh et al. -LSB- 21 -RSB-, che definiscono la compensazione semantica basata sugli obiettivi per un agente. Gli obiettivi di gestione degli errori vengono richiamati in base alle regole della strategia di gestione degli errori, da un componente dedicato per la gestione degli errori -LRB- FHC -RRB- che tiene traccia dell'esecuzione dell'attivit\u00e0. Questi obiettivi sono specificati dal programmatore dell'agente e collegati ai compiti, proprio come il nostro costrutto FAb -LRB- P, PF, PA -RRB- associa metodi di fallimento e interruzione con un piano P. Si noti, tuttavia, che a differenza di entrambi -LSB - 6 -RSB- e la nostra semantica, -LSB- 21 -RSB- collegano la conoscenza della gestione degli errori al livello dell'obiettivo, non del piano. I loro obiettivi di gestione dei guasti possono consistere in obiettivi di stabilizzazione che eseguono una \"pulizia\" localizzata e immediata per ripristinare lo stato dell'agente a uno stato noto e stabile, e obiettivi di compensazione che eseguono azioni di \"annullamento\". Gli obiettivi di compensazione vengono attivati \u200b\u200ball'interruzione di un obiettivo, e quindi non necessariamente al fallimento dell'obiettivo -LRB-, ovvero se l'FHC ordina all'agente di ritentare l'obiettivo fallito e il tentativo ha successo -RRB-. Ci\u00f2 contrasta con la gestione semplicistica degli errori a livello di piano in cui il cosa e il come sono mescolati nella conoscenza delle attivit\u00e0 del dominio. Sebbene il nostro approccio sia definito a livello di piano, la nostra semantica BDI estesa prevede la separazione tra esecuzione e gestione degli errori. Inoltre, l'FHC mantiene esplicitamente le strutture dei dati per tenere traccia dell'esecuzione dell'agente. Sfruttiamo le strutture di esecuzione esistenti e la capacit\u00e0 autoriflessiva di un agente BDI per eseguire la gestione sia degli aborti che degli errori senza costi aggiuntivi. Le regole della strategia di gestione degli errori di FHC -LRB- ad esempio, se ritentare un obiettivo fallito -RRB- sono sostituite da istruzioni nei nostri piani PF e PA, insieme ai gestori di errori predefiniti a livello di meta in base alla natura dell'agente -LRB - ad esempio, commesso ciecamente -RRB-. L'approccio FHC \u00e8 indipendente dall'architettura dell'agente stesso, in contrasto con il nostro lavoro che \u00e8 dedicato al formalismo BDI -LRB- sebbene non legato ad alcun sistema di agenti -RRB-. 14 La Sesta Conf. Congiunta Intl. su agenti autonomi e sistemi multi-agente -LRB- AAMAS 07 -RRB- un protocollo basato sullo stato. Questo approccio, insieme al checkpoint di stato, viene utilizzato per i sistemi multi-agente in -LSB- 22 -RSB-.L'architettura risultante incorpora l'approccio di gestione degli errori all'interno di un'architettura di elaborazione a coppia per il ripristino da crash dell'agente. Altro lavoro sulla gestione delle eccezioni multi-agente include gli agenti di gestione delle eccezioni distribuiti di AOEX -LSB- 5 -RSB- e le sentinelle simili di -LSB- 8 -RSB-. In entrambi i casi, la logica e la conoscenza della gestione degli errori sono disaccoppiate dagli agenti; al contrario, pur separando la gestione delle eccezioni dalla conoscenza specifica del dominio, Unruh et al. L'FHC di e il nostro approccio mantengono entrambi la logica di gestione degli errori all'interno di un agente. 6. CONCLUSIONE E LAVORO FUTURO I compiti e i piani di un agente potrebbero non essere completati con successo, sia per la scelta dell'agente di interromperli -LRB- forse su richiesta di un altro agente di farlo -RRB-, sia per fattori non richiesti che portano al fallimento. In questo articolo abbiamo presentato un approccio basato su procedure che incorpora compiti e piani di interruzione nel ciclo deliberativo di un agente in stile BDI, fornendo cos\u00ec un approccio unificato al fallimento e all'interruzione. Il nostro contributo principale \u00e8 un'analisi dei requisiti per il funzionamento dell'agente per l'interruzione di compiti e piani e una corrispondente semantica operativa per l'interruzione nel linguaggio dell'agente astratto CAN. Stiamo progettando di implementare un'istanza del nostro approccio nel sistema di agenti SPARK -LSB- 9 -RSB- ; in particolare, il lavoro di questo documento costituir\u00e0 la base per il meccanismo di gestione degli aborti di SPARK. Un agente intelligente non solo gestir\u00e0 con garbo compiti e piani infruttuosi, ma deliberer\u00e0 anche sui suoi atteggiamenti cognitivi per decidere la sua prossima linea d'azione. Abbiamo assunto il comportamento predefinito di un agente in stile BDI, in base alla sua natura: ad esempio, riprovare alternative a un piano fallito finch\u00e9 non riesce o finch\u00e9 non rimangono piani alternativi -LRB- nel qual caso fallire l'attivit\u00e0 -RRB- . Il lavoro futuro consiste nel mettere il nostro approccio al servizio di un ragionamento dell'agente pi\u00f9 dinamico, come l'introspezione che un agente in grado di ragionare sugli effetti dell'interazione del compito e sui requisiti di risorse pu\u00f2 realizzare -LSB- 19, 12 -RSB-. A ci\u00f2 si collega la determinazione del costo dell\u2019interruzione di un\u2019attivit\u00e0 o di un piano e l\u2019utilizzo di questo come input per il processo di deliberazione. Ci\u00f2 influenzerebbe in particolare l'impegno dell'agente verso un determinato compito: maggiore \u00e8 il costo, maggiore \u00e8 l'impegno. Un ulteriore elemento di interesse \u00e8 estendere il nostro approccio al fallimento e all'interruzione agli obiettivi di mantenimento -LSB- 1 -RSB-. Per tali obiettivi \u00e8 necessaria una semantica operativa diversa per l'aborto rispetto agli obiettivi di raggiungimento, per corrispondere alla differenza nella semantica degli obiettivi stessi.L'FHC di e il nostro approccio mantengono entrambi la logica di gestione degli errori all'interno di un agente. 6. CONCLUSIONE E LAVORO FUTURO I compiti e i piani di un agente potrebbero non essere completati con successo, sia per la scelta dell'agente di interromperli -LRB- forse su richiesta di un altro agente di farlo -RRB-, sia per fattori non richiesti che portano al fallimento. In questo articolo abbiamo presentato un approccio basato su procedure che incorpora compiti e piani di interruzione nel ciclo deliberativo di un agente in stile BDI, fornendo cos\u00ec un approccio unificato al fallimento e all'interruzione. Il nostro contributo principale \u00e8 un'analisi dei requisiti per il funzionamento dell'agente per l'interruzione di compiti e piani e una corrispondente semantica operativa per l'interruzione nel linguaggio dell'agente astratto CAN. Stiamo progettando di implementare un'istanza del nostro approccio nel sistema di agenti SPARK -LSB- 9 -RSB- ; in particolare, il lavoro di questo documento costituir\u00e0 la base per il meccanismo di gestione degli aborti di SPARK. Un agente intelligente non solo gestir\u00e0 con garbo compiti e piani infruttuosi, ma deliberer\u00e0 anche sui suoi atteggiamenti cognitivi per decidere la sua prossima linea d'azione. Abbiamo assunto il comportamento predefinito di un agente in stile BDI, in base alla sua natura: ad esempio, riprovare alternative a un piano fallito finch\u00e9 non riesce o finch\u00e9 non rimangono piani alternativi -LRB- nel qual caso fallire l'attivit\u00e0 -RRB- . Il lavoro futuro consiste nel mettere il nostro approccio al servizio di un ragionamento dell'agente pi\u00f9 dinamico, come l'introspezione che un agente in grado di ragionare sugli effetti dell'interazione del compito e sui requisiti di risorse pu\u00f2 realizzare -LSB- 19, 12 -RSB-. A ci\u00f2 si collega la determinazione del costo dell\u2019interruzione di un\u2019attivit\u00e0 o di un piano e l\u2019utilizzo di questo come input per il processo di deliberazione. Ci\u00f2 influenzerebbe in particolare l'impegno dell'agente verso un determinato compito: maggiore \u00e8 il costo, maggiore \u00e8 l'impegno. Un ulteriore elemento di interesse \u00e8 estendere il nostro approccio al fallimento e all'interruzione agli obiettivi di mantenimento -LSB- 1 -RSB-. Per tali obiettivi \u00e8 necessaria una semantica operativa diversa per l'aborto rispetto agli obiettivi di raggiungimento, per corrispondere alla differenza nella semantica degli obiettivi stessi.L'FHC di e il nostro approccio mantengono entrambi la logica di gestione degli errori all'interno di un agente. 6. CONCLUSIONE E LAVORO FUTURO I compiti e i piani di un agente potrebbero non essere completati con successo, sia per la scelta dell'agente di interromperli -LRB- forse su richiesta di un altro agente di farlo -RRB-, sia per fattori non richiesti che portano al fallimento. In questo articolo abbiamo presentato un approccio basato su procedure che incorpora compiti e piani di interruzione nel ciclo deliberativo di un agente in stile BDI, fornendo cos\u00ec un approccio unificato al fallimento e all'interruzione. Il nostro contributo principale \u00e8 un'analisi dei requisiti per il funzionamento dell'agente per l'interruzione di compiti e piani e una corrispondente semantica operativa per l'interruzione nel linguaggio dell'agente astratto CAN. Stiamo progettando di implementare un'istanza del nostro approccio nel sistema di agenti SPARK -LSB- 9 -RSB- ; in particolare, il lavoro di questo documento costituir\u00e0 la base per il meccanismo di gestione degli aborti di SPARK. Un agente intelligente non solo gestir\u00e0 con garbo compiti e piani infruttuosi, ma deliberer\u00e0 anche sui suoi atteggiamenti cognitivi per decidere la sua prossima linea d'azione. Abbiamo assunto il comportamento predefinito di un agente in stile BDI, in base alla sua natura: ad esempio, riprovare alternative a un piano fallito finch\u00e9 non riesce o finch\u00e9 non rimangono piani alternativi -LRB- nel qual caso fallire l'attivit\u00e0 -RRB- . Il lavoro futuro consiste nel mettere il nostro approccio al servizio di un ragionamento dell'agente pi\u00f9 dinamico, come l'introspezione che un agente in grado di ragionare sugli effetti dell'interazione del compito e sui requisiti di risorse pu\u00f2 realizzare -LSB- 19, 12 -RSB-. A ci\u00f2 si collega la determinazione del costo dell\u2019interruzione di un\u2019attivit\u00e0 o di un piano e l\u2019utilizzo di questo come input per il processo di deliberazione. Ci\u00f2 influenzerebbe in particolare l'impegno dell'agente verso un determinato compito: maggiore \u00e8 il costo, maggiore \u00e8 l'impegno. Un ulteriore elemento di interesse \u00e8 estendere il nostro approccio al fallimento e all'interruzione agli obiettivi di mantenimento -LSB- 1 -RSB-. Per tali obiettivi \u00e8 necessaria una semantica operativa diversa per l'aborto rispetto agli obiettivi di raggiungimento, per corrispondere alla differenza nella semantica degli obiettivi stessi.il lavoro di questo documento costituir\u00e0 la base per il meccanismo di gestione degli aborti di SPARK. Un agente intelligente non solo gestir\u00e0 con garbo compiti e piani infruttuosi, ma deliberer\u00e0 anche sui suoi atteggiamenti cognitivi per decidere la sua prossima linea d'azione. Abbiamo assunto il comportamento predefinito di un agente in stile BDI, in base alla sua natura: ad esempio, riprovare alternative a un piano fallito finch\u00e9 non riesce o finch\u00e9 non rimangono piani alternativi -LRB- nel qual caso fallire l'attivit\u00e0 -RRB- . Il lavoro futuro consiste nel mettere il nostro approccio al servizio di un ragionamento dell'agente pi\u00f9 dinamico, come l'introspezione che un agente in grado di ragionare sugli effetti dell'interazione del compito e sui requisiti di risorse pu\u00f2 realizzare -LSB- 19, 12 -RSB-. A ci\u00f2 si collega la determinazione del costo dell\u2019interruzione di un\u2019attivit\u00e0 o di un piano e l\u2019utilizzo di questo come input per il processo di deliberazione. Ci\u00f2 influenzerebbe in particolare l'impegno dell'agente verso un determinato compito: maggiore \u00e8 il costo, maggiore \u00e8 l'impegno. Un ulteriore elemento di interesse \u00e8 estendere il nostro approccio al fallimento e all'interruzione agli obiettivi di mantenimento -LSB- 1 -RSB-. Per tali obiettivi \u00e8 necessaria una semantica operativa diversa per l'aborto rispetto agli obiettivi di raggiungimento, per corrispondere alla differenza nella semantica degli obiettivi stessi.il lavoro di questo documento costituir\u00e0 la base per il meccanismo di gestione degli aborti di SPARK. Un agente intelligente non solo gestir\u00e0 con garbo compiti e piani infruttuosi, ma deliberer\u00e0 anche sui suoi atteggiamenti cognitivi per decidere la sua prossima linea d'azione. Abbiamo assunto il comportamento predefinito di un agente in stile BDI, in base alla sua natura: ad esempio, riprovare alternative a un piano fallito finch\u00e9 non riesce o finch\u00e9 non rimangono piani alternativi -LRB- nel qual caso fallire l'attivit\u00e0 -RRB- . Il lavoro futuro consiste nel mettere il nostro approccio al servizio di un ragionamento dell'agente pi\u00f9 dinamico, come l'introspezione che un agente in grado di ragionare sugli effetti dell'interazione del compito e sui requisiti di risorse pu\u00f2 realizzare -LSB- 19, 12 -RSB-. A ci\u00f2 si collega la determinazione del costo dell\u2019interruzione di un\u2019attivit\u00e0 o di un piano e l\u2019utilizzo di questo come input per il processo di deliberazione. Ci\u00f2 influenzerebbe in particolare l'impegno dell'agente verso un determinato compito: maggiore \u00e8 il costo, maggiore \u00e8 l'impegno. Un ulteriore elemento di interesse \u00e8 estendere il nostro approccio al fallimento e all'interruzione agli obiettivi di mantenimento -LSB- 1 -RSB-. Per tali obiettivi \u00e8 necessaria una semantica operativa diversa per l'aborto rispetto agli obiettivi di raggiungimento, per corrispondere alla differenza nella semantica degli obiettivi stessi.", "keyphrases": ["agente intelligente", "fallimento", "Affare", "metodo di pulizia", "metodo di interruzione", "seme oper", "compito", "obiettivo", "costrutto di obiettivo"]}
{"file_name": "I-9", "text": "Logica lineare temporale come base per interazioni flessibili tra agenti. SOMMARIO Le interazioni tra agenti in un sistema aperto come Internet richiedono un significativo grado di flessibilit\u00e0. Un aspetto cruciale dello sviluppo di tali metodi \u00e8 la nozione di impegni, che fornisce un meccanismo per coordinare i comportamenti interattivi tra gli agenti. In questo articolo, esaminiamo un approccio agli impegni del modello con una stretta integrazione con le azioni del protocollo. Ci\u00f2 significa che non \u00e8 necessario disporre di una mappatura esplicita dalle azioni dei protocolli alle operazioni sugli impegni e di un meccanismo esterno per elaborare e far rispettare gli impegni. Mostriamo come gli agenti possono ragionare sugli impegni e sulle azioni del protocollo per ottenere i risultati finali dei protocolli utilizzando un sistema di ragionamento basato sulla logica lineare temporale, che incorpora un ragionamento sia temporale che sensibile alle risorse. Discutiamo anche dell'applicazione di questo framework a scenari come il commercio online. 1. INTRODUZIONE E MOTIVAZIONE Il paradigma dell'agente \u00e8 diventato particolarmente adatto come metafora progettuale per trattare sistemi complessi comprendenti molti componenti, ciascuno avente il proprio filo di controllo e scopi e coinvolti in interazioni dinamiche e complesse. Negli ambienti multi-agente, gli agenti spesso necessitano di interagire tra loro per raggiungere i propri obiettivi. I protocolli vengono utilizzati per regolare le interazioni. Negli approcci tradizionali alla specifica dei protocolli, come quelli che utilizzano macchine a stati finiti o reti di Petri, i protocolli sono spesso sequenze legali predeterminate di comportamenti interattivi. Pertanto, gli agenti sono tenuti ad adattare i loro comportamenti interattivi per avere successo e le interazioni tra agenti non dovrebbero essere costruite in modo rigido. Per raggiungere la flessibilit\u00e0, come caratterizzato da Yolum e Singh in -LSB- 11 -RSB-, i protocolli di interazione dovrebbero garantire che gli agenti abbiano autonomia sui loro comportamenti interattivi e siano liberi da qualsiasi vincolo non necessario. Inoltre, agli agenti dovrebbe essere consentito di adattare le proprie azioni interattive per trarre vantaggio dalle opportunit\u00e0 o gestire le eccezioni che si presentano durante l'interazione. Ad esempio, considera lo scenario seguente per le vendite online. Cus ha l'obiettivo di ottenere da Mer una mazza da cricket prima o poi. Il Cus ha due possibilit\u00e0 di pagamento. Se il Cus utilizza il pagamento a credito, Mer ha bisogno di una banca Ebank per verificare il credito del Cus. Se il credito del Cus viene approvato, Ebank provveder\u00e0 al pagamento del credito. In caso contrario, il Cus potr\u00e0 scegliere di pagare tramite PayPal. L'interazione termina quando la merce viene consegnata e viene organizzato il pagamento. Un approccio flessibile a questo esempio dovrebbe includere diverse funzionalit\u00e0. In secondo luogo, non dovrebbero esserci vincoli inutili sull\u2019ordine in cui le azioni vengono eseguite, ad esempio quale tra l\u2019effettuare i pagamenti e l\u2019invio della mazza da cricket dovrebbe venire per prima. In terzo luogo, la scelta di una sequenza di azioni interattive dovrebbe basarsi sul ragionamento sui significati intrinseci delle azioni del protocollo, che si basano sulla nozione di impegno, vale a direche si riferisce ad una forte promessa ad altri agenti -LRB- s -RRB- di intraprendere alcune linee d'azione. Gli attuali approcci -LSB- 11, 12, 10, 1 -RSB- per ottenere flessibilit\u00e0 utilizzando la nozione di impegno fanno uso di uno strato astratto di impegni. Tuttavia, in questi approcci, deve essere fornita esternamente una mappatura dalle azioni del protocollo alle operazioni sugli impegni, nonch\u00e9 ai meccanismi di gestione e applicazione degli impegni. L'esecuzione delle azioni del protocollo richiede anche l'esecuzione simultanea di operazioni sui relativi impegni. Di conseguenza, il sovraccarico di elaborazione del livello di impegno rende la specifica e l'esecuzione dei protocolli pi\u00f9 complicate e soggette a errori. Manca anche una logica per esprimere con naturalezza aspetti di risorse, scelte interne ed esterne nonch\u00e9 tempi di protocolli. Piuttosto che creare un ulteriore livello di impegno al di fuori delle azioni del protocollo, cerchiamo di ottenere un modello di impegni che sia integrato con le azioni del protocollo. Sia gli impegni che le azioni del protocollo possono quindi essere ragionati in un unico sistema coerente. Per raggiungere questo obiettivo, specifichiamo i protocolli in modo dichiarativo, ovvero cosa si vuole ottenere piuttosto che come gli agenti dovrebbero interagire. Una chiave per questo \u00e8 usare la logica. La logica temporale, in particolare, \u00e8 adatta per descrivere e ragionare sui vincoli temporali mentre la logica lineare -LSB- 3 -RSB- \u00e8 abbastanza adatta per modellare le risorse. Suggeriamo di utilizzare una combinazione di logica lineare e logica temporale per costruire un quadro di interazione basato sull'impegno che consenta un ragionamento sia temporale che relativo alle risorse per i protocolli di interazione. Ci\u00f2 fornisce un meccanismo naturale di manipolazione e ragionamento, nonch\u00e9 meccanismi di applicazione interna degli impegni basati sulla ricerca di prove. La sezione 2 discute il materiale di base della logica lineare, della logica lineare temporale e degli impegni. La sezione 3 introduce il nostro quadro di modellazione e le specifiche dei protocolli. La sezione 4 illustra come utilizzare il nostro framework per un esempio di interazioni di vendita online tra un commerciante, una banca e un cliente. Discuteremo quindi i vantaggi e i limiti dell'utilizzo del nostro framework per modellare i protocolli di interazione e ottenere flessibilit\u00e0 nella Sezione 5. La Sezione 6 presenta le nostre conclusioni e elementi di ulteriore lavoro. 2. BACKGROUND Per aumentare l'autonomia degli agenti rispetto ai loro comportamenti interattivi, i protocolli dovrebbero essere specificati in termini di ci\u00f2 che si vuole ottenere piuttosto che di come gli agenti dovrebbero agire. In altre parole, i protocolli dovrebbero essere specificati in modo dichiarativo. L'uso della logica \u00e8 fondamentale per questo processo di specifica. 2.1 Logica lineare La logica \u00e8 stata utilizzata come formalismo per modellare e ragionare sui sistemi ad agenti. La logica lineare -LSB- 3 -RSB- \u00e8 nota per la modellazione delle risorse e per l'aggiornamento dei processi. Nei sistemi ad agenti \u00e8 stato considerato per supportare la negoziazione e la pianificazione degli agenti mediante la ricerca di prove -LSB- 5, 8 -RSB-. Nella vita reale, le risorse vengono consumate e nuove risorse vengono create.In una logica come quella classica o temporale, tuttavia, una mappatura diretta delle risorse sulle formule \u00e8 problematica. Se modelliamo risorse come A come \"un dollaro\" e B come \"una tavoletta di cioccolato\", allora A = * B nella logica classica viene letto come \"da un dollaro possiamo ottenere una tavoletta di cioccolato\". Per risolvere tali problemi di mappatura risorsa-formula, Girard ha proposto i vincoli su quali formule verranno utilizzate esattamente una volta e non possono pi\u00f9 essere aggiunte o rimosse liberamente nelle derivazioni e quindi trattando le formule logiche lineari come risorse. Nella logica lineare, un'implicazione lineare A - B, tuttavia, consente di rimuovere A dopo aver derivato B, il che significa che il dollaro scompare dopo aver utilizzato un dollaro per acquistare una barretta di cioccolato. La congiunzione classica -LRB- e -RRB- e la disgiunzione -LRB- o -RRB- sono riformulate su diversi usi di contesti - moltiplicativo come combinazione e additivo come condivisione per ottenere quattro connettivi. La capacit\u00e0 di specificare le scelte tramite i connettivi additivi \u00e8 una caratteristica particolarmente utile della logica lineare. A & -LRB- congiunzione additiva -RRB- B, sta per una propria scelta, A o B ma non entrambi. Nei sistemi ad agenti, questa dualit\u00e0 tra scelte interne ed esterne si manifesta con un agente che ha il potere di scegliere tra alternative e l'altro che deve reagire a qualunque scelta venga fatta. Inoltre, durante l\u2019interazione, la capacit\u00e0 di abbinare consumo e offerta di risorse tra gli agenti pu\u00f2 semplificare la specificazione delle allocazioni delle risorse. La logica lineare \u00e8 un meccanismo naturale per fornire questa capacit\u00e0 -LSB- 5 -RSB-. Inoltre, in -LSB-8-RSB- viene sottolineato che la logica lineare viene utilizzata per modellare gli stati degli agenti come insiemi di risorse consumabili e, in particolare, l'implicazione lineare viene utilizzata per modellare le transizioni tra stati e capacit\u00e0 degli agenti. 2.2 Logica lineare temporale Sebbene la logica lineare offra vantaggi alla modellazione e al ragionamento sulle risorse, non affronta in modo naturale i vincoli temporali. La logica temporale, d'altra parte, \u00e8 un sistema formale che affronta la descrizione e il ragionamento sui cambiamenti dei valori di verit\u00e0 delle espressioni logiche nel tempo -LSB- 2 -RSB-. La logica temporale pu\u00f2 essere utilizzata per la specifica e la verifica di programmi concorrenti e reattivi -LSB- 2 -RSB-. La logica lineare temporale -LRB- TLL -RRB- -LSB- 6 -RSB- \u00e8 il risultato dell'introduzione della logica temporale nella logica lineare e quindi \u00e8 attenta alle risorse e si occupa del tempo. Gli operatori temporali utilizzati sono Q -LRB- next -RRB-, \u2751 -LRB- anytime -RRB- e O -LRB- qualche volta -RRB- -LSB- 6 -RSB-. Le formule senza operatori temporali possono essere considerate disponibili solo al momento. Aggiungere Q ad una formula A, cio\u00e8 QA, significa che A pu\u00f2 essere utilizzata solo la volta successiva ed esattamente una volta. Allo stesso modo, \u2751 A significa che A pu\u00f2 essere utilizzato in qualsiasi momento ed esattamente una volta. OA significa che A pu\u00f2 essere utilizzato una volta alla volta. Sebbene sia \u2751 che O si riferiscano a un punto nel tempo, la scelta di quale momento \u00e8 diversa. Per quanto riguarda \u2751, la scelta \u00e8 una scelta interna,a seconda delle proprie capacit\u00e0. Con O la scelta \u00e8 decisa esternamente da altri. 2.3 Impegno Il concetto di impegno sociale \u00e8 stato riconosciuto come fondamentale per l'interazione degli agenti. Infatti, l'impegno sociale fornisce significati intrinseci alle azioni e agli stati del protocollo -LSB- 11 -RSB-. In particolare, la persistenza negli impegni introduce nella considerazione degli agenti un certo livello di prevedibilit\u00e0 delle azioni di altri agenti, che \u00e8 importante quando gli agenti si occupano di questioni di interdipendenza, vincoli globali o La Sesta Internazionale. Conf. Congiunta. condivisione delle risorse -LSB- 7 -RSB-. Gli approcci basati sugli impegni associano le azioni dei protocolli alle operazioni sugli impegni e gli stati del protocollo all'insieme degli impegni effettivi -LSB- 11 -RSB-. Il completamento del protocollo avviene tramite un ragionamento mezzo-fine sulle operazioni di impegno per portare lo stato attuale agli stati finali in cui tutti gli impegni sono risolti. Da allora si determinano le sequenze giuridiche corrispondenti delle azioni interattive. Pertanto, gli approcci migliorano sistematicamente una variet\u00e0 di calcoli legali -LSB- 11 -RSB-. Gli impegni possono essere ridotti a una forma pi\u00f9 fondamentale nota come pre-impegni. Un pre-impegno qui si riferisce a un potenziale impegno che specifica ci\u00f2 che l'agente proprietario \u00e8 disposto a impegnarsi -LSB- 4 -RSB-, come eseguire alcune azioni o raggiungere un particolare stato. Gli agenti possono negoziare pre-impegni inviandone proposte ad altri. Una volta concordato un impegno preliminare, diventa un impegno e il processo passa dalla fase di negoziazione alla fase di impegno, in cui gli agenti agiscono per mantenere i propri impegni.come eseguire alcune azioni o raggiungere un particolare stato. Gli agenti possono negoziare pre-impegni inviandone proposte ad altri. Una volta concordato un impegno preliminare, diventa un impegno e il processo passa dalla fase di negoziazione alla fase di impegno, in cui gli agenti agiscono per mantenere i propri impegni.come eseguire alcune azioni o raggiungere un particolare stato. Gli agenti possono negoziare pre-impegni inviandone proposte ad altri. Una volta concordato un impegno preliminare, diventa un impegno e il processo passa dalla fase di negoziazione alla fase di impegno, in cui gli agenti agiscono per mantenere i propri impegni.", "keyphrases": ["ambiente multi-agente", "comportamento di interazione", "vincolo temporale", "protocollo di interazione", "logica lineare", "congiunto multiplo", "congiunto classico", "livello di previsione", "pre-impegno", "implicito lineare", "protocollo di emergenza", "condizione commessa", "messaggio di richiesta", "relazione causale"]}
{"file_name": "J-3", "text": "Ottimizzazione del budget nelle aste pubblicitarie basate sulla ricerca ABSTRACT Le societ\u00e0 di ricerca su Internet vendono spazi pubblicitari in base alle query di ricerca degli utenti tramite un'asta. Sebbene sia gi\u00e0 stato svolto un lavoro precedente sul processo d'asta e sui suoi aspetti teorici dei giochi, la maggior parte di esso si concentra sulla societ\u00e0 Internet. In questo lavoro, ci concentriamo sugli inserzionisti, che devono risolvere un complesso problema di ottimizzazione per decidere come piazzare offerte sulle keyphrases per massimizzare il loro ritorno -LRB- il numero di clic degli utenti sui loro annunci -RRB- per un determinato budget. Modelliamo l'intero processo e studiamo questo problema di ottimizzazione del budget. Sebbene la maggior parte delle varianti siano NP-difficili, mostriamo, forse sorprendentemente, che la semplice randomizzazione tra due strategie uniformi che fanno offerte equamente su tutte le keyphrases funziona bene. Pi\u00f9 precisamente, questa strategia ottiene almeno una frazione 1 \u2212 1/e del massimo di clic possibili. Come mostrano i nostri esperimenti preliminari, \u00e8 probabile che tali strategie uniformi siano pratiche. Presentiamo anche risultati di inapprossimabilit\u00e0 e algoritmi ottimi per varianti del problema di ottimizzazione del budget. 1. INTRODUZIONE La ricerca online \u00e8 ormai onnipresente e le societ\u00e0 di ricerca su Internet come Google, Yahoo! e MSN consentono ad aziende e * Lavoro svolto durante la visita a Google, Inc., New York, NY. gli individui fanno pubblicit\u00e0 in base alle query di ricerca poste dagli utenti. I media convenzionali, come le stazioni televisive o i giornali, stabiliscono il prezzo dei propri spazi pubblicitari individualmente e gli inserzionisti acquistano quelli che possono permettersi. Al contrario, le societ\u00e0 di ricerca su Internet hanno difficolt\u00e0 a fissare esplicitamente un prezzo per gli annunci pubblicitari che inseriscono in risposta alle domande degli utenti. Pertanto, si affidano al mercato per determinare i prezzi adeguati utilizzando le aste tra gli inserzionisti. \u00c8 un problema impegnativo organizzare l'asta per realizzare un mercato stabile in cui tutte le parti -LRB-, gli inserzionisti, gli utenti e la societ\u00e0 di ricerca Internet -RRB- siano adeguatamente soddisfatti. La prospettiva in questo articolo non \u00e8 quella della societ\u00e0 di ricerca su Internet che visualizza gli annunci pubblicitari, ma piuttosto quella degli inserzionisti. La sfida dal punto di vista di un inserzionista \u00e8 comprendere e interagire con il meccanismo dell'asta. L'inserzionista determina una serie di keyphrases di suo interesse e quindi deve creare annunci, impostare le offerte per ciascuna parola chiave e fornire un budget totale -LRB- spesso giornaliero -RRB-. Quando un utente pone una query di ricerca, la societ\u00e0 di ricerca su Internet determina gli inserzionisti le cui keyphrases corrispondono alla query e che hanno ancora budget residuo, esegue un'asta tra loro e presenta l'insieme di annunci corrispondenti agli inserzionisti che \"vincono\" l'asta. L'inserzionista il cui annuncio appare paga la societ\u00e0 di ricerca su Internet se l'utente fa clic sull'annuncio. L'attenzione in questo documento \u00e8 sulle modalit\u00e0 di offerta degli inserzionisti. Per la scelta specifica delle keyphrases di suo interesse1, un inserzionista desidera ottimizzare l'effetto complessivo della campagna pubblicitaria.Le societ\u00e0 di ricerca su Internet sono favorevoli a1La scelta delle keyphrases \u00e8 legata alla conoscenza del dominio dell'inserzionista, al comportamento dell'utente e a considerazioni strategiche. Le societ\u00e0 di ricerca su Internet forniscono agli inserzionisti riepiloghi del traffico delle query che sono utili per ottimizzare in modo interattivo la scelta delle keyphrases. Non affrontiamo direttamente la scelta delle keyphrases in questo documento, che \u00e8 affrontata altrove -LSB- 13 -RSB-. tutela gli inserzionisti e fornisce statistiche sulla cronologia dei volumi di clic e previsioni sul rendimento futuro di varie keyphrases. 9 Esistono interazioni complesse tra le keyphrases perch\u00e9 una query dell'utente pu\u00f2 corrispondere a due o pi\u00f9 keyphrases, poich\u00e9 l'inserzionista sta cercando di coprire tutte le possibili keyphrases in qualche dominio. In effetti l'inserzionista finisce per competere con se stesso. Di conseguenza, gli inserzionisti devono affrontare un impegnativo problema di ottimizzazione. Lo scopo di questo articolo \u00e8 risolvere questo problema di ottimizzazione. 1.1 Il problema dell'ottimizzazione del budget Presentiamo una breve discussione e formulazione del problema di ottimizzazione affrontato dagli inserzionisti; una descrizione pi\u00f9 dettagliata si trova nella Sezione 2. Un dato inserzionista vede lo stato delle aste per la pubblicit\u00e0 basata sulla ricerca come segue. Esiste un insieme K di keyphrases di interesse; in pratica, anche i piccoli inserzionisti in genere hanno un ampio insieme K. Esiste un insieme Q di query poste dagli utenti. Per ogni query q GQ, ci sono funzioni che forniscono i clicq -LRB- b -RRB- e il costoq -LRB- b -RRB- che risultano dall'offerta di un particolare importo b nell'asta per quella query, che modelliamo in modo pi\u00f9 formale nella sezione successiva. Esiste un grafo bipartito G sui due insiemi di vertici che rappresentano K e Q. Per qualsiasi query q GQ, i vicini di q in K sono le keyphrases che si dice \"corrispondano\" alla query q. 2 Il problema di ottimizzazione del budget \u00e8 il seguente. Dato il grafico G insieme alle funzioni clicksq -LRB-. -RRB- e costoq -LRB-. -RRB- sulle query, nonch\u00e9 un budget U, determinare le offerte bk per ciascuna parola chiave k GK in modo tale che Pq clicq -LRB- bq -RRB- sia massimizzato in base al costo Pq q -LRB- bq -RRB- < U, dove l'\"offerta effettiva\" bq su una query \u00e8 una funzione delle offerte per le keyphrases nell'intorno di q. Anche se possiamo considerare questo problema come un tradizionale problema di ottimizzazione, nella pratica ci sono diverse sfide a seconda dell'accesso dell'inserzionista alle informazioni sulla query e sul grafico e, in effetti, sull'affidabilit\u00e0 di queste informazioni -LRB-, ad esempio, potrebbe essere basata su dati instabili dati storici -RRB-. Pertanto \u00e8 importante trovare soluzioni a questo problema che non solo ottengano molti clic, ma siano anche semplici, robuste e meno dipendenti dalle informazioni. In questo articolo definiamo la nozione di strategia \"uniforme\", che \u00e8 essenzialmente una strategia che fa offerte in modo uniforme su tutte le keyphrases. Poich\u00e9 questo tipo di strategia elimina la necessit\u00e0 di sapere qualcosa sui dettagli del grafico e aggrega in modo efficace le funzioni di clic e costo nelle query,\u00e8 abbastanza robusto e quindi desiderabile nella pratica. Ci\u00f2 che sorprende \u00e8 che la strategia uniforme effettivamente funziona bene, cosa che dimostreremo. 1.2 I nostri risultati principali e panoramica tecnica Presentiamo risultati positivi e negativi per il problema di ottimizzazione del budget. In particolare mostriamo: 9 Quasi tutte le formulazioni del problema sono NP-Hard. In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.Ci\u00f2 che sorprende \u00e8 che la strategia uniforme effettivamente funziona bene, cosa che dimostreremo. 1.2 I nostri risultati principali e panoramica tecnica Presentiamo risultati positivi e negativi per il problema di ottimizzazione del budget. In particolare mostriamo: 9 Quasi tutte le formulazioni del problema sono NP-Hard. In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.Ci\u00f2 che sorprende \u00e8 che la strategia uniforme effettivamente funziona bene, cosa che dimostreremo. 1.2 I nostri risultati principali e panoramica tecnica Presentiamo risultati positivi e negativi per il problema di ottimizzazione del budget. In particolare mostriamo: 9 Quasi tutte le formulazioni del problema sono NP-Hard. In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.2 I nostri risultati principali e panoramica tecnica Presentiamo risultati positivi e negativi per il problema di ottimizzazione del budget. In particolare mostriamo: 9 Quasi tutte le formulazioni del problema sono NP-Hard. In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.2 I nostri risultati principali e panoramica tecnica Presentiamo risultati positivi e negativi per il problema di ottimizzazione del budget. In particolare mostriamo: 9 Quasi tutte le formulazioni del problema sono NP-Hard. In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.In casi leggermente pi\u00f9 generali della formulazione precedente, in cui i clic hanno pesi, il problema \u00e8 inapprossimabile migliore di un fattore 1 -- 1e, a meno che P = NP. 9 Forniamo un algoritmo di approssimazione -LRB- 1 -- 1/e -RRB- - per il problema di ottimizzazione del bilancio. La strategia trovata dall'algoritmo \u00e8 una strategia uniforme a due offerte, il che significa che randomizza tra l'offerta di un valore b1 su tutte le keyphrases e l'offerta di un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.e offrendo un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.e offrendo un altro valore b2 su tutte le keyphrases fino all'esaurimento del budget3. Mostriamo che questo rapporto di approssimazione \u00e8 stretto per strategie uniformi. Forniamo anche un algoritmo di approssimazione -LRB- 1/2 -RRB- - che offre una strategia uniforme a singola offerta, utilizzando un solo valore b1. -LRB- Questo \u00e8 stretto per le strategie uniformi a singola offerta. -RRB- Queste strategie possono essere calcolate in un tempo quasi lineare in JQJ + JKJ, la dimensione dell'input. Le strategie uniformi possono sembrare ingenue a prima vista perch\u00e9 le keyphrases variano in modo significativo nelle loro funzioni di clic e costo e potrebbe esserci un'interazione complessa tra loro quando pi\u00f9 keyphrases sono rilevanti per una query. Dopotutto, l'ottimale pu\u00f2 configurare offerte arbitrarie su ciascuna delle keyphrases. Anche nel caso semplice in cui il grafico corrisponde a una corrispondenza, l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.l'algoritmo ottimale prevede l'immissione di offerte diverse su keyphrases diverse tramite un imballaggio simile a uno zaino -LRB- Sezione 2 -RRB-. Quindi, potrebbe sorprendere che una semplice strategia uniforme a due offerte sia efficace al 63% o pi\u00f9 rispetto a quella ottimale. La nostra dimostrazione del rapporto di approssimazione 1 -- 1/e si basa su un'analisi contraddittoria. Definiamo un LP rivelatore di fattori -LRB- Sezione 4 -RRB- dove le soluzioni primarie corrispondono a istanze possibili e le soluzioni duali corrispondono a distribuzioni sulle strategie di offerta. Abbiamo condotto simulazioni utilizzando dati reali delle aste di Google. I risultati di queste simulazioni, evidenziati alla fine della Sezione 4, suggeriscono che strategie di offerta uniformi potrebbero essere utili nella pratica. 8. OSSERVAZIONI CONCLUSIVE Un'altra generalizzazione interessante \u00e8 considerare i pesi sui clic, che \u00e8 un modo per modellare le conversioni. -LRB- Una conversione corrisponde ad un'azione da parte dell'utente che ha cliccato per raggiungere il sito dell'inserzionista; ad esempio, una vendita o la registrazione di un account. -RRB- Infine, abbiamo considerato questo sistema come una scatola nera che restituisce clic in funzione dell'offerta, mentre in realt\u00e0 si tratta di un complesso gioco ripetuto che coinvolge pi\u00f9 inserzionisti. In -LSB- 3 -RSB-, \u00e8 stato dimostrato che quando un insieme di inserzionisti utilizza una strategia simile a quella che suggeriamo qui, con un'asta al primo prezzo leggermente modificata, i prezzi si avvicinano a un equilibrio di mercato ben compreso.", "keyphrases": ["bilancio ottimale", "asta pubblicitaria basata sulla ricerca", "Internet", "pubblicit\u00e0", "teoria del gioco", "eurista dell'intrigo", "parola chiave", "strategie di offerta uniformi", "Vickrei Clark Grove", "lp", "gener secondo prezzo"]}
{"file_name": "I-18", "text": "Collaborazione tra uno sciame di satelliti ABSTRACT Il documento tratta della pianificazione a bordo di uno sciame di satelliti attraverso la comunicazione e la negoziazione. Il nostro obiettivo \u00e8 definire comportamenti individuali che si traducano in un comportamento globale che soddisfi i requisiti della missione. Presenteremo la formalizzazione del problema, un protocollo di comunicazione, un metodo di risoluzione basato su regole decisionali reattive e i primi risultati. 1. INTRODUZIONE Sono state sviluppate architetture multi-agente per sciami di satelliti -LSB- 36, 38, 42 -RSB- ma vengono fatte forti ipotesi sulle capacit\u00e0 deliberative e di comunicazione per costruire un piano collettivo. In un context multi-agente, gli agenti che costruiscono un piano collettivo devono essere in grado di cambiare i propri obiettivi, riallocare le risorse e reagire ai cambiamenti dell'ambiente e alle scelte degli altri. Tuttavia, questo passaggio richiede elevate capacit\u00e0 di comunicazione e calcolo. Per allentare i vincoli di comunicazione, viene preso in considerazione il coordinamento basato su norme e convenzioni -LSB- 16 -RSB- o strategie -LSB- 17 -RSB-. Le norme vincolano gli agenti nelle loro decisioni in modo tale da ridurre le possibilit\u00e0 di conflitto. Le strategie sono regole decisionali private che consentono a un agente di trarre vantaggio dal mondo della conoscenza senza comunicazione. Tuttavia, la comunicazione \u00e8 ancora necessaria per condividere informazioni e costruire congetture e piani collettivi. La comunicazione pu\u00f2 essere ottenuta attraverso un approccio stigmergico -LRB- attraverso l'ambiente -RRB- o attraverso lo scambio di messaggi e un protocollo. Un protocollo definisce le interazioni tra agenti e non pu\u00f2 essere disaccoppiato dal suo obiettivo, ad esempio scambiare informazioni, trovare un compromesso, assegnare compiti e cos\u00ec via. I protocolli possono essere visti come un'astrazione di un'interazione -LSB- 9 -RSB-. Tuttavia non sempre un agente pu\u00f2 comunicare con un altro agente oppure le possibilit\u00e0 di comunicazione sono limitate a brevi intervalli di tempo. A livello individuale, gli agenti sono deliberativi per creare un piano locale, ma a livello collettivo utilizzano regole decisionali normative per coordinarsi tra loro. Presenteremo le caratteristiche del nostro problema, un protocollo di comunicazione, un metodo per l'allocazione delle richieste e, infine, le strategie di collaborazione. 7. ESPERIMENTI Le simulazioni di sciami di satelliti sono state implementate in JAVA con la piattaforma JADE -LSB- 3 -RSB-. Il pianificatore di bordo \u00e8 realizzato con programmazione lineare tramite ILOG CPLEX -LSB- 1 -RSB-. Lo scenario di simulazione implementa 3 satelliti su orbite di 6 ore. Sono stati considerati due scenari: il primo con un set di 40 richieste con basso tasso di mutua esclusione e conflitto e il secondo con un set di 74 richieste con alto tasso di mutua esclusione e conflitto. Nel caso di bassa mutua esclusione e tasso di conflitto -LRB- Tabella 1 -RRB-, le simulazioni centralizzate e isolate portano allo stesso numero di osservazioni, con le stesse priorit\u00e0 medie. L\u2019isolamento che porta ad un costo inferiore \u00e8 dovuto all\u2019elevato numero di ridondanze:molti agenti eseguono la stessa richiesta a costi diversi. La simulazione informata riduce il numero di licenziamenti ma aumenta leggermente il costo medio per lo stesso motivo. Possiamo notare che l'uso di 5Ad esempio, l'agente esperto di grado 1 si ritira a causa della strategia altruista e il costo aumenta di a nel caso peggiore, quindi l'agente esperto di grado 2 si ritira a causa della strategia altruista e del costo aumenta di e nel caso peggiore. Quindi nel caso peggiore il costo \u00e8 aumentato di 2e. 292 La Sesta Intl.. Conf. congiunta. Tabella 1: Scenario 1 - risultati della simulazione di 40 richieste Tabella 2: Scenario 2 - risultati della simulazione di 74 richieste Le strategie di collaborazione consentono di ridurre notevolmente il numero di licenziamenti ma il numero di osservazioni diminuisce a causa del vincolo creato dagli impegni. Inoltre anche il costo medio aumenta. Tuttavia ogni ridondanza evitata corrisponde a risorse risparmiate per realizzare le richieste generate a bordo durante la simulazione. Nel caso di elevata mutua esclusione e tasso di conflitto -LRB- Tabella 2 -RRB-, esistono differenze degne di nota tra le simulazioni centralizzate e isolate. Possiamo notare che tutte le simulazioni informate -LRB- con o senza strategie -RRB- consentono di eseguire pi\u00f9 osservazioni di quanto facciano gli agenti isolati con meno ridondanze. Allo stesso modo, possiamo notare che tutte le politiche riducono il costo medio contrariamente al primo scenario. La politica drastica \u00e8 interessante perch\u00e9 non solo consente di effettuare pi\u00f9 osservazioni rispetto agli agenti isolati, ma consente di ridurre notevolmente il costo medio con il minor numero di licenziamenti. Per quanto riguarda il numero di messaggi scambiati, durante le simulazioni si contano 12 incontri tra 2 agenti. Nel peggiore dei casi, ad ogni incontro ogni agente invia N informazioni sulle richieste pi\u00f9 3N informazioni sulle intenzioni degli agenti pi\u00f9 1 messaggio di fine comunicazione, dove N \u00e8 il numero totale di richieste. Di conseguenza, nel caso peggiore vengono scambiati 3864 messaggi per le simulazioni da 40 richieste e 7128 messaggi per le simulazioni da 74 richieste. Questi numeri sono molto pi\u00f9 alti del numero di messaggi effettivamente scambiati. Possiamo notare che le simulazioni informate, che comunicano solo richieste, consentono una riduzione maggiore. Nel caso generale, l'utilizzo della comunicazione e delle strategie consente di ridurre le ridondanze e di risparmiare risorse ma aumenta il costo medio: se una richiesta viene realizzata, gli agenti che la conoscono non la pianificano, anche se il suo costo pu\u00f2 essere successivamente ridotto. Non \u00e8 il caso degli agenti isolati. L\u2019utilizzo di strategie su problemi poco vincolati come lo scenario 1 vincola troppo gli agenti e provoca un ulteriore aumento dei costi. Le strategie sono pi\u00f9 utili su problemi altamente vincolati come lo scenario 2. Sebbene gli agenti si limitino al numero di osservazioni, il costo medio \u00e8 ampiamente ridotto. 8.CONCLUSIONE E LAVORO FUTURO Uno sciame di satelliti di osservazione \u00e8 un sistema multiagente cooperativo con forti vincoli in termini di capacit\u00e0 di comunicazione e calcolo. Al fine di aumentare i risultati della missione globale, proponiamo un approccio ibrido: deliberativo per la pianificazione individuale e reattivo per la collaborazione. Gli agenti ragionano sia sulle richieste da eseguire che sulle intenzioni degli altri agenti -LRB- candidature -RRB-. Un protocollo di comunicazione epidemica utilizza tutte le opportunit\u00e0 di comunicazione per aggiornare queste informazioni. Regole decisionali reattive -LRB- strategie -RRB- sono proposte per risolvere i conflitti che possono sorgere tra agenti. Attraverso la messa a punto delle strategie -LRB- \u03b1, e e \u03bb -RRB- e il loro intreccio plastico all'interno del protocollo, \u00e8 possibile coordinare gli agenti senza comunicazioni aggiuntive: il numero di messaggi scambiati rimane quasi lo stesso tra simulazioni informate e simulazioni che implementano strategie. Sono state effettuate alcune simulazioni per validare sperimentalmente questi protocolli e i primi risultati sono promettenti ma sollevano molti interrogativi. Qual \u00e8 il compromesso tra il tasso di vincolo del problema e la necessit\u00e0 di strategie? In che misura il numero degli esuberi e il costo medio sono influenzati dalla messa a punto delle strategie? I lavori futuri si concentreranno su nuove strategie per risolvere nuovi conflitti, in particolare quelli che sorgono quando si allenta il presupposto di indipendenza tra le richieste. Un secondo punto \u00e8 tenere conto della complessit\u00e0 del problema di pianificazione iniziale. In effetti, l\u2019approccio di pianificazione scelto si traduce in un\u2019esplosione combinatoria con grandi insiemi di richieste: per problemi pi\u00f9 complessi deve essere considerato un approccio in qualsiasi momento o completamente reattivo.in particolare quelli che sorgono quando si allenta il presupposto di indipendenza tra le richieste. Un secondo punto \u00e8 tenere conto della complessit\u00e0 del problema di pianificazione iniziale. In effetti, l\u2019approccio di pianificazione scelto si traduce in un\u2019esplosione combinatoria con grandi insiemi di richieste: per problemi pi\u00f9 complessi deve essere considerato un approccio in qualsiasi momento o completamente reattivo.in particolare quelli che sorgono quando si allenta il presupposto di indipendenza tra le richieste. Un secondo punto \u00e8 tenere conto della complessit\u00e0 del problema di pianificazione iniziale. In effetti, l\u2019approccio di pianificazione scelto si traduce in un\u2019esplosione combinatoria con grandi insiemi di richieste: per problemi pi\u00f9 complessi deve essere considerato un approccio in qualsiasi momento o completamente reattivo.", "keyphrases": ["piano di bordo", "sciame satellitare", "comune e negoziato", "regola decisiva reattiva", "informare l'applicazione del sistema", "sistema multiag", "allocazione di compiti e risorse", "architettura oggetto", "teamag", "immersione", "formica potenziale"]}
{"file_name": "H-16", "text": "L'impatto della memorizzazione nella cache sui motori di ricerca ABSTRACT In questo articolo studiamo i compromessi nella progettazione di sistemi di memorizzazione nella cache efficienti per i motori di ricerca Web. Esploreremo l'impatto di diversi approcci, come la memorizzazione nella cache statica o dinamica e la memorizzazione nella cache dei risultati delle query rispetto alla memorizzazione nella cache degli elenchi di invio. Utilizzando un registro delle query che copre un anno intero, esploriamo i limiti della memorizzazione nella cache e dimostriamo che la memorizzazione nella cache degli elenchi di invio pu\u00f2 ottenere tassi di successo pi\u00f9 elevati rispetto alla memorizzazione nella cache delle risposte alle query. Proponiamo un nuovo algoritmo per la memorizzazione nella cache statica degli elenchi di invio, che supera i metodi precedenti. Studiamo anche il problema di trovare il modo ottimale per dividere la cache statica tra risposte ed elenchi di post. Infine, misuriamo come le modifiche nel registro delle query influiscono sull'efficacia della memorizzazione nella cache statica, data la nostra osservazione che la distribuzione delle query cambia lentamente nel tempo. I nostri risultati e le nostre osservazioni sono applicabili a diversi livelli della gerarchia di accesso ai dati, ad esempio, per uno strato di memoria/disco o uno strato di broker/server remoto. 1. INTRODUZIONE Milioni di query vengono inviate ogni giorno ai motori di ricerca Web e gli utenti hanno grandi aspettative sulla qualit\u00e0 e sulla velocit\u00e0 delle risposte. In tale context, per ottenere tempi di risposta rapidi e aumentare il throughput delle query, l'utilizzo di una cache \u00e8 fondamentale. La memorizzazione nella cache pu\u00f2 essere applicata a diversi livelli con crescenti latenze di risposta o requisiti di elaborazione. La decisione su cosa memorizzare nella cache \u00e8 offline -LRB- statico -RRB- o online -LRB- dinamico -RRB-. Una cache statica si basa su informazioni storiche e viene periodicamente aggiornata. Una cache dinamica sostituisce le voci in base alla sequenza delle richieste. Quando arriva una nuova richiesta, il sistema di cache decide se eliminare alcune voci dalla cache in caso di cache miss. Tali decisioni online si basano su una politica di cache e in passato sono state studiate diverse politiche. Per un motore di ricerca, ci sono due modi possibili per utilizzare una memoria cache: Memorizzazione delle risposte nella cache: quando il motore restituisce risposte a una particolare query, pu\u00f2 decidere di archiviare queste risposte per risolvere query future. Termini di memorizzazione nella cache: quando il motore valuta una particolare query, pu\u00f2 decidere di archiviare in memoria gli elenchi di invio dei termini della query coinvolti. Spesso l'intero set di elenchi di invio non rientra nella memoria e, di conseguenza, il motore deve selezionare un piccolo set da mantenere in memoria e accelerare l'elaborazione delle query. Restituire una risposta a una query gi\u00e0 esistente nella cache \u00e8 pi\u00f9 efficiente che calcolare la risposta utilizzando elenchi di invio memorizzati nella cache. D'altra parte, le query mai viste prima si verificano pi\u00f9 spesso dei termini mai visti prima, il che implica un tasso di errore pi\u00f9 elevato per le risposte memorizzate nella cache. La memorizzazione nella cache degli elenchi di invio presenta ulteriori sfide. Poich\u00e9 gli elenchi di invio hanno dimensioni variabili, memorizzarli nella cache dinamicamente non \u00e8 molto efficiente, a causa della complessit\u00e0 in termini di efficienza e spazio e della distribuzione distorta del flusso di query, come mostrato pi\u00f9 avanti. La memorizzazione nella cache statica degli elenchi di invio pone ancora pi\u00f9 sfide:quando si decide quali termini memorizzare nella cache, ci si trova di fronte al compromesso tra termini interrogati frequentemente e termini con elenchi di invio piccoli che occupano poco spazio. Infine, prima di decidere di adottare una politica di caching statica \u00e8 opportuno analizzare il flusso di query per verificare che le sue caratteristiche non cambino rapidamente nel tempo. Figura 1: un livello di memorizzazione nella cache in un'architettura di ricerca distribuita. In questo articolo esploriamo i compromessi nella progettazione di ciascun livello di cache, dimostrando che il problema \u00e8 lo stesso e cambiano solo pochi parametri. In generale, presupponiamo che ogni livello di memorizzazione nella cache in un'architettura di ricerca distribuita sia simile a quello mostrato nella Figura 1. Utilizziamo un registro delle query che copre un anno intero per esplorare i limiti della memorizzazione dinamica nella cache delle risposte alle query o della pubblicazione di elenchi per i termini delle query. Pi\u00f9 concretamente, le nostre conclusioni principali sono che: \u2022 La memorizzazione nella cache delle risposte alle query produce percentuali di successo inferiori rispetto alla memorizzazione nella cache degli elenchi di invio per i termini della query, ma \u00e8 pi\u00f9 veloce perch\u00e9 non \u00e8 necessaria la valutazione della query. Forniamo un framework per l'analisi del compromesso tra caching statico delle risposte alle query e elenchi di invio; \u2022 La memorizzazione nella cache statica dei termini pu\u00f2 essere pi\u00f9 efficace della memorizzazione nella cache dinamica con, ad esempio, LRU. Forniamo algoritmi basati sul problema KNAPSACK per selezionare le liste di invio da inserire in una cache statica e mostriamo miglioramenti rispetto al lavoro precedente, ottenendo un tasso di successo superiore al 90%; \u2022 Le modifiche alla distribuzione delle query nel tempo hanno un impatto minimo sulla memorizzazione nella cache statica. Le sezioni 2 e 3 riassumono il lavoro correlato e caratterizzano i set di dati che utilizziamo. La sezione 4 discute le limitazioni della memorizzazione nella cache dinamica. Le sezioni 5 e 6 introducono rispettivamente gli algoritmi per la memorizzazione nella cache degli elenchi di invio e un quadro teorico per l'analisi della memorizzazione nella cache statica. La sezione 7 discute l'impatto dei cambiamenti nella distribuzione delle query sulla memorizzazione nella cache statica e la sezione 8 fornisce osservazioni conclusive. 2. LAVORO CORRELATO Esiste un'ampia mole di lavoro dedicata all'ottimizzazione delle query. Esempi pi\u00f9 recenti dimostrano che i primi k documenti per una query possono essere restituiti senza la necessit\u00e0 di valutare l'insieme completo degli elenchi di invio -LSB- 1, 4, 15 -RSB-. Sebbene questi approcci cerchino di migliorare l\u2019efficienza dell\u2019elaborazione delle query, differiscono dal nostro lavoro attuale in quanto non considerano la memorizzazione nella cache. Markatos -LSB- 10 -RSB- mostra l'esistenza della localit\u00e0 temporale nelle query e confronta le prestazioni di diverse politiche di memorizzazione nella cache. Fagni et al. seguire il lavoro di Markatos mostrando che combinando politiche di caching statiche e dinamiche insieme a una politica di prefetch adattiva si ottiene un elevato tasso di successo -LSB- 7 -RSB-. Diversamente dal nostro lavoro, considerano la memorizzazione nella cache e il precaricamento delle pagine dei risultati. Saraiva et al. proporre una nuova architettura per i motori di ricerca Web utilizzando un sistema di caching dinamico a due livelli -LSB- 13 -RSB-. Il loro obiettivo per tali sistemi \u00e8 stato quello di migliorare i tempi di risposta per i motori gerarchici. Nella loro architettura, entrambi i livelli utilizzano una politica di sfratto LRU.Scoprono che la cache di secondo livello pu\u00f2 ridurre efficacemente il traffico sul disco, aumentando cos\u00ec il throughput complessivo. Long e Suel propongono un sistema di caching strutturato secondo tre diversi livelli -LSB- 9 -RSB-. Il livello intermedio contiene le coppie di termini ricorrenti e memorizza le intersezioni delle corrispondenti liste invertite. Questi ultimi due articoli sono correlati al nostro in quanto sfruttano diverse strategie di memorizzazione nella cache a diversi livelli della gerarchia della memoria. Infine, il nostro algoritmo di memorizzazione nella cache statica per la pubblicazione di elenchi nella Sezione 5 utilizza il rapporto frequenza/dimensione per valutare la bont\u00e0 di un elemento da memorizzare nella cache. Idee simili sono state utilizzate nel context del caching dei file -LSB- 17 -RSB-, del caching del Web -LSB- 5 -RSB- e persino del caching degli elenchi di invio -LSB- 9 -RSB-, ma in tutti i casi in modo dinamico collocamento. Per quanto ne sappiamo, siamo i primi a utilizzare questo approccio per la memorizzazione nella cache statica degli elenchi di invio. 8. CONCLUSIONI Il caching \u00e8 una tecnica efficace nei motori di ricerca per migliorare i tempi di risposta, ridurre il carico sui processori di query e migliorare l'utilizzo della larghezza di banda della rete. Presentiamo i risultati sia sul caching dinamico che su quello statico. La memorizzazione nella cache dinamica delle query ha un'efficacia limitata a causa dell'elevato numero di errori obbligatori causati dal numero di query uniche o poco frequenti. I nostri risultati mostrano che nel nostro registro del Regno Unito, il tasso minimo di errori \u00e8 del 50% utilizzando una strategia di set di lavoro. La memorizzazione nella cache dei termini \u00e8 pi\u00f9 efficace rispetto al tasso di mancata riuscita, raggiungendo valori fino al 12%. Proponiamo inoltre un nuovo algoritmo per il caching statico degli elenchi di inserzioni che supera i precedenti algoritmi di caching statico e algoritmi dinamici come LRU e LFU, ottenendo valori di hit rate superiori di oltre il 10% rispetto a queste strategie. Presentiamo un framework per l'analisi del compromesso tra la memorizzazione nella cache dei risultati delle query e la memorizzazione nella cache degli elenchi di invio, e simuliamo diversi tipi di architetture. I nostri risultati mostrano che per gli ambienti centralizzati e LAN esiste un'allocazione ottimale dei risultati delle query di memorizzazione nella cache e della memorizzazione nella cache degli elenchi di invio, mentre per gli scenari WAN in cui prevale il tempo di rete \u00e8 pi\u00f9 importante memorizzare nella cache i risultati delle query. Figura 14: Impatto delle modifiche alla distribuzione sulla memorizzazione nella cache statica degli elenchi di invio.e anche il caching degli elenchi di inserzioni -LSB- 9 -RSB-, ma in ogni caso in un ambiente dinamico. Per quanto ne sappiamo, siamo i primi a utilizzare questo approccio per la memorizzazione nella cache statica degli elenchi di invio. 8. CONCLUSIONI Il caching \u00e8 una tecnica efficace nei motori di ricerca per migliorare i tempi di risposta, ridurre il carico sui processori di query e migliorare l'utilizzo della larghezza di banda della rete. Presentiamo i risultati sia sul caching dinamico che su quello statico. La memorizzazione nella cache dinamica delle query ha un'efficacia limitata a causa dell'elevato numero di errori obbligatori causati dal numero di query uniche o poco frequenti. I nostri risultati mostrano che nel nostro registro del Regno Unito, il tasso minimo di errori \u00e8 del 50% utilizzando una strategia di set di lavoro. La memorizzazione nella cache dei termini \u00e8 pi\u00f9 efficace rispetto al tasso di mancata riuscita, raggiungendo valori fino al 12%. Proponiamo inoltre un nuovo algoritmo per il caching statico degli elenchi di inserzioni che supera i precedenti algoritmi di caching statico e algoritmi dinamici come LRU e LFU, ottenendo valori di hit rate superiori di oltre il 10% rispetto a queste strategie. Presentiamo un framework per l'analisi del compromesso tra la memorizzazione nella cache dei risultati delle query e la memorizzazione nella cache degli elenchi di invio, e simuliamo diversi tipi di architetture. I nostri risultati mostrano che per gli ambienti centralizzati e LAN esiste un'allocazione ottimale dei risultati delle query di memorizzazione nella cache e della memorizzazione nella cache degli elenchi di invio, mentre per gli scenari WAN in cui prevale il tempo di rete \u00e8 pi\u00f9 importante memorizzare nella cache i risultati delle query. Figura 14: Impatto delle modifiche alla distribuzione sulla memorizzazione nella cache statica degli elenchi di invio.e anche il caching degli elenchi di inserzioni -LSB- 9 -RSB-, ma in ogni caso in un ambiente dinamico. Per quanto ne sappiamo, siamo i primi a utilizzare questo approccio per la memorizzazione nella cache statica degli elenchi di invio. 8. CONCLUSIONI Il caching \u00e8 una tecnica efficace nei motori di ricerca per migliorare i tempi di risposta, ridurre il carico sui processori di query e migliorare l'utilizzo della larghezza di banda della rete. Presentiamo i risultati sia sul caching dinamico che su quello statico. La memorizzazione nella cache dinamica delle query ha un'efficacia limitata a causa dell'elevato numero di errori obbligatori causati dal numero di query uniche o poco frequenti. I nostri risultati mostrano che nel nostro registro del Regno Unito, il tasso minimo di errori \u00e8 del 50% utilizzando una strategia di working set. La memorizzazione nella cache dei termini \u00e8 pi\u00f9 efficace rispetto al tasso di mancata riuscita, raggiungendo valori fino al 12%. Proponiamo inoltre un nuovo algoritmo per il caching statico degli elenchi di inserzioni che supera i precedenti algoritmi di caching statico e algoritmi dinamici come LRU e LFU, ottenendo valori di hit rate superiori di oltre il 10% rispetto a queste strategie. Presentiamo un framework per l'analisi del compromesso tra la memorizzazione nella cache dei risultati delle query e la memorizzazione nella cache degli elenchi di invio, e simuliamo diversi tipi di architetture. I nostri risultati mostrano che per gli ambienti centralizzati e LAN esiste un'allocazione ottimale dei risultati delle query di memorizzazione nella cache e della memorizzazione nella cache degli elenchi di invio, mentre per gli scenari WAN in cui prevale il tempo di rete \u00e8 pi\u00f9 importante memorizzare nella cache i risultati delle query. Figura 14: Impatto delle modifiche alla distribuzione sulla memorizzazione nella cache statica degli elenchi di invio.", "keyphrases": ["sistema di cache efficiente", "motore di ricerca web", "cache statica", "cache dinamica", "risultato della query sulla cache", "elenco dei post nella cache", "cache statica", "risposta ed elenco dei post", "registro delle query", "effetto della cache statica", "distribuzione della queri", "gerarchia di accesso ai dati", "strato del disco", "livello server remoto"]}
{"file_name": "J-32", "text": "Equilibri di Nash in Graphical Games on Trees Revisited * I giochi grafici sono stati proposti come modello teorico dei giochi di reti distribuite su larga scala di agenti non cooperativi. Quando il numero di giocatori \u00e8 elevato e il grafico sottostante ha un livello basso, forniscono un modo conciso per rappresentare i guadagni dei giocatori. \u00c8 stato recentemente dimostrato che il problema di trovare gli equilibri di Nash in un gioco grafico generale di grado 3 con due azioni per giocatore \u00e8 completo per la classe di complessit\u00e0 PPAD, indicando che \u00e8 improbabile che esista un algoritmo tempo-polinomiale per questo problema. In questo articolo studiamo la complessit\u00e0 dei giochi grafici con due azioni per giocatore su alberi a gradi limitati. Questa impostazione \u00e8 stata presa in considerazione per la prima volta da Kearns, Littman e Singh, che hanno proposto un algoritmo basato sulla programmazione dinamica che calcola tutti gli equilibri di Nash di tali giochi. Il tempo di esecuzione del loro algoritmo \u00e8 esponenziale, sebbene gli equilibri approssimativi possano essere calcolati in modo efficiente. Successivamente, Littman, Kearns e Singh hanno proposto una modifica a questo algoritmo in grado di trovare un singolo equilibrio di Nash in tempo polinomiale. Mostriamo che questo algoritmo modificato non \u00e8 corretto: l'output non \u00e8 sempre un equilibrio di Nash. Proponiamo quindi un nuovo algoritmo basato sulle idee di Kearns et al. e calcola tutti gli equilibri di Nash in tempo quadratico se il grafico di input \u00e8 un percorso e in tempo polinomiale se \u00e8 un grafico arbitrario di massimo grado 2. Inoltre, il nostro algoritmo pu\u00f2 essere utilizzato per calcolare gli equilibri di Nash di giochi grafici su alberi arbitrari, ma il tempo di esecuzione pu\u00f2 essere esponenziale, anche quando l'albero ha un grado limitato. Mostriamo che questo \u00e8 inevitabile: qualsiasi algoritmo di questo tipo richieder\u00e0 tempo esponenziale, anche su alberi a gradi limitati con larghezza di percorso 2. \u00c8 una questione aperta se il nostro algoritmo funziona in tempo polinomiale su grafici con larghezza di percorso 1, ma lo mostriamo trovare un equilibrio di Nash per un gioco grafico a 2 azioni in cui il grafico sottostante ha grado massimo 3 e la larghezza del percorso costante \u00e8 PPAD-completa -LRB- quindi \u00e8 improbabile che sia trattabile -RRB-. * Questa ricerca \u00e8 supportata dalle borse di ricerca EPSRC \"Algorithmics of Network-sharing Games\" e \"Discontinuous Behavior in the Complexity of randomized Algorithms\". 1. INTRODUZIONE I giochi grafici sono stati introdotti negli articoli di Kearns et al. -LSB-8 -RSB- e Littman et al. -LSB- 9 -RSB- come rappresentazione sintetica di giochi con un gran numero di giocatori. La rappresentazione in forma normale classica -LRB- o in forma matriciale -RRB- ha una dimensione esponenziale nel numero di giocatori, il che la rende inadatta ai giochi distribuiti su larga scala. Un gioco grafico associa ciascun giocatore a un vertice di un grafo sottostante G, e il profitto per quel giocatore \u00e8 una funzione delle azioni scelte da lui stesso e dai suoi vicini in G; se G ha grado basso, questo \u00e8 un modo conciso per rappresentare un gioco con molti giocatori. I documenti -LSB- 8,9 -RSB- fornisce un algoritmo di programmazione dinamica per trovare equilibri di Nash in giochi grafici in cui ci sono due azioni per giocatore e G \u00e8 un albero. Il primo di questi articoli descrive un algoritmo generico per questo problema che pu\u00f2 essere specializzato in due modi: come un algoritmo che calcola approssimazioni a tutti gli equilibri di Nash in un polinomio temporale nella dimensione dell'input e nella qualit\u00e0 dell'approssimazione, o come un algoritmo in tempo esponenziale che permette il calcolo esatto di tutti gli equilibri di Nash in G. In -LSB- 9 -RSB-, gli autori propongono una modifica a quest'ultimo algoritmo che mira a trovare un singolo equilibrio di Nash in tempo polinomiale. Questo non funziona del tutto, come mostreremo nella Sezione 3, sebbene introduca un'idea utile. 1.1 Background L'algoritmo generico di -LSB- 8 -RSB- \u00e8 costituito da due fasi che chiameremo passaggio upstream e passaggio downstream; 1 il primo inizia dalle foglie dell'albero e termina alla radice, mentre il secondo inizia dalla radice e termina alle foglie. esiste un equilibrio di Nash nel gioco grafico a valle di V -LRB- compreso -RRB- dato che W gioca w -LRB- per una definizione pi\u00f9 tecnica si rimanda alla Sezione 2 -RRB-. L'algoritmo generico non affronta il problema di rappresentare la migliore politica di risposta; infatti, la differenza pi\u00f9 importante tra le due istanziazioni dell'algoritmo generico descritto in -LSB- 8 -RSB- sta nel loro approccio a questo problema. Il calcolo viene eseguito induttivamente: la migliore politica di risposta per V \u00e8 calcolata in base alle migliori politiche di risposta dei figli di V U1,..., Uk. Alla fine del passaggio a monte, tutti i figli della radice hanno calcolato le loro migliori politiche di risposta. All'inizio del passaggio a valle, la radice seleziona la sua strategia e informa i suoi figli della sua scelta. Seleziona anche una strategia per ogni bambino. Una condizione necessaria e sufficiente affinch\u00e9 l'algoritmo proceda \u00e8 che la strategia della radice sia la risposta migliore alle strategie dei suoi figli e, per ciascun figlio, la strategia scelta sia una delle potenziali risposte migliori precalcolate alla strategia scelta della radice. L'equilibrio poi si propaga a valle, con ciascun vertice che seleziona le azioni dei suoi figli. L'azione del bambino viene scelta come una qualsiasi strategia tra le potenziali migliori risposte precalcolate alla strategia scelta dal genitore. Per delimitare il tempo di esecuzione di questo algoritmo, il documento -LSB- 8 -RSB- mostra che qualsiasi politica di risposta migliore pu\u00f2 essere rappresentata come un'unione di un numero esponenziale di rettangoli; l'algoritmo di approssimazione temporale polinomiale si ottiene combinando questa rappresentazione con una griglia di dimensioni polinomiali. 1.2 I nostri risultati Uno dei principali contributi del nostro articolo \u00e8 mostrare che l'algoritmo proposto da -LSB- 9 -RSB- non \u00e8 corretto. Nella Sezione 3 descriviamo un semplice esempio per il quale l'algoritmo di -LSB- 9 -RSB- produce un vettore di strategie che non costituisce un equilibrio di Nash del gioco sottostante. Nelle sezioni 4,5 e 6 mostriamo come correggere l'algoritmo di -LSB- 9 -RSB- in modo che produca sempre un output corretto. La sezione 4 considera il caso in cui il grafo sottostante sia un cammino di lunghezza n. In questo caso, mostriamo che il numero di rettangoli in ciascuna delle politiche di risposta migliore \u00e8 O -LRB- n2 -RRB-. Questo ci d\u00e0 un algoritmo O -LRB- n3 -RRB- per trovare un equilibrio di Nash e per calcolare una rappresentazione di tutti gli equilibri di Nash. -LRB- Questo algoritmo \u00e8 un caso speciale dell'algoritmo generico di -LSB- 8 -RSB- -- mostriamo che funziona in tempo polinomiale quando il grafico sottostante \u00e8 un percorso. -RRB- Possiamo migliorare il tempo di esecuzione dell'algoritmo generico utilizzando le idee di -LSB- 9 -RSB-. In particolare, diamo un algoritmo O -LRB- n2 -RRB- per trovare un equilibrio di Nash di un gioco grafico su un cammino di lunghezza n. Invece di memorizzare le politiche di risposta migliore, questo algoritmo memorizza sottoinsiemi opportunamente definiti, che, seguendo -LSB- 9 -RSB-, chiamiamo politiche di breakpoint -LRB- modificando la definizione secondo necessit\u00e0 -RRB-. Otteniamo il seguente teorema TEOREMA 1. Esiste un algoritmo O -LRB- n2 -RRB- che trova un equilibrio di Nash di un gioco grafico con due azioni per giocatore su un percorso di n vertici. Esiste un algoritmo O -LRB- n3 -RRB- che calcola una rappresentazione di tutti gli equilibri di Nash di un gioco del genere. Nella Sezione 5 estendiamo i risultati della Sezione 4 a grafi generali di grado2, ottenendo il seguente teorema. TEOREMA 2. Esiste un algoritmo tempo-polinomiale che trova un equilibrio di Nash di un gioco grafico con due azioni per giocatore su un grafico di grado massimo 2. Nella Sezione 6 estendiamo il nostro algoritmo in modo che possa essere utilizzato per trovare un equilibrio di Nash di un gioco grafico su un albero arbitrario. Anche quando l'albero ha un grado limitato, il tempo di esecuzione pu\u00f2 essere esponenziale. Mostriamo che questo \u00e8 inevitabile costruendo una famiglia di giochi grafici su alberi a gradi limitati per i quali le politiche di risposta migliore di alcuni vertici hanno dimensione esponenziale, e qualsiasi algoritmo twopass -LRB- cio\u00e8 un algoritmo che \u00e8 simile nello spirito a quello di -LSB- 8 -RSB- -RRB- deve memorizzare quasi tutti i punti delle politiche di risposta migliore. In particolare mostriamo quanto segue. TEOREMA 3. Esiste una famiglia infinita di giochi grafici su alberi a gradi limitati con ampiezza del percorso 2 tale che qualsiasi algoritmo a due passaggi per trovare equilibri di Nash su questi alberi richiede tempo e spazio esponenziali. \u00c8 interessante notare che gli alberi utilizzati nella dimostrazione del Teorema 3 hanno larghezza di percorso 2, cio\u00e8 sono molto vicini ad essere percorsi. \u00c8 una questione aperta se il nostro algoritmo venga eseguito in tempo polinomiale per grafici di ampiezza del percorso 1. Questa domanda pu\u00f2 essere vista come una generalizzazione di un problema di geometria computazionale molto naturale: lo descriviamo in maggior dettaglio nella Sezione 8. Nella Sezione 7, fornire un risultato di intrattabilit\u00e0 teorica della complessit\u00e0 per il problema di trovare un equilibrio di Nash di un gioco grafico su un grafo con larghezza di percorso piccola. Dimostriamo il seguente teorema. TEOREMA 4.Consideriamo il problema di trovare un equilibrio di Nash per un gioco grafico in cui il grafico sottostante ha grado massimo 3 e larghezza del percorso k. Esiste una costante k tale che questo problema sia completo di PPAD. Il Teorema 4 limita la misura in cui possiamo sfruttare le propriet\u00e0 `` path-like '' del grafico sottostante, al fine di trovare equilibri di Nash. Per dimostrare il Teorema 4, utilizziamo i recenti risultati di completezza PPAD per i giochi, in particolare gli articoli -LSB- 7, 4 -RSB- che mostrano che il problema di trovare equilibri di Nash in giochi grafici di grado d -LRB- per d > 3 -RRB- \u00e8 computazionalmente equivalente al problema di risolvere giochi in forma normale con r-giocatori -LRB- per r > 4 -RRB-, entrambi PPAD-completi. 8. PROBLEMI APERTI Il problema pi\u00f9 importante lasciato aperto da questo articolo \u00e8 se sia possibile trovare un equilibrio di Nash di un gioco grafico su un albero a gradi limitati in tempo polinomiale. La nostra costruzione mostra che qualsiasi algoritmo a due passaggi che memorizza esplicitamente le policy dei breakpoint necessita di tempo e spazio esponenziali. Tuttavia, non preclude l'esistenza di un algoritmo basato su un'idea simile, ma, invece di calcolare l'intera politica dei breakpoint per ciascun vertice, utilizza un piccolo numero di passaggi aggiuntivi attraverso il grafico per decidere quale -LRB- polinomio- dovrebbero essere calcolate le parti dimensionate -RRB- di ciascuna policy di breakpoint. In particolare, tale algoritmo pu\u00f2 essere basato sull'algoritmo di approssimazione di -LSB- 8 -RSB-, dove il valore di e \u00e8 scelto in modo adattivo. Un'altra domanda interessante \u00e8 legata al fatto che il grafico per il quale abbiamo costruito una politica di breakpoint di dimensione esponenziale ha larghezza di percorso 2, mentre i nostri risultati positivi riguardano un percorso, cio\u00e8 un grafico di larghezza di percorso 1. Non \u00e8 chiaro se per qualsiasi percorso limitato -grado grafico della larghezza del percorso 1 il tempo di esecuzione di -LRB- la versione basata sulla politica del punto di interruzione di -RRB- il nostro algoritmo sar\u00e0 polinomiale. In particolare, \u00e8 istruttivo considerare un grafo ``caterpillar'', ovvero il grafo che si pu\u00f2 ottenere da Tn eliminando i vertici S1,..., Sn. Ci\u00f2 implica che il problema di delimitare la dimensione della politica di risposta migliore -LRB- o, in alternativa, della politica del punto di interruzione -RRB-, pu\u00f2 essere visto come una generalizzazione del seguente problema di geometria computazionale, che riteniamo possa essere di interesse indipendente: PROBLEMA 1. Se s\u00ec, pu\u00f2 darsi che in questo insieme non ci sia alcun percorso con un numero polinomiale di giri che collega gli estremi del segmento originale? Ci\u00f2 implica che, anche per un caterpillar, la politica di risposta migliore pu\u00f2 essere esponenzialmente ampia. Tuttavia, nel nostro esempio -LRB- che \u00e8 omesso da questa versione del documento a causa di vincoli di spazio -RRB-, esiste un percorso di dimensioni polinomiali attraverso la politica di risposta migliore, ovvero non dimostra che la politica del punto di interruzione sia necessariamente dimensione esponenziale. Se si riesce a dimostrare che \u00e8 sempre cos\u00ec,potrebbe essere possibile adattare questa prova per dimostrare che pu\u00f2 esserci un divario esponenziale tra le dimensioni delle politiche di risposta migliore e le politiche dei punti di interruzione.", "keyphrases": ["gioco grafico", "rete di distribuzione su larga scala", "equilibrio di Nash", "grado", "Algoritmo di base del programma dinamico", "ppad-complet", "albero a gradi legati", "algoritmo del gene", "responspoli", "passaggio a valle", "politica del punto di rottura"]}
{"file_name": "I-16", "text": "Un agente di offerta avanzato per la selezione della pubblicit\u00e0 sui display pubblici ABSTRACT In questo articolo presentiamo un agente di offerta avanzato che partecipa ad aste con offerta sigillata al primo prezzo per allocare spazi pubblicitari su BluScreen, un sistema sperimentale di pubblicit\u00e0 pubblica che rileva gli utenti attraverso la presenza dei loro Dispositivi abilitati Bluetooth. Il nostro agente di offerta \u00e8 in grado di costruire modelli probabilistici sia del comportamento degli utenti che visualizzano gli annunci, sia delle aste a cui partecipa. Quindi utilizza questi modelli per massimizzare l'esposizione che ricevono i suoi annunci. Valutiamo l'efficacia di questo agente di offerta attraverso la simulazione rispetto a una serie di meccanismi di selezione alternativi tra cui una semplice strategia di offerta, un'allocazione casuale e un'allocazione ottimale centralizzata con perfetta previsione. Il nostro agente di offerta supera significativamente sia la strategia di offerta semplice che l'allocazione casuale e, in una popolazione mista di agenti, \u00e8 in grado di esporre i suoi annunci al 25% di utenti in pi\u00f9 rispetto alla strategia di offerta semplice. Inoltre, la sua performance si colloca entro il 7,5% rispetto a quella dell\u2019allocazione ottimale centralizzata, nonostante il context altamente incerto in cui deve operare. 1. INTRODUZIONE I display elettronici vengono sempre pi\u00f9 utilizzati negli ambienti pubblici, come aeroporti, centri citt\u00e0 e negozi al dettaglio, per pubblicizzare prodotti commerciali o per intrattenere e informare i passanti. sono stati proposti display pubblici interattivi. In quanto tali, questi sistemi presuppongono una conoscenza preliminare del pubblico di destinazione e richiedono che un singolo utente abbia accesso esclusivo al display o che gli utenti portino dispositivi di localizzazione specifici in modo che la loro presenza possa essere identificata -LSB- 6, 11 -RSB- . Tuttavia, questi approcci non funzionano negli spazi pubblici, dove non esiste alcuna conoscenza preliminare degli utenti che possono visualizzare il display e dove tali display devono reagire alla presenza di pi\u00f9 utenti contemporaneamente. Al contrario, Payne et al. hanno sviluppato un sistema di visualizzazione pubblica intelligente, denominato BluScreen, che rileva e traccia gli utenti attraverso i dispositivi abilitati Bluetooth che portano con s\u00e9 ogni giorno -LSB- 8 -RSB-. All'interno di questo sistema, viene utilizzato un meccanismo di asta multi-agente decentralizzato per allocare in modo efficiente il tempo pubblicitario su ciascun display pubblico. Ogni annuncio \u00e8 rappresentato da un singolo agente pubblicitario che mantiene una cronologia degli utenti che sono gi\u00e0 stati esposti all'annuncio. Questo agente cerca quindi di acquisire cicli pubblicitari -LRB- durante i quali pu\u00f2 visualizzare la sua pubblicit\u00e0 sui display pubblici -RRB- presentando offerte a un agente del mercato che implementa un'asta ad offerte sigillate. Il valore di queste offerte si basa sul numero di utenti attualmente presenti davanti allo schermo, sulla cronologia di questi utenti e su una stima derivata esternamente del valore dell'esposizione di un annuncio pubblicitario a un utente. In questo articolo presentiamo un agente di offerta avanzato che estende in modo significativo la sofisticazione di questo approccio.In particolare, consideriamo il context pi\u00f9 generale in cui \u00e8 impossibile determinare una valutazione a priori per esporre un annuncio pubblicitario a un utente. Inoltre, \u00e8 probabile che ci\u00f2 avvenga anche nel caso di nuovi impianti commerciali, dove la limitata esperienza di mercato rende impossibile una stima. L'agente pubblicitario ha quindi semplicemente il compito di utilizzare questo budget al massimo effetto -LRB-, ovvero di ottenere la massima visibilit\u00e0 pubblicitaria possibile entro questo periodo di tempo -RRB-. Ora, per raggiungere questo obiettivo, l'agente pubblicitario deve essere in grado di modellare il comportamento degli utenti per prevedere il numero che sar\u00e0 presente in ogni futuro ciclo pubblicitario. Inoltre, deve anche comprendere l'ambiente d'asta in cui compete, in modo da poter utilizzare al meglio il suo budget limitato. Pertanto, nello sviluppo di un agente di offerta avanzato che raggiunge questo obiettivo, facciamo avanzare lo stato dell'arte in quattro modi chiave: 1. Permettiamo agli agenti pubblicitari di modellare l'arrivo e la partenza degli utenti come processi di Poisson indipendenti e di effettuare stime di massima verosimiglianza delle velocit\u00e0 di questi processi in base alle loro osservazioni. Mostriamo come questi agenti possono quindi calcolare il numero previsto di utenti che saranno presenti durante qualsiasi ciclo pubblicitario futuro. 2. Utilizzando un approccio teorico della decisione consentiamo agli agenti pubblicitari di modellare la probabilit\u00e0 di vincere una determinata asta quando viene offerto un importo specifico. Per rappresentare questa probabilit\u00e0 viene utilizzata la forma cumulativa della distribuzione gamma, e i suoi parametri vengono adattati utilizzando osservazioni sia del prezzo di chiusura delle aste precedenti, sia delle offerte presentate dallo stesso agente pubblicitario. 3. Mostriamo che la nostra assunzione esplicita secondo cui l'agente pubblicitario non trae alcun vantaggio aggiuntivo dal mostrare un annuncio pubblicitario a un singolo utente pi\u00f9 di una volta, fa s\u00ec che l'utilit\u00e0 attesa di ciascun ciclo pubblicitario futuro dipenda dal risultato atteso di tutte le aste che precedono Esso. Presentiamo quindi un algoritmo di ottimizzazione stocastica basato sulla ricottura simulata che consente all'agente pubblicitario di calcolare la sequenza ottimale di offerte che massimizza la sua utilit\u00e0 attesa. 4. Il resto di questo documento \u00e8 organizzato come segue: La sezione 2 discute il lavoro correlato in cui gli agenti e i mercati basati su aste vengono utilizzati per allocare spazi pubblicitari. La sezione 3 descrive il prototipo del sistema BluScreen che motiva il nostro lavoro. Nella sezione 4 presentiamo una descrizione dettagliata del meccanismo di assegnazione dell'asta e nella sezione 5 descriviamo la nostra strategia di offerta avanzata per gli agenti pubblicitari. Nella sezione 6 presentiamo una validazione empirica del nostro approccio e, infine, concludiamo nella sezione 7. 2. LAVORI CORRELATI L'attrattiva commerciale della pubblicit\u00e0 mirata \u00e8 stata ampiamente dimostrata su Internet, dove i sistemi di raccomandazione e i banner pubblicitari contestuali sono la norma - LSB-1-RSB-. I tentativi di applicare questi approcci al mondo reale sono stati molto pi\u00f9 limitati. Gerding et al.presentare un sistema simulato -LRB- CASy -RRB- in cui un meccanismo di asta Vickrey viene utilizzato per vendere spazi pubblicitari all'interno di un centro commerciale elettronico modellato -LSB- 2 -RSB-. L'asta viene utilizzata per classificare una serie di possibili annunci pubblicitari forniti da diversi punti vendita e gli annunci pubblicitari con il ranking migliore vengono selezionati per la presentazione su schermi pubblici. Il feedback viene fornito attraverso le successive informazioni sulle vendite, consentendo al modello di creare un profilo delle preferenze dell'utente. Tuttavia, a differenza del BluScreen Figura 1: Un prototipo BluScreen distribuito. Il sistema che qui consideriamo, non \u00e8 adatto per fare pubblicit\u00e0 a pi\u00f9 individui contemporaneamente, poich\u00e9 richiede l'interazione esplicita con un singolo utente per acquisire le preferenze dell'utente. L'identificazione dell'utente si basa su badge a infrarossi e sensori incorporati all'interno di un ambiente d'ufficio. Quando pi\u00f9 utenti passano davanti al display, un sistema centralizzato confronta i profili dell'utente per identificare aree di interesse comuni e viene mostrato il contenuto che corrisponde a questo interesse comune. Pertanto, mentre CASy \u00e8 un sistema simulato che consente agli inserzionisti di competere per attirare l'attenzione di un singolo utente, GroupCast \u00e8 un sistema prototipo che rileva la presenza di gruppi di utenti e seleziona i contenuti in base ai loro profili. Nonostante le loro somiglianze, nessuno dei due sistemi affronta l'argomento che ci interessa qui: come allocare lo spazio pubblicitario tra inserzionisti concorrenti che si confrontano con un pubblico di pi\u00f9 individui sui quali non esistono informazioni di profilo a priori. Pertanto, nella sezione successiva descriviamo il prototipo del sistema BluScreen che motiva il nostro lavoro. 7. CONCLUSIONI In questo articolo, abbiamo presentato una strategia di offerta avanzata per l'utilizzo da parte degli agenti pubblicitari all'interno del sistema pubblicitario BluScreen. Questa strategia di offerta ha consentito agli agenti pubblicitari di modellare e prevedere l'arrivo e la partenza degli utenti, e anche di modellare il loro successo all'interno di un'asta con offerta sigillata al primo prezzo osservando sia le offerte che loro stessi hanno presentato sia l'offerta vincente. L'ex Sesta Intl.. Conf. congiunta. Figura 8: Confronto di una popolazione equamente mista di agenti pubblicitari che utilizzano strategie di offerta semplici e avanzate su una serie di impostazioni di parametri. Viene calcolata la media dei risultati su 50 simulazioni e le barre di errore indicano l'errore standard nella media. Figura 9: Confronto di una popolazione eterogenea di agenti pubblicitari che utilizzano strategie di offerta semplici e avanzate. Viene calcolata la media dei risultati su 50 simulazioni e le barre di errore indicano l'errore standard nella media. \u00c8 stato dimostrato che l'utilit\u00e0 attesa, misurata come il numero di utenti a cui l'agente pubblicitario espone la sua pubblicit\u00e0, dipende da questi fattori, e ha portato a un'espressione complessa in cui l'utilit\u00e0 attesa di ciascuna asta dipendeva dal successo o meno delle aste precedenti. Abbiamo presentato un algoritmo basato sulla ricottura simulata per risolvere la strategia di offerta ottimale e, nella simulazione,\u00e8 stato dimostrato che questa strategia di offerta supera in modo significativo una strategia di offerta semplice che non presentava nessuna di queste funzionalit\u00e0. La sua performance si \u00e8 avvicinata molto a quella di un\u2019allocazione centrale ottimale, con perfetta conoscenza dell\u2019arrivo e della partenza degli utenti, nonostante l\u2019ambiente incerto in cui la strategia deve operare. Questo lavoro continuer\u00e0 a essere svolto insieme all'implementazione di pi\u00f9 prototipi BluScreen per acquisire ulteriore esperienza nel mondo reale.", "keyphrases": ["agente di offerte anticipate", "schermo blu", "sperimentare un sistema di pubblicit\u00e0 pubblica", "Bluetooth", "modello probabilistico", "centralis optim alloc", "distribuire artifici intellig", "Meccanismo di aste multi-agente decentralis", "processo di poisson indipendente", "approccio teorico della decisione", "Algoritmo di ottimizzazione stocastica"]}
{"file_name": "J-18", "text": "Mediatori nelle aste di posizione ABSTRACT Un mediatore \u00e8 un'entit\u00e0 affidabile, che pu\u00f2 agire per conto degli agenti in un dato gioco. Un mediatore per\u00f2 non pu\u00f2 imporre l'utilizzo dei suoi servizi, e ogni agente \u00e8 libero di partecipare direttamente al gioco. In questo articolo introduciamo uno studio sui mediatori per giochi con informazione incompleta e lo applichiamo al context delle aste di posizione, un argomento centrale nel commercio elettronico. Le aste di posizione VCG, che attualmente non vengono utilizzate nella pratica, possiedono alcune interessanti propriet\u00e0 teoriche, come l\u2019ottimizzazione del surplus sociale e l\u2019adozione di strategie dominanti. Queste propriet\u00e0 potrebbero non essere soddisfatte dalle aste di posizione attuali e dalle loro varianti. Ci concentriamo quindi sulla ricerca di mediatori che permettano di trasformare le aste di posizione attuale in aste di posizione VCG. Richiediamo che accettare i servizi del mediatore e riferire onestamente al mediatore formi un equilibrio ex post, che soddisfa la seguente condizione di razionalit\u00e0: il payoff di un agente non pu\u00f2 essere negativo indipendentemente dalle azioni intraprese dagli agenti che non hanno scelto dai servizi del mediatore, o dagli agenti che segnalano falsi tipi al mediatore. Dimostriamo l'esistenza di tali mediatori desiderati per le aste di posizione al prezzo successivo -LRB- simili a Google -RRB-, cos\u00ec come per una classe pi\u00f9 ricca di aste di posizione, comprese tutte le aste di posizione al prezzo k, k > 1. Per k = 1, l'asta di posizione self-price, mostriamo che l'esistenza di tale mediatore dipende dalla regola di spareggio utilizzata nell'asta. 1. INTRODUZIONE Consideriamo un'interazione in un sistema multi-agente, in cui ogni giocatore possiede alcune informazioni private, chiamate tipo del giocatore. Ad esempio, in un'interazione d'asta, il tipo di giocatore \u00e8 la sua valutazione o, in aste pi\u00f9 complesse, la sua funzione di valutazione. Questa interazione \u00e8 modellata come un gioco con informazioni incomplete. Questo gioco \u00e8 chiamato gioco bayesiano, quando al sistema viene aggiunta una misura di probabilit\u00e0 comunemente nota sui profili dei tipi. Altrimenti si parla di gioco pre-bayesiano. In questo articolo trattiamo solo i giochi pre-bayesiani. Consideriamo il seguente semplice esempio di gioco pre-bayesiano, che possiede un equilibrio ex post. Il gioco \u00e8 indicato con H.", "keyphrases": ["asta", "mediat", "equilibrio ex post", "agente", "inserire un'asta", "commercio di elettroni", "classe pi\u00f9 ricca di aste di posizione", "asta di acquisto al prezzo successivo", "sistema multiagente", "strategia t", "funzione di uscita vcg", "asta di acquisto a prezzo autonomo"]}
{"file_name": "H-29", "text": "Stima e uso dell'incertezza nel feedback di pseudo-rilevanza ABSTRACT I metodi di feedback di pseudo-rilevanza esistenti in genere eseguono una media sui documenti pi\u00f9 recuperati, ma ignorano un'importante dimensione statistica: il rischio o la varianza associata ai singoli modelli di documenti o alla loro combinazione. Trattando il metodo di feedback di base come una scatola nera e il modello di feedback di output come una variabile casuale, stimiamo una distribuzione a posteriori per il modello di feedback ricampionando i documenti pi\u00f9 recuperati di una determinata query, utilizzando la media o la modalit\u00e0 a posteriori come valore avanzato modello di feedback. Eseguiamo quindi una combinazione di modelli su diversi modelli avanzati, ciascuno basato su una query leggermente modificata campionata dalla query originale. Troviamo che il ricampionamento dei documenti aiuta ad aumentare la precisione del modello di feedback individuale rimuovendo i termini non significativi, mentre il campionamento dalla query migliora la robustezza -LRB- prestazioni nel caso peggiore -RRB- enfatizzando i termini relativi a pi\u00f9 aspetti della query. Il risultato \u00e8 un algoritmo di meta-feedback che \u00e8 allo stesso tempo pi\u00f9 robusto e pi\u00f9 preciso rispetto al metodo di base forte originale. 1. INTRODUZIONE L'incertezza \u00e8 una caratteristica intrinseca del recupero delle informazioni. Anche se la query fosse perfettamente specificata, il linguaggio nei documenti della raccolta \u00e8 intrinsecamente complesso e ambiguo e far corrispondere efficacemente tale linguaggio \u00e8 di per s\u00e9 un problema formidabile. In questo modo, gli algoritmi di recupero possono tentare di quantificare il rischio o l\u2019incertezza associati alle loro classifiche di output, o migliorare la stabilit\u00e0 o la precisione dei loro calcoli interni. Gli attuali algoritmi per il feedback di pseudo-rilevanza -LRB- PRF -RRB- tendono a seguire lo stesso metodo di base sia che utilizziamo algoritmi basati sullo spazio vettoriale come la formula di Rocchio -LSB- 16 -RSB-, sia che utilizziamo approcci pi\u00f9 recenti di modellazione del linguaggio come come Modelli di Rilevanza -LSB- 10 -RSB-. Innanzitutto, da una query iniziale viene ottenuto un insieme dei documenti pi\u00f9 recuperati e si presume che si approssimino a un insieme di documenti rilevanti. Successivamente, un singolo vettore del modello di feedback viene calcolato in base a una sorta di media, centroide o aspettativa sull'insieme di modelli di documento possibilmente rilevanti. Ad esempio, i vettori del documento possono essere combinati con uguale peso, come in Rocchio, o in base alla verosimiglianza della query, come pu\u00f2 essere fatto utilizzando il Modello di Rilevanza. L\u2019uso di un\u2019aspettativa \u00e8 ragionevole per ragioni pratiche e teoriche, ma di per s\u00e9 ignora informazioni potenzialmente preziose sul rischio del modello di feedback. La nostra ipotesi principale in questo articolo \u00e8 che la stima dell\u2019incertezza nel feedback \u00e8 utile e porta a migliori modelli di feedback individuale e modelli combinati pi\u00f9 robusti. Pertanto, proponiamo un metodo per stimare l'incertezza associata a un modello di feedback individuale in termini di distribuzione a posteriori sui modelli linguistici. Per fare ci\u00f2, variamo sistematicamente gli input al metodo di feedback di base e adattiamo una distribuzione di Dirichlet all'output.Usiamo la media o modalit\u00e0 a posteriori come stima del modello di feedback migliorato. Questo processo \u00e8 mostrato nella Figura 1. Come mostreremo pi\u00f9 avanti, la media e la modalit\u00e0 possono variare in modo significativo rispetto al modello a feedback singolo proposto dal metodo di base. Eseguiamo anche combinazioni di modelli utilizzando diversi modelli linguistici di feedback migliorati ottenuti da un piccolo numero di nuove query campionate dalla query originale. Il peso di un modello combina due fattori complementari: la probabilit\u00e0 del modello di generare la query e la varianza del modello, con i modelli ad alta varianza che ottengono un peso inferiore. ` Ad esempio, un vettore di parametri atteso condizionato dall'osservazione della query \u00e8 formato dai documenti recuperati in alto, che vengono trattati come stringhe di addestramento -LRB- vedere -LSB- 10 -RSB-, p. 62 -RRB-. Figura 1: Stima dell'incertezza del modello di feedback per una singola query. 4. LAVORO CORRELATO Il nostro approccio \u00e8 legato al lavoro precedente in diverse aree del recupero delle informazioni e dell'apprendimento automatico. Questi studi utilizzano l'idea di creare pi\u00f9 sottoquery e quindi esaminare la natura della sovrapposizione nei documenti e/o nei termini di espansione che risultano da ciascuna sottoquery. La combinazione dei modelli viene eseguita utilizzando l'euristica. In particolare, gli studi di Amati et al. e Carpineto et al. hanno studiato la combinazione di termini provenienti da metodi distributivi individuali utilizzando un'euristica di riclassificazione dei termini. In una serie di argomenti TREC hanno riscontrato un'ampia variazione media nella distanza tra i termini dei diversi metodi di espansione. Il loro metodo combinato ha dato modesti miglioramenti positivi nella precisione media. L'idea di esaminare la sovrapposizione tra elenchi di termini suggeriti \u00e8 stata utilizzata anche nei primi approcci di espansione delle query. Per quanto riguarda i documenti, il recente lavoro di Zhou & Croft -LSB- 21 -RSB- ha esplorato l'idea di aggiungere rumore ai documenti, riassegnarli e utilizzare la stabilit\u00e0 delle classifiche risultanti come stima della difficolt\u00e0 delle query. Ci\u00f2 \u00e8 legato al nostro utilizzo del campionamento dei documenti per stimare il rischio del modello di feedback costruito dai diversi insiemi di documenti pi\u00f9 recuperati. Sakai et al. -LSB- 17 -RSB- ha proposto un approccio per migliorare la robustezza del feedback di pseudo-rilevanza utilizzando un metodo chiamato campionamento selettivo. Greiff, Morgan e Ponte -LSB- 8 -RSB- hanno esplorato il ruolo della varianza nella ponderazione dei termini. In una serie di simulazioni che hanno semplificato il problema in documenti con due caratteristiche, hanno scoperto che la precisione media diminuisce all'aumentare della varianza della frequenza dei termini (rumore elevato). La riduzione del peso dei termini con varianza elevata ha portato a una migliore precisione media. Ci\u00f2 sembra in accordo con i nostri risultati per i modelli di feedback individuali. Le stime della varianza dell'output sono state recentemente utilizzate per una migliore classificazione del text. Lee et al. -LSB- 11 -RSB- ha utilizzato stime della varianza specifiche della query degli output del classificatore per eseguire una migliore combinazione di modelli. Invece di usare il campionamento,sono stati in grado di derivare espressioni in forma chiusa per la varianza del classificatore assumendo classificatori di base utilizzando tipi semplici di reti di inferenza. Ando e Zhang hanno proposto un metodo che chiamano feedback strutturale -LSB- 3 -RSB- e hanno mostrato come applicarlo all'espansione delle query per il TREC Genomics Track. Hanno utilizzato r variazioni della query per ottenere R diversi insiemi Sr di documenti di alto livello che sono stati intersecati con i documenti di alto livello ottenuti dalla query originale qorig. Per ogni Si viene calcolato il vettore baricentro normalizzato \u02c6wi dei documenti. L'analisi delle componenti principali -LRB- PCA -RRB- viene quindi applicata a \u02c6wi per ottenere la matrice 4 -RRB- di H vettori singolari sinistri \u03c6h utilizzati per ottenere la nuova query espansa L'uso della varianza come misura di qualit\u00e0 del modello di feedback avviene indirettamente attraverso l'applicazione della PCA. Sarebbe interessante studiare le connessioni tra questo approccio e il nostro metodo di modelfitting. Infine, negli approcci di modellazione del linguaggio al feedback, Tao e Zhai -LSB- 18 -RSB- descrivono un metodo per un feedback pi\u00f9 robusto che consente a ciascun documento di avere un feedback \u03b1 diverso. I pesi del feedback vengono derivati \u200b\u200bautomaticamente utilizzando EM regolarizzato. Un equilibrio pi\u00f9 o meno uguale tra modello di query e di espansione \u00e8 implicito nella loro condizione di arresto EM. Propongono di adattare il parametro di arresto \u03b7 in base a una funzione di qualche misura di qualit\u00e0 dei documenti di feedback. 5. CONCLUSIONI Abbiamo presentato un nuovo approccio al feedback di pseudo-rilevanza basato sul campionamento di documenti e query. Tali stime della varianza, ad esempio, possono essere naturalmente utilizzate in un quadro bayesiano per migliorare la stima e la combinazione dei modelli. Sebbene il nostro studio utilizzi l\u2019approccio della modellazione del linguaggio come quadro di riferimento per gli esperimenti, facciamo poche ipotesi sul funzionamento effettivo dell\u2019algoritmo di feedback. Riteniamo probabile che qualsiasi algoritmo di feedback di base ragionevolmente efficace trarrebbe beneficio dal nostro approccio. I nostri risultati sulle raccolte TREC standard mostrano che il nostro quadro migliora la robustezza di un forte metodo di feedback di base su una variet\u00e0 di raccolte, senza sacrificare la precisione media. Fornisce inoltre guadagni piccoli ma costanti nella precisione top10. Nel lavoro futuro, prevediamo un\u2019indagine su come la variazione dell\u2019insieme di metodi di campionamento utilizzati e il numero di campioni controlli il compromesso tra robustezza, accuratezza ed efficienza.query espansa L'uso della varianza come misura della qualit\u00e0 del modello di feedback avviene indirettamente attraverso l'applicazione della PCA. Sarebbe interessante studiare le connessioni tra questo approccio e il nostro metodo di modelfitting. Infine, negli approcci di modellazione del linguaggio al feedback, Tao e Zhai -LSB- 18 -RSB- descrivono un metodo per un feedback pi\u00f9 robusto che consente a ciascun documento di avere un feedback \u03b1 diverso. I pesi del feedback vengono derivati \u200b\u200bautomaticamente utilizzando EM regolarizzato. Un equilibrio pi\u00f9 o meno uguale tra modello di query e di espansione \u00e8 implicito nella loro condizione di arresto EM. Propongono di adattare il parametro di arresto \u03b7 in base a una funzione di qualche misura di qualit\u00e0 dei documenti di feedback. 5. CONCLUSIONI Abbiamo presentato un nuovo approccio al feedback di pseudo-rilevanza basato sul campionamento di documenti e query. Tali stime della varianza, ad esempio, possono essere naturalmente utilizzate in un quadro bayesiano per migliorare la stima e la combinazione dei modelli. Sebbene il nostro studio utilizzi l\u2019approccio della modellazione del linguaggio come quadro di riferimento per gli esperimenti, facciamo poche ipotesi sul funzionamento effettivo dell\u2019algoritmo di feedback. Riteniamo probabile che qualsiasi algoritmo di feedback di base ragionevolmente efficace trarrebbe beneficio dal nostro approccio. I nostri risultati sulle raccolte TREC standard mostrano che il nostro quadro migliora la robustezza di un forte metodo di feedback di base su una variet\u00e0 di raccolte, senza sacrificare la precisione media. Fornisce inoltre guadagni piccoli ma costanti nella precisione top10. Nel lavoro futuro, prevediamo un\u2019indagine su come la variazione dell\u2019insieme di metodi di campionamento utilizzati e il numero di campioni controlli il compromesso tra robustezza, accuratezza ed efficienza.query espansa L'uso della varianza come misura della qualit\u00e0 del modello di feedback avviene indirettamente attraverso l'applicazione della PCA. Sarebbe interessante studiare le connessioni tra questo approccio e il nostro metodo di modelfitting. Infine, negli approcci di modellazione del linguaggio al feedback, Tao e Zhai -LSB- 18 -RSB- descrivono un metodo per un feedback pi\u00f9 robusto che consente a ciascun documento di avere un feedback \u03b1 diverso. I pesi del feedback vengono derivati \u200b\u200bautomaticamente utilizzando EM regolarizzato. Un equilibrio pi\u00f9 o meno uguale tra modello di query e di espansione \u00e8 implicito nella loro condizione di arresto EM. Propongono di adattare il parametro di arresto \u03b7 in base a una funzione di qualche misura di qualit\u00e0 dei documenti di feedback. 5. CONCLUSIONI Abbiamo presentato un nuovo approccio al feedback di pseudo-rilevanza basato sul campionamento di documenti e query. Tali stime della varianza, ad esempio, possono essere naturalmente utilizzate in un quadro bayesiano per migliorare la stima e la combinazione dei modelli. Sebbene il nostro studio utilizzi l\u2019approccio della modellazione del linguaggio come quadro di riferimento per gli esperimenti, facciamo poche ipotesi sul funzionamento effettivo dell\u2019algoritmo di feedback. Riteniamo probabile che qualsiasi algoritmo di feedback di base ragionevolmente efficace trarrebbe beneficio dal nostro approccio. I nostri risultati sulle raccolte TREC standard mostrano che il nostro quadro migliora la robustezza di un forte metodo di feedback di base su una variet\u00e0 di raccolte, senza sacrificare la precisione media. Fornisce inoltre guadagni piccoli ma costanti nella precisione top10. Nel lavoro futuro, prevediamo un\u2019indagine su come la variazione dell\u2019insieme di metodi di campionamento utilizzati e il numero di campioni controlli il compromesso tra robustezza, accuratezza ed efficienza.I nostri risultati sulle raccolte TREC standard mostrano che il nostro quadro migliora la robustezza di un forte metodo di feedback di base su una variet\u00e0 di raccolte, senza sacrificare la precisione media. Fornisce inoltre guadagni piccoli ma costanti nella precisione top10. Nel lavoro futuro, prevediamo un\u2019indagine su come la variazione dell\u2019insieme di metodi di campionamento utilizzati e il numero di campioni controlli il compromesso tra robustezza, accuratezza ed efficienza.I nostri risultati sulle raccolte TREC standard mostrano che il nostro quadro migliora la robustezza di un forte metodo di feedback di base su una variet\u00e0 di raccolte, senza sacrificare la precisione media. Fornisce inoltre guadagni piccoli ma costanti nella precisione top10. Nel lavoro futuro, prevediamo un\u2019indagine su come la variazione dell\u2019insieme di metodi di campionamento utilizzati e il numero di campioni controlli il compromesso tra robustezza, accuratezza ed efficienza.", "keyphrases": ["metodo di feedback", "distribuzione posteriore", "migliorare il modello di feedback", "informare il recupero", "queri si espande", "distribuzione probabile", "feedback pseudo-rilevante", "algoritmo a base spaziale vettoriale", "rischio", "modello di feedback", "stima incerta", "modello linguistico", "distribuzione del feedback"]}
{"file_name": "H-20", "text": "Rilevamento di nuovi eventi basato su albero di indicizzazione ed entit\u00e0 nominate ABSTRACT Rilevamento di nuovi eventi -LRB- NED -RRB- mira a rilevare da uno o pi\u00f9 flussi di notizie quello che \u00e8 riportato su un nuovo evento -LRB- cio\u00e8 non riportato in precedenza - RRB-. Con l\u2019enorme volume di notizie oggi disponibili, c\u2019\u00e8 una crescente necessit\u00e0 di un sistema NED in grado di rilevare nuovi eventi in modo pi\u00f9 efficiente e accurato. In questo articolo proponiamo un nuovo modello NED per velocizzare il compito del NED utilizzando dinamicamente l'albero di indicizzazione delle notizie. Inoltre, sulla base dell'osservazione che termini di tipo diverso hanno effetti diversi per il compito NED, vengono proposti approcci di riponderazione di due termini per migliorare l'accuratezza del NED. Nel primo approccio, proponiamo di adattare dinamicamente i pesi dei termini in base ai cluster di storie precedenti e nel secondo approccio, proponiamo di utilizzare statistiche sui dati di addestramento per apprendere il modello di riponderazione delle entit\u00e0 denominate per ciascuna classe di storie. I risultati sperimentali su due set di dati del Linguistic Data Consortium -LRB- LDC -RRB- TDT2 e TDT3 mostrano che il modello proposto pu\u00f2 migliorare significativamente sia l'efficienza che l'accuratezza del compito NED, rispetto al sistema di base e ad altri sistemi esistenti. 1. INTRODUZIONE Il rilevamento di nuovi eventi -LRB- NED -RRB- \u00e8 uno dei cinque compiti in TDT. Un argomento \u00e8 definito come \"un evento o un'attivit\u00e0 fondamentale, insieme ad eventi e attivit\u00e0 direttamente correlati\" -LSB- 2 -RSB-. Un Evento \u00e8 definito come \"qualcosa -LRB- non banale -RRB- che accade in un certo luogo in un certo momento\" -LSB- 3 -RSB-. Le notizie utili sono solitamente sepolte in una massa di dati generati ogni giorno. Pertanto, i sistemi NED sono molto utili per le persone che hanno bisogno di rilevare nuove informazioni dal flusso di notizie in tempo reale. Queste esigenze della vita reale si verificano spesso in ambiti quali i mercati finanziari, l\u2019analisi delle notizie e la raccolta di informazioni. Nella maggior parte dei sistemi all'avanguardia -LRB- attualmente -RRB-NED, ogni notizia disponibile viene confrontata con tutte le storie ricevute in precedenza. Se tutte le somiglianze tra loro non superano una soglia, allora la storia innesca un nuovo evento. Il problema principale del NED \u00e8 identificare se due storie riguardano lo stesso argomento. Ovviamente, questi sistemi non possono trarre vantaggio dalle informazioni sugli argomenti. Altri sistemi organizzano le storie precedenti in cluster -LRB- ogni cluster corrisponde a un argomento -RRB-, e la nuova storia viene confrontata con i cluster precedenti invece che con le storie. In questo modo \u00e8 possibile ridurre significativamente i tempi di confronto. Questo perch\u00e9 a volte le storie all'interno di un argomento si allontanano molto l'una dall'altra, il che potrebbe portare a una bassa somiglianza tra una storia e il suo argomento. D'altra parte, alcuni sistemi NED proposti hanno cercato di migliorare la precisione facendo un uso migliore delle entit\u00e0 denominate -LSB- 10, 11, 12, 13 -RSB-. Tuttavia, nessuno dei sistemi ha considerato che termini di tipo diverso -LRB- ad es. Sostantivo, Verbo o Nome di persona -RRB- hanno effetti diversi per diverse classi di storie nel determinare se due storie riguardano lo stesso argomento. Per esempio,i nomi dei candidati alle elezioni -LRB- Nome della persona -RRB- sono molto importanti per le storie di classe elettorale; i luoghi -LRB- Nome del luogo -RRB- in cui si sono verificati gli incidenti sono importanti per le storie della classe di incidenti. -LRB- 2 -RRB- Come utilizzare al meglio le informazioni sull'argomento -LRB- del cluster -RRB- per migliorare la precisione? -LRB- 3 -RRB- Come ottenere una migliore rappresentazione delle notizie attraverso una migliore comprensione delle entit\u00e0 denominate. Spinti da questi problemi, abbiamo proposto tre approcci in questo documento. -LRB- 1 -RRB- Per rendere pi\u00f9 veloce la procedura di rilevamento, proponiamo una nuova procedura NED basata su un albero di indicizzazione delle notizie creato dinamicamente. L'albero di indicizzazione delle storie viene creato assemblando insieme storie simili per formare gruppi di notizie in diverse gerarchie in base ai loro valori di somiglianza. I confronti tra la storia attuale e i cluster precedenti potrebbero aiutare a trovare la storia pi\u00f9 simile in tempi di confronto inferiori. La nuova procedura pu\u00f2 ridurre la quantit\u00e0 di tempi di confronto senza compromettere la precisione. -LRB- 2 -RRB- Utilizziamo i cluster del primo piano nell'albero di indicizzazione come argomenti di notizie, in cui i pesi dei termini vengono adattati dinamicamente in base alla distribuzione dei termini nei cluster. In questo approccio, le informazioni sul cluster -LRB- sull'argomento -RRB- vengono utilizzate correttamente, in modo da evitare il problema della decentralizzazione del tema. -LRB- 3 -RRB- Sulla base delle osservazioni sulle statistiche ottenute dai dati di training, abbiamo scoperto che termini di diversi tipi -LRB- ad es. Sostantivo e Verbo -RRB- hanno effetti diversi per diverse classi di storie nel determinare se due storie sono su lo stesso argomento. E proponiamo di utilizzare la statistica per ottimizzare i pesi dei termini di diverso tipo in una storia a seconda della classe di notizie a cui appartiene la storia. Il resto del lavoro \u00e8 organizzato come segue. Iniziamo questo articolo riassumendo il lavoro precedente in NED nella sezione 2. La sezione 3 presenta il modello di base per NED utilizzato dalla maggior parte dei sistemi attuali. La sezione 4 descrive la nostra nuova procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Nella sezione 5 vengono proposti metodi di riponderazione di due termini per migliorare l\u2019accuratezza del NED. La sezione 6 fornisce i nostri dati sperimentali e le metriche di valutazione. Concluderemo infine con i risultati sperimentali nella Sezione 7, e le conclusioni e il lavoro futuro nella Sezione 8. 2. LAVORI CORRELATI Papka et al. proposto clustering a passaggio singolo su NED -LSB- 6 -RSB-. Quando veniva incontrata una nuova storia, questa veniva elaborata immediatamente per estrarre le caratteristiche dei termini e veniva creata una rappresentazione della query del contenuto della storia. Quindi \u00e8 stato confrontato con tutte le query precedenti. Se il documento non generava alcuna query superando una soglia, veniva contrassegnato come nuovo evento. Lam et al costruiscono precedenti rappresentazioni di query di cluster di storie, ciascuno dei quali corrisponde a un argomento -LSB- 7 -RSB-. In questo modo avvengono i confronti tra storie e cluster. Negli ultimi anni, la maggior parte del lavoro si concentra sulla proposta di metodi migliori per il confronto delle storie e la rappresentazione dei documenti. Sono stati mostrati buoni miglioramenti rispetto ai parametri di riferimento del TDT.Stokes et al. -LSB- 9 -RSB- ha utilizzato una combinazione di prove provenienti da due distinte rappresentazioni del contenuto di un documento. Una delle rappresentazioni era il solito vettore di text libero, l'altra utilizzava catene lessicali -LRB- create utilizzando WordNet -RRB- per costruire un altro vettore di termini. Successivamente le due rappresentazioni vengono combinate in modo lineare. Un aumento marginale dell\u2019efficacia \u00e8 stato ottenuto quando \u00e8 stata utilizzata la rappresentazione combinata. Sono stati fatti alcuni sforzi su come utilizzare le entit\u00e0 denominate per migliorare il NED. Yang et al. ha dato alla posizione entit\u00e0 denominate un peso quattro volte maggiore rispetto ad altri termini ed entit\u00e0 denominate -LSB- 10 -RSB-. Il gruppo di ricerca DOREMI ha combinato somiglianze semantiche di nomi di persona, nomi di localit\u00e0 e tempo insieme alla somiglianza testuale -LSB- 11 -RSB- -LSB- 12 -RSB-. Il gruppo di ricerca UMass -LSB- 13 -RSB- ha diviso la rappresentazione del documento in due parti: entit\u00e0 denominate ed entit\u00e0 non denominate. Si \u00e8 scoperto che alcune classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 denominate, mentre alcune altre classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 senza nome. Sia -LSB- 10 -RSB- che -LSB- 13 -RSB- hanno utilizzato la tecnica di categorizzazione del text per classificare in anticipo le notizie. In -LSB- 13 -RSB- le notizie vengono inizialmente classificate automaticamente, quindi viene testata la sensibilit\u00e0 dei nomi e dei termini diversi dai nomi per NED per ciascuna classe. In -LSB- 10 -RSB- i termini frequenti per ciascuna classe vengono rimossi dalla rappresentazione del documento. Nel loro lavoro non \u00e8 stata studiata l'efficacia di diversi tipi di nomi -LRB- o termini con diversi POS -RRB- per NED in diverse classi di notizie. 8. CONCLUSIONE Nel nostro modello abbiamo proposto una procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Riduce i tempi di confronto a circa un settimo del metodo tradizionale senza compromettere la precisione NED. Abbiamo anche presentato due estensioni al modello base TF-IDF. La prima estensione viene effettuata adattando i pesi dei termini in base alla distribuzione dei termini tra l'intero corpus e un insieme di storie di cluster. E la seconda estensione al modello TF-IDF di base \u00e8 un migliore utilizzo dei tipi di termini -LRB- tipi di entit\u00e0 denominate e parte della velocit\u00e0 -RRB- in base alle categorie di notizie. I nostri risultati sperimentali sui set di dati TDT2 e TDT3 mostrano che entrambe le due estensioni contribuiscono in modo significativo al miglioramento della precisione. Per il lavoro futuro, vogliamo raccogliere da Internet una serie di notizie che si estendono per un periodo pi\u00f9 lungo e integrare le informazioni temporali nell'attivit\u00e0 NED. Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.Un aumento marginale dell\u2019efficacia \u00e8 stato ottenuto quando \u00e8 stata utilizzata la rappresentazione combinata. Sono stati fatti alcuni sforzi su come utilizzare le entit\u00e0 denominate per migliorare il NED. Yang et al. ha dato alla posizione entit\u00e0 denominate un peso quattro volte maggiore rispetto ad altri termini ed entit\u00e0 denominate -LSB- 10 -RSB-. Il gruppo di ricerca DOREMI ha combinato somiglianze semantiche di nomi di persona, nomi di localit\u00e0 e tempo insieme alla somiglianza testuale -LSB- 11 -RSB- -LSB- 12 -RSB-. Il gruppo di ricerca UMass -LSB- 13 -RSB- ha diviso la rappresentazione del documento in due parti: entit\u00e0 denominate ed entit\u00e0 non denominate. Si \u00e8 scoperto che alcune classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 denominate, mentre alcune altre classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 senza nome. Sia -LSB- 10 -RSB- che -LSB- 13 -RSB- hanno utilizzato la tecnica di categorizzazione del text per classificare in anticipo le notizie. In -LSB- 13 -RSB- le notizie vengono inizialmente classificate automaticamente, quindi viene testata la sensibilit\u00e0 dei nomi e dei termini diversi dai nomi per NED per ciascuna classe. In -LSB- 10 -RSB- i termini frequenti per ciascuna classe vengono rimossi dalla rappresentazione del documento. Nel loro lavoro non \u00e8 stata studiata l'efficacia di diversi tipi di nomi -LRB- o termini con diversi POS -RRB- per NED in diverse classi di notizie. 8. CONCLUSIONE Nel nostro modello abbiamo proposto una procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Riduce i tempi di confronto a circa un settimo del metodo tradizionale senza compromettere la precisione NED. Abbiamo anche presentato due estensioni al modello base TF-IDF. La prima estensione viene effettuata adattando i pesi dei termini in base alla distribuzione dei termini tra l'intero corpus e un insieme di storie di cluster. E la seconda estensione al modello TF-IDF di base \u00e8 un migliore utilizzo dei tipi di termini -LRB- tipi di entit\u00e0 denominate e parte della velocit\u00e0 -RRB- in base alle categorie di notizie. I nostri risultati sperimentali sui set di dati TDT2 e TDT3 mostrano che entrambe le due estensioni contribuiscono in modo significativo al miglioramento della precisione. Per il lavoro futuro, vogliamo raccogliere da Internet una serie di notizie che si estendono per un periodo pi\u00f9 lungo e integrare le informazioni temporali nell'attivit\u00e0 NED. Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.Un aumento marginale dell\u2019efficacia \u00e8 stato ottenuto quando \u00e8 stata utilizzata la rappresentazione combinata. Sono stati fatti alcuni sforzi su come utilizzare le entit\u00e0 denominate per migliorare il NED. Yang et al. ha dato alla posizione entit\u00e0 denominate un peso quattro volte maggiore rispetto ad altri termini ed entit\u00e0 denominate -LSB- 10 -RSB-. Il gruppo di ricerca DOREMI ha combinato somiglianze semantiche di nomi di persona, nomi di localit\u00e0 e tempo insieme alla somiglianza testuale -LSB- 11 -RSB- -LSB- 12 -RSB-. Il gruppo di ricerca UMass -LSB- 13 -RSB- ha diviso la rappresentazione del documento in due parti: entit\u00e0 denominate ed entit\u00e0 non denominate. Si \u00e8 scoperto che alcune classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 denominate, mentre alcune altre classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 senza nome. Sia -LSB- 10 -RSB- che -LSB- 13 -RSB- hanno utilizzato la tecnica di categorizzazione del text per classificare in anticipo le notizie. In -LSB- 13 -RSB- le notizie vengono inizialmente classificate automaticamente, quindi viene testata la sensibilit\u00e0 dei nomi e dei termini diversi dai nomi per NED per ciascuna classe. In -LSB- 10 -RSB- i termini frequenti per ciascuna classe vengono rimossi dalla rappresentazione del documento. Nel loro lavoro non \u00e8 stata studiata l'efficacia di diversi tipi di nomi -LRB- o termini con diversi POS -RRB- per NED in diverse classi di notizie. 8. CONCLUSIONE Nel nostro modello abbiamo proposto una procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Riduce i tempi di confronto a circa un settimo del metodo tradizionale senza compromettere la precisione NED. Abbiamo anche presentato due estensioni al modello base TF-IDF. La prima estensione viene effettuata adattando i pesi dei termini in base alla distribuzione dei termini tra l'intero corpus e un insieme di storie di cluster. E la seconda estensione al modello TF-IDF di base \u00e8 un migliore utilizzo dei tipi di termini -LRB- tipi di entit\u00e0 denominate e parte della velocit\u00e0 -RRB- in base alle categorie di notizie. I nostri risultati sperimentali sui set di dati TDT2 e TDT3 mostrano che entrambe le due estensioni contribuiscono in modo significativo al miglioramento della precisione. Per il lavoro futuro, vogliamo raccogliere da Internet una serie di notizie che si estendono per un periodo pi\u00f9 lungo e integrare le informazioni temporali nell'attivit\u00e0 NED. Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.mentre alcune altre classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 senza nome. Sia -LSB- 10 -RSB- che -LSB- 13 -RSB- hanno utilizzato la tecnica di categorizzazione del text per classificare in anticipo le notizie. In -LSB- 13 -RSB- le notizie vengono inizialmente classificate automaticamente, quindi viene testata la sensibilit\u00e0 dei nomi e dei termini diversi dai nomi per NED per ciascuna classe. In -LSB- 10 -RSB- i termini frequenti per ciascuna classe vengono rimossi dalla rappresentazione del documento. Nel loro lavoro non \u00e8 stata studiata l'efficacia di diversi tipi di nomi -LRB- o termini con diversi POS -RRB- per NED in diverse classi di notizie. 8. CONCLUSIONE Nel nostro modello abbiamo proposto una procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Riduce i tempi di confronto a circa un settimo del metodo tradizionale senza compromettere la precisione NED. Abbiamo anche presentato due estensioni al modello base TF-IDF. La prima estensione viene effettuata adattando i pesi dei termini in base alla distribuzione dei termini tra l'intero corpus e un insieme di storie di cluster. E la seconda estensione al modello TF-IDF di base \u00e8 un migliore utilizzo dei tipi di termini -LRB- tipi di entit\u00e0 denominate e parte della velocit\u00e0 -RRB- in base alle categorie di notizie. I nostri risultati sperimentali sui set di dati TDT2 e TDT3 mostrano che entrambe le due estensioni contribuiscono in modo significativo al miglioramento della precisione. Per il lavoro futuro, vogliamo raccogliere da Internet una serie di notizie che si estendono per un periodo pi\u00f9 lungo e integrare le informazioni temporali nell'attivit\u00e0 NED. Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.mentre alcune altre classi di notizie potrebbero ottenere prestazioni migliori utilizzando la rappresentazione di entit\u00e0 senza nome. Sia -LSB- 10 -RSB- che -LSB- 13 -RSB- hanno utilizzato la tecnica di categorizzazione del text per classificare in anticipo le notizie. In -LSB- 13 -RSB- le notizie vengono inizialmente classificate automaticamente, quindi viene testata la sensibilit\u00e0 dei nomi e dei termini diversi dai nomi per NED per ciascuna classe. In -LSB- 10 -RSB- i termini frequenti per ciascuna classe vengono rimossi dalla rappresentazione del documento. Nel loro lavoro non \u00e8 stata studiata l'efficacia di diversi tipi di nomi -LRB- o termini con diversi POS -RRB- per NED in diverse classi di notizie. 8. CONCLUSIONE Nel nostro modello abbiamo proposto una procedura di rilevamento basata sull'albero di indicizzazione delle notizie. Riduce i tempi di confronto a circa un settimo del metodo tradizionale senza compromettere la precisione NED. Abbiamo anche presentato due estensioni al modello base TF-IDF. La prima estensione viene effettuata adattando i pesi dei termini in base alla distribuzione dei termini tra l'intero corpus e un insieme di storie di cluster. E la seconda estensione al modello TF-IDF di base \u00e8 un migliore utilizzo dei tipi di termini -LRB- tipi di entit\u00e0 denominate e parte della velocit\u00e0 -RRB- in base alle categorie di notizie. I nostri risultati sperimentali sui set di dati TDT2 e TDT3 mostrano che entrambe le due estensioni contribuiscono in modo significativo al miglioramento della precisione. Per il lavoro futuro, vogliamo raccogliere da Internet una serie di notizie che si estendono per un periodo pi\u00f9 lungo e integrare le informazioni temporali nell'attivit\u00e0 NED. Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.Poich\u00e9 l'argomento \u00e8 un cluster di notizie a grana grossa, vogliamo anche perfezionare la granularit\u00e0 del cluster a livello di evento e identificare diversi eventi e le loro relazioni all'interno di un argomento.", "keyphrases": ["rilevamento di nuovi eventi", "flusso di nuove storie", "volume di nuovo", "nuovo albero degli indici", "approccio di riponderazione dei termini", "ned accurati", "peso del termine", "statalista", "dati del treno", "modalit\u00e0 di riponderazione del nome dell'entit\u00e0", "classe di stori", "consorzio di dati linguistici", "sistema di base", "esistere sistema"]}
{"file_name": "H-7", "text": "Efficiente modellazione gerarchica bayesiana degli utenti per sistemi di raccomandazione ABSTRACT Un sistema di raccomandazione personalizzato basato sui contenuti apprende i profili specifici dell'utente dal feedback degli utenti in modo da poter fornire informazioni su misura per gli interessi di ogni singolo utente. Un sistema che serve milioni di utenti pu\u00f2 apprendere un profilo utente migliore per un nuovo utente, o per un utente con poco feedback, prendendo in prestito informazioni da altri utenti attraverso l'uso di un modello gerarchico bayesiano. Apprendere i parametri del modello per ottimizzare la verosimiglianza dei dati congiunti di milioni di utenti \u00e8 molto costoso dal punto di vista computazionale. L'algoritmo EM comunemente utilizzato converge molto lentamente a causa della scarsit\u00e0 dei dati nelle applicazioni IR. Questo articolo propone una nuova tecnica di apprendimento rapido per apprendere un gran numero di profili utente individuali. L'efficacia e l'efficienza dell'algoritmo proposto sono giustificate dalla teoria e dimostrate sui dati degli utenti effettivi di Netflix e MovieLens. 1. INTRODUZIONE Ad esempio, i negozi online, come Amazon e Netflix, forniscono consigli personalizzati per prodotti o servizi aggiuntivi in \u200b\u200bbase alla cronologia di un utente. Uno dei principali argomenti di personalizzazione studiati nella comunit\u00e0 del recupero delle informazioni sono i sistemi di raccomandazione personale basati sui contenuti. Questi sistemi apprendono le raccomandazioni basate sui contenuti pro'pro'specifiche dell'utente, chiamate anche file di file adattivi, dal feedback degli utenti in modo da poter consigliare informazioni su misura per gli interessi di ogni singolo utente senza richiedere all'utente di effettuare una query esplicita. L'apprendimento dei profili utente \u00e8 il problema principale di questi sistemi. Un profilo utente \u00e8 solitamente un classificatore in grado di identificare se un documento \u00e8 rilevante per l'utente o meno, oppure un modello di regressione che indica quanto un documento sia rilevante per l'utente. Una delle principali sfide nella creazione di un sistema di raccomandazione o personalizzazione \u00e8 che il profilo appreso per un particolare utente \u00e8 solitamente di bassa qualit\u00e0 quando la quantit\u00e0 di dati di quel particolare utente \u00e8 ridotta. Questo \u00e8 noto come il problema della \u201cpartenza a freddo\u201d. Ci\u00f2 significa che qualsiasi nuovo utente deve sopportare prestazioni iniziali scarse finch\u00e9 non viene fornito un feedback sufficiente da parte dell'utente per ottenere un profilo utente affidabile. Sono state condotte molte ricerche per migliorare l'accuratezza della classificazione quando la quantit\u00e0 di dati di addestramento etichettati \u00e8 ridotta. L'approccio di apprendimento semi-supervisionato combina insieme dati senza etichetta ed etichettati per raggiungere questo obiettivo -LSB- 26 -RSB-. Un altro approccio utilizza la conoscenza del dominio. Il terzo approccio consiste nel prendere in prestito i dati di addestramento da altre risorse -LSB- 5 -RSB- -LSB- 7 -RSB-. L\u2019efficacia di questi diversi approcci \u00e8 mista, a causa della precisione con cui le ipotesi del modello sottostante si adattano ai dati. Un approccio ben accolto per migliorare le prestazioni del sistema di raccomandazione per un particolare utente \u00e8 prendere in prestito informazioni da altri utenti attraverso un approccio di modellazione gerarchica bayesiana.Diversi ricercatori hanno dimostrato che questo approccio bilancia efficacemente le informazioni condivise con quelle specifiche dell'utente, alleviando cos\u00ec le scarse prestazioni iniziali per ciascun utente -LSB- 27 -RSB- -LSB- 25 -RSB-. Per apprendere un modello gerarchico bayesiano, il sistema solitamente cerca di trovare i parametri del modello pi\u00f9 probabili per i dati forniti. Un sistema di raccomandazioni maturo di solito funziona per milioni di utenti. \u00c8 noto che apprendere i parametri ottimali di un modello gerarchico bayesiano \u00e8 computazionalmente costoso quando ci sono migliaia o milioni di utenti. L'algoritmo EM \u00e8 una tecnica comunemente utilizzata per l'apprendimento dei parametri grazie alla sua semplicit\u00e0 e garanzia di convergenza. Tuttavia, un sistema di raccomandazione basato sui contenuti spesso gestisce i documenti in uno spazio dimensionale molto elevato, in cui ciascun documento \u00e8 rappresentato da un vettore molto sparso. Con un'attenta analisi dell'algoritmo EM in questo scenario -LRB- Sezione 4 -RRB-, troviamo che l'EM tering, o filtraggio collaborativo basato sugli elementi. In questo documento, le parole \u201cfiltraggio\u201d e \u201craccomandazione\u201d sono usate in modo intercambiabile. L\u2019algoritmo converge molto lentamente a causa della scarsit\u00e0 delle variabili di input. Troviamo anche che aggiornare il parametro del modello ad ogni iterazione EM \u00e8 costoso anche con una complessit\u00e0 computazionale di O -LRB- MK -RRB-, dove M \u00e8 il numero di utenti e K \u00e8 il numero di dimensioni. Questo articolo modifica l'algoritmo EM standard per creare un algoritmo di apprendimento migliorato, che chiameremo \"algoritmo EM modificato\". ''Ci\u00f2 riduce notevolmente il calcolo in una singola iterazione EM e ha anche il vantaggio di aumentare la velocit\u00e0 di convergenza dell'algoritmo di apprendimento. La tecnica proposta non \u00e8 solo ben supportata dalla teoria, ma anche dai risultati sperimentali. L'organizzazione delle restanti parti di questo documento \u00e8 la seguente: La sezione 3 descrive il quadro di modellazione della regressione lineare gerarchica bayesiana utilizzato per le raccomandazioni basate sul contenuto. La sezione 4 descrive come apprendere i parametri del modello utilizzando l'algoritmo EM standard, insieme alla nuova tecnica proposta in questo articolo. Il context sperimentale ed i risultati utilizzati per validare la tecnica di apprendimento proposta sono riportati nelle Sezioni 5 e 6. 2. LAVORI CORRELATI Fornire raccomandazioni personalizzate agli utenti \u00e8 stato identificato come un problema molto importante nella comunit\u00e0 IR sin dagli anni '70. Gli approcci utilizzati per risolvere questo problema possono essere grossolanamente classificati in due categorie principali: filtraggio basato sui contenuti e filtraggio collaborativo. Il filtraggio basato sul contenuto studia lo scenario in cui un sistema di raccomandazioni monitora un flusso di documenti e invia i documenti che corrispondono a un profilo utente all'utente corrispondente. Il filtraggio collaborativo va oltre il semplice utilizzo del contenuto del documento per consigliare articoli a un utente, sfruttando le informazioni di altri utenti con gusti e preferenze simili in passato.Euristiche basate sulla memoria e approcci basati su modelli sono stati utilizzati nell'attivit\u00e0 di filtraggio collaborativo -LSB- 15 -RSB- -LSB- 8 -RSB- -LSB- 2 -RSB- -LSB- 14 -RSB- -LSB- 12 -RSB- -LSB - 11 -RSB-. Questo articolo contribuisce alla ricerca sulle raccomandazioni basate sui contenuti migliorando l'efficienza e l'efficacia dei modelli lineari gerarchici bayesiani, che hanno una solida base teorica e buone prestazioni empiriche sui compiti di raccomandazione -LSB- 27 -RSB- -LSB- 25 -RSB-. Questo documento non intende confrontare il filtraggio basato sui contenuti con il filtraggio collaborativo o affermare quale sia il migliore. Riteniamo che ciascuno di essi sia complementare all'altro e che il filtraggio basato sui contenuti sia estremamente utile per gestire nuovi documenti/elementi con poco o nessun feedback da parte degli utenti. Analogamente ad altri ricercatori -LSB- 18 -RSB- -LSB- 1 -RSB- -LSB- 21 -RSB-, abbiamo scoperto che un sistema di raccomandazioni sar\u00e0 pi\u00f9 efficace quando entrambe le tecniche sono combinate. 7. CONCLUSIONE L'apprendimento del profilo utente basato sul contenuto \u00e8 un problema importante ed \u00e8 la chiave per fornire raccomandazioni personali a un utente, in particolare per consigliare nuovi articoli con un numero limitato di valutazioni. L'approccio di modellazione gerarchica bayesiana sta diventando un importante approccio di apprendimento del profilo utente grazie alla sua capacit\u00e0 teoricamente giustificata di aiutare un utente attraverso il trasferimento di informazioni dagli altri utenti tramite hyperprior. Questo articolo ha esaminato la debolezza del popolare approccio di apprendimento basato su EM per i modelli lineari gerarchici bayesiani e ha proposto una tecnica di apprendimento migliore chiamata EM modificato. Abbiamo dimostrato che la nuova tecnica \u00e8 teoricamente pi\u00f9 efficiente dal punto di vista computazionale rispetto all\u2019algoritmo EM standard. La valutazione del set di dati Reuters ha mostrato che la nuova tecnica ha funzionato in modo simile all\u2019algoritmo EM standard quando la condizione di scarsit\u00e0 non \u00e8 soddisfatta. Vale la pena ricordare che anche se lo spazio del problema originale non \u00e8 scarso, la scarsit\u00e0 pu\u00f2 essere creata artificialmente quando un sistema di raccomandazione utilizza tecniche di selezione delle caratteristiche specifiche dell\u2019utente per ridurre il rumore e la complessit\u00e0 del modello utente. La tecnica proposta pu\u00f2 anche essere adattata per migliorare l'apprendimento in tale scenario. Abbiamo anche dimostrato che la tecnica proposta pu\u00f2 apprendere mezzo milione di profili utente da 100 milioni di valutazioni in poche ore con una singola CPU. Il nostro lavoro rappresenta un passo importante sulla strada per rendere pi\u00f9 pratici i modelli lineari gerarchici bayesiani. La nuova tecnica proposta pu\u00f2 essere facilmente adattata per funzionare su un cluster di macchine, accelerando cos\u00ec ulteriormente il processo di apprendimento per gestire un sistema su scala pi\u00f9 ampia con centinaia di milioni di utenti. La ricerca ha un grande potenziale per avvantaggiare le persone che utilizzano l\u2019algoritmo EM su molti altri problemi IR, nonch\u00e9 su problemi di apprendimento automatico. L'algoritmo EM \u00e8 una tecnica di apprendimento automatico comunemente utilizzata. Viene utilizzato per trovare i parametri del modello in molti problemi IR in cui i dati di addestramento sono molto scarsi. Anche se ci stiamo concentrando sui modelli lineari gerarchici bayesiani per la raccomandazione e il filtraggio,la nuova idea di utilizzare una soluzione analitica invece di una soluzione numerica per coppie utente-funzionalit\u00e0 non correlate nella fase M potrebbe essere adattata a molti altri problemi.", "keyphrases": ["modello", "base di contenuti", "consigliare il sistema", "regressione lineare", "filtro di collaborazione", "paramet", "imparare la tecnica", "lo", "algoritmo", "classif", "valutare"]}
{"file_name": "I-32", "text": "Un modello di ambiente contraddittorio per agenti razionali limitati in interazioni a somma zero ABSTRACT Gli ambienti multiagente spesso non sono n\u00e9 cooperativi n\u00e9 collaborativi; in molti casi, gli agenti hanno interessi contrastanti, che portano a interazioni contraddittorie. Questo articolo presenta un modello formale di ambiente contraddittorio per agenti razionali limitati che operano in un ambiente a somma zero. In tali ambienti, i tentativi di utilizzare metodi di ricerca classici basati sull'utilit\u00e0 possono sollevare una serie di difficolt\u00e0 -LRB- ad esempio, modellando implicitamente l'avversario come un massimizzatore di utilit\u00e0 onnisciente, piuttosto che sfruttare un modello dell'avversario pi\u00f9 sfumato ed esplicito -RRB-. Definiamo un ambiente contraddittorio descrivendo gli stati mentali di un agente in tale ambiente. Presentiamo quindi assiomi comportamentali che intendono servire come principi di progettazione per la costruzione di tali agenti avversari. Esploriamo l'applicazione del nostro approccio analizzando i file di registro dei giochi Connect-Four completati e presentiamo un'analisi empirica dell'adeguatezza degli assiomi. 1. INTRODUZIONE Prime ricerche sui sistemi multiagente -LRB-MAS-RRB- considerati gruppi cooperativi di agenti; Poich\u00e9 i singoli agenti avevano effettuato la ricerca MAS, tuttavia, presto iniziarono a considerare gli agenti interagenti con interessi individuati, come rappresentanti di diversi esseri umani o organizzazioni con interessi non identici. Quando si verificano questi tipi di interazioni, gli ambienti richiedono un comportamento appropriato da parte degli agenti in essi situati. Chiamiamo questi ambienti ambienti contraddittori e chiamiamo avversari gli agenti in conflitto. Modelli di cooperazione e lavoro di squadra sono stati ampiamente esplorati in MAS attraverso l'assiomatizzazione degli stati mentali -LRB- ad esempio, -LSB- 8, 4, 5 -RSB- -RRB-. Tuttavia, nessuna di queste ricerche si \u00e8 occupata dei domini antagonisti e delle loro implicazioni sul comportamento degli agenti. Il nostro articolo affronta questo problema fornendo un modello formale e assiomatizzato di stato mentale per un sottoinsieme di domini contraddittori, vale a dire semplici ambienti contraddittori a somma zero. Inoltre, i metodi di ricerca tradizionali -LRB- come Min-Max -RRB- non utilizzano un modello dell'avversario, che si \u00e8 dimostrato un valido complemento alla pianificazione avversaria -LSB- 9, 3, 11 -RSB-. In questo articolo sviluppiamo un modello formale e assiomatizzato per agenti razionali limitati che si trovano in un ambiente contraddittorio a somma zero. Il modello utilizza diversi operatori di modalit\u00e0 e le sue basi principali sono il modello SharedPlans -LSB- 4 -RSB- per il comportamento collaborativo. Esploriamo le propriet\u00e0 dell'ambiente e gli stati mentali degli agenti per derivare assiomi comportamentali; questi assiomi comportamentali costituiscono un modello formale che funge da specifica e linea guida di progettazione per la progettazione degli agenti in tali contesti. Investigheremo quindi empiricamente il comportamento del nostro modello utilizzando il gioco da tavolo Connect-Four. Mostriamo che questo gioco \u00e8 conforme alla nostra definizione di ambiente e analizziamo il comportamento dei giocatori utilizzando un ampio insieme di registri delle partite completati 4. LAVORO CORRELATO Tuttavia,tutte queste teorie formali riguardano il lavoro di squadra e la cooperazione degli agenti. Per quanto ne sappiamo, il nostro modello \u00e8 il primo a fornire un modello formalizzato per gli ambienti avversari espliciti e il comportamento degli agenti in esso. Il classico algoritmo di ricerca avversaria Min-Max \u00e8 stato il primo tentativo di integrare l'avversario nello spazio di ricerca con il debole presupposto di un avversario che gioca in modo ottimale. Da allora, sono stati fatti molti sforzi per integrare il modello dell\u2019avversario nella procedura decisionale per prevedere il comportamento futuro. Ulteriore lavoro di pianificazione contraddittoria \u00e8 stato svolto da Willmott et al. -LSB- 13 -RSB-, che ha fornito un approccio di pianificazione avversaria al gioco del GO. La ricerca sopra menzionata ha riguardato la ricerca contraddittoria e l'integrazione dei modelli dell'avversario nei classici metodi di ricerca basati sull'utilit\u00e0. Quel lavoro mostra l'importanza del modellamento dell'avversario e la capacit\u00e0 di sfruttarlo a vantaggio di un agente. Tuttavia, restano valide le limitazioni di base di questi metodi di ricerca; il nostro modello cerca di superare queste limitazioni presentando un modello formale per una nuova specificazione contraddittoria basata sullo stato mentale. 5. CONCLUSIONI Abbiamo presentato un modello di ambiente contraddittorio per un 2Questi sono stati successivamente rimossi dall'analisi finale. agente razionale limitato situato in un ambiente a N giocatori e a somma zero. Abbiamo utilizzato la formalizzazione SharedPlans per definire il modello e gli assiomi che gli agenti possono applicare come linee guida comportamentali. Il modello \u00e8 pensato per essere utilizzato come linea guida per la progettazione di agenti che devono operare in tali ambienti contraddittori. Abbiamo presentato risultati empirici, basati sull'analisi dei file di registro di ConnectFour, che esemplificano il modello e gli assiomi per un'istanza bilaterale dell'ambiente. I risultati che abbiamo presentato sono un primo passo verso un modello ampliato che coprir\u00e0 tutti i tipi di ambienti contraddittori, ad esempio ambienti a somma diversa da zero e ambienti che contengono agenti naturali che non fanno parte del conflitto diretto. Queste e altre sfide saranno affrontate nella ricerca futura.restano valide le limitazioni di base di questi metodi di ricerca; il nostro modello cerca di superare queste limitazioni presentando un modello formale per una nuova specificazione contraddittoria basata sullo stato mentale. 5. CONCLUSIONI Abbiamo presentato un modello di ambiente contraddittorio per un 2Questi sono stati successivamente rimossi dall'analisi finale. agente razionale limitato situato in un ambiente a N giocatori e a somma zero. Abbiamo utilizzato la formalizzazione SharedPlans per definire il modello e gli assiomi che gli agenti possono applicare come linee guida comportamentali. Il modello \u00e8 pensato per essere utilizzato come linea guida per la progettazione di agenti che devono operare in tali ambienti contraddittori. Abbiamo presentato risultati empirici, basati sull'analisi dei file di registro di ConnectFour, che esemplificano il modello e gli assiomi per un'istanza bilaterale dell'ambiente. I risultati che abbiamo presentato sono un primo passo verso un modello ampliato che coprir\u00e0 tutti i tipi di ambienti contraddittori, ad esempio ambienti a somma diversa da zero e ambienti che contengono agenti naturali che non fanno parte del conflitto diretto. Queste e altre sfide saranno affrontate nella ricerca futura.restano valide le limitazioni di base di questi metodi di ricerca; il nostro modello cerca di superare queste limitazioni presentando un modello formale per una nuova specificazione contraddittoria basata sullo stato mentale. 5. CONCLUSIONI Abbiamo presentato un modello di ambiente contraddittorio per un 2Questi sono stati successivamente rimossi dall'analisi finale. agente razionale limitato situato in un ambiente a N giocatori e a somma zero. Abbiamo utilizzato la formalizzazione SharedPlans per definire il modello e gli assiomi che gli agenti possono applicare come linee guida comportamentali. Il modello \u00e8 pensato per essere utilizzato come linea guida per la progettazione di agenti che devono operare in tali ambienti contraddittori. Abbiamo presentato risultati empirici, basati sull'analisi dei file di registro di ConnectFour, che esemplificano il modello e gli assiomi per un'istanza bilaterale dell'ambiente. I risultati che abbiamo presentato sono un primo passo verso un modello ampliato che coprir\u00e0 tutti i tipi di ambienti contraddittori, ad esempio ambienti a somma diversa da zero e ambienti che contengono agenti naturali che non fanno parte del conflitto diretto. Queste e altre sfide saranno affrontate nella ricerca futura.", "keyphrases": ["ambiente multiag", "gli avversari interagiscono", "ambiente avversario", "assioma comportamentale", "istanti bilater e multilater", "funzione di valutazione", "azione beneficiaria", "gioco \"collega quattro\".", "empirstudi", "modello assiomatico", "conteggio a somma zero", "gruppo di trattamento", "valore val", "interagire"]}
{"file_name": "I-29", "text": "Gestione distribuita degli orari flessibili ABSTRACT Consideriamo il problema della gestione degli orari in un ambiente incerto e distribuito. Assumiamo un team di agenti collaborativi, ciascuno responsabile dell'esecuzione di una parte di un programma prestabilito a livello globale, ma nessuno in possesso di una visione globale n\u00e9 del problema n\u00e9 della soluzione. L'obiettivo \u00e8 massimizzare la qualit\u00e0 congiunta ottenuta dalle attivit\u00e0 eseguite da tutti gli agenti, dato che, durante l'esecuzione, eventi imprevisti imporranno modifiche ad alcune attivit\u00e0 prescritte e ridurranno l'utilit\u00e0 di eseguirne altre. Descriviamo un'architettura ad agenti per risolvere questo problema che accoppia due meccanismi di base: -LRB- 1 -RRB- una rappresentazione con \"tempi flessibili\" del programma dell'agente -LRB- utilizzando una rete temporale semplice -RRB- e -LRB - 2 -RRB- una procedura di riprogrammazione incrementale. Il primo protegge dall'incertezza temporale consentendo all'esecuzione di procedere da un insieme di soluzioni ammissibili, mentre il secondo agisce per rivedere il programma dell'agente quando l'esecuzione \u00e8 forzata al di fuori di questo insieme di soluzioni o quando gli eventi di esecuzione riducono il valore atteso di questo fattibile. insieme di soluzioni. Il coordinamento di base con gli altri agenti si ottiene semplicemente comunicando i cambiamenti di programma a quegli agenti con attivit\u00e0 interdipendenti. Quindi, quando il tempo lo consente, l'infrastruttura centrale di risoluzione dei problemi locale viene utilizzata per guidare un processo di generazione e interrogazione di opzioni tra agenti, volto a identificare opportunit\u00e0 di miglioramento della soluzione attraverso il cambiamento congiunto. Utilizzando un simulatore per modellare l'ambiente, confrontiamo le prestazioni del nostro sistema multi-agente con quelle di un solutore MDP centralizzato -LRB- ottimale atteso ma non scalabile -RRB-. 1. INTRODUZIONE I vincoli pratici di molti ambienti applicativi richiedono una gestione distribuita dei piani e delle pianificazioni di esecuzione. In questo articolo, consideriamo il problema della gestione e dell'esecuzione dei programmi in un ambiente incerto e distribuito come definito dal programma DARPA Coordinators. Assumiamo un team di agenti collaborativi, ciascuno responsabile dell'esecuzione di una parte di un programma prestabilito a livello globale, ma nessuno in possesso di una visione globale n\u00e9 del problema n\u00e9 della soluzione. L'obiettivo del team \u00e8 massimizzare la qualit\u00e0 totale di tutte le attivit\u00e0 eseguite da tutti gli agenti, dato che eventi imprevisti imporranno modifiche alle attivit\u00e0 pre-programmate e altereranno l'utilit\u00e0 di eseguirne altre man mano che l'esecuzione si svolge. Per fornire una base per il coordinamento distribuito, ogni agente \u00e8 consapevole delle dipendenze tra le sue attivit\u00e0 pianificate e quelle degli altri agenti. Ad ogni agente viene inoltre assegnato un insieme precalcolato di opzioni di contingenza locale -LRB- fallback -RRB-. Al centro del nostro approccio alla risoluzione di questo problema multi-agente \u00e8 un quadro di pianificazione incrementale con tempi flessibili. In una rappresentazione a tempi flessibili della pianificazione di un agente, gli intervalli di esecuzione associati alle attivit\u00e0 pianificate non sono fissi,ma invece possono fluttuare entro i vincoli imposti di tempo e di sequenza delle attivit\u00e0. Tuttavia il loro utilizzo in contesti di problem solving distribuito \u00e8 stato piuttosto scarso -LRB- -LSB- 7 -RSB- \u00e8 un'eccezione -RRB-, e gli approcci precedenti alla pianificazione multi-agente -LRB- ad esempio, -LSB- 6, 13, 5 -RSB- -RRB- hanno generalmente operato con rappresentazioni a orari fissi degli orari degli agenti. Definiamo un'architettura di agenti incentrata sulla gestione incrementale di una pianificazione temporale flessibile. Il cambiamento locale \u00e8 ac Figura 1: problema TAEMS C a due agenti. realizzato da uno scheduler incrementale, progettato per massimizzare la qualit\u00e0 tentando di ridurre al minimo il cambiamento di pianificazione. A questa infrastruttura di gestione della pianificazione, aggiungiamo due meccanismi per il coordinamento multi-agente. Il coordinamento di base con altri agenti si ottiene mediante la semplice comunicazione delle modifiche all'orario locale ad altri agenti con attivit\u00e0 interdipendenti. A questo si sovrappone un processo di generazione e valutazione di opzioni non locali -LRB- simile per alcuni aspetti a -LSB- 5 -RSB- -RRB-, mirato all'identificazione di opportunit\u00e0 di miglioramento globale attraverso modifiche congiunte agli orari di pi\u00f9 agenti. Iniziamo riassumendo brevemente il problema generale della scheduling distribuito di interesse nel nostro lavoro. Successivamente, presenteremo l'architettura dell'agente che abbiamo sviluppato per risolvere questo problema e ne abbozzeremo il funzionamento. Nelle sezioni seguenti, descriviamo i componenti dell'architettura in maggiore dettaglio, considerando di volta in volta le questioni relative all'esecuzione delle pianificazioni degli agenti, alla revisione incrementale delle pianificazioni degli agenti e al coordinamento delle modifiche della pianificazione tra pi\u00f9 agenti. Forniamo quindi alcuni risultati sperimentali per indicare le prestazioni attuali del sistema. Infine concludiamo con una breve discussione degli attuali progetti di ricerca. 8. STATO E INDIRIZZI I nostri attuali sforzi di ricerca sono mirati ad estendere le capacit\u00e0 dell'agente Anno 1 e ad adattarlo a problemi significativamente pi\u00f9 grandi. Gli obiettivi di valutazione programmatica del secondo anno richiedono la risoluzione di problemi dell'ordine di 100 agenti e 10.000 metodi. Questa scala pone richieste computazionali molto pi\u00f9 elevate su tutti i componenti dell'agente. Abbiamo recentemente completato una reimplementazione dell'agente prototipo progettato per risolvere alcuni problemi di prestazioni riconosciuti. Oltre a verificare che le prestazioni sui problemi dell'anno 1 corrispondano o vengano superate, di recente abbiamo eseguito con successo alcuni test con l'agente su circa 100 problemi dell'agente. Per affrontare in modo completo le diverse problematiche legate all\u2019espansione, stiamo studiando una serie di meccanismi di coordinamento pi\u00f9 avanzati. Per fornire una prospettiva pi\u00f9 globale alle decisioni di pianificazione locale, stiamo introducendo meccanismi per calcolare, comunicare e utilizzare stime dell'impatto non locale dei nodi remoti. Per affrontare meglio il problema della creazione di punti di sincronizzazione tra agenti, stiamo espandendo l'uso dei proprietari delle attivit\u00e0 e dei protocolli specifici di qaf come mezzo per dirigere l'attivit\u00e0 di coordinamento. Finalmente,intendiamo esplorare l'uso di meccanismi di coordinamento pi\u00f9 avanzati guidati da STN, compreso l'uso del disaccoppiamento temporale -LSB-7-RSB- per isolare le azioni di agenti interdipendenti e l'introduzione di programmi di contingenza sensibili alla probabilit\u00e0.", "keyphrases": ["gestire la pianificazione", "ambiente di distribuzione", "architettura dell'agente", "programma", "interdipendenza attiva", "geografia separata", "orario flessibile", "pianta centrale", "gestionale", "pianificazione-esecuzione", "allentamento", "algoritmo del percorso pi\u00f9 breve", "allocazione attiva", "approccio guidato dal conflitto", "sincronizzato ottimista", "coordinamento tra agenti", "eseguire"]}
{"file_name": "C-30", "text": "Punto elenco: Diffusione dei dati a larghezza di banda elevata utilizzando una rete overlay SOMMARIO Negli ultimi anni, le reti overlay sono diventate un'alternativa efficace al multicast IP per comunicazioni punto-multipunto efficienti su Internet. In genere, i nodi si auto-organizzano con l\u2019obiettivo di formare un albero sovrapposto efficiente, che soddisfi gli obiettivi prestazionali senza imporre oneri eccessivi alla rete sottostante. In questo articolo, miriamo alla distribuzione dei dati a larghezza di banda elevata da un'unica fonte a un gran numero di ricevitori. Le applicazioni includono trasferimenti di file di grandi dimensioni e streaming multimediale in tempo reale. Per queste applicazioni, sosteniamo che una mesh sovrapposta, piuttosto che un albero, pu\u00f2 fornire larghezza di banda e affidabilit\u00e0 fondamentalmente pi\u00f9 elevate rispetto alle tipiche strutture ad albero. Questo articolo presenta Bullet, un algoritmo scalabile e distribuito che consente ai nodi sparsi su Internet di auto-organizzarsi in una rete overlay a larghezza di banda elevata. Costruiamo Bullet sulla base della consapevolezza che i dati dovrebbero essere distribuiti in modo disgiunto nei punti strategici della rete. I singoli ricevitori Bullet sono quindi responsabili della localizzazione e del recupero dei dati da pi\u00f9 punti in parallelo. I contributi chiave di questo lavoro includono: i -RRB- un algoritmo che invia dati a diversi punti nella sovrapposizione in modo tale che qualsiasi oggetto dati abbia la stessa probabilit\u00e0 di apparire in qualsiasi nodo, ii -RRB- un algoritmo scalabile e decentralizzato che consente ai nodi di individuare e recuperare elementi di dati mancanti e iii -RRB- un'implementazione e una valutazione completa di Bullet in esecuzione su Internet e in un ambiente di emulazione su larga scala rivela miglioramenti della larghezza di banda fino a un fattore due in una variet\u00e0 di circostanze. Inoltre, abbiamo riscontrato che, rispetto alle soluzioni basate su alberi, Bullet riduce la necessit\u00e0 di eseguire costosi sondaggi della larghezza di banda. In un albero, \u00e8 fondamentale che il genitore di un nodo consegni un'elevata quantit\u00e0 di dati applicativi a ciascun figlio. In Bullet, tuttavia, i nodi ricevono simultaneamente dati da pi\u00f9 fonti in parallelo, rendendo meno importante individuare una singola fonte in grado di sostenere un'elevata velocit\u00e0 di trasmissione. 1. INTRODUZIONE In questo articolo consideriamo il seguente problema generale. Dato un mittente e un ampio insieme di destinatari interessati sparsi su Internet, come possiamo massimizzare la quantit\u00e0 di larghezza di banda consegnata ai destinatari? Il nostro ambito problematico include la distribuzione di software o video e lo streaming multimediale in tempo reale. Tradizionalmente, il multicast IP nativo \u00e8 stato il metodo preferito per fornire contenuti a un insieme di ricevitori in modo scalabile. Tuttavia, una serie di considerazioni, tra cui la scalabilit\u00e0, l\u2019affidabilit\u00e0 e il controllo della congestione, hanno limitato l\u2019implementazione su larga scala del multicast IP. Anche se tutti questi problemi dovessero essere risolti, il multicast IP non considera la larghezza di banda quando costruisce il suo albero di distribuzione. Pi\u00f9 recentemente, gli overlay sono emersi come un'alternativa promettente al multicast per la consegna di dati da punto a multipunto efficiente in rete.Le tipiche strutture sovrapposte tentano di imitare la struttura degli alberi di routing multicast. Nel multicast a livello di rete, tuttavia, i nodi interni sono costituiti da router ad alta velocit\u00e0 con potenza di elaborazione ed estensibilit\u00e0 limitate. Gli overlay, d'altro canto, utilizzano host finali programmabili -LRB- e quindi estensibili -RRB- come nodi interni nell'albero overlay, con questi host che agiscono come ripetitori per pi\u00f9 figli lungo l'albero. Gli overlay si sono rivelati estremamente promettenti per le applicazioni in stile multicast. Tuttavia, noi sosteniamo che una struttura ad albero presenta limitazioni fondamentali sia per il multicast a larghezza di banda elevata che per l'elevata affidabilit\u00e0. Una difficolt\u00e0 con gli alberi \u00e8 che \u00e8 garantito che la larghezza di banda diminuisca monotonicamente scendendo lungo l'albero. Qualsiasi perdita in alto sull'albero ridurr\u00e0 la larghezza di banda disponibile per i ricevitori pi\u00f9 in basso sull'albero. Sono state proposte numerose tecniche per recuperare dalle perdite e quindi migliorare la larghezza di banda disponibile in un albero sovrapposto -LSB- 2, 6 -RSB-. Tuttavia, fondamentalmente, la larghezza di banda disponibile per qualsiasi host \u00e8 limitata dalla larghezza di banda disponibile dal singolo genitore di quel nodo nell'albero. Pertanto, il nostro lavoro parte dalla premessa che il modello per la diffusione dei dati multicast a larghezza di banda elevata dovrebbe essere riesaminato. Invece di inviare copie identiche dello stesso flusso di dati a tutti i nodi di un albero e progettare un meccanismo scalabile per il recupero dalla perdita, proponiamo che i partecipanti a un overlay multicast cooperino per trasmettere strategicamente set di dati disgiunti a vari punti della rete. Qui, il mittente suddivide i dati in blocchi sequenziali. I blocchi vengono ulteriormente suddivisi in singoli oggetti che vengono a loro volta trasmessi in diversi punti della rete. I nodi ricevono comunque una serie di oggetti dai loro genitori, ma sono poi responsabili di individuare i peer che contengono oggetti dati mancanti. Utilizziamo un algoritmo distribuito che mira a distribuire uniformemente la disponibilit\u00e0 degli elementi di dati tra tutti i partecipanti all'overlay. In questo modo evitiamo il problema di localizzare l'\u201cultimo oggetto\u201d, che potrebbe essere disponibile solo in pochi nodi. Per illustrare il comportamento di Bullet , si consideri un semplice overlay a tre nodi con una radice R e due figli A e B. R ha 1 Mbps di larghezza di banda disponibile -LRB- TCP-friendly -RRB- per ciascuno di A e B. Tuttavia, c'\u00e8 \u00e8 anche 1 Mbps di larghezza di banda disponibile tra A e B. In questo esempio, Bullet trasmetterebbe un insieme disgiunto di dati a 1 Mbps a ciascuno di A e B. A e B scoprirebbero quindi ciascuno in modo indipendente la disponibilit\u00e0 di dati disgiunti sul lato remoto eseguire il peer e iniziare lo streaming dei dati tra loro, raggiungendo effettivamente una velocit\u00e0 di recupero di 2 Mbps. D'altra parte, qualsiasi albero di sovrapposizione \u00e8 limitato a fornire al massimo 1 Mbps anche con una tecnica scalabile per il recupero dei dati persi. Qualsiasi soluzione per realizzare il modello di cui sopra deve mantenere una serie di propriet\u00e0. Innanzitutto deve essere compatibile con TCP -LSB- 15 -RSB-. In secondo luogo, deve imporre un basso carico di controllo. Esistono molte possibili fonti di tali spese generali,compreso il rilevamento della larghezza di banda disponibile tra i nodi, l'individuazione dei nodi appropriati per il \"peer\" per il recupero dei dati e la ricezione ridondante degli stessi oggetti dati da pi\u00f9 fonti. In terzo luogo, l\u2019algoritmo dovrebbe essere decentralizzato e scalabile fino a migliaia di partecipanti. A nessun nodo dovrebbe essere richiesto di apprendere o mantenere la conoscenza globale, ad esempio l'appartenenza a un gruppo globale o l'insieme di oggetti dati attualmente disponibili in tutti i nodi. Infine, l\u2019approccio deve essere resistente ai fallimenti individuali. Ad esempio, il guasto di un singolo nodo dovrebbe comportare solo una riduzione temporanea della larghezza di banda fornita a un piccolo sottoinsieme di partecipanti; nessun singolo guasto dovrebbe comportare la perdita completa di dati per una frazione significativa di nodi, come potrebbe essere il caso di un guasto di un singolo nodo \"in alto\" in un albero di sovrapposizione multicast. In questo context, questo articolo presenta la progettazione e la valutazione di Bullet, un algoritmo per la costruzione di una mesh di sovrapposizione che tenta di mantenere le propriet\u00e0 di cui sopra. I nodi bullet iniziano auto-organizzandosi in un albero sovrapposto, che pu\u00f2 essere costruito con una qualsiasi delle numerose tecniche esistenti -LSB- 1, 18, 21, 24, 34 -RSB-. Ciascun nodo Bullet, a partire dalla radice dell'albero sottostante, trasmette quindi un insieme disgiunto di dati a ciascuno dei suoi figli, con l'obiettivo di mantenere una rappresentativit\u00e0 uniforme di ciascun elemento di dati tra tutti i partecipanti. Il livello di disgiunzione \u00e8 determinato dalla larghezza di banda disponibile per ciascuno dei suoi figli. Bullet utilizza quindi un algoritmo scalabile ed efficiente per consentire ai nodi di individuare rapidamente pi\u00f9 peer in grado di trasmettere i dati mancanti al nodo. Pertanto, Bullet sovrappone una mesh a larghezza di banda elevata sopra un albero di sovrapposizione arbitrario. Infine, utilizziamo TFRC -LSB- 15 -RSB- per trasferire i dati sia lungo l'albero di sovrapposizione che tra peer. Un vantaggio importante del nostro approccio \u00e8 che la larghezza di banda fornita dalla mesh Bullet \u00e8 in qualche modo indipendente dalla larghezza di banda disponibile attraverso l'albero di sovrapposizione sottostante. Una limitazione significativa alla creazione di alberi sovrapposti con larghezza di banda elevata \u00e8 il sovraccarico associato al protocollo di costruzione dell'albero. In questi alberi, \u00e8 fondamentale che ogni partecipante individui un genitore tramite sondaggio con un alto livello di larghezza di banda disponibile perch\u00e9 riceve dati da una sola fonte -LRB- il suo genitore -RRB-. Pertanto, anche una volta costruito l\u2019albero, i nodi devono continuare a sondare per adattarsi alle condizioni della rete che cambiano dinamicamente. Sebbene il sondaggio della larghezza di banda sia un'area di ricerca attiva -LSB- 20, 35 -RSB-, risultati accurati generalmente richiedono il trasferimento di una grande quantit\u00e0 di dati per acquisire fiducia nei risultati. Il nostro approccio con Bullet consente ai ricevitori di ottenere un'elevata larghezza di banda in aggregato utilizzando trasferimenti individuali da peer sparsi nel sistema. Pertanto, in Bullet, la larghezza di banda disponibile da ogni singolo peer \u00e8 molto meno importante che in qualsiasi albero ottimizzato per la larghezza di banda. Ulteriore,tutta la larghezza di banda che normalmente verrebbe consumata per la ricerca della larghezza di banda pu\u00f2 essere riallocata per lo streaming di dati attraverso la mesh Bullet. Abbiamo completato un prototipo di Bullet che funziona su una serie di alberi sovrapposti. La nostra valutazione di un overlay da 1.000 nodi eseguito su un'ampia variet\u00e0 di topologie di rete emulate da 20.000 nodi mostra che Bullet pu\u00f2 fornire fino al doppio della larghezza di banda di un albero con larghezza di banda ottimizzata -LRB- utilizzando un algoritmo offline e informazioni sulla topologia di rete globale -RRB- , il tutto pur rimanendo compatibile con TCP. Per queste corse Internet live, riteniamo che Bullet sia in grado di fornire miglioramenti comparabili delle prestazioni della larghezza di banda. In entrambi i casi, il sovraccarico derivante dal mantenimento della mesh Bullet e dall'individuazione dei dati disgiunti appropriati \u00e8 limitato a 30 Kbps per nodo, accettabile per i nostri scenari target con larghezza di banda elevata e su larga scala. Il resto di questo documento \u00e8 organizzato come segue. La sezione 2 presenta i componenti del sistema Bullet tra cui RanSub, distribuzione informata dei contenuti e TFRC. La sezione 3 descrive poi in dettaglio Bullet, un efficiente sistema di distribuzione dei dati per applicazioni ad uso intensivo di larghezza di banda. La sezione 4 valuta le prestazioni di Bullet per una variet\u00e0 di topologie di rete e le confronta con le tecniche multicast esistenti. La sezione 5 colloca il nostro lavoro nel context degli sforzi correlati e la sezione 6 presenta le nostre conclusioni. 5. LAVORI CORRELATI Snoeren et al. -LSB- 36 -RSB- utilizza una mesh di sovrapposizione per ottenere una consegna affidabile e tempestiva di dati mission-critical. In questo sistema, ogni nodo sceglie n ``genitori'' da cui ricevere flussi di pacchetti duplicati. Poich\u00e9 la sua enfasi principale \u00e8 sull'affidabilit\u00e0, il sistema non tenta di migliorare la larghezza di banda fornita ai partecipanti dell'overlay inviando dati disgiunti a ciascun livello. Inoltre, durante il ripristino dal guasto del genitore, limita la scelta dei genitori da parte di un router overlay ai nodi con un numero di livello inferiore al proprio numero di livello. I nodi Kazaa sono organizzati in una struttura scalabile e gerarchica. I singoli utenti ricercano il contenuto desiderato nella struttura e procedono a scaricare contemporaneamente pezzi potenzialmente disgiunti dai nodi che gi\u00e0 lo possiedono. Poich\u00e9 Kazaa non affronta il modello di comunicazione multicast, un'ampia percentuale di utenti che scaricano lo stesso file consumerebbe pi\u00f9 larghezza di banda rispetto ai nodi organizzati nella struttura sovrapposta Bullet. BitTorrent -LSB- 3 -RSB- \u00e8 un altro esempio di sistema di distribuzione di file attualmente utilizzato su Internet. Il tracker pone un limite di scalabilit\u00e0, poich\u00e9 aggiorna continuamente la distribuzione del file a livello di sistema. Similmente a Bullet, BitTorrent incorpora il concetto di \"choking\" in ciascun nodo con l'obiettivo di identificare i destinatari che traggono il massimo vantaggio dal download da quella particolare fonte. FastReplica -LSB- 11 -RSB- affronta il problema della distribuzione affidabile ed efficiente dei file nelle reti di distribuzione dei contenuti -LRB- CDN -RRB-. Nell'algoritmo di base, i nodi sono organizzati in gruppi di dimensione fissa -LRB- n -RRB-,con informazioni complete sull'appartenenza al gruppo su ciascun nodo. Per distribuire il file, un nodo lo divide in n porzioni di uguali dimensioni, invia le porzioni agli altri membri del gruppo e chiede loro di scaricare i pezzi mancanti in parallelo dagli altri membri del gruppo. Poich\u00e9 lungo ciascuno dei collegamenti sovrapposti viene trasmessa solo una porzione fissa del file, l'impatto della congestione \u00e8 inferiore rispetto al caso della distribuzione ad albero. Tuttavia, poich\u00e9 tratta tutti i percorsi allo stesso modo, FastReplica non sfrutta appieno i collegamenti sovrapposti a larghezza di banda elevata nel sistema. Esistono numerosi protocolli che mirano ad aggiungere affidabilit\u00e0 al multicast IP. In Scalable Reliable Multicast -LRB- SRM -RRB- -LSB- 16 -RSB-, i nodi richiedono ritrasmissione multicast per pacchetti persi. Bullet \u00e8 strettamente correlato agli sforzi che utilizzano tecniche di propagazione dei dati epidemici per recuperare dalle perdite nell'albero multicast IP inaffidabile. In pbcast -LSB- 2 -RSB-, un nodo appartiene a un gruppo globale e sceglie periodicamente un sottoinsieme casuale di peer per inviare un digest dei pacchetti ricevuti. Un nodo che riceve il digest risponde al mittente con i pacchetti mancanti in modalit\u00e0 last-in, first-out. Poich\u00e9 lbpcast non richiede un albero sottostante per la distribuzione dei dati e si basa sul modello push-gossiping, il sovraccarico della rete pu\u00f2 essere piuttosto elevato. Rispetto agli sforzi multicast affidabili, Bullet si comporta favorevolmente in termini di sovraccarico della rete perch\u00e9 i nodi non richiedono \"ciecamente\" ritrasmissioni dai loro pari. Invece, Bullet utilizza le visualizzazioni di riepilogo ottenute tramite RanSub per guidare le sue azioni verso nodi con contenuti disgiunti. Inoltre, un nodo Bullet suddivide il carico di ritrasmissione tra tutti i suoi peer. Notiamo che i nodi pbcast contengono un meccanismo per limitare la velocit\u00e0 dei pacchetti ritrasmessi e per inviare pacchetti diversi in risposta allo stesso digest. Tuttavia, ci\u00f2 non garantisce che i pacchetti ricevuti in parallelo da pi\u00f9 peer non siano duplicati. Ancora pi\u00f9 importante, i metodi di ripristino multicast sono limitati dalla larghezza di banda attraverso l'albero, mentre Bullet si sforza di fornire pi\u00f9 larghezza di banda a tutti i ricevitori rendendo i dati deliberatamente disgiunti in tutto l'albero. Narada -LSB- 19 -RSB- crea una mesh con ritardo ottimizzato che interconnette tutti i nodi partecipanti e misura attivamente la larghezza di banda disponibile sui collegamenti overlay. Quindi esegue un protocollo di routing standard sopra la mesh di sovrapposizione per costruire alberi di inoltro utilizzando ciascun nodo come possibile fonte. I nodi Narada mantengono la conoscenza globale di tutti i partecipanti al gruppo, limitando la scalabilit\u00e0 del sistema a diverse decine di nodi. Inoltre, la larghezza di banda disponibile attraverso un albero Narada \u00e8 ancora limitata alla larghezza di banda disponibile da ciascun genitore. D'altra parte, l'obiettivo fondamentale di Bullet \u00e8 aumentare la larghezza di banda attraverso il download di dati disgiunti da pi\u00f9 peer. Overcast -LSB- 21 -RSB- \u00e8 un esempio di un algoritmo di costruzione di alberi sovrapposti efficiente in termini di larghezza di banda. In questo sistema,tutti i nodi si uniscono alla radice e migrano fino al punto dell'albero dove sono ancora in grado di mantenere un livello minimo di larghezza di banda. Si prevede che Bullet sia pi\u00f9 resistente alle partenze dei nodi rispetto a qualsiasi albero, incluso Overcast. Invece di un nodo in attesa di ricevere i dati mancanti da un nuovo genitore, un nodo pu\u00f2 iniziare a ricevere dati dai suoi pari perpendicolari. Il tempo di convergenza con il cielo coperto \u00e8 limitato dalle sonde ai fratelli e agli antenati immediati. Bullet \u00e8 in grado di fornire approssimativamente una larghezza di banda target senza avere un albero completamente convergente. Parallelamente al nostro lavoro, SplitStream -LSB- 9 -RSB- ha anche l'obiettivo di ottenere una diffusione dei dati ad alta larghezza di banda. Funziona suddividendo il flusso multicast in k strisce, trasmettendo ciascuna striscia lungo un albero multicast separato creato utilizzando Scribe -LSB- 34 -RSB-. Forse ancora pi\u00f9 importante, SplitStream presuppone che ci sia sufficiente larghezza di banda disponibile per trasportare ogni stripe su ogni collegamento dell'albero, inclusi i collegamenti tra l'origine dati e le radici dei singoli stripe tree scelti indipendentemente da Scribe. In una certa misura, Bullet e SplitStream sono complementari. Ad esempio, Bullet potrebbe funzionare su ciascuna striscia per massimizzare la larghezza di banda fornita a ciascun nodo lungo ciascuna striscia. CoopNet -LSB- 29 -RSB- considera lo streaming di contenuti live in un ambiente peerto-peer, soggetto a un elevato tasso di abbandono dei nodi. Di conseguenza, il sistema privilegia la resilienza rispetto all\u2019efficienza della rete. Nel caso dello streaming on-demand, CoopNet -LSB- 30 -RSB- risolve il problema del flash-crowd sul server centrale reindirizzando i client in entrata a un numero fisso di nodi che hanno precedentemente recuperato porzioni dello stesso contenuto. Rispetto a CoopNet, Bullet fornisce ai nodi un sottoinsieme uniformemente casuale della distribuzione del file a livello di sistema. 6. CONCLUSIONI Tipicamente, lo streaming di dati sovrapposti a larghezza di banda elevata avviene su un albero di distribuzione. In questo articolo sosteniamo che, in effetti, una mesh sovrapposta \u00e8 in grado di fornire una larghezza di banda fondamentalmente pi\u00f9 elevata. Naturalmente, \u00e8 necessario superare una serie di sfide difficili per garantire che i nodi della rete mesh non ricevano ripetutamente gli stessi dati dai peer. Questo articolo presenta la progettazione e l'implementazione di Bullet, un algoritmo di costruzione overlay scalabile ed efficiente che supera questa sfida per fornire miglioramenti significativi della larghezza di banda rispetto alle tradizionali strutture ad albero. Nello specifico, questo articolo fornisce i seguenti contributi: 9 Presentiamo la progettazione e l'analisi di Bullet, un algoritmo di costruzione dell'overlay che crea una mesh su qualsiasi albero di distribuzione e consente ai partecipanti dell'overlay di ottenere un throughput di larghezza di banda pi\u00f9 elevato rispetto allo streaming di dati tradizionale. Come vantaggio correlato, eliminiamo il sovraccarico necessario per sondare la larghezza di banda disponibile nelle tradizionali tecniche di costruzione di alberi distribuiti. 9 Forniamo una tecnica per recuperare i dati mancanti dai peer in modo scalabile ed efficiente.RanSub diffonde periodicamente riassunti di set di dati ricevuti da un sottoinsieme mutevole e uniformemente casuale di partecipanti globali. 9 Proponiamo un meccanismo per rendere i dati disgiunti e quindi distribuirli in modo uniforme che renda uguale per tutti i nodi la probabilit\u00e0 di trovare un peer contenente dati mancanti. 9 Una valutazione su larga scala di 1.000 partecipanti overlay in esecuzione in una topologia di rete emulata da 20.000 nodi, nonch\u00e9 la sperimentazione sul banco di prova Internet PlanetLab, mostrano che Bullet in esecuzione su un albero casuale pu\u00f2 raggiungere il doppio del throughput dello streaming su una larghezza di banda tradizionale albero.", "keyphrases": ["maglia sovrapposta", "diffusione dei dati", "rete sovrapposta", "multicast ip", "comune multipunto", "distribuzione dati a larghezza di banda elevata", "trasferimento di file di grandi dimensioni", "flusso multimediale in tempo reale", "proiettile", "sonda di larghezza di banda", "peer to peer", "ransub", "consegna dei contenuti", "tfrc"]}
{"file_name": "I-30", "text": "Allocazione distribuita dei compiti nei social network ABSTRACT Questo articolo propone una nuova variante del problema dell'allocazione dei compiti, in cui gli agenti sono connessi in una rete sociale e i compiti arrivano agli agenti distribuiti sulla rete. Mostriamo che la complessit\u00e0 di questo problema rimane NPhard. Inoltre, non \u00e8 approssimabile entro alcuni fattori. Sviluppiamo un algoritmo basato sul protocollo contract-net. Il nostro algoritmo \u00e8 completamente distribuito e presuppone che gli agenti abbiano solo una conoscenza locale di attivit\u00e0 e risorse. Conduciamo una serie di esperimenti per valutare le prestazioni e la scalabilit\u00e0 dell'algoritmo proposto in termini di qualit\u00e0 della soluzione e tempo di calcolo. Tre diversi tipi di reti, vale a dire reti small-world, casuali e senza scala, vengono utilizzate per rappresentare varie relazioni sociali tra agenti in applicazioni realistiche. I risultati dimostrano che il nostro algoritmo funziona bene e che si adatta bene ad applicazioni su larga scala. 1. INTRODUZIONE Negli ultimi anni si \u00e8 lavorato molto sui metodi di allocazione dei compiti e delle risorse, che possono potenzialmente essere applicati a molte applicazioni del mondo reale. Tuttavia, alcune applicazioni interessanti in cui giocano un ruolo le relazioni tra agenti richiedono un modello leggermente pi\u00f9 generale. gamma di metodi di assegnazione dei compiti. La domanda \u00e8 come le VO devono essere composte e ricomposte dinamicamente da singoli agenti, quando \u00e8 necessario eseguire compiti e sottoattivit\u00e0 diversi. Ci\u00f2 verrebbe fatto assegnandoli a diversi agenti, ciascuno dei quali potrebbe essere in grado di eseguire diversi sottoinsiemi di tali compiti. In questo articolo studiamo il problema dell'allocazione dei compiti dalla prospettiva di una struttura interconnessa cos\u00ec complessa. Nello specifico, quindi, consideriamo gli agenti connessi tra loro in una rete sociale. Oltre a modellare la struttura interconnessa tra partner commerciali, il social network introdotto in questo documento pu\u00f2 essere utilizzato anche per rappresentare altri tipi di connessioni o vincoli tra entit\u00e0 autonome che nascono da altri domini applicativi. La sezione successiva fornisce una descrizione formale del problema dell\u2019allocazione dei compiti sui social network. Nella Sezione 3 dimostriamo che la complessit\u00e0 di questo problema rimane NP-hard. Procederemo quindi allo sviluppo di un algoritmo distribuito nella Sezione 4 ed eseguiremo una serie di esperimenti con questo algoritmo, come descritto nella Sezione 5. La Sezione 6 discute il lavoro correlato e la Sezione 7 conclude. 6. LAVORI CORRELATI L'allocazione dei compiti nei sistemi multiagente \u00e8 stata studiata da molti ricercatori negli ultimi anni con presupposti ed enfatizzazioni diverse. Tuttavia, la maggior parte della ricerca condotta fino ad oggi sull'assegnazione dei compiti non considera le connessioni sociali tra gli agenti e studia il problema in modo centralizzato. La Sesta Internazionale Conf. Congiunta. sugli agenti autonomi e sui sistemi multi-agente -LRB- AAMAS 07 -RRB- 505 Figura 6: La qualit\u00e0 dell'algoritmo GDAP per una distribuzione uniforme e distorta dei benefici dei compiti relativa al rapporto delle risorse -LRB- il primo grafico -RRB-,e il grado della rete -LRB- il secondo grafico -RRB-. collocamento. Ad esempio, Kraus et al. -LSB- 12 -RSB- sviluppano un protocollo d'asta che consente agli agenti di formare coalizioni con vincoli di tempo. Si presuppone che ogni agente conosca le capacit\u00e0 di tutti gli altri. Il protocollo proposto \u00e8 centralizzato, in cui un manager \u00e8 responsabile dell'assegnazione dei compiti a tutte le coalizioni. Manisterski et al. -LSB- 14 -RSB- discutere le possibilit\u00e0 di ottenere allocazioni efficienti sia in contesti cooperativi che non cooperativi. Propongono un algoritmo centralizzato per trovare la soluzione ottimale. In contrasto con questo lavoro, introduciamo anche un efficiente protocollo completamente distribuito che tiene conto del social network. L'assegnazione dei compiti \u00e8 stata studiata anche in contesti distribuiti, ad esempio da Shehory e Kraus -LSB- 18 -RSB- e da Lerman e Shehory -LSB- 13 -RSB-. Propongono algoritmi distribuiti con bassa complessit\u00e0 di comunicazione per formare coalizioni in sistemi multiagente su larga scala. Tuttavia, non presuppongono l'esistenza di alcuna rete di agenti. Il lavoro di Sander et al. -LSB- 16 -RSB- introduce algoritmi basati sulla geometria computazionale per l'allocazione distribuita dei compiti in domini geografici. Agli agenti \u00e8 quindi consentito spostarsi e cercare attivamente le attivit\u00e0 e la capacit\u00e0 degli agenti di eseguire attivit\u00e0 \u00e8 omogenea. Per applicare il loro approccio, gli agenti devono avere una certa conoscenza della posizione geografica dei compiti e di alcuni altri agenti. Altro lavoro -LSB- 17 -RSB- propone un meccanismo di localizzazione per sistemi multiagente aperti per allocare compiti ad agenti sconosciuti. In questo approccio ogni agente memorizza nella cache un elenco di agenti che conosce. L'analisi della complessit\u00e0 comunicativa di questo metodo si basa su grafici reticolari, mentre investighiamo come risolvere in modo efficiente l'allocazione dei compiti in un social network, la cui topologia pu\u00f2 essere arbitraria. Le reti sono state impiegate nel context dell'assegnazione dei compiti anche in alcuni altri lavori, ad esempio per limitare la Figura 8: La qualit\u00e0 dell'algoritmo GDAP rispetto al limite superiore. interazioni tra agenti e mediatori -LSB- 1 -RSB-. I mediatori in questo context sono agenti che ricevono l'incarico e hanno connessioni con altri agenti. Suddividono l'attivit\u00e0 in sottoattivit\u00e0 e negoziano con altri agenti per ottenere impegni per eseguire queste sottoattivit\u00e0. Il loro obiettivo \u00e8 modellare il processo decisionale di un solo mediatore. Un altro approccio \u00e8 quello di suddividere la rete in cricche di nodi, che rappresentano coalizioni che gli agenti coinvolti possono utilizzare come meccanismo di coordinamento -LSB- 20 -RSB-. Il focus di questo lavoro \u00e8 la formazione di coalizioni distribuite tra gli agenti, ma nel nostro approccio non abbiamo bisogno che gli agenti formino gruppi prima di assegnare i compiti. Easwaran e Pitt -LSB- 6 -RSB- studiano i `compiti complessi' che richiedono `servizi' per la loro realizzazione. Il problema riguarda l'assegnazione dei sottocompiti ai fornitori di servizi in una catena di fornitura. Un altro studio sull\u2019allocazione dei compiti nelle catene di fornitura \u00e8 -LSB- 21 -RSB-,dove si sostiene che la caratteristica distintiva della formazione della catena di fornitura \u00e8 la scomposizione gerarchica delle sottoattivit\u00e0 -LRB- HSD -RRB-. L'HSD \u00e8 implementato utilizzando reti di dipendenza dalle attivit\u00e0 -LRB-TDN -RRB-, con agenti e merci come nodi e relazioni I/O tra loro come bordi. Qui viene data la rete e il problema \u00e8 selezionare un sottografo, per il quale gli autori propongono un algoritmo basato sul mercato, in particolare una serie di aste. Rispetto a questi lavori, il nostro approccio \u00e8 pi\u00f9 generale, nel senso che siamo in grado di modellare diversi tipi di connessioni o vincoli tra agenti per diversi ambiti problematici oltre alla formazione della catena di fornitura. Infine, i social network sono stati utilizzati nel context della formazione del team. Il lavoro precedente ha mostrato come apprendere quali relazioni sono pi\u00f9 vantaggiose nel lungo periodo -LSB- 8 -RSB- e adattare di conseguenza la rete sociale. Riteniamo che questi risultati possano essere trasferiti anche all\u2019ambito dell\u2019assegnazione dei compiti, lasciandolo come argomento per ulteriori studi. Figura 7: Il tempo di esecuzione dell'algoritmo GDAP. 506 La Sesta Intl.. Conf. congiunta. su Agenti Autonomi e Sistemi Multi-Agente -LRB- AAMAS 07 -RRB- 7. CONCLUSIONI In questo articolo abbiamo studiato il problema dell'allocazione dei compiti in una rete sociale -LRB- STAP -RRB-, che pu\u00f2 essere visto come un nuovo, pi\u00f9 generale problema , variante del TAP. Riteniamo che abbia un grande potenziale per problemi realistici. Abbiamo fornito risultati di complessit\u00e0 nel calcolo della soluzione efficiente per STAP, nonch\u00e9 un limite ai possibili algoritmi di approssimazione. Successivamente, abbiamo presentato un protocollo distribuito, correlato al protocollo contractnet. Abbiamo anche introdotto un algoritmo esponenziale per calcolare la soluzione ottima, nonch\u00e9 un veloce algoritmo upperbound. I risultati presentati in questo articolo mostrano che l'algoritmo distribuito funziona bene in reti di piccole dimensioni, senza scala e casuali e per molte impostazioni diverse. Sono stati condotti anche altri esperimenti -LRB-, ad esempio sulle reti di rete -RRB- e questi risultati sono stati validi su una gamma pi\u00f9 ampia di scenari. Inoltre, abbiamo dimostrato che si adatta bene a reti di grandi dimensioni, sia in termini di qualit\u00e0 che di tempo di calcolo richiesto. I risultati suggeriscono anche che le reti Small-World sono leggermente pi\u00f9 adatte per l\u2019allocazione dei compiti locali, perch\u00e9 non ci sono nodi con pochissimi vicini. Ci sono molte estensioni interessanti al nostro lavoro attuale. In questo articolo ci concentreremo sull\u2019aspetto computazionale nella progettazione dell\u2019algoritmo distribuito. Nel nostro lavoro futuro, vorremmo anche affrontare alcune delle questioni correlate alla teoria dei giochi, come gli agenti strategici, e mostrare le propriet\u00e0 desiderabili di un protocollo distribuito in tale context. Nell'algoritmo attuale presupponiamo che gli agenti possano contattare solo i loro vicini per richiedere risorse, il che potrebbe spiegare perch\u00e9 il nostro algoritmo non funziona cos\u00ec bene nelle reti senza scala come nelle reti del mondo piccolo. Il nostro lavoro futuro potrebbe consentire agli agenti di riallocare i compiti -LRB- sub -RRB-.Siamo interessati a vedere come tali interazioni influenzeranno le prestazioni dell'allocazione dei compiti nei diversi social network. Un terzo argomento interessante su cui lavorare ulteriormente \u00e8 l'aggiunta di informazioni sulla reputazione tra gli agenti. Ci\u00f2 pu\u00f2 aiutare a modellare le mutevoli relazioni commerciali e incentivare gli agenti a seguire il protocollo. Infine, sarebbe interessante studiare esempi nella vita reale del problema dell\u2019assegnazione dei compiti sociali e vedere come si collegano alle reti generate casualmente di diverso tipo studiate in questo articolo. Ringraziamenti.", "keyphrases": ["rete sociale", "relazione sociale", "allocazione dei compiti", "util", "alloc", "algoritmo", "messaggio comune", "comportamento", "sistema multiag", "agente strategico", "interagire"]}
{"file_name": "J-21", "text": "Un modello strategico per i mercati dell'informazione SOMMARIO I mercati dell'informazione, progettati specificamente per aggregare le informazioni dei trader, stanno diventando sempre pi\u00f9 popolari come mezzo per prevedere eventi futuri. Una recente ricerca sui mercati dell'informazione ha portato a due nuovi modelli, regole di punteggio di mercato e mercati parimutuel dinamici. Sviluppiamo un metodo analitico per guidare la progettazione e l'analisi strategica dei mercati dell'informazione. Il nostro contributo centrale \u00e8 un nuovo gioco di scommesse astratto, il gioco di proiezione, che funge da modello utile per i mercati dell\u2019informazione. Dimostriamo che questo gioco pu\u00f2 servire come modello strategico di mercati parimutuel dinamici e cattura anche l'essenza delle strategie nelle regole di punteggio di mercato. Il gioco di proiezione \u00e8 trattabile da analizzare e ha un'attraente visualizzazione geometrica che rende le mosse strategiche e le interazioni pi\u00f9 trasparenti. Lo usiamo per dimostrare diverse propriet\u00e0 strategiche del dinamico mercato del parimutuel. Dimostriamo anche che una forma speciale del gioco delle proiezioni \u00e8 strategicamente equivalente alla regola del punteggio sferico, ed \u00e8 strategicamente simile ad altre regole del punteggio. Infine, illustriamo due applicazioni del modello all'analisi di scenari strategici complessi: analizziamo la precisione di un mercato in cui i trader hanno inerzia, e un mercato in cui un trader pu\u00f2 trarre profitto manipolando le convinzioni di un altro trader. 1. INTRODUZIONE I mercati sono stati a lungo utilizzati come mezzo per gli scambi commerciali. Come effetto collaterale del commercio, i partecipanti al mercato rivelano qualcosa sulle loro preferenze e convinzioni. Ad esempio, in un mercato finanziario, gli agenti acquisterebbero azioni che ritengono sottovalutate e venderebbero azioni che ritengono sopravvalutate. \u00c8 stato osservato da tempo che, poich\u00e9 il prezzo di mercato \u00e8 influenzato da tutte le transazioni che si svolgono, esso aggrega le informazioni private di tutti i trader. Pertanto, in una situazione in cui gli eventi futuri sono incerti e ogni trader potrebbe avere poche informazioni, le informazioni aggregate contenute nei prezzi di mercato possono essere utilizzate per prevedere eventi futuri. Ci\u00f2 ha motivato la creazione di mercati dell'informazione, che sono meccanismi per aggregare le informazioni dei trader su un evento incerto. I mercati dell\u2019informazione possono essere modellati come un gioco in cui i partecipanti scommettono su una serie di possibili risultati, come i risultati di un\u2019elezione presidenziale, acquistando quote dei risultati e ricevendo profitti quando il risultato viene realizzato. Come nei mercati finanziari, i partecipanti mirano a massimizzare il proprio profitto acquistando a basso prezzo e vendendo a prezzo alto. Il vantaggio di mercati dell\u2019informazione ben progettati va oltre l\u2019aggregazione delle informazioni; possono anche essere utilizzati come strumento di copertura, per consentire ai trader di assicurarsi contro i rischi. Recentemente, i ricercatori si sono rivolti al problema di progettare strutture di mercato specificamente per ottenere migliori propriet\u00e0 di aggregazione delle informazioni rispetto ai mercati tradizionali. Sono stati proposti due modelli per i mercati dell'informazione:il Mercato Parimutuel Dinamico -LRB- DPM -RRB- di Pennock -LSB- 10 -RSB- e le Regole di Scoring del Mercato -LRB- MSR -RRB- di Hanson -LSB- 6 -RSB-. Sia il DPM che l\u2019MSR sono stati progettati con l\u2019obiettivo di dare ai trader informati un incentivo al commercio e di rivelare le loro informazioni il prima possibile, controllando allo stesso tempo il sussidio che il market designer deve immettere nel mercato. Una versione del DPM \u00e8 stata implementata in Yahoo! Buzz market -LSB- 8 -RSB- per testare sperimentalmente le propriet\u00e0 di previsione del mercato. L\u2019innovazione nell\u2019MSR consiste nell\u2019utilizzare queste regole di punteggio come strumenti che possono essere scambiati, fornendo cos\u00ec ai trader che dispongono di nuove informazioni un incentivo al commercio. L'MSR doveva essere utilizzato in un mercato di analisi politica in Medio Oriente -LSB- 15 -RSB-, che \u00e8 stato successivamente ritirato. I mercati dell\u2019informazione si basano su trader informati che commerciano per il proprio profitto, quindi \u00e8 fondamentale comprendere le propriet\u00e0 strategiche di questi mercati. Questo non \u00e8 un compito facile, perch\u00e9 i mercati sono complessi e i trader possono influenzare le convinzioni degli altri attraverso le loro operazioni e, quindi, possono potenzialmente ottenere guadagni a lungo termine manipolando il mercato. Per il DPM non siamo a conoscenza di alcuna analisi strategica preventiva di tale natura; infatti, \u00e8 stata scoperta una falla strategica durante il test del DPM in Yahoo! Buzz mercato -LSB- 8 -RSB-. 1.1 I nostri risultati In questo articolo, cerchiamo di sviluppare un metodo analitico per guidare la progettazione e l'analisi strategica dei mercati dell'informazione. Il nostro contributo centrale \u00e8 un nuovo gioco di scommesse astratto, il gioco della proiezione 1, che funge da modello utile per i mercati dell\u2019informazione. Il gioco delle proiezioni \u00e8 concettualmente pi\u00f9 semplice di MSR e DPM e quindi \u00e8 pi\u00f9 facile da analizzare. Inoltre ha un'attraente visualizzazione geometrica, che rende le mosse strategiche e le interazioni pi\u00f9 trasparenti. Presentiamo un'analisi delle strategie e dei profitti ottimali in questo gioco. Successivamente intraprendiamo un'analisi dei costi e dei profitti dei trader nel dinamico mercato del parimutuel. Sorprendentemente, troviamo che il costo di una sequenza di scambi nel DPM \u00e8 identico al costo delle mosse corrispondenti nel gioco di proiezione. Inoltre, se assumiamo che le convinzioni dei trader alla fine della negoziazione corrispondano alla vera probabilit\u00e0 che l'evento venga previsto, i guadagni e i profitti dei trader nel DPM sono identici ai loro guadagni e profitti in un corrispondente gioco di proiezione. Utilizziamo l'equivalenza tra il DPM e il gioco di proiezione per dimostrare che il DPM \u00e8 privo di arbitraggio, dedurre strategie redditizie nel DPM e dimostrare che i vincoli sulle operazioni degli agenti sono necessari per prevenire un crollo strategico. Dimostriamo anche un'equivalenza tra il gioco di proiezione e l'MSR: mostriamo che giocare nell'MSR \u00e8 strategicamente equivalente a giocare in un gioco di proiezione ristretta, almeno per strategie miopi e piccoli scambi. Ci\u00f2 ci consente di utilizzare il gioco delle proiezioni come modello concettuale per le regole di punteggio di mercato. Ulteriore,poich\u00e9 il gioco di proiezione ristretto corrisponde a un DPM con un vincolo commerciale naturale, ci\u00f2 fa luce su un\u2019interessante connessione tra MSR e DPM. Infine, illustriamo come il modello del gioco di proiezione possa essere utilizzato per analizzare il potenziale di manipolazione dei mercati dell\u2019informazione per ottenere guadagni a lungo termine.2 Presentiamo uno scenario esemplificativo in cui tale manipolazione pu\u00f2 verificarsi e suggeriamo regole aggiuntive che potrebbero mitigare la possibilit\u00e0 di manipolazione. Illustriamo anche un'altra applicazione per analizzare come un market maker pu\u00f2 migliorare l'accuratezza della previsione di un mercato in cui i trader non negozieranno a meno che il loro profitto atteso non sia superiore a una soglia. 1.2 Lavoro correlato Numerosi studi hanno dimostrato empiricamente che i prezzi di mercato sono buoni predittori di eventi futuri e sembrano aggregare la saggezza raccolta di tutti i trader -LSB- 2, 3, 12, 1, 5, 16 -RSB-. Numerosi studi recenti hanno affrontato la progettazione della struttura del mercato e le regole commerciali per i mercati dell\u2019informazione, nonch\u00e9 l\u2019incentivo alla partecipazione e altre questioni strategiche. Tuttavia, le questioni strategiche nei mercati dell'informazione sono state studiate anche da Mangold et al. -LSB- 8 -RSB- e da Hanson, Oprea e Porter -LSB- 7 -RSB-. Un prossimo documento di indagine -LSB- 11 -RSB- discute le formulazioni della funzione di costo dei market maker automatizzati. Organizzazione del documento Il resto di questo documento \u00e8 organizzato come segue: Nella Sezione 2, descriviamo il gioco delle proiezioni e analizziamo i costi, i profitti e le strategie ottimali dei giocatori in questo gioco. Nella sezione 3 studiamo il mercato dinamico del parimutuel e mostriamo che il commercio in un DPM \u00e8 equivalente a un gioco di proiezione. Stabiliamo una connessione tra il gioco di proiezione e l'MSR nella Sezione 4. Nella Sezione 5, illustriamo come il gioco di proiezione pu\u00f2 essere utilizzato per analizzare azioni non miopi e potenzialmente manipolative. Presentiamo le nostre conclusioni e suggerimenti per il lavoro futuro nella Sezione 6. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo presentato un semplice gioco geometrico, il gioco delle proiezioni, che pu\u00f2 servire come modello per il comportamento strategico nei mercati dell'informazione, nonch\u00e9 come un strumento per guidare la progettazione di nuovi mercati dell\u2019informazione. Abbiamo utilizzato questo modello per analizzare i costi, i profitti e le strategie di un trader in un mercato dinamico del parimutuel e abbiamo dimostrato che sia il mercato dinamico del parimutuel che la regola del punteggio del mercato sferico sono strategicamente equivalenti al gioco di proiezione ristretta con una leggera distorsione del probabilit\u00e0 a priori. L'analisi generale si basava sul presupposto che i trader non tentano attivamente di fuorviare altri trader per ottenere profitti futuri. Nella sezione 5, invece, analizziamo un piccolo mercato esemplificativo senza questo presupposto. Dimostriamo che il gioco delle proiezioni pu\u00f2 essere utilizzato per analizzare le strategie dei trader in questo scenario e potenzialmente per aiutare a progettare mercati con migliori propriet\u00e0 strategiche. I nostri risultati sollevano diverse domande aperte molto interessanti. in primo luogo,i profitti del gioco di proiezione non possono essere implementati direttamente in situazioni in cui la vera probabilit\u00e0 alla fine non viene rivelata. Infine, di grande interesse \u00e8 l\u2019esistenza di strategie manipolative a lungo raggio nei mercati dell\u2019informazione. L'esempio che abbiamo studiato nella sezione 5 non fa altro che scalfire la superficie di quest'area. Uno studio generale di questa classe di manipolazioni, insieme ad una caratterizzazione dei mercati in cui pu\u00f2 o non pu\u00f2 verificarsi, sarebbe molto utile per la progettazione dei mercati dell'informazione.", "keyphrases": ["informare il mercato", "mercato dinamico parimutuel", "modello di gioco del progetto", "prevedere il mercato", "regola del punteggio di mercato", "regola del punteggio sferico", "strategie di manipolazione a lungo raggio", "scienza-economia sociale e del comportamento", "tempo liquido"]}
{"file_name": "H-18", "text": "Segmentazione degli argomenti con rilevamento degli argomenti condivisi e allineamento di pi\u00f9 documenti ABSTRACT Il rilevamento e il tracciamento degli argomenti -LSB- 26 -RSB- e la segmentazione degli argomenti -LSB- 15 -RSB- svolgono un ruolo importante nell'acquisizione delle informazioni locali e sequenziali dei documenti. Il lavoro precedente in quest'area si concentra solitamente su singoli documenti, sebbene documenti multipli simili siano disponibili in molti domini. In questo articolo, introduciamo un nuovo metodo non supervisionato per il rilevamento di argomenti condivisi e la segmentazione degli argomenti di pi\u00f9 documenti simili basato su informazioni reciproche -LRB- MI -RRB- e informazioni reciproche ponderate -LRB- WMI -RRB- che \u00e8 una combinazione di MI e pesi dei termini. L'idea di base \u00e8 che la segmentazione ottimale massimizza MI -LRB- o WMI -RRB-. Il nostro approccio \u00e8 in grado di rilevare argomenti condivisi tra i documenti. Pu\u00f2 trovare i confini ottimali in un documento e allineare i segmenti tra i documenti allo stesso tempo. Pu\u00f2 anche gestire la segmentazione di un singolo documento come caso speciale di segmentazione e allineamento di pi\u00f9 documenti. I nostri metodi possono identificare e rafforzare i termini indicativi che possono essere utilizzati per la segmentazione e rimuovere parzialmente le keyphrases utilizzando pesi dei termini basati sull'entropia appresa da pi\u00f9 documenti. I nostri risultati sperimentali mostrano che il nostro algoritmo funziona bene per le attivit\u00e0 di segmentazione di un singolo documento, rilevamento di argomenti condivisi e segmentazione di pi\u00f9 documenti. L'utilizzo delle informazioni provenienti da pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni della segmentazione degli argomenti e l'utilizzo di WMI \u00e8 persino migliore dell'utilizzo di MI per la segmentazione di pi\u00f9 documenti. 1. INTRODUZIONE Molti ricercatori hanno lavorato sul rilevamento e sul tracciamento degli argomenti -LRB- TDT -RRB- -LSB- 26 -RSB- e sulla segmentazione degli argomenti negli ultimi dieci anni. La segmentazione degli argomenti intende identificare i confini di un documento con l'obiettivo di catturare la struttura tematica latente. Le attivit\u00e0 di segmentazione degli argomenti di solito rientrano in due categorie -LSB- 15 -RSB-: segmentazione del flusso di text in cui viene identificata la transizione dell'argomento e segmentazione dei documenti coerenti in cui i documenti sono suddivisi in sottoargomenti. Gli approcci tradizionali eseguono la segmentazione degli argomenti sui documenti uno alla volta -LSB- 15, 25, 6 -RSB-. La maggior parte di essi si comporta male in compiti delicati come la segmentazione coerente dei documenti -LSB- 15 -RSB-. Spesso gli utenti finali cercano documenti con contenuti simili. A un livello pi\u00f9 dettagliato, gli utenti potrebbero effettivamente cercare di ottenere sezioni di un documento simili a una sezione particolare che presumibilmente discute un argomento di loro interesse. Pertanto, l\u2019estensione della segmentazione degli argomenti da singoli documenti all\u2019identificazione di segmenti simili da pi\u00f9 documenti simili con lo stesso argomento \u00e8 una direzione naturale e necessaria, e si prevede che la segmentazione degli argomenti su pi\u00f9 documenti avr\u00e0 prestazioni migliori poich\u00e9 vengono utilizzate pi\u00f9 informazioni. Gli approcci tradizionali che utilizzano la misurazione della somiglianza basata sulla frequenza dei termini generalmente partono dallo stesso presupposto che un vocabolario simile tende a trovarsi in un segmento tematico coerente -LSB- 15, 25, 6 -RSB-. Tuttavia,di solito soffrono del problema di identificare le keyphrases. Ad esempio, le ulteriori parole di stop dipendenti dal documento vengono rimosse insieme alle parole di stop generiche in -LSB- 15 -RSB-. Esistono due motivi per cui non rimuoviamo direttamente le stop word. Innanzitutto, identificare le stopword \u00e8 un altro problema -LSB- 12 -RSB- che richiede una stima in ciascun dominio. La rimozione delle stop word comuni pu\u00f2 comportare la perdita di informazioni utili in un dominio specifico. Utilizziamo una classificazione morbida utilizzando i pesi dei termini. L'allineamento degli argomenti di pi\u00f9 documenti simili pu\u00f2 essere ottenuto raggruppando le frasi sullo stesso argomento nello stesso cluster. La segmentazione degli argomenti a documento singolo \u00e8 solo un caso speciale del problema di segmentazione e allineamento degli argomenti a pi\u00f9 documenti. Di solito, i lettori umani possono identificare la transizione dell'argomento in base alle keyphrases e possono ignorare le keyphrases. Ispirandoci a ci\u00f2, diamo a ciascun termine -LRB- o term cluster -RRB- un peso basato sull'entropia tra diversi documenti e diversi segmenti di documenti. Questo approccio non solo pu\u00f2 aumentare il contributo delle keyphrases, ma pu\u00f2 anche diminuire l'effetto delle parole stop comuni, delle parole rumorose e delle parole stop dipendenti dal documento. Queste parole sono comuni in un documento. Molti metodi basati sulla somiglianza delle frasi richiedono che queste parole vengano rimosse prima di poter eseguire la segmentazione dell'argomento -LSB- 15 -RSB-. I nostri risultati nella Figura 3 mostrano che i pesi dei termini sono utili per la segmentazione e l'allineamento degli argomenti di pi\u00f9 documenti. Il contributo principale di questo articolo \u00e8 che introduce un nuovo metodo per la segmentazione degli argomenti utilizzando l'IM e mostra che questo metodo funziona meglio rispetto ai criteri utilizzati in precedenza. Inoltre, abbiamo affrontato il problema della segmentazione degli argomenti e dell'allineamento tra pi\u00f9 documenti, mentre la maggior parte delle ricerche esistenti si concentrava sulla segmentazione di singoli documenti. La segmentazione e l'allineamento di pi\u00f9 documenti possono utilizzare informazioni provenienti da documenti simili e migliorare notevolmente le prestazioni della segmentazione degli argomenti. Ovviamente, il nostro approccio pu\u00f2 gestire singoli documenti come un caso speciale quando pi\u00f9 documenti non sono disponibili. Pu\u00f2 rilevare argomenti condivisi tra i documenti per giudicare se si tratta di pi\u00f9 documenti sullo stesso argomento. Introduciamo anche il nuovo criterio WMI basato sui pesi dei termini appresi da pi\u00f9 documenti simili, che pu\u00f2 migliorare ulteriormente le prestazioni della segmentazione degli argomenti. Proponiamo un algoritmo greedy iterativo basato sulla programmazione dinamica e mostriamo che funziona bene nella pratica. Alcuni dei nostri lavori precedenti sono in -LSB- 24 -RSB-. Il resto di questo documento \u00e8 organizzato come segue: Nella Sezione 2, esaminiamo il lavoro correlato. La sezione 3 contiene una formulazione del problema della segmentazione degli argomenti e dell'allineamento di pi\u00f9 documenti con il termine co-clustering, una revisione del criterio dell'MI per il clustering e infine un'introduzione al WMI. Nella Sezione 4, proponiamo innanzitutto l'algoritmo iterativo greedy di segmentazione degli argomenti e allineamento con il co-clustering dei termini, quindi descriviamo come l'algoritmo pu\u00f2 essere ottimizzato da noi. Figura 1:Illustrazione della segmentazione e dell'allineamento di pi\u00f9 documenti. programmazione dinamica. Nella Sezione 5 vengono descritti gli esperimenti sulla segmentazione di un singolo documento, il rilevamento di argomenti condivisi e la segmentazione di pi\u00f9 documenti e i risultati vengono presentati e discussi per valutare le prestazioni del nostro algoritmo. Le conclusioni e alcune direzioni future del lavoro di ricerca sono discusse nella Sezione 6. 2. LAVORO PRECEDENTE L'apprendimento supervisionato di solito ha buone prestazioni, poich\u00e9 apprende funzioni da insiemi di addestramento etichettati. Tuttavia, spesso ottenere set di formazione di grandi dimensioni con etichette manuali sulle frasi dei documenti \u00e8 proibitivo, quindi sono desiderabili approcci senza supervisione. Alcuni approcci si concentrano anche sulle keyphrases come suggerimenti per le transizioni degli argomenti -LSB- 11 -RSB-. Mentre alcuni metodi esistenti considerano solo le informazioni contenute in singoli documenti -LSB- 6, 15 -RSB-, altri utilizzano pi\u00f9 documenti -LSB- 16, 14 -RSB-. Non ci sono molti lavori in quest'ultima categoria, anche se si prevede che le prestazioni della segmentazione saranno migliori con l'utilizzo di informazioni provenienti da pi\u00f9 documenti. Precedenti ricerche hanno studiato metodi per trovare argomenti condivisi -LSB- 16 -RSB- e segmentazione e riepilogo degli argomenti solo tra una coppia di documenti -LSB- 14 -RSB-. La classificazione e il clustering del text sono un'area di ricerca correlata che classifica i documenti in gruppi utilizzando metodi supervisionati o non supervisionati. I criteri di questi approcci possono essere utilizzati nella questione della segmentazione degli argomenti. Alcuni di questi metodi sono stati estesi all'area della segmentazione degli argomenti, come PLSA -LSB- 5 -RSB- e entropia massima -LSB- 7 -RSB-, ma per quanto ne sappiamo, l'utilizzo dell'MI per la segmentazione degli argomenti non \u00e8 stato studiato . 6. CONCLUSIONI E LAVORO FUTURO Abbiamo proposto un nuovo metodo per la segmentazione e l'allineamento di argomenti multi-documento basato su informazioni reciproche ponderate, che pu\u00f2 anche gestire casi a documento singolo. Abbiamo utilizzato la programmazione dinamica per ottimizzare il nostro algoritmo. Il nostro approccio supera tutti i metodi precedenti sui casi a documento singolo. Inoltre, abbiamo anche dimostrato che eseguire la segmentazione tra pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni. I nostri risultati hanno anche dimostrato che l\u2019utilizzo di informazioni reciproche ponderate pu\u00f2 utilizzare le informazioni di pi\u00f9 documenti per ottenere prestazioni migliori. Abbiamo testato il nostro metodo solo su set di dati limitati. Dovrebbero essere testati pi\u00f9 set di dati, soprattutto quelli complicati. Dovrebbero essere confrontati metodi pi\u00f9 precedenti. Inoltre, le segmentazioni naturali come i paragrafi sono suggerimenti che possono essere utilizzati per trovare i confini ottimali. Pu\u00f2 essere preso in considerazione anche l\u2019apprendimento supervisionato.LAVORO PRECEDENTE L'apprendimento supervisionato di solito ha buone prestazioni, poich\u00e9 apprende funzioni da set di addestramento etichettati. Tuttavia, spesso ottenere set di formazione di grandi dimensioni con etichette manuali sulle frasi dei documenti \u00e8 proibitivo, quindi sono desiderabili approcci senza supervisione. Alcuni approcci si concentrano anche sulle keyphrases come suggerimenti per le transizioni degli argomenti -LSB- 11 -RSB-. Mentre alcuni metodi esistenti considerano solo le informazioni contenute in singoli documenti -LSB- 6, 15 -RSB-, altri utilizzano pi\u00f9 documenti -LSB- 16, 14 -RSB-. Non ci sono molti lavori in quest'ultima categoria, anche se si prevede che le prestazioni della segmentazione saranno migliori con l'utilizzo di informazioni provenienti da pi\u00f9 documenti. Precedenti ricerche hanno studiato metodi per trovare argomenti condivisi -LSB- 16 -RSB- e segmentazione e riepilogo degli argomenti solo tra una coppia di documenti -LSB- 14 -RSB-. La classificazione e il clustering del text sono un'area di ricerca correlata che classifica i documenti in gruppi utilizzando metodi supervisionati o non supervisionati. I criteri di questi approcci possono essere utilizzati nella questione della segmentazione degli argomenti. Alcuni di questi metodi sono stati estesi all'area della segmentazione degli argomenti, come PLSA -LSB- 5 -RSB- e entropia massima -LSB- 7 -RSB-, ma per quanto ne sappiamo, l'utilizzo dell'MI per la segmentazione degli argomenti non \u00e8 stato studiato . 6. CONCLUSIONI E LAVORO FUTURO Abbiamo proposto un nuovo metodo per la segmentazione e l'allineamento di argomenti multi-documento basato su informazioni reciproche ponderate, che pu\u00f2 anche gestire casi a documento singolo. Abbiamo utilizzato la programmazione dinamica per ottimizzare il nostro algoritmo. Il nostro approccio supera tutti i metodi precedenti sui casi a documento singolo. Inoltre, abbiamo anche dimostrato che eseguire la segmentazione tra pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni. I nostri risultati hanno anche dimostrato che l\u2019utilizzo di informazioni reciproche ponderate pu\u00f2 utilizzare le informazioni di pi\u00f9 documenti per ottenere prestazioni migliori. Abbiamo testato il nostro metodo solo su set di dati limitati. Dovrebbero essere testati pi\u00f9 set di dati, soprattutto quelli complicati. Dovrebbero essere confrontati metodi pi\u00f9 precedenti. Inoltre, le segmentazioni naturali come i paragrafi sono suggerimenti che possono essere utilizzati per trovare i confini ottimali. Pu\u00f2 essere preso in considerazione anche l\u2019apprendimento supervisionato.LAVORO PRECEDENTE L'apprendimento supervisionato di solito ha buone prestazioni, poich\u00e9 apprende funzioni da set di addestramento etichettati. Tuttavia, spesso ottenere set di formazione di grandi dimensioni con etichette manuali sulle frasi dei documenti \u00e8 proibitivo, quindi sono desiderabili approcci senza supervisione. Alcuni approcci si concentrano anche sulle keyphrases come suggerimenti per le transizioni degli argomenti -LSB- 11 -RSB-. Mentre alcuni metodi esistenti considerano solo le informazioni contenute in singoli documenti -LSB- 6, 15 -RSB-, altri utilizzano pi\u00f9 documenti -LSB- 16, 14 -RSB-. Non ci sono molti lavori in quest'ultima categoria, anche se si prevede che le prestazioni della segmentazione saranno migliori con l'utilizzo di informazioni provenienti da pi\u00f9 documenti. Precedenti ricerche hanno studiato metodi per trovare argomenti condivisi -LSB- 16 -RSB- e segmentazione e riepilogo degli argomenti solo tra una coppia di documenti -LSB- 14 -RSB-. La classificazione e il clustering del text sono un'area di ricerca correlata che classifica i documenti in gruppi utilizzando metodi supervisionati o non supervisionati. I criteri di questi approcci possono essere utilizzati nella questione della segmentazione degli argomenti. Alcuni di questi metodi sono stati estesi all'area della segmentazione degli argomenti, come PLSA -LSB- 5 -RSB- e entropia massima -LSB- 7 -RSB-, ma per quanto ne sappiamo, l'utilizzo dell'MI per la segmentazione degli argomenti non \u00e8 stato studiato . 6. CONCLUSIONI E LAVORO FUTURO Abbiamo proposto un nuovo metodo per la segmentazione e l'allineamento di argomenti multi-documento basato su informazioni reciproche ponderate, che pu\u00f2 anche gestire casi a documento singolo. Abbiamo utilizzato la programmazione dinamica per ottimizzare il nostro algoritmo. Il nostro approccio supera tutti i metodi precedenti sui casi a documento singolo. Inoltre, abbiamo anche dimostrato che eseguire la segmentazione tra pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni. I nostri risultati hanno anche dimostrato che l\u2019utilizzo di informazioni reciproche ponderate pu\u00f2 utilizzare le informazioni di pi\u00f9 documenti per ottenere prestazioni migliori. Abbiamo testato il nostro metodo solo su set di dati limitati. Dovrebbero essere testati pi\u00f9 set di dati, soprattutto quelli complicati. Dovrebbero essere confrontati metodi pi\u00f9 precedenti. Inoltre, le segmentazioni naturali come i paragrafi sono suggerimenti che possono essere utilizzati per trovare i confini ottimali. Pu\u00f2 essere preso in considerazione anche l\u2019apprendimento supervisionato.La classificazione e il clustering del text sono un'area di ricerca correlata che classifica i documenti in gruppi utilizzando metodi supervisionati o non supervisionati. I criteri di questi approcci possono essere utilizzati nella questione della segmentazione degli argomenti. Alcuni di questi metodi sono stati estesi all'area della segmentazione degli argomenti, come PLSA -LSB- 5 -RSB- e entropia massima -LSB- 7 -RSB-, ma per quanto ne sappiamo, l'utilizzo dell'MI per la segmentazione degli argomenti non \u00e8 stato studiato . 6. CONCLUSIONI E LAVORO FUTURO Abbiamo proposto un nuovo metodo per la segmentazione e l'allineamento di argomenti multi-documento basato su informazioni reciproche ponderate, che pu\u00f2 anche gestire casi a documento singolo. Abbiamo utilizzato la programmazione dinamica per ottimizzare il nostro algoritmo. Il nostro approccio supera tutti i metodi precedenti sui casi a documento singolo. Inoltre, abbiamo anche dimostrato che eseguire la segmentazione tra pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni. I nostri risultati hanno anche dimostrato che l\u2019utilizzo di informazioni reciproche ponderate pu\u00f2 utilizzare le informazioni di pi\u00f9 documenti per ottenere prestazioni migliori. Abbiamo testato il nostro metodo solo su set di dati limitati. Dovrebbero essere testati pi\u00f9 set di dati, soprattutto quelli complicati. Dovrebbero essere confrontati metodi pi\u00f9 precedenti. Inoltre, le segmentazioni naturali come i paragrafi sono suggerimenti che possono essere utilizzati per trovare i confini ottimali. Pu\u00f2 essere preso in considerazione anche l\u2019apprendimento supervisionato.La classificazione e il clustering del text sono un'area di ricerca correlata che classifica i documenti in gruppi utilizzando metodi supervisionati o non supervisionati. I criteri di questi approcci possono essere utilizzati nella questione della segmentazione degli argomenti. Alcuni di questi metodi sono stati estesi all'area della segmentazione degli argomenti, come PLSA -LSB- 5 -RSB- e entropia massima -LSB- 7 -RSB-, ma per quanto ne sappiamo, l'utilizzo dell'MI per la segmentazione degli argomenti non \u00e8 stato studiato . 6. CONCLUSIONI E LAVORO FUTURO Abbiamo proposto un nuovo metodo per la segmentazione e l'allineamento di argomenti multi-documento basato su informazioni reciproche ponderate, che pu\u00f2 anche gestire casi a documento singolo. Abbiamo utilizzato la programmazione dinamica per ottimizzare il nostro algoritmo. Il nostro approccio supera tutti i metodi precedenti sui casi a documento singolo. Inoltre, abbiamo anche dimostrato che eseguire la segmentazione tra pi\u00f9 documenti pu\u00f2 migliorare enormemente le prestazioni. I nostri risultati hanno anche dimostrato che l\u2019utilizzo di informazioni reciproche ponderate pu\u00f2 utilizzare le informazioni di pi\u00f9 documenti per ottenere prestazioni migliori. Abbiamo testato il nostro metodo solo su set di dati limitati. Dovrebbero essere testati pi\u00f9 set di dati, soprattutto quelli complicati. Dovrebbero essere confrontati metodi pi\u00f9 precedenti. Inoltre, le segmentazioni naturali come i paragrafi sono suggerimenti che possono essere utilizzati per trovare i confini ottimali. Pu\u00f2 essere preso in considerazione anche l\u2019apprendimento supervisionato.", "keyphrases": ["rilevamento dell'argomento", "traccia", "segmento tematico", "locali e successivi informati del documento", "documento unico", "documento multiplo", "wmu", "condividere l'argomento", "confini ottimali", "segmento di documento singolo", "segmento multidocumento", "termine indicativo", "fermare la parola", "peso del termine", "eseguire il segmento dell'argomento"]}
{"file_name": "I-6", "text": "Semantica dinamica per linguaggi di comunicazione con agenti ABSTRACT Questo articolo propone la semantica dinamica per linguaggi di comunicazione con agenti -LRB- ACL -RRB- come metodo per affrontare alcuni dei problemi fondamentali associati alla comunicazione con agenti in sistemi aperti multiagente. Basato sull'idea di fornire \"varianti\" semantiche alternative per gli atti linguistici e regole di transizione tra di essi che sono contingenti al comportamento precedente dell'agente, il nostro framework fornisce una nozione migliorata di radicamento della semantica nell'interazione in corso, un semplice meccanismo per distinguere tra compiacente e conforme. comportamento previsto e un modo per specificare meccanismi di sanzione e ricompensa come parte dell'ACL stessa. Estendiamo un quadro comune per la semantica ACL basata sugli impegni per ottenere queste propriet\u00e0, discutiamo i desiderata per la progettazione di una semantica dinamica concreta insieme ad esempi e analizziamo le loro propriet\u00e0. 1. INTRODUZIONE Il campo della ricerca sul linguaggio di comunicazione degli agenti -LRB- ACL -RRB- \u00e8 stato a lungo afflitto da problemi di verificabilit\u00e0 e messa a terra -LSB- 10, 13, 17 -RSB-. Incapaci di proteggersi dagli abusi da parte di agenti dannosi, ingannevoli o malfunzionanti, la semantica mentalistica \u00e8 intrinsecamente inaffidabile e inappropriata per l'uso in MAS aperti in cui agenti con obiettivi potenzialmente contrastanti potrebbero sfruttare deliberatamente le concezioni della semantica dei messaggi dei loro avversari per provocare un determinato comportamento. La semantica basata sugli impegni -LSB- 6, 8, 14 -RSB-, invece, definisce il significato dei messaggi scambiati tra agenti in termini di impegni osservabili pubblicamente, cio\u00e8 impegni a realizzare uno stato di cose o a compiere determinate azioni . Tale semantica risolve il problema della verificabilit\u00e0 poich\u00e9 consente di tracciare lo stato degli impegni esistenti in qualsiasi momento sulla base dei messaggi e delle azioni osservate in modo che qualsiasi osservatore possa, ad esempio, stabilire se un agente ha eseguito un'azione promessa. Inoltre, ci\u00f2 implica che la specifica della semantica non fornisce un'interfaccia ai meccanismi di deliberazione e pianificazione degli agenti e quindi non \u00e8 chiaro come gli agenti razionali sarebbero in grado di decidere se sottoscrivere una semantica ACL suggerita quando viene implementata. Infine, nessuno degli approcci esistenti consente all'ACL di specificare come rispondere a una violazione della sua semantica da parte di singoli agenti. In secondo luogo, gli approcci esistenti non riescono a sfruttare le possibilit\u00e0 di sanzionare e premiare determinati comportamenti in modo inerente alla comunicazione, modificando il significato futuro dei messaggi pronunciati o ricevuti da agenti compiacenti/devianti. In questo articolo proponiamo la semantica dinamica -LRB- DSs -RRB- per ACL come soluzione a questi problemi. La nostra nozione di DS si basa sull'idea molto semplice di definire diverse alternative per il significato dei singoli atti linguistici -LRB- le cosiddette varianti semantiche -RRB- in una specifica semantica ACL e regole di transizione tra stati semantici -LRB- cio\u00e8raccolte di varianti per diversi atti linguistici -RRB- che descrivono il significato attuale dell'ACL. Questi elementi presi insieme danno luogo a una visione simile a FSM delle specifiche ACL in cui ogni singolo stato fornisce una semantica ACL completa e le transizioni di stato sono attivate dal comportamento osservato dell'agente al fine di -LRB- 1 -RRB- riflettere le aspettative future basate sulla precedente esperienza di interazione e -LRB- 2 -RRB- sanzionano o premiano determinati tipi di comportamento. Nel definire un framework DS per le ACL basate sugli impegni, questo documento fornisce tre contributi: 1. Un'estensione della semantica delle ACL basate sugli impegni per fornire una nozione migliorata degli impegni di base nell'interazione dell'agente e per consentire l'utilizzo diretto delle specifiche ACL per la pianificazione- processo decisionale basato sulla razionalit\u00e0. 2. Un modo semplice per distinguere tra comportamento conforme e previsto rispetto a una specifica ACL che consente di ragionare sul potenziale comportamento degli agenti esclusivamente da una prospettiva semantica ACL. 3. Un meccanismo per specificare come il significato si evolve con il comportamento dell'agente e come questo pu\u00f2 essere utilizzato per descrivere meccanismi sanzionatori e premianti intrinseci alla comunicazione essenziali per la progettazione di MAS aperti. Inoltre, discutiamo i desiderata per la progettazione DS che possono essere derivati \u200b\u200bdal nostro quadro, presentare esempi e analizzarne le propriet\u00e0. La parte restante di questo articolo \u00e8 strutturata come segue: La Sezione 2 introduce un quadro formale per la semantica ACL dinamica. Nella sezione 3 presentiamo un'analisi e discussione di questo framework e discutiamo i desiderata per la progettazione di ACL con semantica dinamica. La sezione 4 esamina gli approcci correlati e la sezione 5 conclude. 4. LAVORI CORRELATI Il ragionamento basato sulle aspettative sull'interazione \u00e8 stato proposto per la prima volta in -LSB- 2 -RSB-, considerando l'evoluzione delle aspettative descritte come aspettative probabilistiche della comunicazione e delle sequenze di azioni. Gli stessi autori suggeriscono un quadro pi\u00f9 generale per la semantica della comunicazione basata sulle aspettative -LSB- 9 -RSB-, e sostengono una visione \"consequenzialista\" della semantica basata sulla definizione del significato degli enunciati in termini delle loro conseguenze attese e aggiornando queste aspettative con nuove osservazioni -LSB- 11 -RSB-. Tuttavia, il loro approccio non utilizza una nozione esplicita di impegni che nel nostro quadro media tra la comunicazione e il radicamento basato sul comportamento e fornisce una chiara distinzione tra una nozione normativa di conformit\u00e0 e una nozione pi\u00f9 empirica di aspettativa. Il grounding per la semantica -LRB- mentalistica -RRB- ACL \u00e8 stato studiato in -LSB- 7 -RSB- dove le informazioni grounded sono viste come \"informazioni espresse pubblicamente e accettate come vere da tutti gli agenti che partecipano a una conversazione\" .Come -LSB- 1 -RSB- -LRB- che basa la nozione di \"espresso pubblicamente\" sui ruoli piuttosto che sugli stati interni degli agenti -RRB- la preoccupazione principale di questi autori \u00e8 fornire una base verificabile per determinare la semantica delle espressioni espresse stati mentali e impegni. 11In senso non banale, cio\u00e8 quando alcune prime transizioni sono possibili in linea di principio 106 La Sesta Conf. Congiunta Intl. Il nostro quadro si riferisce anche ai metodi deontici per la specificazione di obblighi, norme e sanzioni. In questo ambito, -LSB- 16 -RSB- \u00e8 l'unico quadro di cui siamo a conoscenza che considera obblighi, norme e sanzioni dinamici. Tuttavia, come abbiamo descritto sopra, utilizziamo esclusivamente l'evoluzione semantica come meccanismo di sanzione e ricompensa, cio\u00e8 a differenza di questo lavoro non assumiamo che gli agenti possano essere direttamente puniti o ricompensati. 5. CONCLUSIONE Questo articolo introduce la semantica dinamica per le ACL come metodo per affrontare alcuni problemi fondamentali della comunicazione degli agenti nei sistemi aperti, con la semplice idea di fondo che diversi corsi di comportamento degli agenti possono dare origine a diverse interpretazioni del significato dei messaggi scambiati tra loro. agenti. Sulla base di un quadro comune di semantica basata sugli impegni, abbiamo presentato una nozione di base per gli impegni basata su nozioni di comportamento conforme e atteso. Abbiamo quindi definito la semantica dinamica come sistemi di transizione di stato su diversi stati semantici che possono essere visti come diverse \"versioni\" della semantica ACL nel senso tradizionale e possono essere facilmente associati a una visione del ragionamento sulla comunicazione basata sulla pianificazione. Pertanto, la nostra attenzione si \u00e8 concentrata sulla semplicit\u00e0 e sulla fornitura di meccanismi per tracciare l'evoluzione semantica in modo algoritmico \"con i piedi per terra\", per garantire l'applicabilit\u00e0 a molti progetti di agenti diversi. Abbiamo discusso le propriet\u00e0 del nostro framework mostrando come pu\u00f2 essere utilizzato come potente meccanismo inerente alla comunicazione per premiare e sanzionare il comportamento degli agenti in sistemi aperti senza compromettere l'autonomia degli agenti, discusso la sua integrazione con i processi di pianificazione degli agenti, i problemi di complessit\u00e0 e presentato un elenco di desiderata per la progettazione di ACL con tale semantica.la semplice idea di fondo \u00e8 che diversi corsi di comportamento dell'agente possono dare origine a diverse interpretazioni del significato dei messaggi scambiati tra gli agenti. Sulla base di un quadro comune di semantica basata sugli impegni, abbiamo presentato una nozione di base per gli impegni basata su nozioni di comportamento conforme e atteso. Abbiamo quindi definito la semantica dinamica come sistemi di transizione di stato su diversi stati semantici che possono essere visti come diverse \"versioni\" della semantica ACL nel senso tradizionale e possono essere facilmente associati a una visione del ragionamento sulla comunicazione basata sulla pianificazione. Pertanto, la nostra attenzione si \u00e8 concentrata sulla semplicit\u00e0 e sulla fornitura di meccanismi per tracciare l'evoluzione semantica in modo algoritmico \"con i piedi per terra\", per garantire l'applicabilit\u00e0 a molti progetti di agenti diversi. Abbiamo discusso le propriet\u00e0 del nostro framework mostrando come pu\u00f2 essere utilizzato come potente meccanismo inerente alla comunicazione per premiare e sanzionare il comportamento degli agenti in sistemi aperti senza compromettere l'autonomia degli agenti, discusso la sua integrazione con i processi di pianificazione degli agenti, i problemi di complessit\u00e0 e presentato un elenco di desiderata per la progettazione di ACL con tale semantica.la semplice idea di fondo \u00e8 che diversi corsi di comportamento dell'agente possono dare origine a diverse interpretazioni del significato dei messaggi scambiati tra gli agenti. Sulla base di un quadro comune di semantica basata sugli impegni, abbiamo presentato una nozione di base per gli impegni basata su nozioni di comportamento conforme e atteso. Abbiamo quindi definito la semantica dinamica come sistemi di transizione di stato su diversi stati semantici che possono essere visti come diverse \"versioni\" della semantica ACL nel senso tradizionale e possono essere facilmente associati a una visione del ragionamento sulla comunicazione basata sulla pianificazione. Pertanto, la nostra attenzione si \u00e8 concentrata sulla semplicit\u00e0 e sulla fornitura di meccanismi per tracciare l'evoluzione semantica in modo algoritmico \"con i piedi per terra\", per garantire l'applicabilit\u00e0 a molti progetti di agenti diversi. Abbiamo discusso le propriet\u00e0 del nostro framework mostrando come pu\u00f2 essere utilizzato come potente meccanismo inerente alla comunicazione per premiare e sanzionare il comportamento degli agenti in sistemi aperti senza compromettere l'autonomia degli agenti, discusso la sua integrazione con i processi di pianificazione degli agenti, i problemi di complessit\u00e0 e presentato un elenco di desiderata per la progettazione di ACL con tale semantica.", "keyphrases": ["agente lingua comune", "Semante dinamico", "ragione sociale", "semantico commit-base", "sistema di transito statale", "adattamento della base di reputazione", "mutuo di attesa", "meccanismo di recupero", "non ridondante"]}
{"file_name": "J-13", "text": "Sulla complessit\u00e0 delle aste combinatorie: grafici degli articoli strutturati e scomposizioni dell'iperalbero ABSTRACT Il problema della determinazione del vincitore nelle aste combinatorie \u00e8 il problema di determinare l'allocazione degli articoli tra gli offerenti che massimizza la somma dei prezzi delle offerte accettate. Sebbene questo problema sia in generale NPhard, \u00e8 noto che \u00e8 fattibile in tempo polinomiale su quelle istanze i cui grafici di elementi associati hanno una larghezza dell'albero limitata -LRB- chiamati grafici di elementi strutturati -RRB-. Formalmente, un grafo di elementi \u00e8 un grafo i cui nodi sono in corrispondenza biunivoca con gli elementi e i bordi sono tali che per ogni offerta gli elementi che si verificano in esso inducono un sottografo connesso. Si noti che molti grafici degli articoli potrebbero essere associati ad una data asta combinatoria, a seconda degli archi selezionati per garantire la connessione. In effetti, la trattabilit\u00e0 di determinare se esiste un grafico di elementi strutturati con una larghezza di albero fissa -LRB- e, in tal caso, calcolarne uno -RRB- \u00e8 stato lasciato come un problema cruciale aperto. In questo articolo, risolviamo questo problema dimostrando che l'esistenza di un grafo di elementi strutturati \u00e8 computazionalmente intrattabile, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questa brutta notizia, investighiamo diversi tipi di requisiti strutturali che possono essere utilizzati per isolare classi trattabili di combinazioni combinatorie. aste. Mostriamo che la nozione di decomposizione dell'iperalbero, una misura della ciclicit\u00e0 dell'ipergrafo recentemente introdotta, risulta essere molto utile qui. Infatti, mostriamo che il problema della determinazione del vincitore \u00e8 risolvibile in tempo polinomiale su istanze le cui interazioni tra offerenti possono essere rappresentate con ipergrafi -LRB- duali -RRB- aventi larghezza dell'iperalbero limitata. Ancora pi\u00f9 sorprendentemente, mostriamo che la classe di istanze trattabili identificata mediante il nostro approccio contiene correttamente la classe di istanze aventi un grafo di elementi strutturati. 1. INTRODUZIONE Aste combinatorie. Le aste combinatorie sono meccanismi ben noti per l'allocazione di risorse e attivit\u00e0 in cui gli offerenti possono fare offerte simultaneamente su combinazioni di articoli. Ci\u00f2 \u00e8 auspicabile quando la valutazione di un offerente di un insieme di articoli non \u00e8 uguale alla somma delle sue valutazioni dei singoli articoli. Un risultato per -LRB- Z, B -RRB- \u00e8 un sottoinsieme b di B tale che l'elemento -LRB- Bi -RRB- n elemento -LRB- Bj -RRB- = 0, per ogni coppia Bi e Bj di offerte in b con i = ~ j. Il problema della determinazione del vincitore. Un problema cruciale per le aste combinatorie \u00e8 determinare il risultato b \u2217 che massimizza la somma dei prezzi di offerta accettati -LRB- ovvero, Bi \u2208 b \u2217 paga -LRB- Bi -RRB- -RRB- su tutti i possibili risultati. Questo problema, chiamato problema di determinazione del vincitore -LRB- ad esempio, -LSB- 11 -RSB- -RRB-, \u00e8 noto per essere intrattabile, in realt\u00e0 NP-difficile -LSB- 17 -RSB-, e persino non approssimabile in tempo polinomiale a meno che NP = ZPP -LSB- 19 -RSB-. Pertanto, non sorprende che siano stati spesi numerosi sforzi per progettare algoritmi praticamente efficienti per aste generali -LRB- ad esempio, -LSB- 20, 5, 2, 8,23 -RSB- -RRB- e identificare classi di istanze in cui la soluzione del problema della determinazione del vincitore \u00e8 fattibile in tempo polinomiale -LRB- ad esempio, -LSB- 15, 22, 12, 21 -RSB- -RRB-. In effetti, vincolare l\u2019interazione degli offerenti si \u00e8 dimostrato utile per identificare classi di aste combinatorie trattabili. Grafici degli articoli. Attualmente, la classe pi\u00f9 generale di aste combinatorie trattabili \u00e8 stata individuata modellando le interazioni tra gli offerenti con la nozione di grafo degli articoli, che \u00e8 un grafico i cui nodi sono in corrispondenza biunivoca con gli articoli, e i bordi sono tali che per qualsiasi Figura 1: Esempio di problema MaxWSP: -LRB- a -RRB- Ipergrafo H -LRB- To, vai -RRB-, e un imballaggio h per esso; -LRB- b -RRB- Grafico primario per H -LRB- To, go -RRB- ; e, -LRB- c, d -RRB- Grafici a due elementi per H -LRB- To, go -RRB-. bid, gli elementi che ricorrono in esso inducono un sottografo connesso. Infatti, il problema della determinazione del vincitore si \u00e8 dimostrato risolvibile in tempo polinomiale se le interazioni tra gli offerenti possono essere rappresentate mediante un grafo strutturato, cio\u00e8 un albero o, pi\u00f9 in generale, un grafo con struttura ad albero -LSB- 3 - RSB- -- larghezza dell'albero formalmente delimitata -LSB- 16 -RSB-. Per avere un'idea di come possano essere costruiti i grafici degli articoli, notiamo che l'interazione degli offerenti in un'asta combinatoria ~ I, B ~ pu\u00f2 essere rappresentata mediante un ipergrafo H -LRB- T, g -RRB- tale che il suo insieme di nodi N -LRB- H -LRB- T, g -RRB- -RRB- coincide con l'insieme di elementi I, e dove i suoi archi E -LRB- H -LRB- T, g -RRB- -RRB- sono precisamente le offerte di gli acquirenti -LCB- articolo -LRB- Bi -RRB- | Bi \u2208 B -RCB-. Un grafo di elemento speciale per ~ I, B ~ \u00e8 il grafo primario di H -LRB- T, g -RRB-, indicato con G -LRB- H -LRB- T, g -RRB- -RRB-, che contiene un arco tra qualsiasi coppia di nodi in qualche iperbordo di H -LRB- T, g -RRB-. Quindi, qualsiasi grafo di elementi per H -LRB- T, g -RRB- pu\u00f2 essere visto come una semplificazione di G -LRB- H -LRB- T, g -RRB- -RRB- ottenuta eliminando alcuni archi, ma preservando la connettivit\u00e0 condizione sui nodi inclusi in ciascun hyperedge. ESEMPIO 1. L'ipergrafo H -LRB- To, go -RRB- riportato in Figura 1. -LRB- a -RRB- \u00e8 una codifica per un'asta combinatoria ~ I0, B0 ~, dove I0 = -LCB- I1,.. ., I5 -RCB-, e l'elemento -LRB- Bi -RRB- = hi, per ogni 1 \u2264 i \u2264 3. Il grafico primario per H -LRB- To, go -RRB- \u00e8 riportato in Figura 1. -LRB- b -RRB-, mentre nella Figura 1 sono riportati due esempi di grafici di elementi. -LRB- c -RRB- e -LRB- d -RRB-, dove i bordi necessari per mantenere la connettivit\u00e0 per h1 sono rappresentati in grassetto. < Problema aperto: calcolo efficiente di grafici di elementi strutturati. Il risultato di trattabilit\u00e0 sopra menzionato sui grafici degli elementi strutturati risulta essere utile in pratica solo quando un grafico degli elementi strutturati viene fornito o pu\u00f2 essere determinato in modo efficiente. Tuttavia, un numero esponenziale di grafici di articoli potrebbe essere associato a un'asta combinatoria e non \u00e8 chiaro come determinare se esiste un grafico di articoli strutturato con una certa larghezza dell'albero -LRB- costante -RRB- e, in caso affermativo,come calcolare in modo efficiente un grafico di elementi cos\u00ec strutturato. Imballaggio del set ponderato. Notiamo che la rappresentazione dell'ipergrafo H -LRB- T, g -RRB- di un'asta combinatoria ~ I, B ~ \u00e8 utile anche per rendere chiara l'analogia tra il problema della determinazione del vincitore e il problema dell'imballaggio del massimo insieme pesato sugli ipergrafi - LRB- ad esempio, -LSB- 17 -RSB- -RRB-. Formalmente, un impaccamento h per un ipergrafo H \u00e8 un insieme di iperspigoli di H tale che per ogni coppia h, h ' \u2208 h con h = ~ h ', vale che h \u2229 h ' = \u2205. Allora, l\u2019insieme delle soluzioni per il problema dell\u2019impacchettamento degli insiemi pesati per H -LRB- T, g -RRB- rispetto a w -LRB- T, g -RRB- coincide con l\u2019insieme delle soluzioni per il problema della determinazione del vincitore su ~ I , B~. ESEMPIO 2. Consideriamo ancora l'ipergrafo H -LRB- To, go -RRB- riportato in Figura 1. -LRB- a -RRB-. Un esempio di imballaggio per H -LRB- To, go -RRB- \u00e8 h = -LCB- h1 -RCB-, che intuitivamente corrisponde a un risultato per ~ I0, B0 ~, dove il banditore ha accettato l'offerta B1. Infatti, i contributi di imballaggio Lo scopo primario di questo articolo \u00e8 quello di identificare grandi classi trattabili per il problema della determinazione del vincitore, che siano, inoltre, polinomialmente riconoscibili. A questo scopo studiamo prima i grafici degli elementi strutturati e risolviamo il problema aperto in -LSB- 3 -RSB-. Il risultato \u00e8 una pessima notizia: \u25ba \u00c8 NP completo verificare se un'asta combinatoria ha un grafo di elementi strutturati di larghezza albero 3. Pi\u00f9 formalmente, lasciando che C -LRB- ig, k -RRB- denotino la classe di tutti gli ipergrafi aventi un albero di elementi di larghezza delimitata da k, dimostriamo che decidendo se un ipergrafo -LRB- associato a un problema di asta combinatoria -RRB- appartiene a C -LRB- ig, 3 -RRB- \u00e8 NP-completo. Alla luce di questo risultato, \u00e8 stato cruciale valutare se esistono altri tipi di requisiti strutturali che possono essere verificati in tempo polinomiale e che possono ancora essere utilizzati per isolare classi trattabili del problema di impacchettamento dell\u2019insieme con peso massimo o, equivalentemente, il vincitore problema di determinazione. E -LRB- H -RRB- -RCB- \u00e8 in E. Mostriamo che MaxWSP \u00e8 trattabile sulla classe di quelle istanze i cui ipergrafi doppi hanno larghezza dell'iperalbero -LSB- 7 -RSB- delimitata da k -LRB- short: classe C -LRB- hw, k -RRB- degli ipergrafi -RRB-. Si noti che una questione chiave della trattabilit\u00e0 \u00e8 considerare la larghezza dell'iperalbero dell'ipergrafo duale H \u00af invece dell'ipergrafo dell'asta H. Infatti, possiamo dimostrare che MaxWSP rimane NP-hard anche quando H \u00e8 aciclico -LRB- cio\u00e8, quando ha larghezza dell'iperalbero 1 -RRB-, anche quando ciascun nodo \u00e8 contenuto al massimo in 3 iperedge. \u25ba Per alcune classi speciali di ipergrafi rilevanti in C -LRB- hw, k -RRB-, progettiamo un algoritmo altamente parallelizzabile per MaxWSP. Ricordiamo, infatti, che LOGCFL \u00e8 la classe di problemi decisionali riducibili in logspace a linguaggi liberi dal context, e che LOGCFL C _ NC2 C _ P -LRB- vedi, ad esempio, -LSB- 9 -RSB- -RRB-. \u25ba Sorprendentemente,mostriamo che nulla va perduto in termini di generalit\u00e0 quando si considera la scomposizione dell'iperalbero di ipergrafi doppi invece della larghezza dell'albero dei grafici degli elementi. Al contrario, il metodo di scomposizione proposto basato sull'iperalbero \u00e8 strettamente pi\u00f9 generale del metodo dei grafici degli elementi strutturati. Infatti, mostriamo che classi di istanze strettamente pi\u00f9 grandi sono trattabili secondo il nostro nuovo approccio rispetto all'approccio dei grafici degli elementi strutturati. Intuitivamente, la difficolt\u00e0 NP nel riconoscere i grafici di elementi strutturati a larghezza limitata non \u00e8 quindi dovuta alla sua grande generalit\u00e0, ma piuttosto ad alcune peculiarit\u00e0 nella sua definizione. \u25ba La dimostrazione dei risultati di cui sopra ci fornisce alcuni spunti interessanti sulla nozione di grafico di elementi strutturati. Infatti, mostriamo che i grafici degli elementi strutturati sono in corrispondenza biunivoca con alcuni tipi speciali di scomposizione dell'iperalbero dell'ipergrafo duale, che chiamiamo decomposizioni dell'iperalbero rigoroso. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute l'intrattabilit\u00e0 dei grafici degli elementi strutturati. La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.il metodo di scomposizione proposto basato sull'iperalbero \u00e8 strettamente pi\u00f9 generale del metodo dei grafici degli elementi strutturati. Infatti, mostriamo che classi di istanze strettamente pi\u00f9 grandi sono trattabili secondo il nostro nuovo approccio rispetto all'approccio dei grafici degli elementi strutturati. Intuitivamente, la difficolt\u00e0 NP nel riconoscere i grafici di elementi strutturati a larghezza limitata non \u00e8 quindi dovuta alla sua grande generalit\u00e0, ma piuttosto ad alcune peculiarit\u00e0 nella sua definizione. \u25ba La dimostrazione dei risultati di cui sopra ci fornisce alcuni spunti interessanti sulla nozione di grafico di elementi strutturati. Infatti, mostriamo che i grafici degli elementi strutturati sono in corrispondenza biunivoca con alcuni tipi speciali di scomposizione dell'iperalbero dell'ipergrafo duale, che chiamiamo decomposizioni dell'iperalbero rigoroso. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute l'intrattabilit\u00e0 dei grafici degli elementi strutturati. La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.il metodo di scomposizione proposto basato sull'iperalbero \u00e8 strettamente pi\u00f9 generale del metodo dei grafici degli elementi strutturati. Infatti, mostriamo che classi di istanze strettamente pi\u00f9 grandi sono trattabili secondo il nostro nuovo approccio rispetto all'approccio dei grafici degli elementi strutturati. Intuitivamente, la difficolt\u00e0 NP nel riconoscere i grafici di elementi strutturati a larghezza limitata non \u00e8 quindi dovuta alla sua grande generalit\u00e0, ma piuttosto ad alcune peculiarit\u00e0 nella sua definizione. \u25ba La dimostrazione dei risultati di cui sopra ci fornisce alcuni spunti interessanti sulla nozione di grafico di elementi strutturati. Infatti, mostriamo che i grafici degli elementi strutturati sono in corrispondenza biunivoca con alcuni tipi speciali di scomposizione dell'iperalbero dell'ipergrafo duale, che chiamiamo decomposizioni dell'iperalbero rigoroso. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute l'intrattabilit\u00e0 dei grafici degli elementi strutturati. La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.la difficolt\u00e0 NP nel riconoscere i grafici di elementi strutturati di larghezza limitata non \u00e8 quindi dovuta alla sua grande generalit\u00e0, ma piuttosto ad alcune peculiarit\u00e0 nella sua definizione. \u25ba La dimostrazione dei risultati di cui sopra ci fornisce alcuni spunti interessanti sulla nozione di grafico di elementi strutturati. Infatti, mostriamo che i grafici degli elementi strutturati sono in corrispondenza biunivoca con alcuni tipi speciali di scomposizione dell'iperalbero dell'ipergrafo duale, che chiamiamo decomposizioni dell'iperalbero rigoroso. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute l'intrattabilit\u00e0 dei grafici degli elementi strutturati. La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.la difficolt\u00e0 NP nel riconoscere i grafici di elementi strutturati di larghezza limitata non \u00e8 quindi dovuta alla sua grande generalit\u00e0, ma piuttosto ad alcune peculiarit\u00e0 nella sua definizione. \u25ba La dimostrazione dei risultati di cui sopra ci fornisce alcuni spunti interessanti sulla nozione di grafico di elementi strutturati. Infatti, mostriamo che i grafici degli elementi strutturati sono in corrispondenza biunivoca con alcuni tipi speciali di scomposizione dell'iperalbero dell'ipergrafo duale, che chiamiamo decomposizioni dell'iperalbero rigoroso. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute l'intrattabilit\u00e0 dei grafici degli elementi strutturati. La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.La Sezione 3 presenta l'algoritmo tempo-polinomiale per risolvere MaxWSP sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata e discute i casi in cui l'algoritmo \u00e8 anche altamente parallelizzabile. Il confronto tra le classi C -LRB- ig, k -RRB- e C -LRB- hw, k -RRB- \u00e8 discusso nella Sezione 4. Infine, nella Sezione 5 traiamo le nostre conclusioni delineando anche indicazioni per ulteriori ricerche. 5. CONCLUSIONI Abbiamo risolto la questione aperta di determinare la complessit\u00e0 del calcolo di un grafico di un articolo strutturato associato ad uno scenario di asta combinatoria. Il risultato \u00e8 una cattiva notizia, poich\u00e9 si \u00e8 scoperto che \u00e8 NP-completo verificare se un'asta combinatoria ha un grafo di elementi strutturati, anche per la larghezza dell'albero 3. Motivati \u200b\u200bda questo risultato, abbiamo studiato l'uso della decomposizione dell'iperalbero -LRB- sul doppio ipergrafo associato allo scenario -RRB- e abbiamo dimostrato che il problema \u00e8 trattabile sulla classe di quelle istanze i cui doppi ipergrafi hanno una larghezza dell'iperalbero limitata. Per alcuni casi speciali, ma rilevanti, viene discusso anche un algoritmo altamente parallelizzabile. \u00c8 interessante notare che \u00e8 emerso anche che la classe dei grafici degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze con larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.\u00e8 inoltre emerso che la classe dei grafi degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze aventi larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.\u00e8 inoltre emerso che la classe dei grafi degli elementi strutturati \u00e8 propriamente contenuta nella classe delle istanze aventi larghezza dell'iperalbero limitata -LRB-, quindi la ragione della loro intrattabilit\u00e0 non \u00e8 la loro generalit\u00e0 -RRB-. In particolare, quest'ultimo risultato \u00e8 stabilito mostrando una precisa relazione tra grafi di elementi strutturati e forme ristrette di decomposizioni di iperalberi -LRB- sull'ipergrafo duale -RRB-, chiamate decomposizioni di query -LRB- vedi, ad esempio, -LSB- 7 -RSB - -RRB-. Alla luce di questa osservazione, notiamo che dimostrare alcuni risultati di approssimabilit\u00e0 per grafici di elementi strutturati richiede una profonda comprensione dell'approssimabilit\u00e0 delle scomposizioni delle query, che attualmente manca in letteratura.", "keyphrases": ["ipergrafo", "asta combinatoria", "decomposizione ipertre", "noto meccanismo per l'allocazione delle risorse e delle attivit\u00e0", "metodo di decomposizione dell'ipertrebase", "ipergrafo hg", "complesso del grafico degli elementi della struttura", "semplicif del grafo primale", "grafico degli elementi della struttura", "Correggi la larghezza dell'albero", "accettare il prezzo dell'offerta", "tempo polinomiale"]}
{"file_name": "C-33", "text": "Negoziazione basata sui premi per fornire informazioni contestuali ABSTRACT Come fornire informazioni contestuali appropriate \u00e8 un problema impegnativo nell'informatica sensibile al context. La maggior parte degli approcci esistenti utilizza un meccanismo di selezione centralizzato per decidere quali informazioni di context sono appropriate. In questo articolo proponiamo un nuovo approccio basato sulla negoziazione con ricompense per risolvere tale problema. I fornitori di context distribuito negoziano tra loro per decidere chi pu\u00f2 fornire il context e come allocare i proventi. Per supportare il nostro approccio, abbiamo progettato un modello di negoziazione concreto con premi. Valutiamo anche il nostro approccio e dimostriamo che \u00e8 effettivamente in grado di scegliere un fornitore di context appropriato e di allocare equamente i proventi. 1. INTRODUZIONE La consapevolezza del context \u00e8 un concetto chiave nel pervasive computing. Il context informa sia il riconoscimento che la mappatura fornendo una visione strutturata e unificata del mondo in cui opera il sistema -LSB- 1 -RSB-. Le applicazioni sensibili al context sfruttano le informazioni di context, come la posizione, le preferenze degli utenti e cos\u00ec via, per adattare i loro comportamenti in risposta alle mutevoli esigenze degli utenti e agli ambienti pervasivi. Tuttavia, un tipo specifico di context pu\u00f2 spesso essere fornito da diversi fornitori di context -sensori LRB- o altre fonti di dati di informazioni contestuali -RRB- con diversi livelli di qualit\u00e0. Ad esempio, poich\u00e9 le applicazioni sensibili al context utilizzano le informazioni sul context per adattare i propri comportamenti, le informazioni sul context inadeguate possono portare a comportamenti inappropriati. Pertanto dovremmo progettare un meccanismo per fornire informazioni di context appropriate per le attuali applicazioni sensibili al context. In ambienti pervasivi, i fornitori di context, considerati entit\u00e0 relativamente indipendenti, hanno i propri interessi. Sperano di ottenere dei guadagni quando forniscono informazioni sul context. Tuttavia, la maggior parte degli approcci esistenti considera i fornitori di context come entit\u00e0 senza interessi personali e utilizza un \"arbitro\" centralizzato fornito dal middleware per decidere chi pu\u00f2 fornire il context appropriato. Pertanto il carico del middleware \u00e8 molto pesante e la sua decisione potrebbe essere ingiusta e danneggiare gli interessi di alcuni fornitori. Inoltre, quando tale \"arbitro\" viene meno, ci\u00f2 causer\u00e0 gravi conseguenze per le applicazioni sensibili al context. In questo articolo, lasciamo che siano i fornitori di context distribuito a decidere chi fornire le informazioni sul context. Poich\u00e9 un'elevata reputazione potrebbe aiutare i fornitori a ottenere maggiori opportunit\u00e0 di fornire context e ottenere maggiori profitti in futuro, i fornitori cercano di ottenere il diritto di fornire un context \"buono\" per migliorare la loro reputazione. Per ottenere tale diritto, i fornitori di context possono accettare di condividere una parte dei proventi con i suoi avversari. Pertanto i fornitori di context negoziano tra loro per raggiungere un accordo su chi pu\u00f2 fornire il context e su come allocare i proventi. Il nostro approccio presenta alcuni vantaggi specifici: 1.Non abbiamo bisogno di un \u201carbitro\u201d fornito dal middleware del pervasive computing per decidere chi fornisce il context. In questo modo si ridurr\u00e0 il carico del middleware. 2. \u00c8 pi\u00f9 ragionevole che i fornitori di context distribuito decidano chi fornisce il context, perch\u00e9 pu\u00f2 evitare le gravi conseguenze causate dalla rottura di un \u201carbitro\u201d centralizzato. 3. Pu\u00f2 garantire gli interessi dei fornitori e fornire un'equa distribuzione dei proventi quando i fornitori negoziano tra loro per raggiungere un accordo sui problemi interessati. 4. Questo approccio consente di scegliere automaticamente il fornitore appropriato. Il modello di negoziazione che abbiamo progettato per supportare il nostro approccio \u00e8 anche un nuovo modello nel campo della negoziazione. Questo modello pu\u00f2 aiutare i negoziatori a raggiungere un accordo nell'attuale processo di negoziazione fornendo alcune garanzie sull'esito del prossimo processo di negoziazione -LRB- cio\u00e8 premi -RRB-. Ci vorr\u00e0 pi\u00f9 tempo per raggiungere un accordo. Inoltre, espande lo spazio negoziale considerato nell'attuale processo negoziale e offre quindi maggiori possibilit\u00e0 per trovare un accordo migliore. La sezione 2 presenta alcune ipotesi. La sezione 3 descrive dettagliatamente il nostro approccio basato sulla negoziazione, comprese le funzioni di utilit\u00e0, il protocollo di negoziazione e le strategie dei fornitori di context. La sezione 4 valuta il nostro approccio. Nella sezione 5 introduciamo alcuni lavori correlati e concludiamo nella sezione 6. 5. LAVORI CORRELATI In -LSB- 4 -RSB-, Huebscher e McCann hanno proposto un progetto di middleware adattivo per applicazioni sensibili al context. Il loro middleware adattivo utilizza funzioni di utilit\u00e0 per scegliere il miglior fornitore di context -LRB- dati i requisiti di QoC delle applicazioni e il QoC di mezzi alternativi di acquisizione del context -RRB-. Nel nostro modello di negoziazione, il calcolo della funzione di utilit\u00e0 Uc \u00e8 stato ispirato da questo approccio. Henricksen e Indulska propongono un approccio alla modellazione e all'utilizzo di informazioni imperfette in -LSB- 3 -RSB-. Caratterizzano vari tipi e fonti di informazioni di context imperfette e presentano una serie di nuovi costrutti di modellazione del context. Descrivono inoltre un'infrastruttura software che supporta la gestione e l'uso di informazioni di context imperfette. -LSB- 10 -RSB- presenta un framework per realizzare la gestione dinamica della coerenza del context. Il framework supporta il rilevamento delle incoerenze basato su un modello di corrispondenza semantica e di attivazione delle incoerenze e la risoluzione delle incoerenze con azioni proattive sulle origini del context. La maggior parte degli approcci per fornire un context appropriato utilizzano un \"arbitro\" centralizzato. Nel nostro approccio, lasciamo che siano i fornitori di context distribuito a decidere chi pu\u00f2 fornire informazioni contestuali appropriate. Il nostro approccio pu\u00f2 ridurre il carico del middleware, perch\u00e9 non abbiamo bisogno che il middleware fornisca un meccanismo di selezione del context. Inoltre, pu\u00f2 garantire gli interessi dei fornitori di context. 6. CONCLUSIONE E LAVORO FUTURO Come fornire le informazioni di context appropriate \u00e8 un problema impegnativo nel pervasive computing.In questo articolo, abbiamo presentato un nuovo approccio basato sulla negoziazione con ricompense per tentare di risolvere questo problema. I fornitori di context distribuito negoziano tra loro per raggiungere un accordo su chi pu\u00f2 fornire il context appropriato e su come allocare i proventi. I risultati dei nostri esperimenti hanno dimostrato che il nostro approccio pu\u00f2 scegliere un fornitore di context appropriato e pu\u00f2 anche garantire gli interessi dei fornitori attraverso un'allocazione dei proventi relativamente equa. In questo articolo considereremo solo come scegliere un fornitore di context appropriato tra due fornitori. Nel lavoro futuro, questo modello di negoziazione verr\u00e0 esteso e pi\u00f9 di due fornitori di context potranno negoziare tra loro per decidere chi \u00e8 il fornitore di context pi\u00f9 appropriato. Nel modello di negoziazione estesa, come progettare strategie di negoziazione efficienti sar\u00e0 un problema impegnativo. Partiamo dal presupposto che il fornitore del context manterr\u00e0 la sua promessa di ricompensa nel successivo processo di negoziazione. In effetti, il fornitore del context potrebbe ingannare il suo avversario e fornire promesse illusorie. Dovremmo risolvere questo problema in futuro.", "keyphrases": ["riconoscimento del context", "context fornito", "negozi", "calcolo in base al context", "modello negoziale concreto", "distribuire applic", "calcolo pervas", "reputazione", "qualit\u00e0 del context", "convince l'argomentazione"]}
{"file_name": "H-12", "text": "Generazione rapida di frammenti di risultati nella ricerca Web ABSTRACT La presentazione di frammenti di documenti distorti dalle query come parte delle pagine dei risultati presentate dai motori di ricerca \u00e8 diventata un'aspettativa degli utenti dei motori di ricerca. In questo articolo esploriamo gli algoritmi e le strutture dati richiesti come parte di un motore di ricerca per consentire la generazione efficiente di snippet distorti dalle query. Iniziamo proponendo e analizzando un metodo di compressione dei documenti che riduce il tempo di generazione degli snippet del 58% rispetto a una linea di base utilizzando la libreria di compressione zlib. Questi esperimenti rivelano che la ricerca di documenti nell'archiviazione secondaria domina il costo totale della generazione di snippet, quindi la memorizzazione nella cache dei documenti nella RAM \u00e8 essenziale per un processo rapido di generazione di snippet. Utilizzando la simulazione, esaminiamo le prestazioni di generazione degli snippet per cache RAM di diverse dimensioni. Infine proponiamo e analizziamo il riordino e la compattazione dei documenti, rivelando uno schema che aumenta il numero di accessi alla cache dei documenti con un effetto solo marginale sulla qualit\u00e0 dello snippet. Questo schema raddoppia effettivamente il numero di documenti che possono essere contenuti in una cache di dimensione fissa. 1. INTRODUZIONE Ogni risultato nell'elenco dei risultati di ricerca fornito dagli attuali motori di ricerca WWW come search.yahoo.com, google.com e search.msn.com contiene in genere il titolo e l'URL del documento effettivo, collegamenti alle versioni live e memorizzate nella cache di il documento e talvolta un'indicazione della dimensione e del tipo di file. Inoltre, vengono solitamente presentati uno o pi\u00f9 snippet, offrendo a chi effettua la ricerca un'anteprima del contenuto del documento. Gli snippet sono brevi frammenti di text estratti dal contenuto del documento -LRB- o dai suoi metadati -RRB-. Uno snippet influenzato dalla query \u00e8 uno snippet estratto selettivamente sulla base della sua relazione con la query dell'utente. L'aggiunta di frammenti informativi ai risultati di ricerca pu\u00f2 aumentare sostanzialmente il loro valore per gli utenti. Snippet accurati consentono al ricercatore di prendere buone decisioni su quali risultati vale la pena accedere e quali possono essere ignorati. Nel migliore dei casi, gli snippet possono ovviare alla necessit\u00e0 di aprire documenti fornendo direttamente la risposta al reale bisogno di informazioni del ricercatore, come i dettagli di contatto di una persona o di un'organizzazione. La generazione di snippet distorti dalle query da parte dei motori di ricerca Web, l'indicizzazione dell'ordine di dieci miliardi di pagine web e la gestione di centinaia di milioni di query di ricerca al giorno impone un carico computazionale molto significativo -LRB- ricordando che ogni ricerca tipicamente genera dieci snippet -RRB-. L'approccio semplicistico di conservare una copia di ciascun documento in un file e generare frammenti aprendo e scansionando i file, funziona quando i tassi di query sono bassi e le raccolte sono piccole, ma non \u00e8 scalabile nella misura richiesta. Il sovraccarico derivante dall'apertura e dalla lettura di dieci file per query oltre all'accesso alla struttura dell'indice per individuarli sarebbe manifestamente eccessivo in caso di carico elevato di query. Anche l\u2019archiviazione di dieci miliardi di file e le corrispondenti centinaia di terabyte di dati \u00e8 fuori dalla portata dei file system tradizionali.Tieni presente che l'utilit\u00e0 degli snippet non \u00e8 affatto limitata alle applicazioni di ricerca dell'intero Web. Una generazione efficiente di snippet \u00e8 importante anche su scala di servizi di ricerca dell'intero governo come www.firstgov.gov -LRB- c. 25 milioni di pagine -RRB- e govsearch.australia.gov.au -LRB- c. 5 milioni di pagine -RRB- e all'interno di grandi aziende come IBM -LSB- 2 -RSB- -LRB- c. 50 milioni di pagine -RRB-. Gli snippet possono essere ancora pi\u00f9 utili nelle applicazioni di ricerca di database o file system in cui non sono presenti URL utili o informazioni sul titolo. Presentiamo un nuovo algoritmo e una struttura compatta a file singolo progettati per la generazione rapida di snippet di alta qualit\u00e0 e confrontiamo le sue prestazioni spazio/temporali con una linea di base ovvia basata sul compressore zlib su vari set di dati. Riportiamo la proporzione del tempo impiegato per le ricerche del disco, le letture del disco e l'elaborazione della CPU; dimostrando che il tempo per localizzare ciascun documento -LRB- tempo di ricerca -RRB- domina, come previsto. Poich\u00e9 il tempo necessario per elaborare un documento nella RAM \u00e8 ridotto rispetto all'individuazione e alla lettura del documento in memoria, potrebbe sembrare che la compressione non sia necessaria. Tuttavia, ci\u00f2 \u00e8 vero solo se non \u00e8 presente la memorizzazione nella cache dei documenti nella RAM. Controllare la RAM dei sistemi fisici per la sperimentazione \u00e8 difficile, quindi utilizziamo la simulazione per dimostrare che la memorizzazione nella cache dei documenti migliora notevolmente le prestazioni di generazione degli snippet. A sua volta, pi\u00f9 documenti possono essere compressi, pi\u00f9 documenti possono essere contenuti nella cache, e quindi pi\u00f9 ricerche su disco possono essere evitate: il classico compromesso di compressione dei dati che viene sfruttato nelle strutture di file invertite e nel calcolo di elenchi di documenti classificati -LSB- 24 -RSB -. Poich\u00e9 \u00e8 importante accedere alla cache dei documenti, esaminiamo gli schemi di compattazione dei documenti, in contrapposizione alla compressione, che impongono un ordinamento a priori delle frasi all'interno di un documento e quindi consentono solo le frasi iniziali nella cache per ciascun documento. Ci\u00f2 porta ad un ulteriore risparmio di tempo, con un impatto solo marginale sulla qualit\u00e0 degli snippet restituiti. 2. LAVORI CORRELATI La generazione di snippet \u00e8 un tipo speciale di riepilogo estrattivo di documenti, in cui frasi, o frammenti di frasi, vengono selezionati per essere inclusi nel riepilogo sulla base del grado in cui corrispondono alla query di ricerca. I primi motori di ricerca Web presentavano frammenti indipendenti dalla query costituiti dai primi k byte del documento risultato. Generarli \u00e8 chiaramente molto pi\u00f9 semplice e molto meno costoso dal punto di vista computazionale rispetto all'elaborazione di documenti per estrarre riepiloghi distorti dalle query, poich\u00e9 non \u00e8 necessario cercare nel documento frammenti di text contenenti termini di query. Per quanto ne sappiamo, Google \u00e8 stato il primo motore di ricerca dell'intero Web a fornire riepiloghi distorti dalle query, ma il riepilogo \u00e8 elencato da Brin e Page -LSB- 1 -RSB- solo sotto il titolo del lavoro futuro. La maggior parte del lavoro sperimentale che utilizza il riepilogo basato sulle query si \u00e8 concentrato sul confronto del loro valore per i ricercatori rispetto ad altri tipi di riepilogo -LSB- 20, 21 -RSB-, piuttosto che sulla generazione efficiente di riepiloghi.Nonostante l\u2019importanza di un\u2019efficiente generazione di riassunti nella ricerca sul Web, in letteratura compaiono pochi algoritmi. White et al -LSB- 21 -RSB- riportano alcuni tempi sperimentali del loro sistema WebDocSum, ma gli stessi algoritmi di generazione degli snippet non sono isolati, quindi \u00e8 difficile dedurre tempi di generazione degli snippet paragonabili ai tempi riportati in questo articolo. Documenti N/M. La quantit\u00e0 totale di RAM richiesta da una singola macchina, quindi, sarebbe N/M -LRB- 8.192 + 10.24 + 8 -RRB- byte. Supponendo che ogni macchina abbia 8 Gb di RAM e che ci siano 20 miliardi di pagine da indicizzare sul Web, per lo Snippet Engine sarebbe necessario un totale di M = 62 macchine. Queste macchine avrebbero inoltre bisogno di accedere a 37 Tb di disco per archiviare le rappresentazioni di documenti compressi che non erano nella cache. In questo lavoro abbiamo deliberatamente evitato di impegnarci in un particolare metodo di punteggio per le frasi nei documenti. Piuttosto, abbiamo riportato i risultati di accuratezza in termini dei quattro componenti che in precedenza si sono rivelati importanti nel determinare gli snippet utili -LSB- 20 -RSB-. Le tecniche di compattazione dei documenti che utilizzano il riordino delle frasi, tuttavia, rimuovono la relazione spaziale tra le frasi, e quindi se una tecnica di punteggio si basa sulla posizione di una frase all'interno di un documento, le tecniche di compattazione aggressive qui riportate non possono essere utilizzate. Poich\u00e9 il tempo di ricerca domina il processo di generazione degli snippet, in questo documento non ci siamo concentrati su questa parte della generazione degli snippet in dettaglio. Esploreremo schemi di compressione alternativi nel lavoro futuro.non ci siamo concentrati in dettaglio su questa parte della generazione dello snippet in questo documento. Esploreremo schemi di compressione alternativi nel lavoro futuro.non ci siamo concentrati in dettaglio su questa parte della generazione dello snippet in questo documento. Esploreremo schemi di compressione alternativi nel lavoro futuro.", "keyphrases": ["motore di ricerca", "generatore di snippet", "cache dei documenti", "misura del grafico di collegamento", "eseguire", "riassunti web", "filesystem per scopi speciali", "ariete", "documento compatto", "frammento di text", "pagina dei risultati finali precalcolata", "Schema di codice vbyte", "impacco semistatico"]}
{"file_name": "H-10", "text": "Clustering regolarizzato per documenti * ABSTRACT Negli ultimi anni, il clustering di documenti ha ricevuto sempre pi\u00f9 attenzione come tecnica importante e fondamentale per l'organizzazione dei documenti senza supervisione, l'estrazione automatica degli argomenti e il recupero o il filtraggio rapido delle informazioni. In questo articolo proponiamo un nuovo metodo per raggruppare i documenti utilizzando la regolarizzazione. A differenza dei tradizionali metodi di clustering regolarizzati a livello globale, il nostro metodo costruisce innanzitutto un predittore di etichette lineari regolarizzate locali per ciascun vettore di documento, quindi combina tutti questi regolatori locali con un regolarizzatore di uniformit\u00e0 globale. Quindi chiamiamo il nostro algoritmo Clustering con regolarizzazione locale e globale -LRB- CLGR -RRB-. Mostreremo che l'appartenenza ai cluster dei documenti pu\u00f2 essere ottenuta mediante la scomposizione degli autovalori di una matrice sparsa simmetrica, che pu\u00f2 essere risolta efficientemente con metodi iterativi. Infine vengono presentate le nostre valutazioni sperimentali su diversi set di dati per mostrare la superiorit\u00e0 del CLGR rispetto ai tradizionali metodi di clustering dei documenti. 1. INTRODUZIONE Il document clustering ha ricevuto sempre pi\u00f9 attenzione come tecnica importante e fondamentale per l'organizzazione dei documenti senza supervisione, l'estrazione automatica degli argomenti e il recupero o il filtraggio rapido delle informazioni. Un buon approccio di clustering dei documenti pu\u00f2 aiutare i computer a organizzare automaticamente il corpus dei documenti in una gerarchia di cluster significativa per una navigazione e una navigazione efficienti, il che \u00e8 molto utile per integrare le carenze delle tradizionali tecnologie di recupero delle informazioni. In questi casi, la navigazione efficiente attraverso una buona gerarchia di cluster sar\u00e0 sicuramente utile. In generale, i metodi di clustering dei documenti possono essere principalmente classificati in due classi: metodi gerarchici e metodi di partizionamento. I metodi gerarchici raggruppano i punti dati in una struttura ad albero gerarchica utilizzando approcci dal basso verso l'alto o dall'alto verso il basso. Ad esempio, il clustering agglomerativo gerarchico -LRB- HAC -RRB- -LSB- 13 -RSB- \u00e8 un tipico metodo di clustering gerarchico dal basso verso l'alto. Inizialmente prende ogni punto dati come un singolo cluster e quindi crea cluster sempre pi\u00f9 grandi raggruppando punti dati simili fino a quando l'intero set di dati non viene incapsulato in un cluster finale. D'altra parte, i metodi di partizionamento scompongono il set di dati in un numero di cluster disgiunti che di solito sono ottimali in termini di alcune funzioni criterio predefinite. Ad esempio, K-mean -LSB- 13 -RSB- \u00e8 un tipico metodo di partizionamento che mira a ridurre al minimo la somma della distanza quadrata tra i punti dati e i corrispondenti centri del cluster. In questo articolo ci concentreremo sui metodi di partizionamento. Negli ultimi decenni sono stati proposti molti metodi per superare i problemi sopra menzionati dei metodi di partizionamento -LSB- 19 -RSB- -LSB- 28 -RSB-. Recentemente, un altro tipo di metodi di partizionamento basati sul clustering su grafici di dati ha suscitato notevole interesse nella comunit\u00e0 del machine learning e del data mining.L'idea di base alla base di questi metodi \u00e8 quella di modellare innanzitutto l'intero set di dati come un grafico ponderato, in cui i nodi del grafico rappresentano i punti dati e i pesi sui bordi corrispondono alle somiglianze tra i punti a coppie. Quindi le assegnazioni dei cluster del set di dati possono essere ottenute ottimizzando alcuni criteri definiti sul grafico. Dopo alcuni allentamenti, questi criteri possono solitamente essere ottimizzati tramite autodecomposizioni, che \u00e8 garantito essere ottimale globale. In questo modo, il clustering spettrale evita efficacemente i problemi dei metodi di partizionamento tradizionali come abbiamo introdotto nello scorso paragrafo. In questo articolo proponiamo un nuovo algoritmo di clustering di documenti che eredita la superiorit\u00e0 del clustering spettrale, ovvero i risultati finali del clustering possono essere ottenuti anche sfruttando l'autostruttura di una matrice simmetrica. Quindi chiamiamo il nostro metodo Clustering con regolarizzazione locale e globale -LRB- CLGR -RRB-. L'idea di incorporare informazioni sia locali che globali nella previsione delle etichette \u00e8 ispirata dai recenti lavori sull'apprendimento semi-supervisionato -LSB- 31 -RSB-, e le nostre valutazioni sperimentali su diversi set di dati di documenti reali mostrano che CLGR funziona meglio di molti stati di -metodi di clustering all'avanguardia. Il resto di questo documento \u00e8 organizzato come segue: nella sezione 2 introdurremo in dettaglio il nostro algoritmo CLGR. I risultati sperimentali su diversi set di dati sono presentati nella sezione 3, seguiti dalle conclusioni e discussioni nella sezione 4. 4. CONCLUSIONI E LAVORI FUTURI In questo articolo, abbiamo derivato un nuovo algoritmo di clustering chiamato clustering con regolarizzazione locale e globale. Il nostro metodo preserva il merito degli algoritmi di apprendimento locale e del clustering spettrale. I nostri esperimenti mostrano che l'algoritmo proposto supera la maggior parte degli algoritmi all'avanguardia su molti set di dati di riferimento. In futuro, ci concentreremo sulla selezione dei parametri e sui problemi di accelerazione dell'algoritmo CLGR.L'idea di incorporare informazioni sia locali che globali nella previsione delle etichette \u00e8 ispirata dai recenti lavori sull'apprendimento semi-supervisionato -LSB- 31 -RSB-, e le nostre valutazioni sperimentali su diversi set di dati di documenti reali mostrano che CLGR funziona meglio di molti stati di -metodi di clustering all'avanguardia. Il resto di questo documento \u00e8 organizzato come segue: nella sezione 2 introdurremo in dettaglio il nostro algoritmo CLGR. I risultati sperimentali su diversi set di dati sono presentati nella sezione 3, seguiti dalle conclusioni e discussioni nella sezione 4. 4. CONCLUSIONI E LAVORI FUTURI In questo articolo, abbiamo derivato un nuovo algoritmo di clustering chiamato clustering con regolarizzazione locale e globale. Il nostro metodo preserva il merito degli algoritmi di apprendimento locale e del clustering spettrale. I nostri esperimenti mostrano che l'algoritmo proposto supera la maggior parte degli algoritmi all'avanguardia su molti set di dati di riferimento. In futuro, ci concentreremo sulla selezione dei parametri e sui problemi di accelerazione dell'algoritmo CLGR.L'idea di incorporare informazioni sia locali che globali nella previsione delle etichette \u00e8 ispirata dai recenti lavori sull'apprendimento semi-supervisionato -LSB- 31 -RSB-, e le nostre valutazioni sperimentali su diversi set di dati di documenti reali mostrano che CLGR funziona meglio di molti stati di -metodi di clustering all'avanguardia. Il resto di questo documento \u00e8 organizzato come segue: nella sezione 2 introdurremo in dettaglio il nostro algoritmo CLGR. I risultati sperimentali su diversi set di dati sono presentati nella sezione 3, seguiti dalle conclusioni e discussioni nella sezione 4. 4. CONCLUSIONI E LAVORI FUTURI In questo articolo, abbiamo derivato un nuovo algoritmo di clustering chiamato clustering con regolarizzazione locale e globale. Il nostro metodo preserva il merito degli algoritmi di apprendimento locale e del clustering spettrale. I nostri esperimenti mostrano che l'algoritmo proposto supera la maggior parte degli algoritmi all'avanguardia su molti set di dati di riferimento. In futuro, ci concentreremo sulla selezione dei parametri e sui problemi di accelerazione dell'algoritmo CLGR.", "keyphrases": ["gruppo di documenti", "regolare", "regolare globale", "gerarchie dei cluster", "spettro", "ricerca specifica", "metodo della gerarchia", "metodo delle parti", "previsione dell'etichetta", "stima della funzione", "collettore"]}
{"file_name": "C-32", "text": "BuddyCache: storage di oggetti ad alte prestazioni per applicazioni collaborative ad elevata coerenza in una WAN * SOMMARIO Le applicazioni collaborative forniscono un ambiente di lavoro condiviso per gruppi di client in rete che collaborano su un'attivit\u00e0 comune. Richiedono una forte coerenza per i dati persistenti condivisi e un accesso efficiente agli oggetti a grana fine. Queste propriet\u00e0 sono difficili da fornire nelle reti geografiche a causa dell'elevata latenza della rete. BuddyCache \u00e8 un nuovo approccio di caching transazionale che migliora la latenza di accesso a oggetti persistenti condivisi per applicazioni collaborative ad alta coerenza in ambienti di rete ad alta latenza. La sfida \u00e8 migliorare le prestazioni fornendo al contempo le propriet\u00e0 di correttezza e disponibilit\u00e0 di un protocollo di caching transazionale in presenza di guasti dei nodi e peer lenti. Abbiamo implementato un prototipo di BuddyCache e valutato le sue prestazioni. I risultati analitici, confermati dalle misurazioni del prototipo BuddyCache utilizzando il benchmark multiutente 007, indicano che per le latenze tipiche di Internet, ad esempio comprese tra 40 e 80 millisecondi di andata e ritorno al server di archiviazione, i peer che utilizzano BuddyCache possono ridurre fino al 50% la latenza di accesso agli oggetti condivisi rispetto all'accesso diretto ai server remoti. 1. INTRODUZIONE Tuttavia, le applicazioni distribuite possono funzionare male in ambienti di rete geografica. I problemi relativi alla larghezza di banda della rete miglioreranno nel prossimo futuro, ma il miglioramento della latenza della rete \u00e8 fondamentalmente limitato. BuddyCache \u00e8 una nuova tecnica di memorizzazione nella cache degli oggetti che risolve il problema della latenza di rete per applicazioni collaborative in ambienti di rete geografica. Le applicazioni collaborative forniscono un ambiente di lavoro condiviso per gruppi di utenti in rete che collaborano su un compito comune, ad esempio un team di ingegneri che supervisiona congiuntamente un progetto di costruzione. Le applicazioni collaborative ad elevata coerenza, ad esempio i sistemi CAD, utilizzano sistemi di storage di oggetti transazionali client/server per garantire un accesso coerente ai dati persistenti condivisi. Fino ad ora, tuttavia, gli utenti hanno raramente preso in considerazione l'idea di utilizzare sistemi di storage di rete coerenti su reti geografiche poich\u00e9 le prestazioni sarebbero inaccettabili -LSB- 24 -RSB-. Per i sistemi di storage transazionali, il costo elevato delle interazioni di rete su vasta scala per mantenere la coerenza dei dati \u00e8 il costo principale che limita le prestazioni e pertanto, negli ambienti di rete su vasta area, le applicazioni collaborative sono state adattate per utilizzare sistemi di storage di consistenza pi\u00f9 debole -LSB- 22 -RSB-. Adattare un'applicazione per utilizzare un sistema di archiviazione a consistenza debole richiede uno sforzo significativo poich\u00e9 l'applicazione deve essere riscritta per gestire una semantica diversa del sistema di archiviazione. Se fosse possibile accedere agli oggetti persistenti condivisi con bassa latenza, si potrebbe aprire un nuovo campo di applicazioni distribuite a forte coerenza. Web caching cooperativo -LSB- 10, 11,15 -RSB- \u00e8 un approccio ben noto per ridurre l'interazione del client con un server consentendo a un client di ottenere oggetti mancanti da un altro client anzich\u00e9 dal server. Tuttavia, le tecniche di web caching cooperativo non forniscono due importanti propriet\u00e0 necessarie alle applicazioni collaborative, ovvero una forte coerenza e un accesso efficiente agli oggetti a grana fine. I sistemi di caching cooperativo degli oggetti -LSB- 2 -RSB- forniscono queste propriet\u00e0. Tuttavia, si basano sull'interazione con il server per fornire una coerenza della cache a grana fine che eviti il \u200b\u200bproblema della falsa condivisione quando gli accessi a oggetti non correlati sembrano essere in conflitto perch\u00e9 si verificano sulla stessa pagina fisica. L'interazione con il server aumenta la latenza. Il contributo di questo lavoro sta estendendo le tecniche di caching cooperativo per fornire una forte coerenza e un accesso efficiente agli oggetti a grana fine in ambienti ad ampia area. Gli ingegneri utilizzano un'applicazione CAD collaborativa per rivedere e aggiornare documenti di progettazione complessi. I documenti condivisi vengono archiviati in server di repository transazionali presso la sede aziendale. Gli ingegneri utilizzano workstation che eseguono client di repository. Le postazioni di lavoro sono interconnesse tramite una veloce Ethernet locale ma la connessione di rete ai server del repository domestico \u00e8 lenta. Per migliorare la latenza di accesso, i client recuperano gli oggetti dai server di repository, li memorizzano nella cache e vi accedono localmente. Un protocollo di coerenza garantisce che le cache del client rimangano coerenti quando gli oggetti vengono modificati. Il problema di prestazioni che deve affrontare l'applicazione collaborativa \u00e8 il coordinamento con l'accesso coerente dei server agli oggetti condivisi. Con BuddyCache, un gruppo di client che collaborano vicini, connessi al repository di archiviazione tramite un collegamento ad alta latenza, pu\u00f2 evitare interazioni con il server se oggetti, aggiornamenti o informazioni di coerenza necessari sono disponibili in alcuni client del gruppo. BuddyCache presenta due principali sfide tecniche. Una sfida \u00e8 come fornire un accesso efficiente agli oggetti a grana fine condivisi nel gruppo collaborativo senza imporre un sovraccarico di prestazioni sull'intero sistema di memorizzazione nella cache. L'altra sfida \u00e8 supportare la coerenza della cache a grana fine in presenza di nodi lenti e guasti. BuddyCache utilizza un approccio di ''reindirizzamento'' simile a quello utilizzato nei sistemi di web caching cooperativi -LSB- 11 -RSB-. Un server di reindirizzamento, interposto tra i client e i server remoti, funziona sulla stessa rete del gruppo collaboratore e, quando possibile, sostituisce la funzione dei server remoti. Se la richiesta del client non pu\u00f2 essere soddisfatta localmente, il reindirizzatore la inoltra a un server remoto. Quando uno dei client del gruppo recupera un oggetto condiviso dal repository, \u00e8 probabile che l'oggetto sia necessario ad altri client. BuddyCache reindirizza le richieste successive per questo oggetto al client di memorizzazione nella cache. Allo stesso modo, quando un cliente crea o modifica un oggetto condiviso, \u00e8 probabile che i nuovi dati siano di potenziale interesse per tutti i membri del gruppo.BuddyCache utilizza il reindirizzamento per supportare l'aggiornamento peer, una tecnica leggera di \"multicast a livello di applicazione\" che fornisce ai membri del gruppo un accesso coerente ai nuovi dati impegnati all'interno del gruppo che collabora senza imporre un sovraccarico aggiuntivo all'esterno del gruppo. Tuttavia, in un sistema transazionale, il reindirizzamento interferisce con la disponibilit\u00e0 degli oggetti condivisi. Solo commit \u00e8 una tecnica di convalida utilizzata da BuddyCache per evitare le dipendenze client indesiderate che riducono la disponibilit\u00e0 degli oggetti quando alcuni nodi client nel gruppo sono lenti o i client falliscono in modo indipendente. Una caratteristica saliente del commit solista \u00e8 il supporto della convalida a grana fine utilizzando informazioni di coerenza a grana grossa poco costose. Abbiamo progettato e implementato un prototipo BuddyCache e ne abbiamo studiato i vantaggi e i costi in termini di prestazioni utilizzando la modellazione analitica e le misurazioni del sistema. Abbiamo confrontato le prestazioni del sistema di storage con e senza BuddyCache e considerato il modo in cui il rapporto costi-benefici viene influenzato dalla latenza della rete. Questi forti miglioramenti delle prestazioni potrebbero rendere i sistemi di storage di oggetti transazionali pi\u00f9 attraenti per le applicazioni collaborative in ambienti di vasta portata. 2. LAVORI CORRELATI Le tecniche di caching cooperativo -LSB- 20, 16, 13, 2, 28 -RSB- forniscono accesso alle cache dei client per evitare un'elevata latenza di accesso al disco in un ambiente in cui server e client funzionano su una rete locale veloce. Queste tecniche utilizzano il server per fornire il reindirizzamento e non considerano i problemi di elevata latenza di rete. Le tecniche di Web caching cooperativo, -LRB- ad es. -LSB- 11, 15 -RSB- -RRB- indagano i problemi relativi al mantenimento di una directory di oggetti memorizzati nella cache nelle cache proxy vicine in un ambiente ad ampia area, utilizzando protocolli di directory distribuiti per tenere traccia dei cambiamenti della cache. Questo lavoro non considera i problemi di aggiornamenti simultanei coerenti per oggetti a grana fine condivisi. Questa soluzione a livello di trasporto multicast \u00e8 orientata alla semantica a singolo scrittore degli oggetti web. Al contrario, BuddyCache utilizza il multicast \"a livello di applicazione\" e un protocollo di coerenza affidabile del mittente per fornire miglioramenti simili della latenza di accesso per gli oggetti transazionali. La soluzione multicast a livello applicativo in un sistema middleware \u00e8 stata descritta da Pendarakis, Shi e Verma in -LSB- 27 -RSB-. Lo schema supporta piccoli gruppi multi-mittente adatti per applicazioni collaborative e considera i problemi di coerenza in presenza di errori, ma non supporta una coerenza elevata o una condivisione a grana fine. Il protocollo utilizza i lease per fornire richiamate con tolleranza agli errori e sfrutta le cache vicine per ridurre il costo delle estensioni del lease. Lo studio utilizza la simulazione per indagare i problemi di latenza e tolleranza agli errori nello schema di coerenza gerarchico basato sull'elusione. Al contrario, il nostro lavoro utilizza l\u2019implementazione e l\u2019analisi per valutare i costi e i benefici del reindirizzamento e degli aggiornamenti granulari in un sistema ottimistico. Anderson,Eastham e Vahdat in WebFS -LSB- 29 -RSB- presentano un protocollo di coerenza globale del file system che consente ai client di scegliere in base al file tra ricevere aggiornamenti o invalidazioni. Gli aggiornamenti e gli invalidamenti vengono trasmessi in multicast su canali separati e i client si iscrivono a uno dei canali. Il protocollo sfrutta metodi specifici dell'applicazione, ad esempio la politica last-writer-wins per applicazioni broadcast, per gestire aggiornamenti simultanei ma \u00e8 limitato ai file system. BuddyCache fornisce miglioramenti simili alla larghezza di banda quando gli oggetti sono disponibili nella cache del gruppo. 7. CONCLUSIONE Le applicazioni collaborative forniscono un ambiente di lavoro condiviso per gruppi di clienti in rete che collaborano su un compito comune. Richiedono una forte coerenza per i dati persistenti condivisi e un accesso efficiente agli oggetti a grana fine. Queste propriet\u00e0 sono difficili da fornire in una rete geografica a causa dell'elevata latenza della rete. Questo articolo descrive BuddyCache, una nuova tecnica di caching cooperativo transazionale -LSB- 20, 16, 13, 2, 28 -RSB- che migliora la latenza di accesso a oggetti persistenti condivisi per applicazioni collaborative ad alta coerenza in ambienti di rete ad alta latenza. La tecnica migliora le prestazioni ma fornisce forti propriet\u00e0 di correttezza e disponibilit\u00e0 in presenza di guasti dei nodi e client lenti. Il reindirizzamento, tuttavia, pu\u00f2 interferire con la disponibilit\u00e0 degli oggetti. Solo commit \u00e8 una nuova tecnica di convalida che consente a un client in un gruppo di eseguire il commit indipendentemente dai peer lenti o non riusciti. Fornisce una convalida a grana fine utilizzando informazioni poco costose sulla versione a grana grossa. Abbiamo progettato e implementato il prototipo BuddyCache nel sistema di storage di oggetti transazionali distribuiti Thor -LSB- 23 -RSB- e valutato i vantaggi e i costi del sistema su una gamma di latenze di rete. accesso a grana fine e ad elevata coerenza in ambienti ad alta latenza, 2. un'implementazione del prototipo del sistema che produce notevoli miglioramenti in termini di prestazioni rispetto al sistema di base, 3. valutazione analitica e basata sulla misurazione delle prestazioni dei costi e dei benefici delle nuove tecniche che catturano il costo delle prestazioni dominante, elevata latenza di rete.una nuova tecnica di caching cooperativo transazionale -LSB- 20, 16, 13, 2, 28 -RSB- che migliora la latenza di accesso a oggetti persistenti condivisi per applicazioni collaborative a forte coerenza in ambienti di rete ad alta latenza. La tecnica migliora le prestazioni ma fornisce forti propriet\u00e0 di correttezza e disponibilit\u00e0 in presenza di guasti dei nodi e client lenti. Il reindirizzamento, tuttavia, pu\u00f2 interferire con la disponibilit\u00e0 degli oggetti. Solo commit \u00e8 una nuova tecnica di convalida che consente a un client in un gruppo di eseguire il commit indipendentemente dai peer lenti o non riusciti. Fornisce una convalida a grana fine utilizzando informazioni poco costose sulla versione a grana grossa. Abbiamo progettato e implementato il prototipo BuddyCache nel sistema di storage di oggetti transazionali distribuiti Thor -LSB- 23 -RSB- e valutato i vantaggi e i costi del sistema su una gamma di latenze di rete. accesso a grana fine e ad elevata coerenza in ambienti ad alta latenza, 2. un'implementazione del prototipo del sistema che produce notevoli miglioramenti in termini di prestazioni rispetto al sistema di base, 3. valutazione analitica e basata sulla misurazione delle prestazioni dei costi e dei benefici delle nuove tecniche che catturano il costo delle prestazioni dominante, elevata latenza di rete.una nuova tecnica di caching cooperativo transazionale -LSB- 20, 16, 13, 2, 28 -RSB- che migliora la latenza di accesso a oggetti persistenti condivisi per applicazioni collaborative a forte coerenza in ambienti di rete ad alta latenza. La tecnica migliora le prestazioni ma fornisce forti propriet\u00e0 di correttezza e disponibilit\u00e0 in presenza di guasti dei nodi e client lenti. Il reindirizzamento, tuttavia, pu\u00f2 interferire con la disponibilit\u00e0 degli oggetti. Solo commit \u00e8 una nuova tecnica di convalida che consente a un client in un gruppo di eseguire il commit indipendentemente dai peer lenti o non riusciti. Fornisce una convalida a grana fine utilizzando informazioni poco costose sulla versione a grana grossa. Abbiamo progettato e implementato il prototipo BuddyCache nel sistema di storage di oggetti transazionali distribuiti Thor -LSB- 23 -RSB- e valutato i vantaggi e i costi del sistema su una gamma di latenze di rete. accesso a grana fine e ad elevata coerenza in ambienti ad alta latenza, 2. un'implementazione del prototipo del sistema che produce notevoli miglioramenti in termini di prestazioni rispetto al sistema di base, 3. valutazione analitica e basata sulla misurazione delle prestazioni dei costi e dei benefici delle nuove tecniche che catturano il costo delle prestazioni dominante, elevata latenza di rete.", "keyphrases": ["sistema di archiviazione degli oggetti", "collaborazione forte-consistente applic", "rete geografica", "web cache di cooper", "quota a grana fine", "effettuare transazioni", "propriet\u00e0 di tolleranza agli errori", "buddycach", "Il dominio esegue il costo", "sistema ottimista", "recupero dei pari", "benchmark oo7 multiutente"]}
{"file_name": "J-17", "text": "Progettazione di meccanismi veritieri per scheduling multidimensionale tramite monotonicit\u00e0 del ciclo ABSTRACT Consideriamo il problema della minimizzazione del makespan su m macchine non correlate nel context della progettazione di meccanismi algoritmici, dove le macchine sono gli attori strategici. Questo \u00e8 un dominio di scheduling multidimensionale e gli unici risultati positivi noti per la minimizzazione del makespan in tale dominio sono O -LRB- m -RRB- - meccanismi veritieri di approssimazione -LSB- 22, 20 -RSB-. Studiamo un caso speciale ben motivato di questo problema, in cui il tempo di elaborazione di un lavoro su ciascuna macchina pu\u00f2 essere \"basso\" o \"alto\", e i valori basso e alto sono pubblici e dipendenti dal lavoro. Ci\u00f2 preserva la multidimensionalit\u00e0 del dominio e generalizza l'impostazione delle macchine limitate -LRB- ovvero -LCB- pj, \u221e -RCB- -RRB- nello scheduling. Forniamo una tecnica generale per convertire qualsiasi algoritmo di approssimazione c in un meccanismo di approssimazione veritiera 3c. Questo \u00e8 uno dei pochi risultati conosciuti che mostra come esportare algoritmi di approssimazione per un problema multidimensionale in meccanismi veritieri in stile scatola nera. Quando i valori basso e alto sono gli stessi per tutti i lavori, elaboriamo un meccanismo veritiero deterministico a 2 approssimazioni. Questi sono i primi meccanismi veritieri con garanzie di prestazione non banali per un dominio di scheduling multidimensionale. Le nostre costruzioni sono nuove sotto due aspetti. Innanzitutto, non utilizziamo n\u00e9 facciamo affidamento su definizioni di prezzo esplicite per dimostrarne la veridicit\u00e0; progettiamo invece algoritmi che soddisfano la monotonicit\u00e0 del ciclo. La monotonia del ciclo -LSB- 23 -RSB- \u00e8 una condizione necessaria e sufficiente per la veridicit\u00e0, \u00e8 una generalizzazione della monotonia del valore per domini multidimensionali. Tuttavia, mentre la monotonia del valore \u00e8 stata utilizzata ampiamente e con successo per progettare meccanismi veritieri in domini unidimensionali, il nostro \u00e8 il primo lavoro che sfrutta la monotonia del ciclo in un context multidimensionale. In secondo luogo, i nostri meccanismi randomizzati si ottengono costruendo prima un meccanismo di verit\u00e0 frazionaria per un rilassamento frazionato del problema, e poi convertendolo in un meccanismo di aspettativa veritiera. Questo si basa su una tecnica di -LSB- 16 -RSB- e mostra l'utilit\u00e0 dei meccanismi frazionari nella progettazione di meccanismi veritieri. 1. INTRODUZIONE La progettazione del meccanismo studia le costruzioni algoritmiche in presenza di attori strategici che detengono gli input dell'algoritmo. La progettazione del meccanismo algoritmico si \u00e8 concentrata principalmente su contesti in cui il pianificatore o progettista sociale desidera massimizzare il benessere sociale -LRB- o, in modo equivalente, minimizzare il costo sociale -RRB-, o su contesti di asta in cui la massimizzazione delle entrate \u00e8 l'obiettivo principale. In questo articolo consideriamo un obiettivo alternativo nel context della schedulazione delle macchine, vale a dire la minimizzazione del makespan. Ci sono n lavori o attivit\u00e0 che devono essere assegnati a m macchine, dove ogni lavoro deve essere assegnato esattamente a una macchina. Quindi, affrontiamo il problema attraverso la progettazione del meccanismo:il social designer, titolare dell'insieme dei compiti da assegnare, deve specificare, oltre ad un calendario, adeguati compensi ai giocatori in modo da incentivarli a rivelare i loro reali tempi di lavorazione. Un tale meccanismo \u00e8 chiamato meccanismo veritiero. Corrisponde invece alla massimizzazione del benessere minimo e alla nozione di equit\u00e0 max-min, e sembra essere un problema molto pi\u00f9 difficile dal punto di vista della progettazione del meccanismo. In particolare, la celebre famiglia di meccanismi VCG -LSB- 26, 9, 10 -RSB- non si applica qui e dobbiamo ideare nuove tecniche. La possibilit\u00e0 di costruire un meccanismo veritiero per la minimizzazione del makespan \u00e8 fortemente legata alle ipotesi sui tempi di elaborazione dei giocatori, in particolare sulla \"dimensionalit\u00e0\" del dominio. Nisan e Ronen hanno considerato l'impostazione di macchine non correlate in cui i valori pij possono essere arbitrari. Questo \u00e8 un dominio multidimensionale, poich\u00e9 il valore privato di un giocatore \u00e8 il suo intero vettore di tempi di elaborazione -LRB- pij -RRB- j. Sono noti pochissimi risultati positivi per i domini multidimensionali in generale, e gli unici risultati positivi noti per la scheduling multidimensionale sono O -LRB- m -RRB- - meccanismi veritieri di approssimazione -LSB- 22, 20 -RSB-. Sottolineiamo che, indipendentemente dalle considerazioni computazionali, anche l'esistenza di un meccanismo veritiero con un rapporto di approssimazione -LRB- significativamente migliore di m -RRB- non \u00e8 nota per alcun dominio di scheduling di questo tipo. Sul lato negativo, -LSB- 22 -RSB- ha mostrato che nessun meccanismo deterministico veritiero pu\u00f2 raggiungere un rapporto di approssimazione migliore di 2, e ha rafforzato questo limite inferiore a m per due classi specifiche di meccanismi deterministici. Recentemente, -LSB- 20 -RSB- ha esteso questo limite inferiore a meccanismi randomizzati e -LSB- 8 -RSB- ha migliorato il limite inferiore deterministico. In netto contrasto con lo stato di cose di cui sopra, sono noti risultati positivi molto pi\u00f9 forti -LRB- e molti altri -RRB- per un caso speciale del problema delle macchine non correlate, vale a dire l'impostazione delle macchine correlate. Qui abbiamo pij = pj/si per ogni i, j, dove pj \u00e8 di dominio pubblico, e la velocit\u00e0 si \u00e8 l'unico parametro privato della macchina i. Questo presupposto rende il dominio delle tipologie dei giocatori unidimensionale. La veridicit\u00e0 in tali domini equivale a una conveniente condizione di monotonicit\u00e0 del valore -LSB- 21, 3 -RSB-, che sembra rendere significativamente pi\u00f9 semplice la progettazione di meccanismi veritieri in tali domini. Archer e Tardos -LSB- 3 -RSB- hanno prima considerato l'impostazione delle macchine correlate e hanno fornito un meccanismo randomizzato di 3 approssimazioni veritiere. Il divario tra i domini monodimensionali e multidimensionali \u00e8 forse meglio esemplificato dal fatto che -LSB- 3 -RSB- ha dimostrato che esiste un meccanismo veritiero che produce sempre una pianificazione ottimale. -LRB- Ricordiamo che nel context delle macchine multidimensionali non correlate, \u00e8 impossibile ottenere un meccanismo veritiero con un rapporto di approssimazione migliore di 2. -RRB- Vari risultati di follow-up -LSB- 2,4, 1, 13 -RSB- hanno rafforzato la nozione di veridicit\u00e0 e/o migliorato il rapporto di approssimazione. Tali difficolt\u00e0 nel passaggio dal context unidimensionale a quello multidimensionale sorgono anche in altri contesti di progettazione di meccanismi -LRB- ad esempio, aste combinatorie -RRB-. Pertanto, oltre all\u2019importanza specifica della pianificazione in ambienti strategici, le idee derivanti dalla pianificazione multidimensionale possono anche avere un impatto nel context pi\u00f9 generale della progettazione di meccanismi veritieri per domini multidimensionali. In questo articolo consideriamo il problema di minimizzazione del makespan per un caso speciale di macchine non correlate, dove il tempo di elaborazione di un lavoro \u00e8 \"basso\" o \"alto\" su ciascuna macchina. Chiamiamo questo modello il caso dei \"due valori dipendenti dal lavoro\". Questo modello generalizza l'impostazione classica delle ``macchine limitate'', dove pij \u2208 -LCB- Lj, \u221e -RCB- che \u00e8 stato ben studiato algoritmicamente. Un caso speciale del nostro modello \u00e8 quando Lj = L e Hj = H per tutti i lavori j, che denotiamo semplicemente come modello di pianificazione \"a due valori\". Entrambi i nostri domini sono multidimensionali, poich\u00e9 le macchine non sono correlate: un lavoro pu\u00f2 essere basso su una macchina e alto sull'altra, mentre un altro lavoro pu\u00f2 seguire lo schema opposto. Pertanto, le informazioni private di ciascuna macchina sono un vettore che specifica quali lavori sono bassi e alti su di essa. Pertanto, mantengono la propriet\u00e0 fondamentale alla base della durezza della progettazione di meccanismi veritieri per macchine non correlate e, studiando queste impostazioni speciali, speriamo di ottenere alcune intuizioni che saranno utili per affrontare il problema generale. I nostri risultati e tecniche Presentiamo vari risultati positivi per i nostri domini di pianificazione multidimensionale. Il nostro primo risultato \u00e8 un metodo generale per convertire qualsiasi algoritmo di capprossimazione per l'impostazione di due valori dipendente dal lavoro in un meccanismo di approssimazione 3c veritiero nelle aspettative. Questo \u00e8 uno dei pochissimi risultati conosciuti che utilizza un algoritmo di approssimazione in stile scatola nera per ottenere un meccanismo veritiero per un problema multidimensionale. Il nostro risultato implica che esiste un meccanismo di aspettativa veritiera con 3 approssimazioni per l'impostazione Lj-Hj. Il nostro secondo risultato si applica all'impostazione a due valori -LRB- Lj = L, Hj = H -RRB-, per la quale miglioriamo sia il rapporto di approssimazione che rafforziamo la nozione di veridicit\u00e0. Otteniamo un meccanismo veritiero deterministico a 2 approssimazioni -LRB- insieme ai prezzi -RRB- per questo problema. Questi sono i primi meccanismi veritieri con garanzie di prestazione non banali per un dominio di scheduling multidimensionale. In aggiunta a ci\u00f2, osserviamo che anche questa impostazione apparentemente semplice non ammette meccanismi veritieri che restituiscono un programma ottimale -LRB- a differenza del caso delle macchine correlate -RRB-. Sfruttando la multidimensionalit\u00e0 del dominio, dimostriamo che nessun meccanismo deterministico veritiero pu\u00f2 ottenere un rapporto di approssimazione migliore di 1.14 rispetto al makespan -LRB- indipendentemente da considerazioni computazionali -RRB-.La tecnica principale, e una delle novit\u00e0, alla base delle nostre costruzioni e prove, \u00e8 che non ci basiamo su specifiche esplicite di prezzo per dimostrare la veridicit\u00e0 dei nostri meccanismi. Invece sfruttiamo alcune condizioni di monotonicit\u00e0 algoritmica che caratterizzano la veridicit\u00e0 per progettare prima un algoritmo implementabile, cio\u00e8 un algoritmo per il quale esistono prezzi che garantiscono la veridicit\u00e0, e poi troviamo questi prezzi -LRB- approfondendo ulteriormente la prova di implementabilit\u00e0 -RRB-. Questo tipo di analisi \u00e8 stato il metodo scelto nella progettazione di meccanismi veritieri per domini unidimensionali, dove la monotonicit\u00e0 del valore produce una caratterizzazione conveniente che consente di concentrarsi sul lato algoritmico del problema -LRB- vedere, ad esempio, -LSB- 3 , 7, 4, 1, 13 -RSB- -RRB-. Il nostro lavoro \u00e8 il primo a sfruttare le condizioni di monotonicit\u00e0 per la progettazione di meccanismi veritieri in domini arbitrari. La condizione di monotonicit\u00e0 da noi utilizzata, talvolta chiamata monotonicit\u00e0 del ciclo, \u00e8 stata proposta per la prima volta da Rochet -LSB- 23 -RSB- -LRB- vedi anche -LSB- 11 -RSB- -RRB-. \u00c8 una generalizzazione della monotonicit\u00e0 dei valori e caratterizza completamente la veridicit\u00e0 in ogni ambito. I nostri metodi e le nostre analisi dimostrano i potenziali benefici di questa caratterizzazione e mostrano che la monotonicit\u00e0 del ciclo pu\u00f2 essere utilizzata efficacemente per ideare meccanismi veritieri per domini multidimensionali. Consideriamo, ad esempio, il nostro primo risultato che mostra che qualsiasi algoritmo di approssimazione c pu\u00f2 essere \"esportato\" in un meccanismo di approssimazione 3c veritiero nelle aspettative. A livello di generalit\u00e0 di un algoritmo di approssimazione arbitraria, sembra improbabile che si possa elaborare dei prezzi per dimostrare la veridicit\u00e0 del meccanismo costruito. Ma la monotonicit\u00e0 del ciclo ci consente di dimostrare tale affermazione. In effetti, alcune condizioni basate solo sull\u2019algoritmo sottostante -LRB- e non sui prezzi -RRB- sembrano necessarie per dimostrare tale affermazione generale. Il metodo per convertire gli algoritmi di approssimazione in meccanismi veritieri coinvolge un'altra idea nuova. Il nostro meccanismo randomizzato si ottiene costruendo prima un meccanismo veritiero che restituisce una pianificazione frazionaria. Passare a un dominio frazionario ci consente di \"inserire\" la veridicit\u00e0 nell'algoritmo di approssimazione in modo piuttosto semplice, perdendo un fattore 2 nel rapporto di approssimazione. Utilizziamo quindi un'opportuna procedura di arrotondamento randomizzato per convertire l'assegnazione frazionaria in un'assegnazione integrale casuale. Ci\u00f2 preserva la veridicit\u00e0, ma perdiamo un altro fattore additivo pari al rapporto di approssimazione. La nostra costruzione utilizza ed estende alcune osservazioni di Lavi e Swamy -LSB- 16 -RSB- e dimostra ulteriormente i vantaggi dei meccanismi frazionari nella progettazione di meccanismi veritieri. Lavoro correlato Nisan e Ronen -LSB- 22 -RSB- hanno considerato per primi il problema della minimizzazione del makespan per macchine non correlate. Hanno dato un risultato positivo con l'approssimazione m e hanno dimostrato vari limiti inferiori. Questo \u00e8 stato migliorato in -LSB- 2, 4,1, 13 -RSB- a: un meccanismo randomizzato con approssimazione 2 -LSB- 2 -RSB- ; un FPTAS per qualsiasi numero fisso di macchine dato da Andelman, Azar e Sorani -LSB- 1 -RSB-, e un meccanismo deterministico a 3 approssimazioni di Kov \u00b4 acs -LSB- 13 -RSB-. Il problema algoritmico -LRB- cio\u00e8, senza richiedere la veridicit\u00e0 -RRB- della minimizzazione del makespan su macchine non correlate \u00e8 ben compreso e sono noti vari algoritmi con approssimazione 2. Lenstra, Shmoys e Tardos -LSB- 18 -RSB- hanno fornito il primo di questi algoritmi. Shmoys e Tardos -LSB- 25 -RSB- hanno successivamente fornito un algoritmo di approssimazione 2 per il problema di assegnazione generalizzato, una generalizzazione in cui esiste un costo cij per assegnare un lavoro j a una macchina i, e l'obiettivo \u00e8 minimizzare il costo soggetto a a vincolato al makespan. Recentemente, Kumar, Marathe, Parthasarathy e Srinivasan -LSB- 14 -RSB- hanno fornito un algoritmo di arrotondamento randomizzato che produce gli stessi limiti. Usiamo la loro procedura nel nostro meccanismo randomizzato. La caratterizzazione della veridicit\u00e0 per domini arbitrari in termini di monotonicit\u00e0 del ciclo sembra essere stata osservata per la prima volta da Rochet -LSB- 23 -RSB- -LRB- vedi anche Gui et al. -LSB- 11 -RSB- -RRB-. Ci\u00f2 generalizza la condizione di valore-monotonicit\u00e0 per domini unidimensionali data da Myerson -LSB- 21 -RSB- e riscoperta da -LSB- 3 -RSB-. Come accennato in precedenza, questa condizione \u00e8 stata sfruttata numerose volte per ottenere meccanismi veritieri per domini unidimensionali -LSB- 3, 7, 4, 1, 13 -RSB-. Per i domini convessi -LRB-, cio\u00e8 l'insieme di valori privati \u200b\u200bdi ciascun giocatore \u00e8 convesso -RRB-, \u00e8 noto che la monotonicit\u00e0 del ciclo \u00e8 implicata da una condizione pi\u00f9 semplice, chiamata monotonicit\u00e0 debole -LSB- 15, 6, 24 -RSB-. Ma anche questa condizione pi\u00f9 semplice non ha trovato molta applicazione nella progettazione di meccanismi veritieri per problemi multidimensionali. Obiettivi diversi dalla massimizzazione del benessere sociale e della massimizzazione delle entrate hanno ricevuto pochissima attenzione nella progettazione del meccanismo. Nel context delle aste combinatorie, sono stati brevemente studiati i problemi di massimizzazione del valore minimo ricevuto da un giocatore e di calcolo di un'allocazione che minimizzi l'invidia. Lavi, Mu'alem e Nisan -LSB- 15 -RSB- hanno dimostrato che il primo obiettivo non pu\u00f2 essere implementato in modo veritiero; Bezakova e Dani -LSB- 5 -RSB- hanno fornito un meccanismo di approssimazione 0,5 per due giocatori con valutazioni aggiuntive. Questi limiti inferiori sono stati rafforzati in -LSB- 20 -RSB-. 2. PRELIMINARI 2.1 Il dominio di schedulazione Nel nostro problema di schedulazione, ci vengono dati n lavori e m macchine, e ogni lavoro deve essere assegnato esattamente a una macchina. Nell'impostazione delle macchine non correlate, ciascuna macchina i \u00e8 caratterizzata da un vettore di tempi di elaborazione -LRB- pij -RRB- j, dove pij ER \u2265 0 U -LCB- oo -RCB- denota il tempo di elaborazione di i per il lavoro j con il valore oo specificando che non posso elaborare j. Consideriamo due casi speciali di questo problema: 1. Il caso a due valori dipendente dal lavoro, dove pij E -LCB- Lj, Hj -RCB- per ogni i, j, con Lj < Hj, e i valori Lj, Hj sono conosciuto.Ci\u00f2 generalizza il classico modello di schedulazione delle macchine vincolate, dove Hj = oo. 2. Diciamo che un lavoro j \u00e8 basso sulla macchina i se pij = Lj, e alto se pij = Hj. Utilizzeremo i termini pianificazione e assegnazione in modo intercambiabile. Considereremo anche algoritmi randomizzati e algoritmi che restituiscono un'assegnazione frazionaria. Indichiamo il carico della macchina i -LRB- sotto un dato incaricoj xijpij, e il makespan di un programma \u00e8 definito come il carico massimo su qualsiasi macchina, cio\u00e8 maxi li. L'obiettivo del problema di minimizzazione del makespan \u00e8 assegnare i lavori alle macchine in modo da ridurre al minimo il makespan della pianificazione. 2.2 Progettazione del meccanismo Consideriamo il problema della minimizzazione del makespan nei domini di scheduling sopra menzionati nel context della progettazione del meccanismo. Il design dei meccanismi studia contesti strategici in cui il progettista sociale deve garantire la cooperazione delle diverse entit\u00e0 coinvolte nella procedura algoritmica. Seguendo il lavoro di Nisan e Ronen -LSB- 22 -RSB-, consideriamo le macchine come attori o agenti strategici. Il progettista sociale detiene l'insieme dei lavori che devono essere assegnati, ma non conosce i tempi di elaborazione -LRB- true -RRB- di questi lavori sulle diverse macchine. Ogni macchina \u00e8 un'entit\u00e0 egoista, che conosce privatamente il proprio tempo di elaborazione per ogni lavoro. Consideriamo meccanismi di rivelazione diretta: ogni macchina riporta il suo vettore -LRB- eventualmente falso -RRB- dei tempi di elaborazione, il meccanismo quindi calcola un programma e distribuisce i pagamenti ai giocatori -LRB- cio\u00e8 alle macchine -RRB- per compensarli il costo sostenuto per l'elaborazione dei lavori assegnati. Un meccanismo -LRB- a rivelazione diretta -RRB- consiste quindi di una tupla -LRB- x, P -RRB- : x specifica la pianificazione e P = -LCB- Pi -RCB- specifica i pagamenti distribuiti alle macchine, dove sia x che Pis sono funzioni dei tempi di elaborazione riportati p = -LRB- pij -RRB- i, j. Il meccanismo deve quindi incentivare le macchine/giocatori a rivelare in modo veritiero i propri tempi di elaborazione tramite i pagamenti. Ci\u00f2 \u00e8 reso preciso utilizzando la nozione di veridicit\u00e0 della strategia dominante. Per dirla in parole, in un meccanismo veritiero, nessuna macchina pu\u00f2 migliorare la propria utilit\u00e0 dichiarando un tempo di elaborazione falso, indipendentemente da ci\u00f2 che dichiarano le altre macchine. Considereremo anche meccanismi frazionari che restituiscono un incarico frazionario e meccanismi randomizzati a cui \u00e8 consentito lanciare monete e in cui l'assegnazione e i pagamenti possono essere variabili casuali. La nozione di veridicit\u00e0 per un meccanismo frazionario \u00e8 la stessa della Definizione 2.1, dove x1, x2 sono ora assegnazioni frazionarie. Per un meccanismo randomizzato, considereremo la nozione di veridicit\u00e0 nell'aspettativa -LSB- 3 -RSB-, il che significa che una macchina -LRB- player -RRB- massimizza la sua utilit\u00e0 attesa dichiarando il suo vero vettore del tempo di elaborazione. Per i nostri due domini di scheduling, l'ipotesi informativa \u00e8 che i valori Lj, Hj siano noti pubblicamente.L'informazione privata di una macchina \u00e8 quali lavori hanno valore Lj -LRB- o L -RRB- e quali hanno valore Hj -LRB- o H -RRB- su di essa. Sottolineiamo che entrambi i nostri domini sono multidimensionali, poich\u00e9 ogni macchina ha bisogno di specificare un vettore che indichi quali lavori sono bassi e alti su di essa.", "keyphrases": ["progettazione meccanica", "algoritmo approssimativo", "programma", "pianificazione multidimensionale", "ciclo monotono", "makespan minimo", "algoritmo", "meccanico casuale", "noi della meccanica delle frazioni", "progettazione meccanica della verit\u00e0", "dominio della frazione"]}
{"file_name": "I-26", "text": "Processo decisionale sequenziale nella ricerca economica bilaterale parallela ABSTRACT Questo articolo presenta un modello di ricerca economica bilaterale in cui gli agenti sono alla ricerca di partnership vantaggiose a coppie. In ogni fase di ricerca, ciascuno degli agenti viene abbinato casualmente a diversi altri agenti in parallelo e decide se accettare una potenziale partnership con uno di loro. La caratteristica distintiva del modello proposto \u00e8 che gli agenti non sono limitati a mantenere un protocollo decisionale sincronizzato -LRB- istantaneo -RRB- e possono accettare e rifiutare sequenzialmente le partnership all'interno della stessa fase di ricerca. Analizziamo le dinamiche che guidano le strategie degli agenti verso un equilibrio stabile nel nuovo modello e mostriamo che la strategia di ricerca proposta domina debolmente quella attualmente in uso per il modello di ricerca economica parallela a due lati. Identificando diverse caratteristiche uniche dell'equilibrio riusciamo a delimitare in modo efficiente lo spazio strategico che deve essere esplorato dagli agenti e proporre un mezzo efficiente per estrarre le strategie di equilibrio distribuito in ambienti comuni. 1. INTRODUZIONE Una ricerca economica bilaterale \u00e8 un meccanismo distribuito per formare partnership a coppie di agenti -LSB- 5 -RSB-.1 In ogni fase del processo, ciascuno degli agenti \u00e8 abbinato casualmente a un altro agente 1Si noti che il concetto di La ''ricerca'' qui \u00e8 molto diversa dalla definizione classica di ''ricerca'' in AI. Mentre la ricerca dell\u2019intelligenza artificiale \u00e8 un processo attivo in cui un agente trova una sequenza di azioni che lo porteranno dallo stato iniziale a uno stato obiettivo, la ricerca economica si riferisce all\u2019identificazione del miglior agente con cui impegnarsi in una partnership. e i due interagiscono bilateralmente per apprendere i vantaggi racchiusi in una partnership tra loro. L'interazione non implica contrattazione, quindi ciascun agente deve semplicemente scegliere tra accettare o rifiutare la partnership con l'altro agente. Un tipico mercato in cui avviene questo tipo di ricerca bilaterale \u00e8 il mercato matrimoniale -LSB- 22 -RSB-. La letteratura recente suggerisce varie applicazioni software basate su agenti in cui ha luogo una ricerca -LRB- distribuita su due lati, cio\u00e8 senza meccanismi di corrispondenza centralizzati -RRB-. Una classe importante di tali applicazioni comprende i mercati secondari per lo scambio di risorse non sfruttate. Ad esempio, attraverso una ricerca bilaterale, gli agenti, che rappresentano diversi fornitori di servizi, possono scambiarsi la larghezza di banda inutilizzata -LSB-21 -RSB- e i satelliti di comunicazione possono trasferire la comunicazione con una maggiore copertura geografica. La ricerca bilaterale basata sugli agenti pu\u00f2 essere trovata anche nelle applicazioni di acquirenti e venditori negli eMarkets e nelle applicazioni peer-to-peer. La natura bilaterale della ricerca suggerisce che una partnership tra una coppia di agenti si forma solo se \u00e8 reciprocamente accettata. Formando una partnership gli agenti ottengono un'utilit\u00e0 immediata e interrompono la loro ricerca. Quando si riprende la ricerca,d'altro canto, potrebbe essere trovato un partner pi\u00f9 adatto, tuttavia alcune risorse dovranno essere consumate per mantenere il processo di ricerca. In questo articolo ci concentriamo su una classe specifica di problemi di search match a due facce, in cui la prestazione della partnership si applica a entrambe le parti, cio\u00e8 entrambe ottengono la stessa utilit\u00e0 -LSB- 13 -RSB-. Lo scenario di pari utilit\u00e0 \u00e8 solitamente applicabile in ambiti in cui i partner traggono vantaggio dalla sinergia tra loro. In tutte queste applicazioni, due agenti qualsiasi possono formare una partnership e la performance di una determinata partnership dipende dalle competenze o dalle caratteristiche dei suoi membri. Inoltre, lo scenario di pari utilit\u00e0 pu\u00f2 valere anche ogni volta che esiste un'opzione per pagamenti collaterali e l'utilit\u00e0 complessiva della partnership \u00e8 equamente divisa tra i due agenti che la compongono -LSB- 22 -RSB-. Sebbene la letteratura sulla ricerca bilaterale offra un'analisi completa dell'equilibrio per vari modelli, presuppone che la ricerca degli agenti sia condotta in modo puramente sequenziale: ciascun agente localizza e interagisce con un altro agente nel suo ambiente alla volta -LSB- 5, 22 -RSB-. Tuttavia, quando la ricerca viene assegnata ad agenti software autonomi, \u00e8 possibile utilizzare una strategia di ricerca migliore. Qui un agente pu\u00f2 trarre vantaggio dalle sue esclusive capacit\u00e0 intrinseche di filtraggio ed elaborazione delle informazioni e dalla sua capacit\u00e0 di mantenere in modo efficiente -LRB- rispetto alle persone -RRB- interazioni simultanee con diversi altri agenti in ogni fase della sua ricerca. Tale utilizzo delle interazioni parallele nella ricerca \u00e8 favorevole ogni volta che il costo medio2 per interazione con un altro agente, quando si interagisce in parallelo con un gruppo di altri agenti, \u00e8 inferiore al costo di mantenere un'interazione alla volta -LRB- ovvero, vantaggio della dimensione -RRB-. Ad esempio, l'analisi dei costi associati alla valutazione di potenziali partnership tra fornitori di servizi rivela sia componenti fisse che variabili quando si utilizza la ricerca parallela, quindi il costo medio per interazione diminuisce all'aumentare del numero di interazioni parallele -LSB- 21 -RSB-. Nonostante i vantaggi identificati per le interazioni parallele in domini adiacenti -LRB- ad esempio, nella ricerca economica unilaterale -LSB- 7, 16 -RSB- -RRB-, un primo tentativo di modellare un processo di abbinamento ripetuto a coppie in cui gli agenti sono capaci di il mantenimento dell'interazione con diversi altri agenti contemporaneamente \u00e8 stato introdotto solo di recente -LSB- 21 -RSB-. Tuttavia, gli agenti di quel modello fondamentale sono tenuti a sincronizzare il loro processo decisionale. Pertanto ciascun agente, esaminate le opportunit\u00e0 disponibili in una specifica fase di ricerca, deve comunicare a tutti gli altri agenti la propria decisione se impegnarsi in una partnership -LRB- al massimo con uno di essi -RRB- oppure rifiutare la partnership -LRB- con gli altri -RRB-. Questa restrizione intrinseca impone una limitazione significativa al comportamento strategico degli agenti. Nel nostro modello,gli agenti sono liberi di notificare agli altri agenti le loro decisioni in modo asincrono. L'approccio asincrono consente agli agenti di rivalutare la propria strategia, in base a ogni nuova risposta che ricevono dagli agenti con cui interagiscono. Il nuovo modello \u00e8 un modello a coppie molto pi\u00f9 realistico e, come mostriamo nella sezione di analisi, \u00e8 sempre preferito da ogni singolo agente che partecipa al processo. In assenza di altri modelli economici di ricerca parallela bilaterale, utilizziamo il modello che si basa su un processo decisionale istantaneo -LRB- sincrono -RRB- -LSB- 21 -RSB- -LRB- indicato con I-DM per tutto il resto del il documento -RRB- come punto di riferimento per valutare l'utilit\u00e0 della nostra proposta di strategia decisionale sequenziale -LRB- asincrona -RRB- -LRB- denotata S-DM -RRB-. I principali contributi di questo articolo sono triplici: in primo luogo, modelliamo e analizziamo formalmente un processo di ricerca bilaterale in cui gli agenti non hanno vincoli decisionali temporali riguardanti il \u200b\u200brifiuto o l'impegno verso potenziali partnership che incontrano in parallelo -LRB- la S -DM modello -RRB-. Questo modello \u00e8 un modello di ricerca generale che pu\u00f2 essere applicato in vari domini -LRB- non necessariamente basati su agenti software -RRB-. In secondo luogo, dimostriamo che la strategia SDM degli agenti domina debolmente la strategia I-DM, quindi ogni agente ha un incentivo a deviare verso la strategia S-DM quando tutti gli altri agenti utilizzano la strategia I-DM. Infine, utilizzando un'innovativa presentazione ricorsiva delle probabilit\u00e0 di accettazione di diverse potenziali partnership, identifichiamo le caratteristiche uniche delle strategie di equilibrio nel nuovo modello. Questi vengono utilizzati per fornire un mezzo computazionale appropriato che faciliti il \u200b\u200bcalcolo della strategia di equilibrio degli agenti. Quest'ultimo contributo \u00e8 Riusciamo a estrarre le nuove strategie di equilibrio degli agenti senza aumentare la complessit\u00e0 computazionale rispetto al modello I-DM. In tutto l'articolo dimostriamo le diverse propriet\u00e0 del nuovo modello e lo confrontiamo con il modello I-DM utilizzando un ambiente sintetico artificiale. Nella sezione seguente presentiamo formalmente il modello S-DM. Nella Sezione 3 vengono forniti un'analisi dell'equilibrio e i mezzi computazionali per trovare la strategia di equilibrio. Nella Sezione 4 esaminiamo la letteratura relativa alla MAS e alla teoria della ricerca economica. 4. LAVORI CORRELATI La ricerca economica bilaterale di partenariati nella letteratura sull'IA \u00e8 un sottodominio della formazione di coalizioni8. Come nel caso generale 8L'uso del termine ''partnership'' in questo context si riferisce all'accordo tra due singoli agenti di cooperare in un modo predefinito. Ad esempio, nell'applicazione acquirente-venditore una partnership \u00e8 definita come una transazione concordata tra le due parti -LSB- 9 -RSB-. caso della formazione di una coalizione,gli agenti hanno l'incentivo a formare partenariati quando non sono in grado di eseguire un compito da soli o quando la partnership pu\u00f2 migliorare le loro utilit\u00e0 individuali -LSB- 14 -RSB-. In letteratura si possono trovare vari meccanismi di abbinamento centralizzati -LSB- 6, 2, 8 -RSB-. Tuttavia, in molti ambienti MAS, in assenza di un meccanismo di abbinamento centrale affidabile, il processo di abbinamento \u00e8 completamente distribuito. Mentre \u00e8 risaputo che la ricerca in ambienti basati su agenti \u00e8 costosa -LSB- 11, 21, 1 -RSB-, la maggior parte dei meccanismi di formazione di coalizioni proposti presuppone che un agente possa analizzare tutte le opportunit\u00e0 di partnership nel suo ambiente necessarie o avere accesso ai corrispondenti centrali o agli agenti intermedi -LSB- 6 -RSB-. L'incorporazione della ricerca costosa in questo context \u00e8 piuttosto rara -LSB- 21 -RSB- e, per quanto ne sappiamo, un modello distribuito di ricerca bilaterale per partner simile al modello S-DM non \u00e8 stato studiato fino ad oggi. La teoria classica della ricerca economica -LRB- -LSB- 15, 17 -RSB-, e i riferimenti in essa contenuti -RRB- affrontano ampiamente il problema di un ricercatore che opera in un ambiente costoso, cercando di massimizzare la sua utilit\u00e0 a lungo termine. In questi modelli, classificati come ricerca unilaterale, l'attenzione \u00e8 posta sulla definizione delle strategie ottimali per il ricercatore, presupponendo l'assenza di attivit\u00e0 di ricerca reciproca -LRB-, cio\u00e8 nessuna influenza sull'ambiente -RRB-. Qui viene spesso applicata la procedura di ricerca sequenziale, che consente al ricercatore di indagare su una singola opportunit\u00e0 -LSB- 15 -RSB- o su pi\u00f9 opportunit\u00e0 -LSB- 7, 19 -RSB- alla volta. Sebbene quest'ultimo metodo abbia dimostrato di essere vantaggioso per il ricercatore, non \u00e8 mai stato utilizzato nei modelli di ricerca \"a due lati\" che seguirono -LRB- dove le attivit\u00e0 di ricerca duali sono modellate -RRB- -LSB- 22, 5, 18 -RSB-. Pertanto, in questi modelli, le strategie di equilibrio sono sempre sviluppate partendo dal presupposto che gli agenti interagiscono con gli altri in modo sequenziale -LRB- cio\u00e8 con un agente alla volta -RRB-. Un primo tentativo di integrare la ricerca parallela in un modello di ricerca bilaterale \u00e8 riportato in -LSB- 21 -RSB-, come dettagliato nella sezione introduttiva. I modelli presentati in quest\u2019area non associano il processo di formazione della coalizione ai costi di ricerca, che \u00e8 l\u2019essenza dell\u2019analisi che la teoria della ricerca economica mira a fornire. Inoltre, anche nei modelli di contrattazione a coppie ripetute -LSB-10 -RSB- gli agenti sono sempre limitati ad avviare una singola interazione contrattuale alla volta.L'incorporazione della ricerca costosa in questo context \u00e8 piuttosto rara -LSB- 21 -RSB- e, per quanto ne sappiamo, un modello distribuito di ricerca bilaterale per partner simile al modello S-DM non \u00e8 stato studiato fino ad oggi. La teoria classica della ricerca economica -LRB- -LSB- 15, 17 -RSB-, e i riferimenti in essa contenuti -RRB- affrontano ampiamente il problema di un ricercatore che opera in un ambiente costoso, cercando di massimizzare la sua utilit\u00e0 a lungo termine. In questi modelli, classificati come ricerca unilaterale, l'attenzione \u00e8 posta sulla definizione delle strategie ottimali per il ricercatore, presupponendo l'assenza di attivit\u00e0 di ricerca reciproca -LRB-, cio\u00e8 nessuna influenza sull'ambiente -RRB-. Qui viene spesso applicata la procedura di ricerca sequenziale, che consente al ricercatore di indagare su una singola opportunit\u00e0 -LSB- 15 -RSB- o su pi\u00f9 opportunit\u00e0 -LSB- 7, 19 -RSB- alla volta. Sebbene quest'ultimo metodo abbia dimostrato di essere vantaggioso per il ricercatore, non \u00e8 mai stato utilizzato nei modelli di ricerca \"a due lati\" che seguirono -LRB- dove le attivit\u00e0 di ricerca duali sono modellate -RRB- -LSB- 22, 5, 18 -RSB-. Pertanto, in questi modelli, le strategie di equilibrio sono sempre sviluppate partendo dal presupposto che gli agenti interagiscono con gli altri in modo sequenziale -LRB- cio\u00e8 con un agente alla volta -RRB-. Un primo tentativo di integrare la ricerca parallela in un modello di ricerca bilaterale \u00e8 riportato in -LSB- 21 -RSB-, come dettagliato nella sezione introduttiva. I modelli presentati in quest\u2019area non associano il processo di formazione della coalizione ai costi di ricerca, che \u00e8 l\u2019essenza dell\u2019analisi che la teoria della ricerca economica mira a fornire. Inoltre, anche nei modelli di contrattazione a coppie ripetute -LSB-10 -RSB- gli agenti sono sempre limitati ad avviare una singola interazione contrattuale alla volta.L'incorporazione della ricerca costosa in questo context \u00e8 piuttosto rara -LSB- 21 -RSB- e, per quanto ne sappiamo, un modello distribuito di ricerca bilaterale per partner simile al modello S-DM non \u00e8 stato studiato fino ad oggi. La teoria classica della ricerca economica -LRB- -LSB- 15, 17 -RSB-, e i riferimenti in essa contenuti -RRB- affrontano ampiamente il problema di un ricercatore che opera in un ambiente costoso, cercando di massimizzare la sua utilit\u00e0 a lungo termine. In questi modelli, classificati come ricerca unilaterale, l'attenzione \u00e8 posta sulla definizione delle strategie ottimali per il ricercatore, presupponendo l'assenza di attivit\u00e0 di ricerca reciproca -LRB-, cio\u00e8 nessuna influenza sull'ambiente -RRB-. Qui viene spesso applicata la procedura di ricerca sequenziale, che consente al ricercatore di indagare su una singola opportunit\u00e0 -LSB- 15 -RSB- o su pi\u00f9 opportunit\u00e0 -LSB- 7, 19 -RSB- alla volta. Sebbene quest'ultimo metodo abbia dimostrato di essere vantaggioso per il ricercatore, non \u00e8 mai stato utilizzato nei modelli di ricerca \"a due lati\" che seguirono -LRB- dove le attivit\u00e0 di ricerca duali sono modellate -RRB- -LSB- 22, 5, 18 -RSB-. Pertanto, in questi modelli, le strategie di equilibrio sono sempre sviluppate partendo dal presupposto che gli agenti interagiscono con gli altri in modo sequenziale -LRB- cio\u00e8 con un agente alla volta -RRB-. Un primo tentativo di integrare la ricerca parallela in un modello di ricerca bilaterale \u00e8 riportato in -LSB- 21 -RSB-, come dettagliato nella sezione introduttiva. I modelli presentati in quest\u2019area non associano il processo di formazione della coalizione ai costi di ricerca, che \u00e8 l\u2019essenza dell\u2019analisi che la teoria della ricerca economica mira a fornire. Inoltre, anche nei modelli di contrattazione a coppie ripetute -LSB-10 -RSB- gli agenti sono sempre limitati ad avviare una singola interazione contrattuale alla volta.Un primo tentativo di integrare la ricerca parallela in un modello di ricerca bilaterale \u00e8 riportato in -LSB- 21 -RSB-, come dettagliato nella sezione introduttiva. I modelli presentati in quest\u2019area non associano il processo di formazione della coalizione ai costi di ricerca, che \u00e8 l\u2019essenza dell\u2019analisi che la teoria della ricerca economica mira a fornire. Inoltre, anche nei modelli di contrattazione a coppie ripetute -LSB-10 -RSB- gli agenti sono sempre limitati ad avviare una singola interazione contrattuale alla volta.Un primo tentativo di integrare la ricerca parallela in un modello di ricerca bilaterale \u00e8 riportato in -LSB- 21 -RSB-, come dettagliato nella sezione introduttiva. I modelli presentati in quest\u2019area non associano il processo di formazione della coalizione ai costi di ricerca, che \u00e8 l\u2019essenza dell\u2019analisi che la teoria della ricerca economica mira a fornire. Inoltre, anche nei modelli di contrattazione a coppie ripetute -LSB-10 -RSB- gli agenti sono sempre limitati ad avviare una singola interazione contrattuale alla volta.", "keyphrases": ["partnership di coppia", "decis", "applicazione peer-to-peer", "processo informativo", "util", "costo della ricerca", "scenario multiequilibrio", "strategie di equilibrio", "interagire parallelamente", "metodologia vincolata", "formato coalizione", "formato di partenariato", "associazione", "ambiente costiero", "eseguire la ricerca", "decisione istantanea", "fare le successive decisioni", "ricerca bilaterale"]}
{"file_name": "H-8", "text": "Robuste raccolte di test per la valutazione del recupero ABSTRACT Metodi a basso costo per acquisire giudizi di pertinenza possono essere un vantaggio per i ricercatori che devono valutare nuovi compiti o argomenti di recupero ma non hanno le risorse per esprimere migliaia di giudizi. Sebbene questi giudizi siano molto utili per una valutazione una tantum, non \u00e8 chiaro se ci si possa fidare di essi quando riutilizzati per valutare nuovi sistemi. In questo lavoro definiamo formalmente cosa significa che i giudizi siano riutilizzabili: la fiducia nella valutazione di nuovi sistemi pu\u00f2 essere valutata accuratamente da un insieme esistente di giudizi di rilevanza. Presentiamo quindi un metodo per integrare un insieme di giudizi di rilevanza con stime di rilevanza che non richiedono alcuno sforzo aggiuntivo da parte del valutatore. L'utilizzo di questo metodo garantisce praticamente la riusabilit\u00e0: con appena cinque giudizi per argomento presi da soli due sistemi, possiamo valutare in modo affidabile un insieme pi\u00f9 ampio di dieci sistemi. Anche i pi\u00f9 piccoli insiemi di giudizi possono essere utili per la valutazione di nuovi sistemi. 1. INTRODUZIONE Consideriamo un ricercatore di recupero di informazioni che ha inventato un nuovo compito di recupero. Ha costruito un sistema per eseguire l'attivit\u00e0 e desidera valutarlo. Poich\u00e9 il compito \u00e8 nuovo, \u00e8 improbabile che esistano giudizi di rilevanza. Non ha il tempo o le risorse per giudicare ogni documento, e nemmeno ogni documento recuperato. Pu\u00f2 solo giudicare i documenti che sembrano essere pi\u00f9 informativi e fermarsi quando ha un ragionevole grado di fiducia nelle sue conclusioni. Ma cosa succede quando sviluppa un nuovo sistema e ha bisogno di valutarlo? Oppure un altro gruppo di ricerca decide di implementare un sistema per svolgere il compito? Possono riutilizzare in modo affidabile le sentenze originali? Possono valutare senza pi\u00f9 giudizi di pertinenza? La valutazione \u00e8 un aspetto importante della ricerca sul recupero delle informazioni, ma \u00e8 solo un problema parzialmente risolto: per la maggior parte dei compiti di recupero, \u00e8 impossibile giudicare la rilevanza di ogni documento; ce ne sono semplicemente troppi. La soluzione utilizzata dal NIST alla TREC -LRB- Text REtrieval Conference -RRB- \u00e8 il metodo di pooling -LSB- 19, 20 -RSB-: tutti i sistemi concorrenti contribuiscono con N documenti a un pool e ogni documento in quel pool viene giudicato. Questo metodo crea grandi serie di giudizi riutilizzabili per l'addestramento o la valutazione di nuovi sistemi che non contribuiscono al pool -LSB- 21 -RSB-. Questa soluzione non \u00e8 adeguata per il nostro ipotetico ricercatore. Il metodo di pooling fornisce migliaia di giudizi di pertinenza, ma richiede molte ore di annotatore pagato da LRB e RRB. Come vedremo, i giudizi prodotti da questi metodi possono influenzare in modo significativo la valutazione di un nuovo insieme di sistemi. Tornando alla nostra ipotetica ricercatrice, potr\u00e0 riutilizzare i suoi giudizi di rilevanza? Per prima cosa dobbiamo definire formalmente cosa significa essere \u201criutilizzabili\u201d. Nel lavoro precedente, la riusabilit\u00e0 \u00e8 stata testata semplicemente valutando l'accuratezza di una serie di giudizi di pertinenza nella valutazione di sistemi invisibili. Abbiamo bisogno di una definizione pi\u00f9 attenta di riusabilit\u00e0. Nello specifico,la questione della riusabilit\u00e0 non \u00e8 quanto accuratamente possiamo valutare i nuovi sistemi. Un \"avversario malintenzionato\" pu\u00f2 sempre produrre una nuova classifica che non ha recuperato nessuno dei documenti giudicati. La vera domanda \u00e8 quanta fiducia abbiamo nelle nostre valutazioni e, cosa ancora pi\u00f9 importante, se possiamo fidarci delle nostre stime di fiducia. Anche se la fiducia non \u00e8 elevata, finch\u00e9 possiamo fidarci di essa, possiamo identificare quali sistemi necessitano di pi\u00f9 giudizi per aumentare la fiducia. Qualsiasi insieme di giudizi, non importa quanto piccolo, diventa in una certa misura riutilizzabile. Raccolte di test piccole e riutilizzabili potrebbero avere un enorme impatto sulla ricerca sul recupero delle informazioni. I gruppi di ricerca potrebbero condividere i giudizi di pertinenza che hanno formulato \"internamente\" per studi pilota, nuovi compiti o nuovi argomenti. La quantit\u00e0 di dati a disposizione dei ricercatori crescerebbe esponenzialmente nel tempo. 6. CONCLUSIONI E LAVORO FUTURO In questo lavoro abbiamo offerto la prima definizione formale dell'idea comune di \u201criusabilit\u00e0\u201d di una raccolta di test e presentato un modello in grado di raggiungere la riusabilit\u00e0 con insiemi di giudizi di rilevanza molto piccoli. Le stime di confidenza di RTC, oltre ad essere accurate, forniscono una guida per ottenere giudizi aggiuntivi: concentrarsi sul giudizio dei documenti in base ai confronti con confidenza pi\u00f9 bassa. Nel lungo periodo, vediamo piccoli insiemi di giudizi rilevanti. Tabella 5: Accuratezza, W, media \u03c4 e numero mediano di giudizi per tutti gli 8 insiemi di test. I risultati sono altamente coerenti tra i set di dati. Poich\u00e9 i ricercatori condividono i concetti, ciascun gruppo apporta alcuni giudizi in pi\u00f9 per acquisire maggiore fiducia nei propri sistemi particolari. Con il passare del tempo, il numero di giudizi cresce fino a raggiungere il 100% di fiducia in ogni valutazione ed \u00e8 disponibile una raccolta completa di test per l'attivit\u00e0. Potrebbe essere applicato alla valutazione su una raccolta di test dinamici come definita da Soboroff -LSB- 18 -RSB-. Il modello che abbiamo presentato nella Sezione 3 non \u00e8 affatto l\u2019unica possibilit\u00e0 per creare una solida raccolta di test. Oltre all'aggregazione degli esperti, potremmo stimare le probabilit\u00e0 osservando le somiglianze tra i documenti. Questa \u00e8 un\u2019area ovvia per l\u2019esplorazione futura. Disponiamo di molti altri risultati sperimentali per i quali purtroppo non abbiamo avuto spazio, ma che rafforzano l'idea che RTC \u00e8 estremamente robusto: con solo pochi giudizi per argomento, possiamo valutare accuratamente la fiducia in qualsiasi confronto a coppie di sistemi.Raccolte di test piccole e riutilizzabili potrebbero avere un enorme impatto sulla ricerca sul recupero delle informazioni. I gruppi di ricerca potrebbero condividere i giudizi di pertinenza che hanno formulato \"internamente\" per studi pilota, nuovi compiti o nuovi argomenti. La quantit\u00e0 di dati a disposizione dei ricercatori crescerebbe esponenzialmente nel tempo. 6. CONCLUSIONI E LAVORO FUTURO In questo lavoro abbiamo offerto la prima definizione formale dell'idea comune di \u201criusabilit\u00e0\u201d di una raccolta di test e presentato un modello in grado di raggiungere la riusabilit\u00e0 con insiemi di giudizi di rilevanza molto piccoli. Le stime di confidenza di RTC, oltre ad essere accurate, forniscono una guida per ottenere giudizi aggiuntivi: concentrarsi sul giudizio dei documenti in base ai confronti con confidenza pi\u00f9 bassa. Nel lungo periodo, vediamo piccoli insiemi di giudizi rilevanti. Tabella 5: Accuratezza, W, media \u03c4 e numero mediano di giudizi per tutti gli 8 insiemi di test. I risultati sono altamente coerenti tra i set di dati. Poich\u00e9 i ricercatori condividono i concetti, ciascun gruppo apporta alcuni giudizi in pi\u00f9 per acquisire maggiore fiducia nei propri sistemi particolari. Con il passare del tempo, il numero di giudizi cresce fino a raggiungere il 100% di fiducia in ogni valutazione ed \u00e8 disponibile una raccolta completa di test per l'attivit\u00e0. Potrebbe essere applicato alla valutazione su una raccolta di test dinamici come definita da Soboroff -LSB- 18 -RSB-. Il modello che abbiamo presentato nella Sezione 3 non \u00e8 affatto l\u2019unica possibilit\u00e0 per creare una solida raccolta di test. Oltre all'aggregazione degli esperti, potremmo stimare le probabilit\u00e0 osservando le somiglianze tra i documenti. Questa \u00e8 un\u2019area ovvia per l\u2019esplorazione futura. Disponiamo di molti altri risultati sperimentali per i quali purtroppo non abbiamo avuto spazio, ma che rafforzano l'idea che RTC \u00e8 estremamente robusto: con solo pochi giudizi per argomento, possiamo valutare accuratamente la fiducia in qualsiasi confronto a coppie di sistemi.Raccolte di test piccole e riutilizzabili potrebbero avere un enorme impatto sulla ricerca sul recupero delle informazioni. I gruppi di ricerca potrebbero condividere i giudizi di pertinenza che hanno formulato \"internamente\" per studi pilota, nuovi compiti o nuovi argomenti. La quantit\u00e0 di dati a disposizione dei ricercatori crescerebbe esponenzialmente nel tempo. 6. CONCLUSIONI E LAVORO FUTURO In questo lavoro abbiamo offerto la prima definizione formale dell'idea comune di \u201criusabilit\u00e0\u201d di una raccolta di test e presentato un modello in grado di raggiungere la riusabilit\u00e0 con insiemi di giudizi di rilevanza molto piccoli. Le stime di confidenza di RTC, oltre ad essere accurate, forniscono una guida per ottenere giudizi aggiuntivi: concentrarsi sul giudizio dei documenti in base ai confronti con confidenza pi\u00f9 bassa. Nel lungo periodo, vediamo piccoli insiemi di giudizi rilevanti. Tabella 5: Accuratezza, W, media \u03c4 e numero mediano di giudizi per tutti gli 8 insiemi di test. I risultati sono altamente coerenti tra i set di dati. Poich\u00e9 i ricercatori condividono i concetti, ciascun gruppo apporta alcuni giudizi in pi\u00f9 per acquisire maggiore fiducia nei propri sistemi particolari. Con il passare del tempo, il numero di giudizi cresce fino a raggiungere il 100% di fiducia in ogni valutazione ed \u00e8 disponibile una raccolta completa di test per l'attivit\u00e0. Potrebbe essere applicato alla valutazione su una raccolta di test dinamici come definita da Soboroff -LSB- 18 -RSB-. Il modello che abbiamo presentato nella Sezione 3 non \u00e8 affatto l\u2019unica possibilit\u00e0 per creare una solida raccolta di test. Oltre all'aggregazione degli esperti, potremmo stimare le probabilit\u00e0 osservando le somiglianze tra i documenti. Questa \u00e8 un\u2019area ovvia per l\u2019esplorazione futura. Disponiamo di molti altri risultati sperimentali per i quali purtroppo non abbiamo avuto spazio, ma che rafforzano l'idea che RTC \u00e8 estremamente robusto: con solo pochi giudizi per argomento, possiamo valutare accuratamente la fiducia in qualsiasi confronto a coppie di sistemi.il numero di giudizi cresce fino a raggiungere il 100% di fiducia in ogni valutazione e c'\u00e8 una raccolta completa di test per l'attivit\u00e0. Potrebbe essere applicato alla valutazione su una raccolta di test dinamici come definita da Soboroff -LSB- 18 -RSB-. Il modello che abbiamo presentato nella Sezione 3 non \u00e8 affatto l\u2019unica possibilit\u00e0 per creare una solida raccolta di test. Oltre all'aggregazione degli esperti, potremmo stimare le probabilit\u00e0 osservando le somiglianze tra i documenti. Questa \u00e8 un\u2019area ovvia per l\u2019esplorazione futura. Disponiamo di molti altri risultati sperimentali per i quali purtroppo non abbiamo avuto spazio, ma che rafforzano l'idea che RTC \u00e8 estremamente robusto: con solo pochi giudizi per argomento, possiamo valutare accuratamente la fiducia in qualsiasi confronto a coppie di sistemi.il numero di giudizi cresce fino a raggiungere il 100% di fiducia in ogni valutazione e c'\u00e8 una raccolta completa di test per l'attivit\u00e0. Potrebbe essere applicato alla valutazione su una raccolta di test dinamici come definita da Soboroff -LSB- 18 -RSB-. Il modello che abbiamo presentato nella Sezione 3 non \u00e8 affatto l\u2019unica possibilit\u00e0 per creare una solida raccolta di test. Oltre all'aggregazione degli esperti, potremmo stimare le probabilit\u00e0 osservando le somiglianze tra i documenti. Questa \u00e8 un\u2019area ovvia per l\u2019esplorazione futura. Disponiamo di molti altri risultati sperimentali per i quali purtroppo non abbiamo avuto spazio, ma che rafforzano l'idea che RTC \u00e8 estremamente robusto: con solo pochi giudizi per argomento, possiamo valutare accuratamente la fiducia in qualsiasi confronto a coppie di sistemi.", "keyphrases": ["informare il recupero", "valutare", "giudizio rilevante", "riutilizzabile", "confronto con il livello di confidenza pi\u00f9 basso", "mtc", "rtc", "aspettarsi", "varianc", "distribuzione dei rev"]}
{"file_name": "H-21", "text": "Classificazione robusta di query rare utilizzando la conoscenza del web ABSTRACT Proponiamo una metodologia per costruire un sistema pratico e robusto di classificazione delle query in grado di identificare migliaia di classi di query con ragionevole precisione, trattando in tempo reale il volume di query di un motore di ricerca web commerciale. Utilizziamo una tecnica di feedback cieco: data una query, ne determiniamo l'argomento classificando i risultati della ricerca web recuperati dalla query. Motivati \u200b\u200bdalle esigenze della pubblicit\u00e0 associata alla ricerca, ci concentriamo principalmente su query rare, che sono le pi\u00f9 difficili dal punto di vista dell'apprendimento automatico, ma che nel complesso rappresentano una parte considerevole del traffico dei motori di ricerca. La valutazione empirica conferma che la nostra metodologia produce un\u2019accuratezza di classificazione notevolmente pi\u00f9 elevata rispetto a quanto riportato in precedenza. Riteniamo che la metodologia proposta porter\u00e0 a una migliore corrispondenza degli annunci online con query rare e, nel complesso, a una migliore esperienza utente. 1. INTRODUZIONE Una cosa, tuttavia, \u00e8 rimasta costante: le persone utilizzano query molto brevi. Diversi studi stimano la lunghezza media di una query di ricerca tra 2,4 e 2,7 parole, che a detta di tutti possono contenere solo una piccola quantit\u00e0 di informazioni. I motori di ricerca commerciali fanno un ottimo lavoro nell'interpretare queste brevi stringhe, ma non sono ancora -LRB-! -RRB- onnisciente. Pertanto, l'utilizzo di ulteriori conoscenze esterne per aumentare le query pu\u00f2 contribuire notevolmente a migliorare i risultati della ricerca e l'esperienza dell'utente. In questo studio presentiamo una metodologia per la classificazione delle query, in cui il nostro obiettivo \u00e8 classificare le query in una tassonomia commerciale di query web con circa 6000 nodi. Date tali classificazioni, \u00e8 possibile utilizzarle direttamente per fornire risultati di ricerca migliori e annunci pi\u00f9 mirati. Il problema della classificazione delle query \u00e8 estremamente difficile a causa della brevit\u00e0 delle query. Si noti, tuttavia, che in molti casi un essere umano che guarda una query di ricerca e i relativi risultati riesce notevolmente a dargli un senso. Naturalmente, l\u2019enorme volume delle query di ricerca non si presta alla supervisione umana, e quindi abbiamo bisogno di fonti alternative di conoscenza del mondo. I motori di ricerca indicizzano quantit\u00e0 colossali di informazioni e come tali possono essere considerati archivi di conoscenza molto completi. Seguendo l'euristica sopra descritta, proponiamo di utilizzare i risultati della ricerca stessi per ottenere ulteriori approfondimenti per l'interpretazione delle query. A tal fine, utilizziamo il paradigma del feedback di pseudo rilevanza e presupponiamo che i risultati di ricerca principali siano pertinenti alla query. Certamente non tutti i risultati sono ugualmente rilevanti, quindi utilizziamo elaborati schemi di voto per ottenere una conoscenza affidabile della query. Ai fini di questo studio inviamo innanzitutto la query specificata a un motore di ricerca web generale e raccogliamo un numero di URL con il punteggio pi\u00f9 alto. Eseguiamo la scansione delle pagine Web indicate da questi URL e classifichiamo queste pagine. Infine, utilizziamo queste classificazioni delle pagine dei risultati per classificare la query originale.La nostra valutazione empirica conferma che l'utilizzo dei risultati della ricerca Web in questo modo produce miglioramenti sostanziali nell'accuratezza della classificazione delle query. Tieni presente che in un'implementazione pratica della nostra metodologia all'interno di un motore di ricerca commerciale, tutte le pagine indicizzate possono essere preclassificate utilizzando la normale pipeline di elaborazione del text e indicizzazione. Pertanto, in fase di esecuzione dobbiamo solo eseguire la procedura di votazione, senza eseguire alcuna scansione o classificazione. Questo sovraccarico aggiuntivo \u00e8 minimo e pertanto l'utilizzo dei risultati della ricerca per migliorare la classificazione delle query \u00e8 del tutto fattibile in fase di esecuzione. Un altro aspetto importante del nostro lavoro risiede nella scelta delle query. Il volume delle query nei motori di ricerca di oggi segue la familiare legge del potere, secondo la quale alcune query appaiono molto spesso mentre la maggior parte delle query appare solo poche volte. Sebbene le singole query in questa lunga coda siano rare, insieme rappresentano una massa considerevole di tutte le ricerche. Tuttavia, le query \"tail\" semplicemente non hanno abbastanza occorrenze per consentire l'apprendimento statistico in base alla query. Pertanto, dobbiamo aggregare tali query in qualche modo e ragionare a livello di cluster di query aggregati. Una scelta naturale per tale aggregazione \u00e8 classificare le query in una tassonomia topica. Sapere quali nodi della tassonomia sono pi\u00f9 rilevanti per una determinata query ci aiuter\u00e0 a fornire lo stesso tipo di supporto per le query rare e per quelle frequenti. Di conseguenza, in questo lavoro ci concentreremo sulla classificazione di query rare, la cui corretta classificazione potrebbe essere particolarmente vantaggiosa. I primi studi sull'interpretazione delle query si concentravano sull'aumento delle query attraverso dizionari esterni -LSB- 22 -RSB-. Anche studi pi\u00f9 recenti -LSB- 18, 21 -RSB- hanno tentato di raccogliere ulteriori conoscenze dal Web. Nello specifico, i lavori precedenti nel campo utilizzavano tassonomie di classificazione delle query molto piccole, composte solo da poche decine di nodi, che non consentono un'ampia specificit\u00e0 per la pubblicit\u00e0 online -LSB- 11 -RSB-. Innanzitutto, creiamo il classificatore di query direttamente per la tassonomia di destinazione, invece di utilizzare una struttura ausiliaria secondaria; ci\u00f2 semplifica notevolmente la manutenzione e lo sviluppo della tassonomia. La tassonomia utilizzata in questo lavoro \u00e8 due ordini di grandezza pi\u00f9 ampia di quella utilizzata negli studi precedenti. La valutazione empirica dimostra che la nostra metodologia per l'utilizzo della conoscenza esterna raggiunge miglioramenti maggiori rispetto a quelli precedentemente riportati. Poich\u00e9 la nostra tassonomia \u00e8 considerevolmente pi\u00f9 ampia, il problema di classificazione che dobbiamo affrontare \u00e8 molto pi\u00f9 difficile, rendendo i miglioramenti che otteniamo particolarmente notevoli. Riportiamo inoltre i risultati di uno studio empirico approfondito di diversi schemi di voto e diversi livelli di conoscenza -LRB-, ad esempio, utilizzando riepiloghi di ricerca rispetto a intere pagine scansionate -RRB-. Abbiamo scoperto che la scansione dei risultati di ricerca produce una conoscenza pi\u00f9 approfondita e porta a miglioramenti maggiori rispetto ai semplici riepiloghi. Questo risultato \u00e8 in contrasto con i risultati precedenti nella classificazione delle query -LSB- 20 -RSB-,ma \u00e8 supportato dalla ricerca nella classificazione dei testi tradizionali -LSB- 5 -RSB-. 4. LAVORO CORRELATO Anche se la lunghezza media delle query di ricerca \u00e8 in costante aumento nel tempo, una query tipica \u00e8 ancora inferiore a 3 parole. Di conseguenza, molti ricercatori hanno studiato possibili modi per arricchire le query con informazioni aggiuntive. Una direzione importante per migliorare le query \u00e8 attraverso l'espansione delle query. Questo pu\u00f2 essere fatto utilizzando dizionari elettronici e thesauri -LSB- 22 -RSB-, oppure tramite tecniche di feedback sulla pertinenza che fanno uso di alcuni risultati di ricerca con il punteggio pi\u00f9 alto. I primi lavori nel recupero delle informazioni si concentravano sulla revisione manuale dei risultati restituiti -LSB- 16, 15 -RSB-. Pi\u00f9 recentemente, gli studi sull'aumento delle query si sono concentrati sulla classificazione delle query, presupponendo che tali classificazioni siano utili per un'interpretazione pi\u00f9 mirata delle query. Infatti, Kowalczyk et al. -LSB- 10 -RSB- ha scoperto che l'utilizzo delle classi di query ha migliorato le prestazioni di recupero dei documenti. Gli studi sul campo perseguono approcci diversi per ottenere informazioni aggiuntive sulle query. Beitzel et al. -LSB- 1 -RSB- utilizzava l'apprendimento semi-supervisionato cos\u00ec come i dati senza etichetta -LSB- 2 -RSB-. Gravano et al. -LSB- 6 -RSB- query classificate rispetto alla localit\u00e0 geografica per determinare se il loro intento \u00e8 locale o globale. La KDD Cup del 2005 sulla classificazione delle query web ha ispirato un'altra linea di ricerca, focalizzata sull'arricchimento delle query utilizzando motori e directory di ricerca web -LSB- 11, 18, 20, 9, 21 -RSB-. La specifica dell'attivit\u00e0 KDD forniva una piccola tassonomia -LRB- 67 nodi -RRB- insieme a una serie di query etichettate e rappresentava una sfida per utilizzare questi dati di addestramento per creare un classificatore di query. Diversi team hanno utilizzato il Web per arricchire le query e fornire pi\u00f9 context per la classificazione. Le principali domande di ricerca di questo approccio sono -LRB- 1 -RRB- come costruire un classificatore di documenti, -LRB- 2 -RRB- come tradurre le sue classificazioni nella tassonomia di destinazione e -LRB- 3 -RRB- come determinare la classe di query in base alle classificazioni dei documenti. La soluzione vincente della KDD Cup -LSB- 18 -RSB- proponeva l'utilizzo di un insieme di classificatori insieme alla ricerca su pi\u00f9 motori di ricerca. Per risolvere il problema -LRB- 1 -RRB- sopra, la loro soluzione ha utilizzato il progetto Open Directory -LRB- ODP -RRB- per produrre un classificatore di documenti basato su ODP. La gerarchia ODP \u00e8 stata quindi mappata nella tassonomia di destinazione utilizzando corrispondenze di parole sui singoli nodi. \u00c8 stato creato un classificatore di documenti per la tassonomia di destinazione utilizzando le pagine nella tassonomia ODP che appaiono nei nodi mappati al particolare nodo di destinazione. Pertanto, i documenti Web sono stati prima classificati rispetto alla gerarchia ODP e le loro classificazioni sono state successivamente mappate alla tassonomia di destinazione per la classificazione delle query. Rispetto a questo approccio, abbiamo risolto il problema della classificazione dei documenti direttamente nella tassonomia di destinazione utilizzando le query per produrre il classificatore di documenti come descritto nella Sezione 2.Ci\u00f2 semplifica il processo ed elimina la necessit\u00e0 di mappare tra tassonomie. Ci\u00f2 semplifica anche la manutenzione e lo sviluppo della tassonomia. Utilizzando questo approccio, siamo stati in grado di ottenere buone prestazioni in una tassonomia su scala molto ampia. Abbiamo anche valutato alcune alternative su come combinare le classificazioni dei singoli documenti durante la classificazione effettiva della query. In un articolo di follow-up -LSB- 19 -RSB-, Shen et al. ha proposto un quadro per la classificazione delle query basato sul collegamento tra due tassonomie. In questo approccio, il problema di non avere un classificatore di documenti per i risultati web viene risolto utilizzando un set di training disponibile per documenti con una tassonomia diversa. A questo scopo viene utilizzata una tassonomia intermedia con un set di addestramento -LRB- ODP -RRB-. Al contrario, abbiamo creato direttamente un classificatore di documenti per la tassonomia di destinazione, senza utilizzare documenti di una tassonomia intermedia. Sebbene non siamo stati in grado di confrontare direttamente i risultati a causa dell'uso di diverse tassonomie -LRB-, abbiamo utilizzato una tassonomia molto pi\u00f9 ampia -RRB-, i nostri risultati di precisione e richiamo sono costantemente pi\u00f9 elevati anche nel set di query pi\u00f9 difficili. 5. CONCLUSIONI La classificazione delle query \u00e8 un importante compito di recupero delle informazioni. \u00c8 probabile che la classificazione accurata delle query di ricerca avvantaggi una serie di attivit\u00e0 di livello superiore come la ricerca sul Web e la corrispondenza degli annunci. Poich\u00e9 le query di ricerca sono generalmente brevi, da sole contengono informazioni insufficienti per un'adeguata precisione di classificazione. Per affrontare questo problema, abbiamo proposto una metodologia per utilizzare i risultati della ricerca come fonte di conoscenza esterna. A tal fine, inviamo la query a un motore di ricerca e presupponiamo che una pluralit\u00e0 di risultati di ricerca con il posizionamento pi\u00f9 elevato siano pertinenti alla query. La classificazione di questi risultati ci consente quindi di classificare la query originale con una precisione sostanzialmente maggiore. I risultati della nostra valutazione empirica hanno definitivamente confermato che l\u2019utilizzo del Web come deposito della conoscenza del mondo fornisce informazioni preziose sulla questione e aiuta nella sua corretta classificazione. Inoltre, la tassonomia utilizzata in questo studio \u00e8 di circa 2 ordini di grandezza maggiore di quella utilizzata nei lavori precedenti. Quando si utilizzano i risultati di ricerca \u00e8 possibile utilizzare solo i riassunti dei risultati forniti da 3. Poich\u00e9 nel campo della classificazione delle query non sono ancora stati stabiliti e concordati parametri di riferimento, il confronto diretto dei risultati \u00e8 certamente complicato. il motore di ricerca o eseguire effettivamente la scansione delle pagine dei risultati per una conoscenza ancora pi\u00f9 approfondita. Nel complesso, le prestazioni di classificazione delle query sono state le migliori quando si utilizzavano le pagine completamente scansionate -LRB- Tabella 1 -RRB-. Questi risultati sono coerenti con studi precedenti -LSB-5 -RSB-, che hanno scoperto che l'utilizzo di pagine completamente scansionate \u00e8 migliore per la classificazione dei documenti rispetto all'utilizzo solo di brevi riepiloghi. I nostri risultati, tuttavia, sono diversi da quelli riportati da Shen et al. -LSB- 19 -RSB-, che ha riscontrato che i riepiloghi producono risultati migliori.Attribuiamo le nostre osservazioni all'utilizzo di uno schema di voto pi\u00f9 elaborato tra le classificazioni dei singoli risultati di ricerca, nonch\u00e9 all'utilizzo di un insieme pi\u00f9 difficile di query rare. In questo studio abbiamo utilizzato due principali motori di ricerca, A e B. \u00c8 interessante notare che abbiamo riscontrato notevoli distinzioni nella qualit\u00e0 dei loro risultati. In particolare, per il motore A i risultati complessivi erano migliori quando si utilizzavano le pagine complete dei risultati di ricerca scansionate, mentre per il motore B sembra essere pi\u00f9 vantaggioso utilizzare i riepiloghi dei risultati. Ci\u00f2 implica che mentre la qualit\u00e0 dei risultati di ricerca restituiti dal motore A \u00e8 apparentemente migliore, il motore B svolge un lavoro migliore nel riassumere le pagine. Abbiamo anche scoperto che i risultati migliori sono stati ottenuti utilizzando pagine completamente scansionate ed eseguendo la votazione tra le loro singole classificazioni. D'altra parte, per i proprietari di un motore di ricerca, la classificazione dell'intera pagina \u00e8 molto pi\u00f9 efficiente, poich\u00e9 \u00e8 facile preelaborare tutte le pagine indicizzate classificandole una volta nella tassonomia fissa -LRB- -RRB-. Quindi, le classificazioni delle pagine vengono ottenute come parte dei metadati associati a ciascun risultato di ricerca e la classificazione delle query pu\u00f2 essere quasi istantanea. Quando si utilizzano i riassunti sembra che i risultati migliori si ottengano prima concatenando i singoli riassunti in un meta-documento e poi utilizzando la sua classificazione nel suo complesso. Coerentemente con la nostra intuizione, l\u2019utilizzo di un numero troppo basso di risultati di ricerca produce una conoscenza utile ma insufficiente, mentre l\u2019utilizzo di troppi risultati di ricerca porta all\u2019inclusione di pagine Web marginalmente rilevanti. I risultati migliori sono stati ottenuti utilizzando i 40 risultati di ricerca principali. In questo lavoro classifichiamo innanzitutto i risultati della ricerca e poi utilizziamo le loro classificazioni direttamente per classificare la query originale. In alternativa si possono utilizzare le classificazioni dei risultati di ricerca come caratteristiche per apprendere un classificatore di secondo livello. Abbiamo intenzione di approfondire ulteriormente questa direzione nel nostro lavoro futuro. Se il motore di ricerca classifica le pagine scansionate durante l'indicizzazione, al momento della query dobbiamo solo recuperare queste classificazioni ed effettuare la votazione. Per concludere, riteniamo che la nostra metodologia per l'utilizzo dei risultati di ricerca sul Web sia molto promettente per migliorare sostanzialmente l'accuratezza delle query di ricerca sul Web. Riteniamo che i nostri risultati avranno applicazioni immediate per migliorare la gestione delle query rare, sia per migliorare i risultati della ricerca sia per produrre annunci pubblicitari pi\u00f9 corrispondenti. Nelle nostre ulteriori ricerche prevediamo anche di utilizzare le informazioni sulla sessione per sfruttare la conoscenza delle query precedenti per classificare meglio quelle successive.Ci\u00f2 implica che mentre la qualit\u00e0 dei risultati di ricerca restituiti dal motore A \u00e8 apparentemente migliore, il motore B svolge un lavoro migliore nel riassumere le pagine. Abbiamo anche scoperto che i risultati migliori sono stati ottenuti utilizzando pagine completamente scansionate ed eseguendo la votazione tra le loro singole classificazioni. D'altra parte, per i proprietari di un motore di ricerca, la classificazione dell'intera pagina \u00e8 molto pi\u00f9 efficiente, poich\u00e9 \u00e8 facile preelaborare tutte le pagine indicizzate classificandole una volta nella tassonomia fissa -LRB- -RRB-. Quindi, le classificazioni delle pagine vengono ottenute come parte dei metadati associati a ciascun risultato di ricerca e la classificazione delle query pu\u00f2 essere quasi istantanea. Quando si utilizzano i riassunti sembra che i risultati migliori si ottengano prima concatenando i singoli riassunti in un meta-documento e poi utilizzando la sua classificazione nel suo insieme. Coerentemente con la nostra intuizione, l\u2019utilizzo di un numero troppo basso di risultati di ricerca produce una conoscenza utile ma insufficiente, mentre l\u2019utilizzo di troppi risultati di ricerca porta all\u2019inclusione di pagine Web marginalmente rilevanti. I risultati migliori sono stati ottenuti utilizzando i 40 risultati di ricerca principali. In questo lavoro classifichiamo innanzitutto i risultati della ricerca e poi utilizziamo le loro classificazioni direttamente per classificare la query originale. In alternativa si possono utilizzare le classificazioni dei risultati di ricerca come caratteristiche per apprendere un classificatore di secondo livello. Abbiamo intenzione di approfondire ulteriormente questa direzione nel nostro lavoro futuro. Se il motore di ricerca classifica le pagine scansionate durante l'indicizzazione, al momento della query dobbiamo solo recuperare queste classificazioni ed effettuare la votazione. Per concludere, riteniamo che la nostra metodologia per l'utilizzo dei risultati di ricerca sul Web sia molto promettente per migliorare sostanzialmente l'accuratezza delle query di ricerca sul Web. Riteniamo che i nostri risultati avranno applicazioni immediate per migliorare la gestione delle query rare, sia per migliorare i risultati della ricerca sia per produrre annunci pubblicitari pi\u00f9 corrispondenti. Nelle nostre ulteriori ricerche prevediamo anche di utilizzare le informazioni sulla sessione per sfruttare la conoscenza delle query precedenti per classificare meglio quelle successive.Ci\u00f2 implica che mentre la qualit\u00e0 dei risultati di ricerca restituiti dal motore A \u00e8 apparentemente migliore, il motore B svolge un lavoro migliore nel riassumere le pagine. Abbiamo anche scoperto che i risultati migliori sono stati ottenuti utilizzando pagine completamente scansionate ed eseguendo la votazione tra le loro singole classificazioni. D'altra parte, per i proprietari di un motore di ricerca, la classificazione dell'intera pagina \u00e8 molto pi\u00f9 efficiente, poich\u00e9 \u00e8 facile preelaborare tutte le pagine indicizzate classificandole una volta nella tassonomia fissa -LRB- -RRB-. Quindi, le classificazioni delle pagine vengono ottenute come parte dei metadati associati a ciascun risultato di ricerca e la classificazione delle query pu\u00f2 essere quasi istantanea. Quando si utilizzano i riassunti sembra che i risultati migliori si ottengano prima concatenando i singoli riassunti in un meta-documento e poi utilizzando la sua classificazione nel suo complesso. Coerentemente con la nostra intuizione, l\u2019utilizzo di un numero troppo basso di risultati di ricerca produce una conoscenza utile ma insufficiente, mentre l\u2019utilizzo di troppi risultati di ricerca porta all\u2019inclusione di pagine Web marginalmente rilevanti. I risultati migliori sono stati ottenuti utilizzando i 40 risultati di ricerca principali. In questo lavoro classifichiamo innanzitutto i risultati della ricerca e poi utilizziamo le loro classificazioni direttamente per classificare la query originale. In alternativa si possono utilizzare le classificazioni dei risultati di ricerca come caratteristiche per apprendere un classificatore di secondo livello. Abbiamo intenzione di approfondire ulteriormente questa direzione nel nostro lavoro futuro. Se il motore di ricerca classifica le pagine scansionate durante l'indicizzazione, al momento della query dobbiamo solo recuperare queste classificazioni ed effettuare la votazione. Per concludere, riteniamo che la nostra metodologia per l'utilizzo dei risultati di ricerca sul Web sia molto promettente per migliorare sostanzialmente l'accuratezza delle query di ricerca sul Web. Riteniamo che i nostri risultati avranno applicazioni immediate per migliorare la gestione delle query rare, sia per migliorare i risultati della ricerca sia per produrre annunci pubblicitari pi\u00f9 corrispondenti. Nelle nostre ulteriori ricerche prevediamo anche di utilizzare le informazioni sulla sessione per sfruttare la conoscenza delle query precedenti per classificare meglio quelle successive.e l'utilizzo di troppi risultati di ricerca porta all'inclusione di pagine Web marginalmente rilevanti. I risultati migliori sono stati ottenuti utilizzando i 40 risultati di ricerca principali. In questo lavoro classifichiamo innanzitutto i risultati della ricerca e poi utilizziamo le loro classificazioni direttamente per classificare la query originale. In alternativa si possono utilizzare le classificazioni dei risultati di ricerca come caratteristiche per apprendere un classificatore di secondo livello. Abbiamo intenzione di approfondire ulteriormente questa direzione nel nostro lavoro futuro. Se il motore di ricerca classifica le pagine scansionate durante l'indicizzazione, al momento della query dobbiamo solo recuperare queste classificazioni ed effettuare la votazione. Per concludere, riteniamo che la nostra metodologia per l'utilizzo dei risultati di ricerca sul Web sia molto promettente per migliorare sostanzialmente l'accuratezza delle query di ricerca sul Web. Riteniamo che i nostri risultati avranno applicazioni immediate per migliorare la gestione delle query rare, sia per migliorare i risultati della ricerca sia per produrre annunci pubblicitari pi\u00f9 corrispondenti. Nelle nostre ulteriori ricerche prevediamo anche di utilizzare le informazioni sulla sessione per sfruttare la conoscenza delle query precedenti per classificare meglio quelle successive.e l'utilizzo di troppi risultati di ricerca porta all'inclusione di pagine Web marginalmente rilevanti. I risultati migliori sono stati ottenuti utilizzando i 40 risultati di ricerca principali. In questo lavoro classifichiamo innanzitutto i risultati della ricerca e poi utilizziamo le loro classificazioni direttamente per classificare la query originale. In alternativa si possono utilizzare le classificazioni dei risultati di ricerca come caratteristiche per apprendere un classificatore di secondo livello. Abbiamo intenzione di approfondire ulteriormente questa direzione nel nostro lavoro futuro. Se il motore di ricerca classifica le pagine scansionate durante l'indicizzazione, al momento della query dobbiamo solo recuperare queste classificazioni ed effettuare la votazione. Per concludere, riteniamo che la nostra metodologia per l'utilizzo dei risultati di ricerca sul Web sia molto promettente per migliorare sostanzialmente l'accuratezza delle query di ricerca sul Web. Riteniamo che i nostri risultati avranno applicazioni immediate per migliorare la gestione delle query rare, sia per migliorare i risultati della ricerca sia per produrre annunci pubblicitari pi\u00f9 corrispondenti. Nelle nostre ulteriori ricerche prevediamo anche di utilizzare le informazioni sulla sessione per sfruttare la conoscenza delle query precedenti per classificare meglio quelle successive.", "keyphrases": ["queri classif", "motore di ricerca", "cerca annunci", "apprendimento automatico", "feedback rilevante", "schema di voto", "strisciare", "tassonomi dell'argomento", "punteggio affine", "condizione probabile", "adattare", "informare il recupero"]}
{"file_name": "C-4", "text": "Recupero e controllo delle perdite intra-flusso per ABSTRACT Le reti a commutazione di pacchetto, come Internet, non offrono una trasmissione affidabile di pacchetti ad applicazioni con vincoli in tempo reale, come la voce. Pertanto, la perdita di pacchetti compromette l'utilit\u00e0 a livello di applicazione. Per la voce questa compromissione dell'utilit\u00e0 \u00e8 duplice: da un lato, anche brevi sequenze di pacchetti persi possono diminuire significativamente la capacit\u00e0 del ricevitore di nascondere la perdita di pacchetti e il segnale vocale in uscita viene interrotto. D'altra parte, alcuni pacchetti possono essere particolarmente sensibili alla perdita poich\u00e9 contengono informazioni pi\u00f9 importanti in termini di percezione dell'utente rispetto ad altri pacchetti. Per prima cosa sviluppiamo un modello end-to-end basato sulle lunghezze di perdita con cui possiamo descrivere la distribuzione delle perdite all'interno di un modello. Queste metriche a livello di pacchetto sono quindi collegate a metriche di qualit\u00e0 vocale oggettiva a livello di utente. Utilizzando questo framework, scopriamo che per codec basati su campioni a bassa compressione -LRB- PCM -RRB- con occultamento delle perdite, le perdite di pacchetti isolate possono essere nascoste bene, mentre le perdite di burst hanno un impatto percettivo maggiore. Per i codec basati su frame ad alta compressione -LRB- G. 729 -RRB- da un lato l'impatto della perdita viene amplificato attraverso la propagazione dell'errore causata dalle memorie del filtro del decodificatore, dall'altro tali schemi di codifica aiutano a eseguire l'occultamento della perdita mediante estrapolazione dello stato del decodificatore. Contrariamente ai codec basati su campioni, dimostriamo che le prestazioni di occultamento possono tuttavia \"rompersi\" nelle transizioni all'interno del segnale vocale. Proponiamo quindi meccanismi che differenziano i pacchetti all'interno di dati vocali per ridurre al minimo l'impatto della perdita di pacchetti. Definiamo questi metodi come recupero e controllo delle perdite. A livello end-to-end ha luogo l'identificazione dei pacchetti sensibili alla perdita -LRB- mittente -RRB- nonch\u00e9 l'occultamento della perdita -LRB- ricevitore -RRB-. Gli schemi di supporto hop-by-hop consentono quindi di -LRB- statisticamente -RRB- scambiare la perdita di un pacchetto, che \u00e8 considerato pi\u00f9 importante, con un altro dello stesso flusso che \u00e8 di minore importanza. Poich\u00e9 entrambi i sistemi richiedono lo stesso costo in termini di trasmissione di rete, \u00e8 possibile ottenere un guadagno nella percezione dell'utente. Abbiamo dimostrato che \u00e8 possibile ottenere miglioramenti significativi nella qualit\u00e0 del parlato e che \u00e8 possibile evitare dati aggiuntivi e ritardi, pur mantenendo un servizio di rete praticamente identico al miglior sforzo a lungo termine. 1. INTRODUZIONE Considerando che in tempo reale pu\u00f2 verificarsi una perdita di pacchetti, l'impatto della perdita pu\u00f2 variare in modo significativo a seconda di quali pacchetti vengono persi all'interno di un flusso. Di seguito distinguiamo due ragioni per una sensibilit\u00e0 alla perdita cos\u00ec variabile: Sensibilit\u00e0 temporale: la cui perdita \u00e8 correlata nel tempo pu\u00f2 portare a interruzioni del servizio. Per la voce, poich\u00e9 un singolo pacchetto contiene tipicamente diversi frame vocali -LRB- -RRB-, questo effetto \u00e8 quindi pi\u00f9 significativo che ad esempio per il video. Si traduce fondamentalmente in perdite di pacchetti isolate rispetto a perdite che si verificano a raffica. Figura 1 :Funzioni di utilit\u00e0 schematiche dipendenti dalla perdita di pacchetti pi\u00f9 o meno importanti -LRB- -1 -RRB- pi\u00f9 importanti per quanto riguarda la percezione dell'utente rispetto ad altri dello stesso flusso. Consideriamo un flusso con due tipi di frame di importanza percettiva molto diversa -LRB-, abbiamo la stessa dimensione, frequenza e nessuna interdipendenza tra i frame -RRB-. In caso di perdita del 50% dei pacchetti, la qualit\u00e0 percettiva varia enormemente tra il luogo in cui viene ricevuto il 50% dei frame con elevata importanza percettiva e il luogo in cui viene ricevuto il 50% dei frame meno importanti. Il supporto di rete per flussi multimediali in tempo reale pu\u00f2 da un lato mirare ad offrire un servizio, che per\u00f2, se implementato all'interno della rete pa&et-switched, risulter\u00e0 costoso per il gestore della rete e quindi per l'utente. D\u2019altro canto, all\u2019interno di un servizio in perdita, occorre tenere conto dei vincoli di sensibilit\u00e0 di cui sopra. Consideriamo ora il caso in cui il 50% dei pacchetti di flusso identificati come pi\u00f9 importanti -LRB- designati da o meno importanti a causa di uno qualsiasi dei vincoli di sensibilit\u00e0 di cui sopra. La Figura 1 a -RRB- mostra una funzione di utilit\u00e0 generica che descrive il livello di qualit\u00e0 del servizio dipendente dalla percentuale di pacchetti persi. Per il traffico multimediale in tempo reale, tale utilit\u00e0 dovrebbe corrispondere alla qualit\u00e0 video/voce percepita. Se l'importanza relativa dei pacchetti non \u00e8 nota al sistema di trasmissione, i tassi di perdita per i pacchetti e -1 sono uguali. A causa della sensibilit\u00e0 troppo proporzionale dei pacchetti alla perdita e della dipendenza delle prestazioni di recupero della perdita finale dai pacchetti, la funzione di utilit\u00e0 diminuisce significativamente in modo non lineare -LRB- approssimato nella figura a tratti funzioni lineari -RRB- con tasso di perdita crescente. La Figura 1 b -RRB- presenta il caso in cui tutti i pacchetti sono protetti a scapito di -1 Il decadimento della funzione di utilit\u00e0 -LRB- per tassi di perdita < 50 % -RRB- \u00e8 ridotto, perch\u00e9 i pacchetti sono protetti e il end-to-end il recupero delle perdite pu\u00f2 quindi operare correttamente in un intervallo pi\u00f9 ampio di tassi di perdita indicati dall'area ombreggiata. Ci\u00f2 si traduce in un discreto degrado dell'utilit\u00e0 dell'applicazione. Si noti che maggiore \u00e8 la non linearit\u00e0 del contributo di utilit\u00e0 dei pacchetti \u00e8 la deviazione -LRB- dalla curva tratteggiata in Fig. 1 a -RRB-, maggiore \u00e8 il potenziale guadagno di utilit\u00e0 quando la protezione \u00e8 abilitata. I risultati relativi all'effettiva utilit\u00e0 della qualit\u00e0 percepita per le applicazioni multimediali mostrano tale comportamento non lineare *. Poich\u00e9 i meccanismi devono essere implementati all'interno della rete -LRB- hopby-hop -RRB- e/o nei sistemi finali -LRB- end-to-end -RRB-, abbiamo un altro asse di classificazione. L'adattamento del mittente all'attuale stato di congestione della rete, uno schema di prevenzione delle perdite LRB, \u00e8 difficile da applicare alla voce. Considerando che i flussi vocali sono molto bassi, il costo relativo di trasmissione delle informazioni di feedback \u00e8 -LRB- rispetto ad esempio a un flusso video -RRB-. Il maggiore per\u00f2la mancanza di un codec \u00e8 veramente scalabile in termini di output e corrispondente qualit\u00e0 percettiva. quando si presuppone la disponibilit\u00e0 di potenza di calcolo, \u00e8 possibile scegliere in modo permanente il codec pi\u00f9 basso senza diminuire effettivamente la qualit\u00e0 percettiva. Per le perdite su base end-to-end, a causa dei vincoli di ritardo in tempo reale, sono stati proposti schemi a ciclo aperto come Forward Error Correction -LRB-FEC-RRB-. Sebbene attraenti perch\u00e9 possono essere utilizzati oggi su Internet, sono anche presentano diversi inconvenienti. La quantit\u00e0 di informazioni ridondanti deve essere adattiva per evitare di sottrarre larghezza di banda ad altri flussi. L'uso della ridondanza ha anche implicazioni sull'adattamento del ritardo -LRB- -LSB- lo -RSB- -RRB- impiegato per de-jitter i pacchetti al ricevitore. Si noti che i tipi di sensibilit\u00e0 alle perdite presentati si applicano anche ai risultati ottenuti che confermano la forma della curva di \"utilit\u00e0 complessiva\" mostrata in Fig. 1, chiaramente le funzioni di utilit\u00e0 del \"sotto\". i flussi e le loro relazioni sono pi\u00f9 complessi e solo approssimativamente additivi. Tabella 1: Probabilit\u00e0 di stato e di transizione calcolate per una traccia Internet end-to-end utilizzando un modello di Markov generale -LRB- terzo ordine -RRB- di Yajnik et. al.. che sono potenziati da meccanismi di recupero delle perdite end-to-end. I meccanismi end-to-end possono ridurre e spostare tali sensibilit\u00e0, ma non possono avvicinarsi alla loro eliminazione. Pertanto in questo lavoro si presuppone che venga scelto il tasso pi\u00f9 basso possibile che fornisca la qualit\u00e0 desiderata. Non viene utilizzato n\u00e9 feedback/adattamento n\u00e9 ridondanza, tuttavia a livello end-to-end ha luogo l'identificazione/contrassegno dei pacchetti sensibili alla perdita -LRB- mittente -RRB- nonch\u00e9 l'occultamento della perdita -LRB- ricevitore -RRB-. Gli schemi di supporto hop-by-hop consentono quindi di scambiare la perdita di un pacchetto, considerato pi\u00f9 importante, con un altro dello stesso flusso che \u00e8 di minore importanza. Utilizziamo effettivi e misuriamo la loro utilit\u00e0 in presenza di perdita di pacchetti utilizzando misurazioni oggettive della qualit\u00e0 vocale. Il documento \u00e8 strutturato come segue: La Sezione 2 introduce le metriche a livello di pacchetto e utente. Utilizziamo questi parametri per descrivere la sensibilit\u00e0 del traffico alla perdita di pacchetti nella sezione 3. La sezione 4 introduce brevemente un algoritmo di gestione delle code che pu\u00f2 essere utilizzato per il controllo della perdita intra-flusso. Nella sezione 5 presentiamo i risultati che documentano le prestazioni dei meccanismi proposti sia a livello end-to-end che by-hop. La sezione 6 conclude il documento.L'uso della ridondanza ha anche implicazioni sull'adattamento del ritardo -LRB- -LSB- lo -RSB- -RRB- impiegato per de-jitter i pacchetti al ricevitore. Si noti che i tipi di sensibilit\u00e0 alle perdite presentati si applicano anche ai risultati ottenuti che confermano la forma della curva di \"utilit\u00e0 complessiva\" mostrata in Fig. 1, chiaramente le funzioni di utilit\u00e0 del \"sotto\". i flussi e le loro relazioni sono pi\u00f9 complessi e solo approssimativamente additivi. Tabella 1: Probabilit\u00e0 di stato e di transizione calcolate per una traccia Internet end-to-end utilizzando un modello di Markov generale -LRB- terzo ordine -RRB- di Yajnik et. al.. che sono potenziati da meccanismi di recupero delle perdite end-to-end. I meccanismi end-to-end possono ridurre e spostare tali sensibilit\u00e0, ma non possono avvicinarsi alla loro eliminazione. Pertanto in questo lavoro si presuppone che venga scelto il tasso pi\u00f9 basso possibile che fornisca la qualit\u00e0 desiderata. Non viene utilizzato n\u00e9 feedback/adattamento n\u00e9 ridondanza, tuttavia a livello end-to-end ha luogo l'identificazione/contrassegno dei pacchetti sensibili alla perdita -LRB- mittente -RRB- nonch\u00e9 l'occultamento della perdita -LRB- ricevitore -RRB-. Gli schemi di supporto hop-by-hop consentono quindi di scambiare la perdita di un pacchetto, considerato pi\u00f9 importante, con un altro dello stesso flusso che \u00e8 di minore importanza. Utilizziamo effettivi e misuriamo la loro utilit\u00e0 in presenza di perdita di pacchetti utilizzando misurazioni oggettive della qualit\u00e0 vocale. Il documento \u00e8 strutturato come segue: La Sezione 2 introduce le metriche a livello di pacchetto e utente. Utilizziamo questi parametri per descrivere la sensibilit\u00e0 del traffico alla perdita di pacchetti nella sezione 3. La sezione 4 introduce brevemente un algoritmo di gestione delle code che pu\u00f2 essere utilizzato per il controllo della perdita intra-flusso. Nella sezione 5 presentiamo i risultati che documentano le prestazioni dei meccanismi proposti sia a livello end-to-end che by-hop. La sezione 6 conclude il documento.L'uso della ridondanza ha anche implicazioni sull'adattamento del ritardo -LRB- -LSB- lo -RSB- -RRB- impiegato per de-jitter i pacchetti al ricevitore. Si noti che i tipi di sensibilit\u00e0 alle perdite presentati si applicano anche ai risultati ottenuti che confermano la forma della curva di \"utilit\u00e0 complessiva\" mostrata in Fig. 1, chiaramente le funzioni di utilit\u00e0 del \"sotto\". i flussi e le loro relazioni sono pi\u00f9 complessi e solo approssimativamente additivi. Tabella 1: Probabilit\u00e0 di stato e di transizione calcolate per una traccia Internet end-to-end utilizzando un modello di Markov generale -LRB- terzo ordine -RRB- di Yajnik et. al.. che sono potenziati da meccanismi di recupero delle perdite end-to-end. I meccanismi end-to-end possono ridurre e spostare tali sensibilit\u00e0, ma non possono avvicinarsi alla loro eliminazione. Pertanto in questo lavoro si presuppone che venga scelto il tasso pi\u00f9 basso possibile che fornisca la qualit\u00e0 desiderata. Non viene utilizzato n\u00e9 feedback/adattamento n\u00e9 ridondanza, tuttavia a livello end-to-end ha luogo l'identificazione/contrassegno dei pacchetti sensibili alla perdita -LRB- mittente -RRB- nonch\u00e9 l'occultamento della perdita -LRB- ricevitore -RRB-. Gli schemi di supporto hop-by-hop consentono quindi di scambiare la perdita di un pacchetto, considerato pi\u00f9 importante, con un altro dello stesso flusso che \u00e8 di minore importanza. Utilizziamo effettivi e misuriamo la loro utilit\u00e0 in presenza di perdita di pacchetti utilizzando misurazioni oggettive della qualit\u00e0 vocale. Il documento \u00e8 strutturato come segue: La Sezione 2 introduce le metriche a livello di pacchetto e utente. Utilizziamo questi parametri per descrivere la sensibilit\u00e0 del traffico alla perdita di pacchetti nella sezione 3. La sezione 4 introduce brevemente un algoritmo di gestione delle code che pu\u00f2 essere utilizzato per il controllo della perdita intra-flusso. Nella sezione 5 presentiamo i risultati che documentano le prestazioni dei meccanismi proposti sia a livello end-to-end che by-hop. La sezione 6 conclude il documento.considerato pi\u00f9 importante, rispetto ad un altro dello stesso flusso che ha minore importanza. Utilizziamo effettivi e misuriamo la loro utilit\u00e0 in presenza di perdita di pacchetti utilizzando misurazioni oggettive della qualit\u00e0 vocale. Il documento \u00e8 strutturato come segue: La Sezione 2 introduce le metriche a livello di pacchetto e utente. Utilizziamo questi parametri per descrivere la sensibilit\u00e0 del traffico alla perdita di pacchetti nella sezione 3. La sezione 4 introduce brevemente un algoritmo di gestione delle code che pu\u00f2 essere utilizzato per il controllo della perdita intra-flusso. Nella sezione 5 presentiamo i risultati che documentano le prestazioni dei meccanismi proposti sia a livello end-to-end che by-hop. La sezione 6 conclude il documento.considerato pi\u00f9 importante, rispetto ad un altro dello stesso flusso che ha minore importanza. Utilizziamo effettivi e misuriamo la loro utilit\u00e0 in presenza di perdita di pacchetti utilizzando misurazioni oggettive della qualit\u00e0 vocale. Il documento \u00e8 strutturato come segue: La Sezione 2 introduce le metriche a livello di pacchetto e utente. Utilizziamo questi parametri per descrivere la sensibilit\u00e0 del traffico alla perdita di pacchetti nella sezione 3. La sezione 4 introduce brevemente un algoritmo di gestione delle code che pu\u00f2 essere utilizzato per il controllo della perdita intra-flusso. Nella sezione 5 presentiamo i risultati che documentano le prestazioni dei meccanismi proposti sia a livello end-to-end che by-hop. La sezione 6 conclude il documento.", "keyphrases": ["modello end-to-end", "codec base campione", "recupero e controllo delle perdite", "sensibilit\u00e0 alla perdita", "supporto di rete per contenuti multimediali in tempo reale", "qualit\u00e0 del servizio", "recupero delle perdite end-to-end", "traffico voip", "controllo delle perdite intraflusso", "metrica a livello di pacchetto", "modello di gener markov", "sensibilit\u00e0 del traffico voip", "algoritmo di gestione della coda", "codec frame-base"]}
{"file_name": "C-22", "text": "Raccolta di metriche di runtime per l'adattamento supportato dal middleware di applicazioni mobili ABSTRACT Questo documento propone, implementa e valuta in termini di prestazioni nel caso peggiore, una strategia di raccolta di metriche online per facilitare l'adattamento dell'applicazione tramite la mobilit\u00e0 degli oggetti utilizzando un framework di oggetti mobili e un middleware di supporto. La soluzione si basa su una rappresentazione astratta del sistema di oggetti mobili, che contiene contenitori che aggregano metriche per ciascun componente specifico inclusi gestori host, runtime e oggetti mobili. Una caratteristica fondamentale della soluzione \u00e8 la specifica di molteplici criteri configurabili per controllare la misurazione e la propagazione dei parametri attraverso il sistema. La piattaforma MobJeX \u00e8 stata utilizzata come base per l'implementazione e il test con una serie di test di laboratorio condotti per misurare la scalabilit\u00e0, l'efficienza e l'applicazione di semplici criteri di misurazione e propagazione per ridurre i costi di raccolta. 1. INTRODUZIONE Un adattamento efficace richiede informazioni dettagliate e aggiornate sia sul sistema che sul software stesso. Le metriche relative alle informazioni a livello di sistema -LRB- ad es. processore, memoria e carico di rete -RRB- sono indicate come metriche ambientali -LSB- 5 -RSB-, mentre le metriche che rappresentano il comportamento dell'applicazione sono indicate come metriche software -LSB- 8 -RSB- . Inoltre, il tipo di metrica richiesta per eseguire l'adattamento dipende dal tipo di adattamento richiesto. Ad esempio, l'adattamento basato sul servizio, in cui la qualit\u00e0 del servizio o il comportamento del servizio vengono modificati in risposta ai cambiamenti nell'ambiente di runtime, generalmente richiede metriche ambientali dettagliate ma solo metriche software semplici -LSB- 4 -RSB-. D'altro canto, l'adattamento tramite la mobilit\u00e0 degli oggetti -LSB- 6 -RSB- richiede anche metriche software dettagliate -LSB- 9 -RSB- poich\u00e9 il posizionamento degli oggetti dipende dalle caratteristiche di esecuzione degli oggetti mobili stessi. Ad eccezione di MobJeX -LSB- 6 -RSB-, i sistemi di oggetti mobili esistenti come Voyager -LSB- 10 -RSB-, FarGo -LSB- 11, 12 -RSB- e JavaParty -LSB- 13 -RSB- non lo fanno forniscono un adattamento automatizzato e pertanto non dispongono del processo di raccolta delle metriche necessario per supportare questo processo. Nel caso di MobJeX, sebbene sia stato implementato un motore di adattamento -LSB- 5 -RSB-, i test preliminari sono stati eseguiti utilizzando metriche sintetiche preimpostate poich\u00e9 c'\u00e8 poco lavoro precedente sulla raccolta dinamica di metriche software nei framework di oggetti mobili, e non esiste alcun mezzo per raccoglierli automaticamente. Di conseguenza, il contributo principale di questo documento \u00e8 una soluzione per la raccolta di metriche dinamiche per supportare l'adattamento tramite la mobilit\u00e0 degli oggetti per le applicazioni mobili. Questo problema non \u00e8 banale poich\u00e9 i tipici framework di oggetti mobili sono costituiti da pi\u00f9 componenti applicativi e middleware e quindi la raccolta delle metriche deve essere eseguita in posizioni diverse e i risultati propagati in modo efficiente al motore di adattamento. Il resto del documento \u00e8 organizzato come segue:La sezione 2 descrive la struttura generale e l'implementazione dei framework di oggetti mobili al fine di comprendere le sfide legate alla raccolta, propagazione e distribuzione dei parametri come descritto nella sezione 3. La sezione 4 descrive alcuni test e risultati iniziali e la sezione 5 si chiude con un riepilogo, conclusioni e discussione del lavoro futuro. 2. BACKGROUND In generale, un'applicazione orientata agli oggetti consiste di oggetti che collaborano per fornire la funzionalit\u00e0 richiesta da un dato dominio problematico. I framework di oggetti mobili consentono ad alcuni di questi oggetti di essere contrassegnati come oggetti mobili, fornendo il supporto middleware per lo spostamento di tali oggetti in fase di esecuzione su altri host. Come minimo, un framework di oggetti mobili con almeno un'applicazione mobile in esecuzione \u00e8 costituito dai seguenti componenti: runtime, oggetti mobili e proxy -LSB- 14 -RSB-, sebbene la terminologia utilizzata dai singoli framework possa differire -LSB- 6, 10-13 -RSB-. Un runtime \u00e8 un processo contenitore per la gestione di oggetti mobili. Ad esempio, in FarGo -LSB- 15 -RSB- questo componente \u00e8 noto come core e nella maggior parte dei sistemi sono necessari runtime separati per consentire l'esecuzione indipendente di diverse applicazioni, sebbene questo non sia il caso di MobJeX, che pu\u00f2 eseguire pi\u00f9 applicazioni in sequenza. un singolo runtime utilizzando i thread. Le applicazioni stesse comprendono oggetti mobili, che interagiscono tra loro tramite proxy -LSB- 14 -RSB-. Durante la migrazione, gli oggetti proxy si spostano con l'oggetto di origine. Il sistema basato su Java MobJeX, utilizzato come piattaforma di implementazione per la soluzione di raccolta delle metriche descritta in questo documento, aggiunge una serie di componenti middleware aggiuntivi. In primo luogo, un gestore host -LRB- noto come servizio in MobJeX -RRB- fornisce un punto centrale di comunicazione eseguendo su una porta conosciuta in base all'host, facilitando cos\u00ec l'enumerazione o la ricerca di componenti come runtime o oggetti mobili. In secondo luogo, MobJeX ha un contenitore di oggetti mobili per applicazione chiamato gestore dei trasporti -LRB- TM -RRB-. Pertanto i gestori host e trasporto sono considerati nella soluzione fornita nella sezione successiva, ma potrebbero essere omessi nel caso generale. Infine, a seconda della modalit\u00e0 di adattamento, MobJeX pu\u00f2 avere un controller di sistema centralizzato che incorpora un motore di adattamento globale per eseguire l'ottimizzazione a livello di sistema. 5. SOMMARIO E CONCLUSIONI Date le sfide legate allo sviluppo di applicazioni mobili che funzionano in ambienti dinamici/eterogenei e il conseguente interesse per l'adattamento delle applicazioni, questo documento ha proposto e implementato una strategia di raccolta di metriche online per assistere tale adattamento utilizzando un framework di oggetti mobili e supportando middleware. Sono stati condotti studi di laboratorio controllati per determinare le prestazioni nel caso peggiore e per mostrare la riduzione dei costi di raccolta quando si applicano criteri di raccolta semplici. Inoltre,ulteriori test hanno fornito un'indicazione iniziale delle caratteristiche degli oggetti applicativi -LRB- basati sul tempo di esecuzione del metodo -RRB- che sarebbero buoni candidati per l'adattamento utilizzando l'implementazione del caso peggiore della strategia di raccolta delle metriche proposta. Una caratteristica fondamentale della soluzione era la specifica di molteplici criteri configurabili per controllare la propagazione dei parametri attraverso il sistema, riducendo cos\u00ec il sovraccarico della raccolta. Inoltre, una tale storia temporale potrebbe anche facilitare decisioni intelligenti riguardanti la raccolta di parametri poich\u00e9, ad esempio, un parametro che \u00e8 noto per essere in gran parte costante non necessita di essere misurato frequentemente. Il lavoro futuro comporter\u00e0 anche la valutazione di un\u2019ampia gamma di scenari di adattamento sul framework MobJeX per quantificare i vantaggi che possono essere ottenuti tramite l\u2019adattamento attraverso la mobilit\u00e0 degli oggetti e quindi dimostrare nella pratica l\u2019efficacia della soluzione descritta in questo documento. Infine, gli autori desiderano esplorare l'applicazione dei concetti di raccolta di metriche descritti in questo articolo a un sistema di gestione del context pi\u00f9 generale e riutilizzabile -LSB- 20 -RSB-.", "keyphrases": ["dati", "applicazione orientata agli oggetti", "quadro di oggetti mobili", "mobjex", "Giava", "metricscontain", "raccolta metrica", "proxi", "performante e scalabile", "misura", "propaganda e consegna", "struttura"]}
{"file_name": "H-26", "text": "Un metodo vettoriale di supporto per ottimizzare la precisione media ABSTRACT L'apprendimento automatico \u00e8 comunemente utilizzato per migliorare i sistemi di recupero classificati. A causa delle difficolt\u00e0 computazionali, sono state sviluppate poche tecniche di apprendimento per ottimizzare direttamente la precisione media media -LRB- MAP -RRB-, nonostante il suo uso diffuso nella valutazione di tali sistemi. Gli approcci esistenti che ottimizzano la MAP non trovano una soluzione ottimale a livello globale o sono computazionalmente costosi. Al contrario, presentiamo un algoritmo di apprendimento SVM generale che trova in modo efficiente una soluzione ottimale a livello globale per un semplice rilassamento della MAP. Valutiamo il nostro approccio utilizzando i corpora TREC 9 e TREC 10 Web Track -LRB- WT10g -RRB-, confrontandoli con SVM ottimizzati per precisione e ROCArea. Nella maggior parte dei casi mostriamo il nostro metodo per produrre miglioramenti statisticamente significativi nei punteggi MAP. 1. INTRODUZIONE I sistemi di recupero delle informazioni pi\u00f9 avanzati utilizzano comunemente tecniche di apprendimento automatico per apprendere le funzioni di classificazione. Tuttavia, la maggior parte degli approcci attuali non ottimizzano la misura di valutazione pi\u00f9 spesso utilizzata, vale a dire la precisione media media -LRB- MAP -RRB-. Invece, gli attuali algoritmi tendono ad adottare uno dei due approcci generali. Il primo approccio \u00e8 apprendere un modello che stima la probabilit\u00e0 che un documento sia rilevante dato che, se risolto in modo efficace, la classifica con la migliore prestazione MAP pu\u00f2 essere facilmente derivata dalle probabilit\u00e0 di rilevanza. Tuttavia, per ottenere un MAP elevato \u00e8 sufficiente trovare un buon ordinamento dei documenti. Di conseguenza, per trovare buone probabilit\u00e0 \u00e8 necessario risolvere un problema pi\u00f9 difficile del necessario, probabilmente richiedendo pi\u00f9 dati di addestramento per ottenere le stesse prestazioni MAP. Il secondo approccio comune \u00e8 apprendere una funzione che massimizza una misura surrogata. Le misure di prestazione ottimizzate includono precisione -LSB- 17, 15 -RSB-, ROCArea -LSB- 1, 5, 10, 11, 13, 21 -RSB- o modifiche di ROCArea -LSB- 4 -RSB- e NDCG -LSB- 2, 3 -RSB-. Imparare un modello da ottimizzare per tali misure potrebbe comportare prestazioni MAP non ottimali. Infatti, sebbene alcuni sistemi precedenti abbiano ottenuto buone prestazioni MAP, \u00e8 noto che n\u00e9 il raggiungimento della precisione ottimale n\u00e9 la ROCArea possono garantire prestazioni MAP ottimali -LSB- 7 -RSB-. In questo articolo presentiamo un approccio generale per l'apprendimento delle funzioni di classificazione che massimizzano le prestazioni della MAP. Nello specifico, presentiamo un algoritmo SVM che ottimizza globalmente il rilassamento della MAP dovuto alla perdita di cerniera. Questo approccio semplifica il processo per ottenere funzioni di classificazione con elevate prestazioni MAP evitando ulteriori passaggi intermedi ed euristiche. Il nuovo algoritmo rende inoltre concettualmente altrettanto semplice ottimizzare le SVM per MAP, come in precedenza era possibile solo per precisione e ROCArea. In contrasto con il recente lavoro di ottimizzazione diretta delle prestazioni MAP di Metzler & Croft -LSB- 16 -RSB- e Caruana et al. -LSB- 6 -RSB-, la nostra tecnica \u00e8 computazionalmente efficiente e trova una soluzione ottimale a livello globale.Descriviamo ora l\u2019algoritmo in dettaglio e diamo prova della correttezza. Successivamente, forniamo un'analisi del tempo di esecuzione. Abbiamo anche sviluppato un pacchetto software che implementa il nostro algoritmo disponibile per l'utente pubblico. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo presentato un metodo SVM che ottimizza direttamente MAP. Fornisce un approccio basato su principi ed evita euristiche difficili da controllare. Abbiamo formulato il problema di ottimizzazione e presentato un algoritmo che trova in modo dimostrabile la soluzione in tempo polinomiale. Abbiamo dimostrato empiricamente che il nostro metodo \u00e8 generalmente superiore o competitivo con i metodi SVM convenzionali. Il nostro nuovo metodo rende concettualmente altrettanto semplice ottimizzare le SVM per MAP, come in precedenza era possibile solo per Accuracy e ROCArea. Poich\u00e9 altri metodi in genere richiedono la messa a punto di pi\u00f9 euristiche, prevediamo anche di addestrare meno modelli prima di trovarne uno che raggiunga buone prestazioni. Il quadro di apprendimento utilizzato dal nostro metodo \u00e8 abbastanza generale.", "keyphrases": ["apprendimento automatico", "sistema di recupero del rango", "imparare la tecnica", "media media esatta", "soluzione ottim", "relax della mappa", "informare il sistema di recupero", "probabile", "misura surrogata", "funzione di perdita", "supervisionare imparare"]}
{"file_name": "H-25", "text": "Feedback sui termini per il recupero delle informazioni con modelli linguistici ABSTRACT In questo articolo feedback basato sul termine occidentale per il recupero delle informazioni nell'approccio di modellazione del linguaggio. Con il feedback sui termini l'utente giudica direttamente la rilevanza dei singoli termini senza interagire con i documenti di feedback, assumendo il pieno controllo del processo di espansione della query. Proponiamo un metodo basato su cluster per selezionare i termini da presentare all'utente per il giudizio, nonch\u00e9 algoritmi efficaci per costruire modelli di linguaggio di query raffinati dal feedback dei termini dell'utente. \u00c8 stato dimostrato che i nostri algoritmi apportano miglioramenti significativi nell'accuratezza del recupero rispetto a una linea di base senza feedback e raggiungono prestazioni paragonabili al feedback sulla pertinenza. Sono utili anche quando in alto non sono presenti documenti rilevanti. 1. INTRODUZIONE Nell'approccio di modellazione del linguaggio al recupero delle informazioni, il feedback \u00e8 spesso modellato come stima di un modello di query migliorato o di un modello di rilevanza basato su una serie di documenti di feedback -LSB- 25, 13 -RSB-. Ci\u00f2 \u00e8 in linea con il modo tradizionale di fornire feedback sulla pertinenza: presentare a un utente documenti/passaggi per un giudizio di pertinenza e quindi estrarre termini dai documenti o passaggi giudicati per espandere la query iniziale. \u00c8 un modo indiretto di cercare assistenza da parte dell'utente per la costruzione del modello di query, nel senso che il modello di query raffinato -LRB- basato su termini -RRB- viene appreso attraverso documenti/passaggi di feedback, che sono strutture di termini di alto livello. Ha lo svantaggio che i termini irrilevanti, che compaiono insieme a quelli rilevanti nel contenuto giudicato, possono essere utilizzati erroneamente per l'espansione della query, causando effetti indesiderati. Ad esempio, per la query TREC \"Risultati del telescopio Hubble\", quando un documento pertinente parla pi\u00f9 della riparazione del telescopio che delle sue scoperte, \u00e8 possibile aggiungere termini irrilevanti come \"passeggiata nello spazio\" nella query modificata. Possiamo considerare un modo pi\u00f9 diretto per coinvolgere un utente nel miglioramento del modello di query, senza un passaggio intermedio di feedback del documento che possa introdurre rumore. L'idea \u00e8 di presentare all'utente un numero ragionevole -RRB- di termini individuali e chiedergli di giudicare la rilevanza di ciascun termine o di specificare direttamente le loro probabilit\u00e0 nel modello di query. Rispetto al tradizionale feedback sulla pertinenza, questo approccio basato sui termini per il perfezionamento del modello di query interattivo presenta numerosi vantaggi. Innanzitutto, l'utente ha un migliore controllo del modello di query finale attraverso la manipolazione diretta dei termini: pu\u00f2 dettare quali termini sono rilevanti, irrilevanti ed eventualmente in quale misura. Ci\u00f2 evita il rischio di introdurre termini indesiderati nel modello di query, anche se a volte l'utente introduce termini di bassa qualit\u00e0. Ci\u00f2 \u00e8 particolarmente utile per la ricerca interattiva ad hoc. In questo caso, il feedback sulla pertinenza \u00e8 inutile, poich\u00e9 non \u00e8 possibile sfruttare alcun documento rilevante, ma il feedback sui termini \u00e8 spesso comunque utile, poich\u00e9 consente di selezionare termini rilevanti da documenti irrilevanti.Durante la nostra partecipazione al TREC 2005 HARD Track e lo studio continuato successivamente, abbiamo esplorato come sfruttare il feedback sui termini da parte dell'utente per costruire modelli di query migliorati per il recupero delle informazioni nell'approccio di modellazione del linguaggio. Abbiamo identificato due sottoattivit\u00e0 chiave del feedback basato sui termini, ovvero la selezione dei termini di presentazione prima del feedback e la costruzione del modello di query post-feedback, con algoritmi efficaci sviluppati per entrambi. Abbiamo imposto una struttura di cluster secondaria sui termini e abbiamo scoperto che una visualizzazione di cluster fornisce ulteriori informazioni sulle esigenze informative dell'utente e fornisce un buon modo di utilizzare il feedback sui termini. Attraverso gli esperimenti abbiamo scoperto che il feedback sui termini migliora significativamente rispetto al livello di base senza feedback, anche se l'utente spesso commette errori nel giudizio di pertinenza. Tra i nostri algoritmi, quello con le migliori prestazioni di recupero \u00e8 TCFB, la combinazione di TFB, l'algoritmo di feedback a termine diretto, e CFB, l'algoritmo di feedback basato su cluster. Abbiamo anche variato il numero di termini di feedback e osservato un miglioramento ragionevole anche con numeri bassi. Infine, confrontando il feedback sui termini con il feedback a livello di documento, abbiamo riscontrato che \u00e8 una valida alternativa a quest'ultimo con prestazioni di recupero competitive. Il resto del lavoro \u00e8 organizzato come segue. La sezione 2 discute alcuni lavori correlati. La sezione 4 delinea il nostro approccio generale al termine feedback. Presentiamo il nostro metodo per la selezione dei termini di presentazione nella Sezione 3 e gli algoritmi per la costruzione del modello di query nella Sezione 5. I risultati dell'esperimento sono forniti nella Sezione 6. La Sezione 7 conclude questo articolo. 2. LAVORI CORRELATI Il feedback sulla pertinenza -LSB- 17, 19 -RSB- \u00e8 stato a lungo riconosciuto come un metodo efficace per migliorare le prestazioni di recupero. Normalmente, i primi N documenti recuperati utilizzando la query originale vengono presentati all'utente per il giudizio, dopodich\u00e9 i termini vengono estratti dai documenti rilevanti giudicati, ponderati in base al loro potenziale di attrarre documenti pi\u00f9 rilevanti e aggiunti al modello di query. La query espansa di solito rappresenta le esigenze di informazioni dell'utente meglio di quella originale, che spesso \u00e8 solo una breve query con parola chiave. Una seconda iterazione di recupero utilizzando questa query modificata produce solitamente un aumento significativo nella precisione del recupero. Nei casi in cui il giudizio sulla reale rilevanza non \u00e8 disponibile e si presuppone che tutti i primi N documenti siano rilevanti, si parla di feedback cieco o pseudo-feedback -LSB- 5, 16 -RSB- e di solito porta comunque un miglioramento delle prestazioni. Poich\u00e9 il documento \u00e8 un'unit\u00e0 di text di grandi dimensioni, quando viene utilizzato per il feedback sulla pertinenza \u00e8 possibile introdurre molti termini irrilevanti nel processo di feedback. Per superare questo problema, viene proposto e dimostrato che il feedback del passaggio migliora le prestazioni del feedback -LSB- 1, 23 -RSB-. Una soluzione pi\u00f9 diretta \u00e8 chiedere all'utente il suo giudizio di pertinenza sui termini di feedback. Ad esempio, in alcuni sistemi di feedback rilevanti come -LSB- 12 -RSB-,esiste una fase di interazione che consente all'utente di aggiungere o rimuovere termini di espansione dopo che sono stati estratti automaticamente dai documenti pertinenti. In molti casi \u00e8 stato riscontrato che il feedback sulla pertinenza dei termini migliora efficacemente le prestazioni di recupero -LSB- 6, 22, 12, 4, 10 -RSB-. Ad esempio, lo studio in -LSB- 12 -RSB- mostra che l'utente preferisce avere una conoscenza esplicita e un controllo diretto di quali termini vengono utilizzati per l'espansione della query e l'interfaccia penetrabile che fornisce questa libert\u00e0 ha dimostrato di funzionare meglio di altre interfacce . Tuttavia, in alcuni altri casi non vi \u00e8 alcun vantaggio significativo -LSB- 3, 14 -RSB-, anche se all'utente piace interagire con i termini di espansione. Si scopre che l'utente non \u00e8 bravo a identificare termini utili per l'espansione della query, quando una semplice interfaccia di presentazione dei termini non \u00e8 in grado di fornire un context semantico sufficiente dei termini di feedback. Il nostro lavoro differisce dai precedenti in due aspetti importanti. Il modo usuale per la presentazione dei termini di feedback \u00e8 semplicemente visualizzare i termini in un elenco. Sono stati fatti alcuni lavori su interfacce utente alternative. In entrambi gli studi, tuttavia, non vi \u00e8 alcuna differenza significativa nelle prestazioni. Nel nostro lavoro adottiamo l'approccio pi\u00f9 semplice di termini + caselle di controllo. Ci concentriamo sulla presentazione dei termini e sulla costruzione del modello di query a partire dai termini di feedback e riteniamo che l'utilizzo di contesti per migliorare la qualit\u00e0 dei termini di feedback dovrebbe essere ortogonale al nostro metodo. 7. CONCLUSIONI In questo articolo abbiamo studiato l'uso del termine feedback per il recupero interattivo delle informazioni nell'approccio di modellazione del linguaggio. Abbiamo proposto un metodo basato su cluster per selezionare i termini di presentazione e algoritmi per stimare modelli di query raffinati dal feedback dei termini degli utenti. Abbiamo riscontrato un miglioramento significativo nell'accuratezza del recupero apportato dal feedback dei termini, nonostante il fatto che un utente spesso commetta errori nel giudizio di pertinenza che incidono sulle sue prestazioni. Abbiamo riscontrato che l'algoritmo con le migliori prestazioni \u00e8 TCFB, che beneficia della combinazione di prove di termini osservati direttamente con TFB e di rilevanza del cluster appresa indirettamente con CFB. Anche quando abbiamo ridotto il numero di termini di presentazione, il feedback dei termini \u00e8 ancora in grado di mantenere gran parte del miglioramento delle prestazioni rispetto al livello di base. Infine, abbiamo confrontato il feedback sui termini con il feedback sulla pertinenza a livello di documento e abbiamo scoperto che le prestazioni di TCFB3C sono alla pari con quest'ultimo con 5 documenti di feedback. Abbiamo considerato il feedback sui termini come una valida alternativa al tradizionale feedback sulla pertinenza, soprattutto quando non sono presenti documenti rilevanti in alto. Proponiamo di estendere il nostro lavoro in diversi modi. Innanzitutto, vogliamo studiare se l'uso di vari contesti pu\u00f2 aiutare l'utente a identificare meglio la pertinenza dei termini, senza sacrificare la semplicit\u00e0 e la compattezza del feedback dei termini. In secondo luogo, attualmente tutti i termini vengono presentati all'utente in un unico batch. Potremmo invece considerare il feedback iterativo sui termini, presentando prima un numero limitato di termini e mostrandone altri dopo aver ricevuto il feedback degli utenti o fermandoci quando la query perfezionata \u00e8 sufficientemente buona.I termini presentati dovrebbero essere selezionati dinamicamente per massimizzare i benefici dell'apprendimento in qualsiasi momento. In terzo luogo, abbiamo in programma di incorporare il feedback sui termini nella nostra barra degli strumenti UCAIR -LSB- 20 -RSB-, un plug-in di Internet Explorer, per farlo funzionare per la ricerca sul web. Siamo anche interessati a studiare come combinare il feedback sui termini con il feedback sulla pertinenza o il feedback implicito. Potremmo, ad esempio, consentire all'utente di modificare dinamicamente i termini in un modello linguistico appreso dai documenti di feedback.", "keyphrases": ["feedback sulla base dei termini", "informare il recupero", "modello linguistico", "queri espande il processo", "modello queri", "interagire con la ricerca ad hoc", "recuperare eseguire", "probabile", "kl-diverg", "termine attuale"]}
{"file_name": "H-9", "text": "Imparare dai registri di ricerca sul Web per organizzare i risultati della ricerca ABSTRACT Un'organizzazione efficace dei risultati della ricerca \u00e8 fondamentale per migliorare l'utilit\u00e0 di qualsiasi motore di ricerca. Il clustering dei risultati della ricerca \u00e8 un modo efficace per organizzare i risultati della ricerca, che consente a un utente di spostarsi rapidamente nei documenti pertinenti. Tuttavia, due carenze di questo approccio fanno s\u00ec che non sempre funzioni bene: -LRB- 1 -RRB- i cluster scoperti non corrispondono necessariamente agli aspetti interessanti di un argomento dal punto di vista dell'utente; e -LRB- 2 -RRB- le etichette dei cluster generate non sono sufficientemente informative per consentire all'utente di identificare il cluster giusto. In questo articolo, proponiamo di affrontare queste due carenze -LRB- 1 -RRB- imparando gli \"aspetti interessanti\" di un argomento dai log di ricerca sul Web e organizzando di conseguenza i risultati della ricerca; e -LRB- 2 -RRB- che genera etichette di cluster pi\u00f9 significative utilizzando le parole di query passate immesse dagli utenti. Valutiamo il nostro metodo proposto sui dati di registro di un motore di ricerca commerciale. Rispetto ai metodi tradizionali di raggruppamento dei risultati di ricerca, il nostro metodo pu\u00f2 fornire una migliore organizzazione dei risultati ed etichette pi\u00f9 significative. 1. INTRODUZIONE L'utilit\u00e0 di un motore di ricerca \u00e8 influenzata da molteplici fattori. Sebbene il fattore principale sia la solidit\u00e0 del modello di recupero sottostante e della funzione di classificazione, anche il modo in cui organizzare e presentare i risultati della ricerca \u00e8 un fattore molto importante che pu\u00f2 influenzare in modo significativo l'utilit\u00e0 di un motore di ricerca. Rispetto alla grande quantit\u00e0 di letteratura sui modelli di recupero, tuttavia, c\u2019\u00e8 relativamente poca ricerca su come migliorare l\u2019efficacia dell\u2019organizzazione dei risultati di ricerca. La strategia pi\u00f9 comune per presentare i risultati di ricerca \u00e8 un semplice elenco classificato. Intuitivamente, una tale strategia di presentazione \u00e8 ragionevole per risultati di ricerca non ambigui e omogenei; in generale, funzionerebbe bene quando i risultati della ricerca sono buoni e un utente pu\u00f2 facilmente trovare molti documenti rilevanti nei primi risultati in classifica. In questi esempi, una visualizzazione raggruppata dei risultati della ricerca sarebbe molto pi\u00f9 utile per un utente rispetto a un semplice elenco classificato. Il clustering \u00e8 utile anche quando i risultati della ricerca sono scadenti, nel qual caso un utente dovrebbe altrimenti scorrere un lungo elenco in sequenza per raggiungere il primo documento rilevante. Come strategia alternativa primaria per la presentazione dei risultati della ricerca, il clustering dei risultati della ricerca \u00e8 stato studiato in modo relativamente approfondito -LSB- 9, 15, 26, 27, 28 -RSB-. L'idea generale praticamente in tutto il lavoro esistente \u00e8 quella di eseguire il clustering su un insieme di risultati di ricerca in cima alla classifica per suddividere i risultati in cluster naturali, che spesso corrispondono a diversi sottoargomenti dell'argomento di ricerca generale. Verr\u00e0 generata un'etichetta per indicare di cosa tratta ciascun cluster. Un utente pu\u00f2 quindi visualizzare le etichette per decidere in quale cluster esaminare. Tuttavia, questa strategia di clustering presenta due carenze che la rendono non sempre efficace: in primo luogo,i cluster scoperti in questo modo non corrispondono necessariamente agli aspetti interessanti di un argomento dal punto di vista dell'utente. Ma i cluster scoperti con i metodi attuali possono suddividere i risultati in \u201ccodici locali\u201d e \u201ccodici internazionali\u201d. '' Tali cluster non sarebbero molto utili per gli utenti ; anche il miglior cluster avrebbe comunque una bassa precisione. In secondo luogo, le etichette dei cluster generate non sono sufficientemente informative da consentire all'utente di identificare il cluster corretto. Ci sono due ragioni per questo problema: -LRB- 1 -RRB- I cluster non corrispondono agli interessi dell'utente, quindi le loro etichette non sarebbero molto significative o utili. Ad esempio, la query ambigua \"giaguaro\" pu\u00f2 significare un animale o un'auto. Un ammasso pu\u00f2 essere etichettato come \"panthera onca\". In questo articolo proponiamo una strategia diversa per il partizionamento dei risultati di ricerca, che affronta queste due carenze imponendo un partizionamento dei risultati di ricerca orientato all'utente. Cerchiamo cio\u00e8 di capire quali aspetti di un argomento di ricerca sono probabilmente interessanti per un utente e di organizzare i risultati di conseguenza. Nello specifico, proponiamo di fare quanto segue: in primo luogo, impareremo gli \"aspetti interessanti\" di argomenti simili dai registri di ricerca e organizzeremo i risultati della ricerca in base a questi \"aspetti interessanti\". Ad esempio, se la query corrente si \u00e8 ripetuta pi\u00f9 volte nei log di ricerca, possiamo vedere quali tipi di pagine visualizzate dagli utenti nei risultati e che tipo di parole vengono utilizzate insieme a tale query. Nel caso in cui la query sia ambigua, come \"jaguar\", possiamo aspettarci di vedere alcuni gruppi chiari corrispondenti a diversi sensi di \"jaguar\". Tali aspetti possono essere molto utili per organizzare i futuri risultati di ricerca su \"auto\". In secondo luogo, genereremo etichette di cluster pi\u00f9 significative utilizzando le parole di query passate immesse dagli utenti. Pertanto possono essere etichette migliori di quelle estratte dai normali contenuti dei risultati di ricerca. Per implementare le idee presentate sopra, ci affidiamo ai log dei motori di ricerca e creiamo una raccolta cronologica contenente le query passate e i clic associati. Data una nuova query, troviamo le query passate correlate dalla raccolta della cronologia e apprendiamo gli aspetti applicando l'algoritmo di clustering a stella -LSB- 2 -RSB- a queste query e clic passati. Possiamo quindi organizzare i risultati della ricerca in questi aspetti utilizzando tecniche di categorizzazione ed etichettare ciascun aspetto in base alla query passata pi\u00f9 rappresentativa nel cluster di query. Valutiamo il nostro metodo per l'organizzazione dei risultati utilizzando i log di un motore di ricerca commerciale. Confrontiamo il nostro metodo con il posizionamento predefinito nei motori di ricerca e il tradizionale raggruppamento dei risultati di ricerca. I risultati mostrano che il nostro metodo \u00e8 efficace per migliorare l'utilit\u00e0 di ricerca e le etichette generate utilizzando le parole di query passate sono pi\u00f9 leggibili di quelle generate utilizzando i tradizionali approcci di clustering. Il resto del lavoro \u00e8 organizzato come segue.Per prima cosa esamineremo il lavoro correlato nella Sezione 2. Nella Sezione 3, descriviamo i dati di registro dei motori di ricerca e la nostra procedura per creare una raccolta di cronologia. Nella Sezione 4 presentiamo il nostro approccio in dettaglio. Descriviamo il set di dati nella Sezione 5 e i risultati sperimentali sono discussi nella Sezione 6. Infine, concludiamo il nostro articolo e discutiamo il lavoro futuro nella Sezione 7. 2. LAVORO CORRELATO Il nostro lavoro \u00e8 strettamente correlato allo studio dei risultati di ricerca in cluster. In -LSB- 9, 15 -RSB-, gli autori hanno utilizzato l'algoritmo Scatter/Gather per raggruppare i principali documenti restituiti da un sistema di recupero delle informazioni tradizionale. I loro risultati convalidano l\u2019ipotesi del cluster -LSB- 20 -RSB- secondo cui i documenti rilevanti tendono a formare cluster. In questi articoli gli autori hanno proposto di raggruppare i risultati di un vero e proprio motore di ricerca in base agli snippet o al contenuto dei documenti restituiti. Vengono confrontati diversi algoritmi di clustering e l'algoritmo Suffix Tree Clustering -LRB- STC -RRB- si \u00e8 dimostrato il pi\u00f9 efficace. Hanno anche dimostrato che l\u2019utilizzo di frammenti \u00e8 efficace quanto l\u2019utilizzo di interi documenti. Tuttavia, una sfida importante del clustering di documenti \u00e8 generare etichette significative per i cluster. Per superare questa difficolt\u00e0, in -LSB- 28 -RSB-, sono stati studiati algoritmi di apprendimento supervisionato per estrarre frasi significative dagli snippet dei risultati di ricerca e queste frasi sono state poi utilizzate per raggruppare i risultati di ricerca. In -LSB- 13 -RSB-, gli autori hanno proposto di utilizzare un algoritmo di clustering monotetico, in cui un documento viene assegnato a un cluster in base a una singola caratteristica, per organizzare i risultati della ricerca, e la singola caratteristica viene utilizzata per etichettare il cluster corrispondente . Il clustering dei risultati di ricerca ha attirato molta attenzione anche nell'industria e nei servizi Web commerciali come Vivisimo -LSB- 22 -RSB-. Tuttavia, in tutti questi lavori, i cluster vengono generati esclusivamente in base ai risultati della ricerca. Pertanto i cluster ottenuti non riflettono necessariamente le preferenze degli utenti e le etichette generate potrebbero non essere informative dal punto di vista dell'utente. I metodi di organizzazione dei risultati della ricerca basati sulla categorizzazione del text sono studiati in -LSB- 6, 8 -RSB-. In questo lavoro, un classificatore di text viene addestrato utilizzando una directory Web e i risultati della ricerca vengono quindi classificati in categorie predefinite. Gli autori hanno progettato e studiato diverse interfacce di categoria e hanno scoperto che le interfacce di categoria sono pi\u00f9 efficaci delle interfacce di elenco. Tuttavia, le categorie predefinite sono spesso troppo generiche per riflettere gli aspetti di granularit\u00e0 pi\u00f9 precisa di una query. In passato i log di ricerca sono stati sfruttati per diversi scopi. Ad esempio, il raggruppamento delle query di ricerca per trovare le domande frequenti -LRB- FAQ -RRB- \u00e8 studiato in -LSB- 24, 4 -RSB-. Nel nostro lavoro esploriamo la cronologia delle query passate per organizzare meglio i risultati della ricerca per le query future. Utilizziamo l'algoritmo di star clustering -LSB- 2 -RSB-, che \u00e8 un approccio basato sulla partizione del grafico, per apprendere aspetti interessanti dai log di ricerca data una nuova query. 7.CONCLUSIONI E LAVORO FUTURO In questo articolo abbiamo studiato il problema dell'organizzazione dei risultati di ricerca in modo orientato all'utente. Per raggiungere questo obiettivo ci affidiamo ai log dei motori di ricerca per apprendere aspetti interessanti dal punto di vista degli utenti. Data una query, recuperiamo le query correlate dalla cronologia delle query passate, apprendiamo gli aspetti raggruppando le query passate e le informazioni sui clic associate e classifichiamo i risultati della ricerca in base agli aspetti appresi. Abbiamo confrontato il nostro metodo basato su log con il tradizionale metodo basato su cluster e con la linea di base del posizionamento nei motori di ricerca. Gli esperimenti mostrano che il nostro metodo basato su log pu\u00f2 costantemente sovraperformare il metodo basato su cluster e migliorare rispetto alla linea di base della classifica, soprattutto quando le query sono difficili o i risultati della ricerca sono diversi. Inoltre, il nostro metodo basato su log pu\u00f2 generare etichette di aspetto pi\u00f9 significative rispetto alle etichette di cluster generate in base ai risultati di ricerca quando raggruppiamo i risultati di ricerca. Esistono diverse direzioni interessanti per estendere ulteriormente il nostro lavoro: in primo luogo, sebbene i risultati dei nostri esperimenti abbiano chiaramente mostrato la promessa dell'idea di imparare dai log di ricerca per organizzare i risultati di ricerca, i metodi che abbiamo sperimentato sono relativamente semplici. Sarebbe interessante esplorare altri metodi potenzialmente pi\u00f9 efficaci. In particolare, speriamo di sviluppare modelli probabilistici per gli aspetti di apprendimento e di organizzazione dei risultati simultaneamente. In secondo luogo, con il modo proposto di organizzare i risultati della ricerca, possiamo aspettarci di ottenere informazioni di feedback informative da un utente -LRB-, ad esempio l'aspetto scelto da un utente per visualizzare -RRB-. Sarebbe quindi interessante studiare come migliorare ulteriormente l'organizzazione dei risultati sulla base di tali informazioni di feedback. Infine, possiamo abbinare un log di ricerca generale con un eventuale log di ricerca personale per personalizzare e ottimizzare l'organizzazione dei risultati di ricerca per ogni singolo utente.sebbene i risultati dei nostri esperimenti abbiano chiaramente mostrato la promessa dell'idea di imparare dai log di ricerca per organizzare i risultati di ricerca, i metodi che abbiamo sperimentato sono relativamente semplici. Sarebbe interessante esplorare altri metodi potenzialmente pi\u00f9 efficaci. In particolare, speriamo di sviluppare modelli probabilistici per gli aspetti di apprendimento e di organizzazione dei risultati simultaneamente. In secondo luogo, con il modo proposto di organizzare i risultati della ricerca, possiamo aspettarci di ottenere informazioni di feedback informative da un utente -LRB-, ad esempio l'aspetto scelto da un utente per visualizzare -RRB-. Sarebbe quindi interessante studiare come migliorare ulteriormente l'organizzazione dei risultati sulla base di tali informazioni di feedback. Infine, possiamo combinare un registro di ricerca generale con un eventuale registro di ricerca personale per personalizzare e ottimizzare l'organizzazione dei risultati di ricerca per ogni singolo utente.sebbene i risultati dei nostri esperimenti abbiano chiaramente mostrato la promessa dell'idea di imparare dai log di ricerca per organizzare i risultati di ricerca, i metodi che abbiamo sperimentato sono relativamente semplici. Sarebbe interessante esplorare altri metodi potenzialmente pi\u00f9 efficaci. In particolare, speriamo di sviluppare modelli probabilistici per gli aspetti di apprendimento e di organizzazione dei risultati simultaneamente. In secondo luogo, con il modo proposto di organizzare i risultati della ricerca, possiamo aspettarci di ottenere informazioni di feedback informative da un utente -LRB-, ad esempio l'aspetto scelto da un utente per visualizzare -RRB-. Sarebbe quindi interessante studiare come migliorare ulteriormente l'organizzazione dei risultati sulla base di tali informazioni di feedback. Infine, possiamo combinare un registro di ricerca generale con un eventuale registro di ricerca personale per personalizzare e ottimizzare l'organizzazione dei risultati di ricerca per ogni singolo utente.", "keyphrases": ["modello di recupero", "funzione di rango", "ambiguo", "vista a grappolo", "che significa etichetta del cluster", "raccolta storica", "domande passate", "clicca attraverso", "Algoritmo degli ammassi stellari", "Algoritmo del cluster dell'albero dei suffissi", "snippet del risultato della ricerca", "Algoritmo del cluster monoteto", "pseudo-documento", "\u00e8 un grafico simile", "parametro di soglia simile", "metodo della base del baricentro", "coseno simile", "prototipo del baricentro", "rango reciproco", "metodo basato su log", "media media esatta"]}
{"file_name": "H-5", "text": "Distillazione dell'informazione basata sull'utilit\u00e0 su documenti sequenziati temporalmente ABSTRACT Questo articolo esamina un nuovo approccio alla distillazione dell'informazione su documenti ordinati temporalmente e propone un nuovo schema di valutazione per tale quadro. Combina i punti di forza e si estende oltre il filtraggio adattivo convenzionale, il rilevamento delle novit\u00e0 e la classificazione dei passaggi non ridondanti rispetto alle esigenze informative di lunga durata -LRB- \"compiti\" con query multiple -RRB-. Il nostro approccio supporta il feedback dettagliato degli utenti evidenziando parti arbitrarie di text e sfrutta tali informazioni per l'ottimizzazione dell'utilit\u00e0 in contesti adattivi. Per i nostri esperimenti, abbiamo definito compiti ipotetici basati su eventi di notizie nel corpus TDT4, con pi\u00f9 query per attivit\u00e0. Per ogni query sono state generate chiavi di risposta -LRB- nuggets -RRB- ed \u00e8 stata utilizzata una procedura semiautomatica per l'acquisizione di regole che consentono di abbinare automaticamente i nugget alle risposte del sistema. Proponiamo inoltre un'estensione della metrica NDCG per valutare l'utilit\u00e0 dei passaggi classificati come combinazione di rilevanza e novit\u00e0. I nostri risultati mostrano incoraggianti miglioramenti dell\u2019utilit\u00e0 utilizzando il nuovo approccio, rispetto ai sistemi di base senza apprendimento incrementale o componenti di rilevamento delle novit\u00e0. 1. INTRODUZIONE Il tracciamento di informazioni nuove e rilevanti da flussi di dati temporali per utenti con esigenze di lunga durata \u00e8 stato un argomento di ricerca impegnativo nel recupero delle informazioni. Il filtraggio adattivo -LRB- AF -RRB- \u00e8 uno di questi compiti di previsione online della rilevanza di ogni nuovo documento rispetto ad argomenti predefiniti. Sulla base della query iniziale e di alcuni esempi positivi -LRB- se disponibili -RRB-, un sistema AF mantiene un profilo per ciascun argomento di interesse e lo aggiorna costantemente in base al feedback dell'utente. Nonostante i risultati sostanziali ottenuti nelle recenti ricerche sul filtraggio adattivo, restano irrisolti problemi significativi su come sfruttare il feedback degli utenti in modo efficace ed efficiente. Nello specifico, i seguenti problemi possono limitare seriamente la reale utilit\u00e0 dei sistemi AF nelle applicazioni del mondo reale: impostazione del filtro adattivo: l'utente reagisce al sistema solo quando il sistema prende una decisione \"s\u00ec\" su un documento, confermando o rifiutando quella decisione. Un'alternativa pi\u00f9 \"attiva\" sarebbe quella di consentire all'utente di formulare pi\u00f9 query per un argomento, rivedere un elenco classificato di documenti candidati -LRB- o passaggi -RRB- per query e fornire feedback sull'elenco classificato, perfezionando cos\u00ec le proprie informazioni necessit\u00e0 e richiedendo graduatorie aggiornate. Quest'ultima forma di interazione con l'utente si \u00e8 rivelata molto efficace nel recupero standard di query ad hoc. Come implementare tale strategia per esigenze informative di lunga durata in contesti di FA \u00e8 una questione aperta per la ricerca. 2. Tuttavia, un utente reale potrebbe essere disposto a fornire feedback pi\u00f9 informativi e dettagliati evidenziando alcune parti di text in un documento recuperato come rilevanti, invece di etichettare l'intero documento come rilevante.Sfruttare in modo efficace un feedback cos\u00ec dettagliato potrebbe migliorare sostanzialmente la qualit\u00e0 di un sistema AF. Per questo, dobbiamo abilitare l\u2019apprendimento supervisionato da pezzi di text etichettati di ampiezza arbitraria invece di consentire solo documenti etichettati. 3. I documenti selezionati dal sistema sono spesso altamente ridondanti. Un sistema AF convenzionale selezionerebbe tutte queste notizie ridondanti per il feedback dell'utente, facendogli perdere tempo e offrendo poco guadagno. Chiaramente, le tecniche per il rilevamento delle novit\u00e0 possono aiutare in linea di principio -LSB- 25, 2, 22 -RSB- a migliorare l'utilit\u00e0 dei sistemi AF. Tuttavia, l'efficacia di tali tecniche a livello di passaggio per rilevare la novit\u00e0 rispetto al feedback -LRB- a grana fine -RRB- dell'utente e per rilevare la ridondanza nelle liste classificate resta da valutare utilizzando una misura di utilit\u00e0 che imita le esigenze di un utente reale. Chiamiamo il nuovo processo distillazione delle informazioni basata sull'utilit\u00e0. Si noti che i corpora di benchmark convenzionali per le valutazioni AF, che hanno giudizi di pertinenza a livello di documento e non definiscono compiti con query multiple, sono insufficienti per valutare il nuovo approccio. Pertanto, abbiamo esteso un corpus di riferimento, la raccolta TDT4 di notizie e trasmissioni televisive, con definizioni di attivit\u00e0, query multiple per attivit\u00e0 e chiavi di risposta per query. Abbiamo condotto i nostri esperimenti su questo corpus TDT4 esteso e abbiamo reso pubblicamente disponibili i dati generati in aggiunta per future valutazioni comparative 1. Per valutare automaticamente le porzioni arbitrarie di text restituite dal sistema utilizzando le nostre chiavi di risposta, abbiamo ulteriormente sviluppato uno schema di valutazione con semi- procedura automatica per l'acquisizione di regole che possono abbinare i nugget alle risposte del sistema. Inoltre, proponiamo un'estensione di NDCG -LRB- Normalized Discounted Cumulated Gain -RRB- -LSB- 9 -RSB- per valutare l'utilit\u00e0 dei passaggi classificati in funzione sia della rilevanza che della novit\u00e0. La sezione 2 delinea il processo di distillazione delle informazioni con un esempio concreto. La sezione 3 descrive i nuclei tecnici del nostro sistema chiamato CAF \u00b4 E -- CMU Adaptive Filtering Engine. La sezione 4 discute le questioni relative alla metodologia di valutazione e propone un nuovo schema. La sezione 5 descrive il corpus TDT4 esteso. La sezione 6 presenta i nostri esperimenti e risultati. La sezione 7 conclude lo studio e offre prospettive future. 7. OSSERVAZIONI CONCLUSIVE Questo articolo presenta la prima indagine sulla distillazione delle informazioni basata sull'utilit\u00e0 con un sistema che apprende le esigenze informative a lungo termine dal feedback dettagliato degli utenti su una sequenza di passaggi classificati. Il nostro sistema, chiamato CAF \u00b4 E, combina il filtraggio adattivo, il rilevamento delle novit\u00e0 e la classificazione dei passaggi antiridondanti in un quadro unificato per l'ottimizzazione delle utilit\u00e0. Abbiamo sviluppato un nuovo schema per la valutazione e il feedback automatizzati basato su una procedura semiautomatica per l'acquisizione di regole che consentono di abbinare automaticamente i nugget alle risposte del sistema.Abbiamo anche proposto un\u2019estensione della metrica NDCG per valutare l\u2019utilit\u00e0 dei passaggi classificati come combinazione ponderata di rilevanza e novit\u00e0. I nostri esperimenti sul corpus di benchmark TDT4 appena annotato mostrano un incoraggiante miglioramento dell'utilit\u00e0 rispetto a Indri e anche rispetto al nostro sistema con l'apprendimento incrementale e il rilevamento delle novit\u00e0 disattivati.", "keyphrases": ["util-base informa distill", "documento d'ordine temporaneo", "rango di passaggio", "adattare il filtro", "recupero ad hoc", "novit\u00e0 rilevate", "nuovo metodo di valutazione", "rispondi kei", "regola di corrispondenza delle pepite", "quadro unificato", "metrica ndcg"]}
{"file_name": "C-8", "text": "Context operativo e trasformazione operativa basata sul context ABSTRACT La trasformazione operativa -LRB- OT -RRB- \u00e8 una tecnica per il mantenimento della coerenza e l'annullamento del gruppo e viene applicata a un numero crescente di applicazioni collaborative. Il fondamento teorico dell'OT \u00e8 cruciale nel determinare la sua capacit\u00e0 di risolvere problemi esistenti e nuovi, nonch\u00e9 la qualit\u00e0 di tali soluzioni. La teoria della causalit\u00e0 \u00e8 stata il fondamento di tutti i precedenti sistemi OT, ma \u00e8 inadeguata a catturare i requisiti essenziali di correttezza. La ricerca precedente aveva inventato varie patch per aggirare questo problema, risultando in algoritmi OT sempre pi\u00f9 intricati e complicati. Dopo aver progettato, implementato e sperimentato una serie di algoritmi OT, abbiamo riflettuto su quanto appreso e abbiamo deciso di sviluppare un nuovo quadro teorico per comprendere e risolvere meglio i problemi OT, riducendone la complessit\u00e0 e supportandone la continua evoluzione. In questo articolo riportiamo i principali risultati di questo sforzo: la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. L'algoritmo COT \u00e8 in grado di supportare sia l'esecuzione che l'annullamento di qualsiasi operazione in qualsiasi momento, senza richiedere funzioni di trasformazione per preservare la propriet\u00e0 di reversibilit\u00e0, propriet\u00e0 di convergenza 2, propriet\u00e0 inverse 2 e 3. L'algoritmo COT non \u00e8 solo pi\u00f9 semplice ed efficiente del precedente controllo OT algoritmi, ma semplifica anche la progettazione delle funzioni di trasformazione. Abbiamo implementato l'algoritmo COT in un motore di collaborazione generico e lo abbiamo utilizzato per supportare una gamma di nuove applicazioni collaborative. 1. INTRODUZIONE La trasformazione operativa -LRB- OT -RRB- \u00e8 stata originariamente inventata per il mantenimento della coerenza negli editor di gruppi di text semplice -LSB- 4 -RSB-. Per supportare in modo efficace ed efficiente le applicazioni esistenti e nuove, dobbiamo continuare a migliorare la capacit\u00e0 e la qualit\u00e0 dell\u2019OT nel risolvere problemi vecchi e nuovi. La solidit\u00e0 delle basi teoriche dell\u2019OT \u00e8 cruciale in questo processo. Tuttavia, la teoria della causalit\u00e0 \u00e8 inadeguata a catturare le condizioni OT essenziali per una corretta trasformazione. La limitazione della teoria della causalit\u00e0 aveva causato problemi di correttezza fin dall'inizio dell'OT. L'algoritmo dOPT \u00e8 stato il primo algoritmo OT e si basava esclusivamente sulle relazioni di concorrenza tra operazioni -LSB- 4 -RSB-: una coppia di operazioni sono trasformabili purch\u00e9 concorrenti. Tuttavia, ricerche successive hanno scoperto che la condizione di concorrenza da sola non \u00e8 sufficiente a garantire la correttezza della trasformazione. Un'altra condizione \u00e8 che le due operazioni concorrenti siano definite sullo stesso stato del documento. Questo enigma \u00e8 stato risolto in vari modi, ma la teoria della causalit\u00e0 e i suoi limiti sono stati ereditati da tutti gli algoritmi OT successivi. La limitazione della teoria della causalit\u00e0 \u00e8 diventata ancora pi\u00f9 evidente quando OT \u00e8 stato applicato per risolvere il problema dell\u2019annullamento nei redattori di gruppo.Il concetto di causalit\u00e0 non \u00e8 adatto a catturare le relazioni tra un'operazione inversa -LRB- come interpretazione di un comando di annullamento a meta livello -RRB- e altre normali operazioni di modifica. Infatti, la relazione di causalit\u00e0 non \u00e8 definita per le operazioni inverse -LRB- vedi Sezione 2 -RRB-. Sono state inventate varie patch per aggirare questo problema, risultando in algoritmi OT pi\u00f9 complessi e complicati -LSB- 18, 21 -RSB-. sostenendone la continua evoluzione. In questo articolo riportiamo i principali risultati di questo sforzo: la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. Innanzitutto, definiamo la dipendenza causale/indipendenza e ne descriviamo brevemente i limiti nella Sezione 2. Successivamente, presentiamo gli elementi chiave della teoria del context operativo, inclusa la definizione di context operativo, relazioni di dipendenza dal context/indipendenza, relazioni basate sul context. condizioni e vettori di context nella Sezione 3. Nella Sezione 4, presentiamo l'algoritmo COT di base per supportare il mantenimento della coerenza -LRB-do -RRB- e l'annullamento del gruppo partendo dal presupposto che le funzioni di trasformazione sottostanti siano in grado di preservare alcune importanti propriet\u00e0 di trasformazione. Quindi, queste propriet\u00e0 di trasformazione e le loro precondizioni sono discusse nella Sezione 5. Le soluzioni COT a queste propriet\u00e0 di trasformazione sono presentate nella Sezione 6. Il confronto tra il lavoro COT e il lavoro OT precedente, i problemi di correttezza OT e il lavoro futuro sono discussi nella Sezione 7. Infine, i principali contributi di questo lavoro sono riassunti nella Sezione 8. 8. CONCLUSIONI Abbiamo contribuito con la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. La teoria del context operativo \u00e8 in grado di catturare le relazioni e le condizioni essenziali per tutti i tipi di operazioni in un sistema OT; fornisce una nuova base per una migliore comprensione e risoluzione dei problemi OT. L'algoritmo COT fornisce soluzioni uniformi sia al mantenimento della coerenza che ai problemi di annullamento; \u00e8 pi\u00f9 semplice e pi\u00f9 efficiente dei precedenti algoritmi di controllo OT con capacit\u00e0 simili; e semplifica significativamente la progettazione delle funzioni di trasformazione. L'algoritmo COT \u00e8 stato implementato in un motore di collaborazione generico e utilizzato per supportare una gamma di nuove applicazioni collaborative -LSB- 24 -RSB-. Le applicazioni nel mondo reale offrono opportunit\u00e0 e sfide entusiasmanti per la futura ricerca OT. La teoria del context operativo e l\u2019algoritmo COT fungeranno da nuove basi per affrontare le sfide tecniche nelle applicazioni OT esistenti ed emergenti.riportiamo i principali risultati di questo sforzo: la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. Innanzitutto, definiamo la dipendenza causale/indipendenza e ne descriviamo brevemente i limiti nella Sezione 2. Successivamente, presentiamo gli elementi chiave della teoria del context operativo, inclusa la definizione di context operativo, relazioni di dipendenza dal context/indipendenza, relazioni basate sul context. condizioni e vettori di context nella Sezione 3. Nella Sezione 4, presentiamo l'algoritmo COT di base per supportare il mantenimento della coerenza -LRB-do -RRB- e l'annullamento del gruppo partendo dal presupposto che le funzioni di trasformazione sottostanti siano in grado di preservare alcune importanti propriet\u00e0 di trasformazione. Quindi, queste propriet\u00e0 di trasformazione e le loro precondizioni sono discusse nella Sezione 5. Le soluzioni COT a queste propriet\u00e0 di trasformazione sono presentate nella Sezione 6. Il confronto tra il lavoro COT e il lavoro OT precedente, i problemi di correttezza OT e il lavoro futuro sono discussi nella Sezione 7. Infine, i principali contributi di questo lavoro sono riassunti nella Sezione 8. 8. CONCLUSIONI Abbiamo contribuito con la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. La teoria del context operativo \u00e8 in grado di catturare le relazioni e le condizioni essenziali per tutti i tipi di operazioni in un sistema OT; fornisce una nuova base per una migliore comprensione e risoluzione dei problemi OT. L'algoritmo COT fornisce soluzioni uniformi sia al mantenimento della coerenza che ai problemi di annullamento; \u00e8 pi\u00f9 semplice e pi\u00f9 efficiente dei precedenti algoritmi di controllo OT con capacit\u00e0 simili; e semplifica significativamente la progettazione delle funzioni di trasformazione. L'algoritmo COT \u00e8 stato implementato in un motore di collaborazione generico e utilizzato per supportare una gamma di nuove applicazioni collaborative -LSB- 24 -RSB-. Le applicazioni nel mondo reale offrono opportunit\u00e0 e sfide entusiasmanti per la futura ricerca OT. La teoria del context operativo e l\u2019algoritmo COT fungeranno da nuove basi per affrontare le sfide tecniche nelle applicazioni OT esistenti ed emergenti.riportiamo i principali risultati di questo sforzo: la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. Innanzitutto, definiamo la dipendenza causale/indipendenza e ne descriviamo brevemente i limiti nella Sezione 2. Successivamente, presentiamo gli elementi chiave della teoria del context operativo, inclusa la definizione di context operativo, relazioni di dipendenza dal context/indipendenza, relazioni basate sul context. condizioni e vettori di context nella Sezione 3. Nella Sezione 4, presentiamo l'algoritmo COT di base per supportare il mantenimento della coerenza -LRB-do -RRB- e l'annullamento del gruppo partendo dal presupposto che le funzioni di trasformazione sottostanti siano in grado di preservare alcune importanti propriet\u00e0 di trasformazione. Quindi, queste propriet\u00e0 di trasformazione e le loro precondizioni sono discusse nella Sezione 5. Le soluzioni COT a queste propriet\u00e0 di trasformazione sono presentate nella Sezione 6. Il confronto tra il lavoro COT e il lavoro OT precedente, i problemi di correttezza OT e il lavoro futuro sono discussi nella Sezione 7. Infine, i principali contributi di questo lavoro sono riassunti nella Sezione 8. 8. CONCLUSIONI Abbiamo contribuito con la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. La teoria del context operativo \u00e8 in grado di catturare le relazioni e le condizioni essenziali per tutti i tipi di operazioni in un sistema OT; fornisce una nuova base per una migliore comprensione e risoluzione dei problemi OT. L'algoritmo COT fornisce soluzioni uniformi sia al mantenimento della coerenza che ai problemi di annullamento; \u00e8 pi\u00f9 semplice e pi\u00f9 efficiente dei precedenti algoritmi di controllo OT con capacit\u00e0 simili; e semplifica significativamente la progettazione delle funzioni di trasformazione. L'algoritmo COT \u00e8 stato implementato in un motore di collaborazione generico e utilizzato per supportare una gamma di nuove applicazioni collaborative -LSB- 24 -RSB-. Le applicazioni nel mondo reale offrono opportunit\u00e0 e sfide entusiasmanti per la futura ricerca OT. La teoria del context operativo e l\u2019algoritmo COT fungeranno da nuove basi per affrontare le sfide tecniche nelle applicazioni OT esistenti ed emergenti.e il lavoro futuro sono discussi nella Sezione 7. Infine, i principali contributi di questo lavoro sono riassunti nella Sezione 8. 8. CONCLUSIONI Abbiamo contribuito con la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. La teoria del context operativo \u00e8 in grado di catturare le relazioni e le condizioni essenziali per tutti i tipi di operazioni in un sistema OT; fornisce una nuova base per una migliore comprensione e risoluzione dei problemi OT. L'algoritmo COT fornisce soluzioni uniformi sia al mantenimento della coerenza che ai problemi di annullamento; \u00e8 pi\u00f9 semplice e pi\u00f9 efficiente dei precedenti algoritmi di controllo OT con capacit\u00e0 simili; e semplifica significativamente la progettazione delle funzioni di trasformazione. L'algoritmo COT \u00e8 stato implementato in un motore di collaborazione generico e utilizzato per supportare una gamma di nuove applicazioni collaborative -LSB- 24 -RSB-. Le applicazioni nel mondo reale offrono opportunit\u00e0 e sfide entusiasmanti per la futura ricerca OT. La teoria del context operativo e l\u2019algoritmo COT fungeranno da nuove basi per affrontare le sfide tecniche nelle applicazioni OT esistenti ed emergenti.e il lavoro futuro sono discussi nella Sezione 7. Infine, i principali contributi di questo lavoro sono riassunti nella Sezione 8. 8. CONCLUSIONI Abbiamo contribuito con la teoria del context operativo e l'algoritmo COT -LRB- Context-based OT -RRB-. La teoria del context operativo \u00e8 in grado di catturare le relazioni e le condizioni essenziali per tutti i tipi di operazioni in un sistema OT; fornisce una nuova base per una migliore comprensione e risoluzione dei problemi OT. L'algoritmo COT fornisce soluzioni uniformi sia al mantenimento della coerenza che ai problemi di annullamento; \u00e8 pi\u00f9 semplice e pi\u00f9 efficiente dei precedenti algoritmi di controllo OT con capacit\u00e0 simili; e semplifica significativamente la progettazione delle funzioni di trasformazione. L'algoritmo COT \u00e8 stato implementato in un motore di collaborazione generico e utilizzato per supportare una gamma di nuove applicazioni collaborative -LSB- 24 -RSB-. Le applicazioni nel mondo reale offrono opportunit\u00e0 e sfide entusiasmanti per la futura ricerca OT. La teoria del context operativo e l\u2019algoritmo COT fungeranno da nuove basi per affrontare le sfide tecniche nelle applicazioni OT esistenti ed emergenti.", "keyphrases": ["trasformazione operata", "culla", "basato sul context ot", "dipendenza causale", "condiz", "concordo relaz", "invers operat", "stato del documento", "origine operat", "trasformare opera", "cluster inverso", "il vettore rappresenta il context operativo", "buffer storico", "esclusa trasformazione"]}
{"file_name": "J-4", "text": "Analisi dei ricavi di una famiglia di regole di posizionamento per le aste di keyphrases ABSTRACT Le aste di keyphrases sono al centro dei modelli di business dei principali motori di ricerca di oggi. Gli inserzionisti fanno offerte per il posizionamento accanto ai risultati di ricerca e pagano per i clic sui loro annunci. Gli inserzionisti vengono generalmente classificati in base a un punteggio che tiene conto delle loro offerte e delle potenziali percentuali di clic. Consideriamo una famiglia di regole di ranking che contiene quelle tipicamente utilizzate per modellare Yahoo! e i progetti di aste di Google come casi speciali. Troviamo che in generale nessuno di questi \u00e8 necessariamente ottimale in termini di entrate in equilibrio e che la scelta della regola di classificazione pu\u00f2 essere guidata considerando la correlazione tra i valori degli offerenti e le percentuali di clic. Proponiamo un approccio semplice per determinare una regola di ranking ottimale per le entrate all'interno della nostra famiglia, tenendo conto degli effetti sulla soddisfazione degli inserzionisti e sull'esperienza dell'utente. Illustriamo l'approccio utilizzando simulazioni Monte-Carlo basate su distribuzioni adattate a Yahoo! dati sull'offerta e sulla percentuale di clic per una parola chiave a volume elevato. 1. INTRODUZIONE I principali motori di ricerca come Google, Yahoo! e MSN vendono annunci pubblicitari mettendo all'asta lo spazio nelle pagine dei risultati di ricerca di keyphrases. Ad esempio, quando un utente cerca sul Web * Questo lavoro \u00e8 stato svolto mentre l'autore era su Yahoo! Ricerca. \"iPod\", gli inserzionisti pi\u00f9 pagati -LRB- ad esempio Apple o Best Buy -RRB- per quella parola chiave possono apparire in una sezione separata \"sponsorizzata\" della pagina sopra o a destra dei risultati algoritmici. In genere, gli annunci pubblicitari che appaiono in una posizione pi\u00f9 alta nella pagina attirano pi\u00f9 attenzione e pi\u00f9 clic da parte degli utenti. Pertanto, a parit\u00e0 di condizioni, gli inserzionisti preferiscono le posizioni pi\u00f9 alte a quelle pi\u00f9 basse. Gli inserzionisti fanno offerte per il posizionamento sulla pagina in un formato stile asta in cui maggiore \u00e8 la loro offerta, maggiore \u00e8 la probabilit\u00e0 che la loro inserzione venga visualizzata sopra gli altri annunci sulla pagina. Per convenzione, gli inserzionisti della ricerca sponsorizzata generalmente fanno offerte e pagano per clic, il che significa che pagano solo quando un utente fa clic sul loro annuncio e non pagano se il loro annuncio viene visualizzato ma non viene cliccato. Overture Services, precedentemente GoTo.com e ora di propriet\u00e0 di Yahoo! Inc., \u00e8 accreditata come pioniera della pubblicit\u00e0 associata alla ricerca sponsorizzata. Il successo di Overture ha spinto diverse aziende ad adottare modelli di business simili, in primis Google, il principale motore di ricerca web oggi. MSN di Microsoft, precedentemente affiliato di Overture, ora gestisce il proprio mercato di aste di keyphrases. Il motore di ricerca valuta le offerte degli inserzionisti e assegna di conseguenza le posizioni sulla pagina. Tieni presente che, sebbene le offerte siano espresse come pagamenti per clic, il motore di ricerca non pu\u00f2 assegnare direttamente i clic, ma piuttosto assegna impressioni o posizionamenti sullo schermo. I clic si riferiscono solo stocasticamente alle impressioni. Fino a poco tempo fa, Yahoo! classificati gli offerenti in ordine decrescente in base ai valori per clic dichiarati dagli inserzionisti,mentre Google si classifica in ordine decrescente in base ai valori per impressione dichiarati dagli inserzionisti. Ci riferiamo a queste regole rispettivamente come \"rank-by-bid\" e \"rank-by-revenue\". ' Analizziamo una famiglia di regole di ranking che contiene le regole di Yahoo! e modelli Google come casi speciali. Consideriamo il rango `Questi sono termini del settore. Vedremo, tuttavia, che il ranking in base alle entrate non \u00e8 necessariamente ottimale in termini di entrate. ing regole in cui gli offerenti sono classificati in ordine decrescente di punteggio eqb, dove e indica la percentuale di clic di un inserzionista -LRB- normalizzata per la posizione -RRB- e b la sua offerta. Si noti che q = 0 corrisponde a Yahoo! e q = 1 corrisponde alla regola di ranking per entrate di Google. La nostra premessa \u00e8 che gli offerenti stanno giocando un equilibrio simmetrico, come definito da Edelman, Ostrovsky e Schwarz -LSB- 3 -RSB- e Varian -LSB- 11 -RSB-. Mostriamo attraverso la simulazione che, sebbene q = 1 fornisca un'allocazione efficiente, impostazioni di q considerevolmente inferiori a 1 possono produrre entrate superiori in equilibrio in determinate condizioni. Il parametro chiave \u00e8 la correlazione tra il valore dell'inserzionista e la percentuale di clic. Se questa correlazione \u00e8 fortemente positiva, allora q pi\u00f9 piccoli sono ottimali in termini di entrate. Le nostre simulazioni si basano su distribuzioni adattate ai dati di Yahoo! aste di keyphrases. Proponiamo che i motori di ricerca stabiliscano soglie di perdita accettabile nella soddisfazione dell'inserzionista e nell'esperienza dell'utente, quindi scelgano il q ottimale in termini di entrate coerente con questi vincoli. Nella Sezione 2 forniamo un modello formale di aste di keyphrases e ne stabiliamo le propriet\u00e0 di equilibrio nella Sezione 3. Nella Sezione 4 notiamo che dare crediti agli agenti che fanno offerte pu\u00f2 avere lo stesso effetto di mettere a punto esplicitamente la regola di ranking. Nella Sezione 5 forniamo una formulazione generale del problema di progettazione dell'asta di keyphrases ottimali come problema di ottimizzazione, in modo analogo all'impostazione dell'asta per singolo articolo. Forniamo quindi alcune informazioni teoriche su come l'ottimizzazione di q pu\u00f2 migliorare le entrate e sul motivo per cui la correlazione tra i valori degli offerenti e le percentuali di clic \u00e8 rilevante. Nella Sezione 6 consideriamo l'effetto di q sulla soddisfazione dell'inserzionista e sull'esperienza dell'utente. Nella Sezione 7 descriviamo le nostre simulazioni e interpretiamo i loro risultati. Lavoro correlato. Entrambi gli articoli definiscono indipendentemente un interessante perfezionamento dell'equilibrio di Nash per le aste di keyphrases e ne analizzano le propriet\u00e0 di equilibrio. Hanno chiamato questo raffinamento \u201cequilibrio localmente privo di invidia\u201d e \u201cequilibrio simmetrico\u201d, rispettivamente. Varian fornisce anche alcune analisi empiriche. Il modello generale di aste di keyphrases utilizzato qui, in cui gli offerenti vengono classificati in base a un peso moltiplicato per la loro offerta, \u00e8 stato introdotto da Aggarwal, Goel e Motwani -LSB- 1 -RSB-. Quel documento collega anche le entrate delle aste di keyphrases in contesti di informazione incompleta con le entrate in equilibrio simmetrico.Iyengar e Kumar -LSB- 5 -RSB- studiano il problema della progettazione di un'asta di keyphrases ottimale in un context di informazioni incomplete e creano anche il collegamento con l'equilibrio simmetrico. Utilizziamo questa connessione quando formuliamo il problema di progettazione dell'asta ottimale nel nostro context. Sono stati i primi a rendersi conto che la correlazione tra i valori degli offerenti e le percentuali di clic dovrebbe essere un parametro chiave che influenza la performance delle entrate di vari meccanismi di classificazione. Per semplicit\u00e0, presuppongono che gli offerenti offrano i loro veri valori, quindi il loro modello \u00e8 molto diverso dal nostro e di conseguenza lo sono anche i loro risultati. Secondo le loro simulazioni, il ranking per entrate \u00e8 sempre -LRB- e debolmente -RRB- domina il ranking per offerta in termini di entrate, mentre i nostri risultati suggeriscono che il ranking per offerta pu\u00f2 fare molto meglio per le correlazioni negative. Lahaie -LSB- 8 -RSB- fornisce un esempio che suggerisce che il ranking per offerta dovrebbe generare maggiori entrate quando i valori e le percentuali di clic sono correlati positivamente, mentre il ranking per entrate dovrebbe produrre risultati migliori quando la correlazione \u00e8 negativa. In questo lavoro approfondiamo questa congettura. 8. CONCLUSIONI In questo lavoro abbiamo esaminato le propriet\u00e0 di ricavo di una famiglia di regole di ranking che contiene le regole di Yahoo! e i modelli Google come casi speciali. In pratica, dovrebbe essere molto semplice spostarsi tra le regole all'interno della famiglia: ci\u00f2 comporta semplicemente la modifica dell'esponente q applicato agli effetti dell'inserzionista. Abbiamo anche dimostrato che, in linea di principio, lo stesso effetto potrebbe essere ottenuto utilizzando i crediti di offerta. Nonostante la semplicit\u00e0 del cambiamento delle regole, le simulazioni hanno rivelato che un\u2019adeguata regolazione di q pu\u00f2 migliorare significativamente le entrate. Nelle simulazioni, i miglioramenti delle entrate sono stati maggiori di quelli ottenibili utilizzando i prezzi di riserva. D\u2019altro canto, abbiamo dimostrato che la soddisfazione dell\u2019inserzionista e l\u2019esperienza dell\u2019utente potrebbero risentirne se q fosse reso troppo piccolo. Sarebbe interessante fare questa analisi per una variet\u00e0 di keyphrases, per vedere se l'impostazione ottimale di q \u00e8 sempre cos\u00ec sensibile al livello di correlazione. Se lo \u00e8, allora utilizzare semplicemente il ranking per offerta dove esiste una correlazione positiva e il ranking per fatturato dove esiste una correlazione negativa potrebbe andare bene in prima approssimazione e gi\u00e0 migliorare le entrate. Sarebbe anche interessante confrontare gli effetti dell'ottimizzazione di q rispetto al prezzo di riserva per le keyphrases che hanno pochi offerenti. In linea di principio il ricavo minimo nell\u2019equilibrio di Nash pu\u00f2 essere trovato mediante programmazione lineare. Tuttavia, nell\u2019equilibrio di Nash possono verificarsi molte allocazioni e per ciascuna di queste \u00e8 necessario risolvere un programma lineare. Non esiste ancora un modo efficiente per enumerare tutte le possibili allocazioni Nash, quindi trovare il ricavo minimo \u00e8 attualmente impossibile. Se questo problema potesse essere risolto, potremmo eseguire simulazioni per l\u2019equilibrio di Nash anzich\u00e9 per l\u2019equilibrio simmetrico, per vedere se le nostre intuizioni sono robuste rispetto alla scelta del concetto di soluzione. Potrebbero essere rilevanti classi pi\u00f9 ampie di regole di classificazione. Ad esempio,\u00e8 possibile introdurre sconti ds e classificare secondo wsbs \u2212 ds ; l\u2019analisi dell\u2019equilibrio si generalizza anche a questo caso. Con questa classe pi\u00f9 ampia il punteggio virtuale pu\u00f2 eguagliare il punteggio, ad esempio nel caso di una distribuzione marginale uniforme sui valori. Figura 4: Entrate, efficienza e rilevanza per diversi punteggi di riserva r, con correlazione di Spearman di 0,4 e q = 1.", "keyphrases": ["entrate", "asta di keyphrases", "posizione ottimale in termini di entrate", "regola del rango", "motore di ricerca", "pubblicit\u00e0", "ricerca sponsor", "classifica per offerta", "classifica per fatturato", "profitto", "entrate pubblicitarie", "parola chiave per la ricerca del prezzo", "problema di progettazione dell'asta ottimale"]}
{"file_name": "C-31", "text": "Apocrita: un sistema distribuito di condivisione file peer-to-peer per intranet ABSTRACT A molte organizzazioni viene richiesto di creare documenti per vari scopi e tali documenti potrebbero dover essere accessibili a tutti i membri dell'organizzazione. Questo accesso potrebbe essere necessario per modificare o semplicemente visualizzare un documento. In alcuni casi questi documenti vengono condivisi tra gli autori, via email, per essere modificati. Ci\u00f2 pu\u00f2 facilmente causare l'invio di una versione errata o la creazione di conflitti tra pi\u00f9 utenti che tentano di apportare modifiche a un documento. Potrebbero esserci anche pi\u00f9 documenti diversi in fase di modifica. All'utente potrebbe essere richiesto di cercare un particolare documento, che alcuni strumenti di ricerca come Google Desktop potrebbero essere una soluzione per i documenti locali ma non troveranno un documento sul computer di un altro utente. Un altro problema sorge quando un documento viene reso disponibile sul computer di un utente e quell'utente \u00e8 offline, nel qual caso il documento non \u00e8 pi\u00f9 accessibile. In questo articolo presentiamo Apocrita, un rivoluzionario sistema di condivisione file P2P distribuito per Intranet. 1. INTRODUZIONE Il paradigma informatico Peer-to-Peer -LRB- P2P -RRB- sta diventando una forma completamente nuova di condivisione reciproca delle risorse su Internet. Con il sempre pi\u00f9 diffuso accesso a Internet a banda larga, la tecnologia P2P \u00e8 finalmente diventata un modo praticabile per condividere documenti e file multimediali. Sul mercato esistono gi\u00e0 programmi che consentono la condivisione di file P2P. Questi programmi consentono a milioni di utenti di condividere file tra loro. I file scaricati richiedono ancora molta gestione manuale da parte dell'utente. L'utente deve comunque inserire i file nella directory corretta, gestire file con pi\u00f9 versioni, eliminare i file quando non sono pi\u00f9 desiderati. Ci sforziamo di rendere pi\u00f9 semplice il processo di condivisione dei documenti all'interno di una Intranet. Molte organizzazioni sono tenute a creare documenti per vari scopi e potrebbe essere necessario che tali documenti siano accessibili a tutti i membri dell'organizzazione. Questo accesso potrebbe essere necessario per modificare o semplicemente visualizzare un documento. In alcuni casi questi documenti vengono inviati tra gli autori, via email, per essere modificati. Ci\u00f2 pu\u00f2 facilmente causare l'invio di una versione errata o la creazione di conflitti tra pi\u00f9 utenti che tentano di apportare modifiche a un documento. Potrebbero esserci anche pi\u00f9 documenti diversi in fase di modifica. All'utente potrebbe essere richiesto di cercare un particolare documento, che alcuni strumenti di ricerca come Google Desktop potrebbero essere una soluzione per i documenti locali ma non troveranno un documento sul computer di un altro utente. Inoltre, alcune organizzazioni non dispongono di un server di condivisione file o dell'infrastruttura di rete necessaria per abilitarne uno. In questo articolo presentiamo Apocrita, un sistema di condivisione file P2P distribuito ed economicamente vantaggioso per tali organizzazioni. Nella sezione 2 presentiamo Apocrita. Il meccanismo e il protocollo di indicizzazione distribuita sono presentati nella Sezione 3. La Sezione 4 presenta il modello di distribuzione peer-topeer. Un prototipo dimostrativo \u00e8 presentato nella Sezione 5,e le valutazioni delle prestazioni sono discussi nella Sezione 6. Il lavoro correlato \u00e8 presentato nella Sezione 7, e infine le conclusioni e il lavoro futuro sono discussi nella Sezione 8. 7. LAVORI CORRELATI Oggi esistono diversi sistemi P2P decentralizzati -LSB- 1, 2, 3 -RSB- che Apocrita presenta alcune delle loro funzionalit\u00e0. Tuttavia, Apocrita dispone anche di funzionalit\u00e0 uniche di ricerca e indicizzazione di nuovi romanzi che rendono questo sistema unico. Ad esempio, Majestic-12 -LSB- 4 -RSB- \u00e8 un progetto di ricerca e indicizzazione distribuito progettato per la ricerca su Internet. Ogni utente installer\u00e0 un client, che \u00e8 responsabile dell'indicizzazione di una porzione del web. Un'area centrale per interrogare l'indice \u00e8 disponibile sulla pagina web Majestic-12. L'indice stesso non viene distribuito, viene distribuito solo l'atto di indicizzare. L'aspetto dell'indicizzazione distribuita di questo progetto \u00e8 strettamente correlato agli obiettivi di Apocrita. YaCy -LSB- 6 -RSB- \u00e8 un'applicazione di ricerca web peer-to-peer. YaCy \u00e8 progettato per mantenere un indice distribuito di Internet. Ha utilizzato una tabella hash distribuita -LRB- DHT -RRB- per mantenere l'indice. Il nodo locale viene utilizzato per eseguire query, ma tutti i risultati restituiti sono accessibili su Internet. YaCy ha utilizzato molti peer e DHT per mantenere un indice distribuito. Apocrita utilizzer\u00e0 anche un indice distribuito nelle implementazioni future e potrebbe trarre vantaggio dall'utilizzo di un'implementazione di DHT. YaCy, tuttavia, \u00e8 progettato come motore di ricerca web e, come tale, risolve un problema molto diverso rispetto ad Apocrita. 8. CONCLUSIONI E LAVORO FUTURO Abbiamo presentato Apocrita, un sistema distribuito di ricerca e indicizzazione P2P destinato agli utenti della rete su una Intranet. Pu\u00f2 aiutare le organizzazioni prive di file server di rete o dell'infrastruttura di rete necessaria per condividere documenti. Elimina la necessit\u00e0 di condividere manualmente i documenti tra gli utenti durante la modifica e riduce la possibilit\u00e0 che vengano distribuite versioni contrastanti. Nonostante queste carenze, l\u2019esperienza acquisita dalla progettazione e dall\u2019implementazione di Apocrita ci ha fornito maggiori informazioni sulla creazione di sistemi distribuiti impegnativi.YaCy \u00e8 progettato per mantenere un indice distribuito di Internet. Ha utilizzato una tabella hash distribuita -LRB- DHT -RRB- per mantenere l'indice. Il nodo locale viene utilizzato per eseguire query, ma tutti i risultati restituiti sono accessibili su Internet. YaCy ha utilizzato molti peer e DHT per mantenere un indice distribuito. Apocrita utilizzer\u00e0 anche un indice distribuito nelle implementazioni future e potrebbe trarre vantaggio dall'utilizzo di un'implementazione di DHT. YaCy, tuttavia, \u00e8 progettato come motore di ricerca web e, come tale, risolve un problema molto diverso rispetto ad Apocrita. 8. CONCLUSIONI E LAVORO FUTURO Abbiamo presentato Apocrita, un sistema distribuito di ricerca e indicizzazione P2P destinato agli utenti della rete su una Intranet. Pu\u00f2 aiutare le organizzazioni prive di file server di rete o dell'infrastruttura di rete necessaria per condividere documenti. Elimina la necessit\u00e0 di condividere manualmente i documenti tra gli utenti durante la modifica e riduce la possibilit\u00e0 che vengano distribuite versioni contrastanti. Nonostante queste carenze, l\u2019esperienza acquisita dalla progettazione e dall\u2019implementazione di Apocrita ci ha fornito maggiori informazioni sulla creazione di sistemi distribuiti impegnativi.YaCy \u00e8 progettato per mantenere un indice distribuito di Internet. Ha utilizzato una tabella hash distribuita -LRB- DHT -RRB- per mantenere l'indice. Il nodo locale viene utilizzato per eseguire query, ma tutti i risultati restituiti sono accessibili su Internet. YaCy ha utilizzato molti peer e DHT per mantenere un indice distribuito. Apocrita utilizzer\u00e0 anche un indice distribuito nelle implementazioni future e potrebbe trarre vantaggio dall'utilizzo di un'implementazione di DHT. YaCy, tuttavia, \u00e8 progettato come motore di ricerca web e, come tale, risolve un problema molto diverso rispetto ad Apocrita. 8. CONCLUSIONI E LAVORO FUTURO Abbiamo presentato Apocrita, un sistema distribuito di ricerca e indicizzazione P2P destinato agli utenti della rete su una Intranet. Pu\u00f2 aiutare le organizzazioni prive di file server di rete o dell'infrastruttura di rete necessaria per condividere documenti. Elimina la necessit\u00e0 di condividere manualmente i documenti tra gli utenti durante la modifica e riduce la possibilit\u00e0 che vengano distribuite versioni contrastanti. Nonostante queste carenze, l\u2019esperienza acquisita dalla progettazione e dall\u2019implementazione di Apocrita ci ha fornito maggiori informazioni sulla creazione di sistemi distribuiti impegnativi.", "keyphrases": ["peer to peer", "sistema di condivisione file", "intranet", "autore", "documento", "apocrita", "jxta", "indice di distribuzione", "modello di distribuzione peer-to-peer", "idl queri", "file indice", "file incom", "ricerca p2p"]}
{"file_name": "C-20", "text": "Migrazione di data center live attraverso WAN: un solido approccio cooperativo consapevole del context ABSTRACT Una preoccupazione significativa per i fornitori di servizi basati su Internet \u00e8 la continua operativit\u00e0 e disponibilit\u00e0 dei servizi nonostante le interruzioni, pianificate o non pianificate. In questo documento sosteniamo un approccio cooperativo e consapevole del context alla migrazione dei data center attraverso le WAN per gestire le interruzioni in modo senza interruzioni. Cerchiamo specificamente di ottenere un'elevata disponibilit\u00e0 dei servizi del data center a fronte di interruzioni sia pianificate che impreviste delle strutture del data center. Utilizziamo tecnologie di virtualizzazione del server per consentire la replica e la migrazione delle funzioni del server. Proponiamo nuove funzioni di rete per consentire la migrazione e la replica dei server su reti geografiche -LRB- ad esempio Internet -RRB-, e infine mostriamo l'utilit\u00e0 della tecnologia di replica dello storage intelligente e dinamica per garantire che le applicazioni abbiano accesso ai dati in caso di interruzioni con obiettivi dei punti di ripristino molto ristretti. 1. INTRODUZIONE Una preoccupazione significativa per i fornitori di servizi basati su Internet \u00e8 la continuit\u00e0 del funzionamento e della disponibilit\u00e0 dei servizi nonostante le interruzioni, pianificate o non pianificate. Un'interruzione relativamente minore pu\u00f2 interrompere e causare disagio a un gran numero di utenti. Oggi questi servizi sono ospitati quasi esclusivamente nei data center. I recenti progressi nelle tecnologie di virtualizzazione dei server -LSB- 8, 14, 22 -RSB- consentono la migrazione in tempo reale dei servizi all'interno di un ambiente di rete locale -LRB- LAN -RRB-. Nell'ambiente LAN, queste tecnologie si sono rivelate uno strumento molto efficace per consentire la gestione del data center in modo senza interruzioni. Non solo pu\u00f2 supportare eventi di manutenzione pianificata -LSB- 8 -RSB-, ma pu\u00f2 anche essere utilizzato in modo pi\u00f9 dinamico per bilanciare automaticamente il carico tra i server fisici in un data center -LSB- 22 -RSB-. Quando si utilizzano queste tecnologie in un ambiente LAN, i servizi vengono eseguiti in un server virtuale e i servizi di migrazione forniti dal framework di virtualizzazione sottostante consentono la migrazione di un server virtuale da un server fisico a un altro, senza tempi di inattivit\u00e0 significativi per il servizio o l'applicazione . In particolare, poich\u00e9 il server virtuale conserva lo stesso indirizzo di rete di prima, le interazioni in corso a livello di rete non vengono interrotte. Allo stesso modo, in un ambiente LAN, i requisiti di archiviazione vengono normalmente soddisfatti tramite Network attached storage -LRB- NAS -RRB- o tramite una Storage Area Network -LRB- SAN -RRB- che \u00e8 ancora raggiungibile dalla nuova posizione del server fisico per consentire accesso continuo allo spazio di archiviazione. Sfortunatamente in un ambiente di vasta area -LRB-WAN-RRB-, la migrazione del server live non \u00e8 cos\u00ec facilmente realizzabile per due motivi: in primo luogo, la migrazione live richiede che il server virtuale mantenga lo stesso indirizzo di rete in modo che, dal punto di vista della connettivit\u00e0 di rete, il server migrato \u00e8 indistinguibile dall'originale. Secondo,sebbene siano stati sviluppati meccanismi di replica remota abbastanza sofisticati nel context del disaster recovery -LSB- 20, 7, 11 -RSB-, questi meccanismi sono poco adatti alla migrazione di data center live, perch\u00e9 in generale le tecnologie disponibili non sono consapevoli dell'applicazione/servizio semantica di livello. In questo documento descriviamo un progetto per la migrazione dei servizi live attraverso le WAN. Il nostro progetto utilizza le tecnologie di virtualizzazione dei server esistenti e propone meccanismi di rete e di archiviazione per facilitare la migrazione attraverso una WAN. L'essenza del nostro approccio \u00e8 la migrazione cooperativa e consapevole del context, in cui un sistema di gestione della migrazione orchestra la migrazione del data center attraverso tutti e tre i sottosistemi coinvolti, vale a dire le piattaforme server, la rete geografica e il sistema di archiviazione su disco. Sebbene concettualmente simile per natura al lavoro basato su LAN descritto sopra, l'utilizzo delle tecnologie di migrazione attraverso una rete geografica presenta sfide uniche e, a nostra conoscenza, non \u00e8 stato raggiunto. Il nostro contributo principale \u00e8 la progettazione di un framework che consentir\u00e0 la migrazione attraverso una WAN di tutti i sottosistemi coinvolti nell'abilitazione dei servizi del data center. Descriviamo nuovi meccanismi nonch\u00e9 estensioni delle tecnologie esistenti per consentire ci\u00f2 e delineare le funzionalit\u00e0 cooperative e consapevoli del context necessarie nei diversi sottosistemi per consentire ci\u00f2. 4. LAVORO CORRELATO Il lavoro precedente su questo argomento rientra in diverse categorie: migrazione della macchina virtuale, replica dello storage e supporto di rete. Al centro della nostra tecnica c'\u00e8 la capacit\u00e0 di incapsulare le applicazioni all'interno di macchine virtuali che possono essere migrate senza tempi di inattivit\u00e0 delle applicazioni -LSB- 15 -RSB-. Come indicato in precedenza, queste tecniche presuppongono che la migrazione venga eseguita su una LAN. La migrazione delle VM \u00e8 stata studiata anche nel sistema Shirako -LSB- 10 -RSB- e per ambienti grid -LSB- 17, 19 -RSB-. L'attuale software della macchina virtuale supporta una funzionalit\u00e0 di sospensione e ripresa che pu\u00f2 essere utilizzata per supportare la migrazione WAN, ma con tempi di inattivit\u00e0 -LSB- 18, 12 -RSB-. Recentemente \u00e8 stata dimostrata la migrazione in tempo reale della WAN utilizzando tunnel IP in -LSB-21 -RSB-, dove viene configurato un tunnel IP dal server di origine a quello di destinazione per inoltrare in modo trasparente i pacchetti da e verso l'applicazione; noi sosteniamo un approccio alternativo che presuppone il supporto del router edge. Un'eccellente descrizione di questi e altri, nonch\u00e9 una tassonomia dettagliata dei diversi approcci per la replica possono essere trovati in -LSB- 11 -RSB-. Il sistema dell'Orsa Minore sostiene che nessun singolo modello di faglia \u00e8 ottimale per tutte le applicazioni e propone di supportare selezioni specifiche del tipo di dati di modelli di faglia e schemi di codifica per la replica -LSB- 1 -RSB-. Nel context del supporto di rete, il nostro lavoro \u00e8 legato all'approccio RouterFarm -LSB- 2 -RSB-, che fa uso di modifiche di rete orchestrate per realizzare una manutenzione quasi senza intoppi sui router edge del provider. Oltre ad essere in un'area applicativa diversa, il nostro approccio differisce dal lavoro della RouterFarm sotto due aspetti. Secondo,a causa dei rigorosi requisiti temporali della migrazione in tempo reale, prevediamo che il nostro approccio richiederebbe una nuova funzionalit\u00e0 del router -LRB- invece di essere realizzabile tramite le interfacce di configurazione esistenti -RRB-. Con uno spirito simile a ROC, consigliamo l'utilizzo di meccanismi che vanno dalla migrazione delle VM in tempo reale alla replica dello storage per supportare interruzioni pianificate e non pianificate nei data center -LRB- piuttosto che la replica completa per mascherare tali guasti -RRB-. 5. CONCLUSIONE Una preoccupazione significativa per i fornitori di servizi basati su Internet \u00e8 la continuit\u00e0 del funzionamento e della disponibilit\u00e0 dei servizi nonostante le interruzioni, pianificate o non pianificate. In questo documento abbiamo sostenuto un approccio cooperativo e consapevole del context alla migrazione dei data center attraverso le WAN per gestire le interruzioni in modo senza interruzioni. Abbiamo cercato di raggiungere un'elevata disponibilit\u00e0 dei servizi del data center a fronte di interruzioni sia pianificate che accidentali delle strutture del data center. Abbiamo sostenuto l'utilizzo di tecnologie di virtualizzazione del server per consentire la replica e la migrazione delle funzioni del server. Abbiamo proposto nuove funzioni di rete per consentire la migrazione e la replica dei server su reti geografiche -LRB- come Internet o una rete privata virtuale geograficamente distribuita -RRB-, e infine abbiamo mostrato l'utilit\u00e0 della tecnologia di replica di archiviazione intelligente e dinamica per garantire che le applicazioni abbiano accesso ai dati in caso di interruzioni con obiettivi di punti di ripristino molto rigorosi.", "keyphrases": ["servizio basato su Internet", "migrazione del data center", "pallido", "lan", "server virtuale", "replica di archiviazione", "replica sincrona", "replica asincrona", "supporto di rete", "storag", "voce su ip", "voip", "database"]}
{"file_name": "J-20", "text": "Algoritmi chiari per i mercati del baratto: abilitare gli scambi di reni a livello nazionale ABSTRACT Nei mercati del baratto, gli agenti cercano di scambiare i loro articoli tra loro, al fine di migliorare le proprie utilit\u00e0. Questi scambi consistono in cicli di agenti, in cui ciascun agente riceve l'elemento dell'agente successivo nel ciclo. Ci concentriamo principalmente sul prossimo mercato nazionale dello scambio di reni, in cui i pazienti con malattie renali possono ottenere donatori compatibili scambiando i propri donatori consenzienti ma incompatibili. Con oltre 70.000 pazienti gi\u00e0 in attesa di un rene da cadavere negli Stati Uniti, questo mercato \u00e8 visto come l\u2019unico modo etico per ridurre significativamente i 4.000 decessi all\u2019anno attribuiti alle malattie renali. Il problema del clearing implica la ricerca di un benessere sociale che massimizzi lo scambio quando viene fissata la durata massima di un ciclo. Sono vietati i cicli lunghi poich\u00e9, per ragioni di incentivazione, tutti i trapianti di un ciclo devono essere eseguiti contemporaneamente. Inoltre, negli scambi di baratto in genere, vengono colpiti pi\u00f9 agenti se si abbandona un ciclo pi\u00f9 lungo. Dimostriamo che il problema di compensazione con questo vincolo di durata del ciclo \u00e8 NP-difficile. Risolverlo esattamente \u00e8 una delle sfide principali nella realizzazione di uno scambio renale nazionale. Presentiamo il primo algoritmo in grado di compensare questi mercati su scala nazionale. La chiave \u00e8 la formulazione incrementale del problema. Adattiamo due paradigmi per l'attivit\u00e0: generazione di vincoli e generazione di colonne. Per ognuno di essi sviluppiamo tecniche che migliorano notevolmente sia il runtime che l'utilizzo della memoria. Concludiamo che la generazione di colonne scala drasticamente meglio della generazione di vincoli. Il nostro algoritmo supporta anche diverse generalizzazioni, come richiesto dagli scambi renali nel mondo reale. Il nostro algoritmo ha sostituito CPLEX come algoritmo di compensazione dell'Alliance for Paired Donation, uno dei principali scambi di reni. Le partite vengono eseguite ogni due settimane e sono gi\u00e0 stati effettuati i trapianti basati sulle nostre ottimizzazioni. 1. INTRODUZIONE Il ruolo dei reni \u00e8 quello di filtrare i rifiuti dal sangue. L'insufficienza renale provoca l'accumulo di questi rifiuti, che porta alla morte in pochi mesi. Un'opzione di trattamento \u00e8 la dialisi, in cui il paziente si reca in ospedale per farsi filtrare il sangue da una macchina esterna. Sono necessarie diverse visite a settimana e ciascuna richiede diverse ore. La qualit\u00e0 della vita in dialisi pu\u00f2 essere estremamente bassa, e infatti molti pazienti scelgono di sospendere la dialisi, portando ad una morte naturale. Solo il 12% dei pazienti in dialisi sopravvive 10 anni -LSB- 23 -RSB-. Invece, il trattamento preferito \u00e8 il trapianto di rene. I trapianti di rene sono di gran lunga i trapianti pi\u00f9 comuni. Sfortunatamente, la domanda di reni supera di gran lunga l\u2019offerta. Negli Stati Uniti nel 2005 sono morte 4.052 persone in attesa di un trapianto di rene salvavita. Durante questo periodo, quasi 30.000 persone sono state aggiunte alla lista d'attesa nazionale, mentre solo 9.913 persone hanno lasciato la lista dopo aver ricevuto un rene da donatore deceduto. Per molti pazienti con malattie renali,la soluzione migliore \u00e8 trovare un donatore vivente, cio\u00e8 una persona sana disposta a donare uno dei suoi due reni. Nel 2005 ci sono state 6.563 donazioni da vivi negli Stati Uniti. e il destinatario previsto sono incompatibili per tipo sanguigno o tissutale. In passato, il donatore incompatibile veniva rimandato a casa, lasciando il paziente in attesa del rene del donatore deceduto. Tuttavia, ora ci sono alcuni scambi renali regionali negli Stati Uniti, in cui i pazienti possono scambiare tra loro i loro donatori incompatibili, per ottenere ciascuno un donatore compatibile. Questi mercati sono esempi di scambi di baratto. In un mercato di scambio del baratto, gli agenti -LRB- pazienti -RRB- cercano di scambiare i loro articoli -LRB- donatori incompatibili -RRB- tra loro. Questi scambi consistono in cicli di agenti, in cui ciascun agente riceve l'elemento dell'agente successivo nel ciclo. Gli scambi di baratto sono onnipresenti: gli esempi includono Peerflix -LRB- DVDs -RRB- -LSB- 11 -RSB-, Read It Swap It -LRB- libri -RRB- -LSB- 12 -RSB- e Intervac -LRB- case vacanza - RRB- -LSB- 9 -RSB-. Da molti anni negli Stati Uniti esiste anche un grande scambio di scarpe -LSB- 10 -RSB-. Le persone con piedi di dimensioni diverse lo usano per evitare di dover acquistare due paia di scarpe. Gli amputati di gamba hanno uno scambio separato per condividere il costo dell'acquisto di un singolo paio di scarpe. Possiamo codificare un mercato di scambio del baratto come un grafo orientato G = -LRB- V, E -RRB- nel modo seguente. Costruisci un vertice per ciascun agente. Aggiungi un vantaggio ponderato e da un agente vi a un altro vj, se vi desidera l'elemento di vj. Il peso we di e rappresenta l'utilit\u00e0 per vi di ottenere l'elemento di vj. Un ciclo c in questo grafico rappresenta un possibile scambio, in cui ciascun agente del ciclo ottiene l'elemento dell'agente successivo. Il peso wc di un ciclo c \u00e8 la somma dei pesi dei suoi bordi. Uno scambio \u00e8 un insieme di cicli disgiunti. Il peso di uno scambio \u00e8 la somma dei suoi pesi ciclici. Uno scambio che massimizza il benessere sociale \u00e8 quello con il massimo peso. La Figura 1 illustra un mercato di esempio con 5 agenti, -LCB- v1, v2,..., v5 -RCB-, in cui tutti gli archi hanno peso 1. Il mercato ha 4 cicli, c1 = -LRB- v1, v2 -RRB -, c2 = -LRB- v2, v3 -RRB-, c3 = -LRB- v3, v4 -RRB- e c4 = -LRB- v1, v2, v3, v4, v5 -RRB- e due -LRB- inclusione -RRB- scambi massimali, ovvero M1 = -LCB- c4 -RCB- e M2 = -LCB- c1, c3 -RCB-. Lo scambio M1 ha sia il peso massimo che la cardinalit\u00e0 massima -LRB-, ovvero include il maggior numero di bordi/vertici -RRB-. Figura 1: Esempio di mercato del baratto. Il problema di compensazione \u00e8 trovare uno scambio di peso massimo costituito da cicli con lunghezza al massimo di una piccola costante L. Questo vincolo sulla lunghezza del ciclo si presenta naturalmente per diverse ragioni. Ad esempio, in uno scambio renale, tutte le operazioni di un ciclo devono essere eseguite contemporaneamente; altrimenti un donatore potrebbe tirarsi indietro dopo che il suo partner incompatibile ha ricevuto un rene. A causa di tali vincoli di risorse, il prossimo mercato nazionale dello scambio renale consentir\u00e0 probabilmente solo cicli di lunghezza 2 e 3.Un\u2019altra motivazione per i cicli brevi \u00e8 che se il ciclo non riesce a scambiarsi, vengono colpiti meno agenti. Ad esempio, il test dell'ultimo minuto in una sostituzione renale spesso rivela nuove incompatibilit\u00e0 che non erano state rilevate nel test iniziale -LRB- in base al quale \u00e8 stato costruito il grafico di compatibilit\u00e0 -RRB-. Nella Sezione 3, mostriamo che -LRB- la versione decisionale di -RRB- il problema di compensazione \u00e8 NP-completo per L > 3. Un approccio potrebbe quindi essere quello di cercare un buon algoritmo euristico o di approssimazione. Tuttavia, per due ragioni, miriamo a un algoritmo esatto basato su una formulazione di programma lineare intero -LRB- ILP -RRB-, che risolviamo utilizzando la ricerca specializzata sugli alberi. 9 In primo luogo, qualsiasi perdita di ottimalit\u00e0 potrebbe portare a morti inutili di pazienti. 9 In secondo luogo, una caratteristica interessante dell'utilizzo di una formula ILP \u00e8 che consente di modellare facilmente una serie di variazioni dell'obiettivo e di aggiungere ulteriori vincoli al problema. Oppure, se per vari -LRB- ad esempio ragioni etiche -RRB- si richiede uno scambio di cardinalit\u00e0 massima, si pu\u00f2 almeno in un secondo passaggio trovare la soluzione -LRB- tra tutte le soluzioni di cardinalit\u00e0 massima -RRB- che ha il minor numero di 3 -cicli. Altre variazioni che si possono risolvere includono la ricerca di varie forme di raccolte di cicli \"tolleranti ai guasti\" -LRB- non disgiunte -RRB- nel caso in cui alcune coppie che si pensava fossero compatibili si rivelano dopotutto incompatibili. In questo articolo presentiamo il primo algoritmo in grado di compensare questi mercati su scala nazionale. Le codifiche ILP semplici sono troppo grandi per essere costruite anche sull'hardware attuale, per non parlare di come risolverle. La chiave quindi \u00e8 la formulazione incrementale del problema. Adattiamo due paradigmi per l'attivit\u00e0: generazione di vincoli e generazione di colonne. Per ciascuno, sviluppiamo una serie di tecniche -LRB- principalmente specifiche per problemi -RRB- che migliorano notevolmente sia il runtime che l'utilizzo della memoria. 1.1 Lavoro precedente Diversi articoli recenti hanno utilizzato simulazioni e algoritmi di marketclearing per esplorare l'impatto di uno scambio renale nazionale -LSB- 13, 20, 6, 14, 15, 17 -RSB-. Ad esempio, utilizzando l'algoritmo di corrispondenza massima di Edmond -LSB- 4 -RSB-, -LSB- 20 -RSB- mostra che un mercato nazionale di scambio a coppie -LRB- che utilizza solo cicli di lunghezza 2 -RRB- comporterebbe pi\u00f9 trapianti, tempi di attesa ridotti e risparmio di 750 milioni di dollari in costi sanitari in 5 anni. Questi risultati sono conservativi in \u200b\u200bdue modi. Innanzitutto, il mercato simulato conteneva solo 4.000 pazienti iniziali, con l\u2019aggiunta di 250 pazienti ogni 3 mesi. Ci \u00e8 stato riferito che il mercato potrebbe essere quasi il doppio di questa dimensione. In secondo luogo, gli scambi sono stati limitati a cicli di lunghezza 2 -LRB- perch\u00e9 \u00e8 tutto ci\u00f2 che pu\u00f2 essere modellato come corrispondenza massima e risolto utilizzando l'algoritmo di Edmonds -RRB-. Consentire cicli di lunghezza 3 porta a ulteriori guadagni significativi.Ci\u00f2 \u00e8 stato dimostrato sui mercati dello scambio renale con 100 pazienti utilizzando CPLEX per risolvere una codifica a programma intero del problema di compensazione -LSB- 15 -RSB-. In questo articolo presentiamo un algoritmo alternativo per questo programma intero in grado di liberare mercati con oltre 10.000 pazienti -LRB- e lo stesso numero di donatori disponibili -RRB-. Consentire cicli di durata superiore a 3 spesso non porta a nessun miglioramento nella dimensione dello scambio -LSB- 15 -RSB-. -LRB- Inoltre, in un modello teorico semplificato, qualsiasi scambio renale pu\u00f2 essere convertito in uno con cicli di lunghezza massima 4 -LSB- 15 -RSB-. -RRB- Anche se questo non vale per gli scambi generali di baratto, o anche per tutti i mercati dello scambio di reni, nella Sezione 5.2.3 ci avvaliamo dell'osservazione che cicli brevi sono sufficienti per aumentare notevolmente la velocit\u00e0 del nostro algoritmo. Ad alto livello, il problema della compensazione per gli scambi di baratto \u00e8 simile al problema della compensazione -LRB- ovvero il problema della determinazione del vincitore -RRB- nelle aste combinatorie. In entrambi i contesti, l\u2019idea \u00e8 quella di raccogliere tutte le informazioni pertinenti sugli agenti in un punto di compensazione centrale ed eseguire un algoritmo di compensazione centralizzato per determinare l\u2019allocazione. Entrambi i problemi sono NP-difficili. Entrambi vengono risolti al meglio utilizzando le tecniche di ricerca degli alberi. Dal 1999, \u00e8 stato svolto un lavoro significativo nel campo dell'informatica e della ricerca operativa su algoritmi di ricerca degli alberi ottimali pi\u00f9 rapidi per la liquidazione delle aste combinatorie. Tuttavia, il problema della compensazione dello scambio renale -LRB- con un limite di 3 o pi\u00f9 sulla dimensione del ciclo -RRB- \u00e8 diverso dal problema della compensazione dell'asta combinatoria in modi significativi. La differenza pi\u00f9 importante \u00e8 che le formulazioni naturali del problema dell\u2019asta combinatoria tendono ad adattarsi facilmente alla memoria, quindi nella pratica il tempo rappresenta il collo di bottiglia. Al contrario, le formulazioni naturali del problema dello scambio renale -LRB- con L = 3 -RRB- richiedono almeno spazio cubico nel numero di pazienti per modellare anche, e quindi la memoria diventa un collo di bottiglia molto prima del tempo quando si utilizza la ricerca ad albero standard. , come branch-andcut in CPLEX, per affrontare il problema. Pertanto, gli approcci sviluppati per le aste combinatorie non sono in grado di gestire il problema dello scambio renale. 1.2 Struttura del documento Il resto del documento \u00e8 organizzato come segue. La sezione 2 illustra il processo mediante il quale generiamo dati di mercato realistici sullo scambio renale, al fine di confrontare gli algoritmi di compensazione. La sezione 3 contiene la prova che il problema della decisione di compensazione del mercato \u00e8 NP-completo. Le sezioni 4 e 5 contengono ciascuna una formulazione ILP del problema del clearing. In quelle sezioni vengono inoltre descritte in dettaglio le nostre tecniche utilizzate per risolvere tali programmi su istanze di grandi dimensioni. La sezione 6 presenta esperimenti sulle varie tecniche. La sezione 7 discute la recente messa in campo del nostro algoritmo. Infine, presentiamo le nostre conclusioni nella Sezione 8 e suggeriamo future direzioni di ricerca. 7.UTILIZZARE LA TECNOLOGIA Il nostro algoritmo e la nostra implementazione hanno sostituito CPLEX come algoritmo di compensazione dell'Alliance for Paired Donation, uno dei principali scambi di reni, nel dicembre 2006. Conduciamo un match run ogni due settimane e i primi trapianti basati sulle nostre soluzioni sono gi\u00e0 stati eseguiti stato condotto. Sebbene ci siano attualmente -LRB- per ragioni politiche/interpersonali -RRB- almeno quattro scambi renali negli Stati Uniti, tutti comprendono che uno scambio nazionale unificato e non frammentato salverebbe pi\u00f9 vite. Stiamo discutendo con altri scambi renali interessati ad adottare la nostra tecnologia. In questo modo, si spera, la nostra tecnologia -LRB- e i processi attorno ad essa -RRB- serviranno come substrato che alla fine aiuter\u00e0 a unificare gli scambi. Almeno la scalabilit\u00e0 computazionale non \u00e8 pi\u00f9 un ostacolo. 8. CONCLUSIONE E RICERCA FUTURA In questo lavoro abbiamo sviluppato gli algoritmi esatti pi\u00f9 scalabili per gli scambi di baratto fino ad oggi, con particolare attenzione al prossimo mercato nazionale dello scambio di reni in cui i pazienti con malattie renali saranno abbinati a donatori compatibili scambiando il proprio donatori consenzienti ma incompatibili. Con oltre 70.000 pazienti gi\u00e0 in attesa di un rene da cadavere negli Stati Uniti, questo mercato \u00e8 visto come l\u2019unico modo etico per ridurre significativamente i 4.000 decessi all\u2019anno attribuiti alle malattie renali. Il nostro lavoro presenta il primo algoritmo in grado di compensare questi mercati su scala nazionale. Risolve in modo ottimale il problema dello scambio renale con 10.000 coppie di donatore. La migliore tecnologia precedente -LRB- vanilla CPLEX -RRB- non \u00e8 in grado di gestire istanze oltre le 900 coppie donatore-fattone perch\u00e9 esaurisce la memoria. La chiave del nostro miglioramento \u00e8 la formulazione incrementale del problema. Abbiamo adattato due paradigmi per l'attivit\u00e0: generazione di vincoli e generazione di colonne. Per ciascuno di essi abbiamo sviluppato una serie di tecniche che migliorano sostanzialmente sia il runtime che l'utilizzo della memoria. Alcune tecniche utilizzano osservazioni specifiche del dominio mentre altre sono indipendenti dal dominio. Concludiamo che la generazione di colonne scala notevolmente meglio della generazione di vincoli. Indubbiamente, si potrebbero utilizzare ulteriori regolazioni dei parametri e forse ulteriori tecniche di miglioramento della velocit\u00e0 per rendere l\u2019algoritmo ancora pi\u00f9 veloce. Il nostro algoritmo supporta anche diverse generalizzazioni, come desiderato dagli scambi renali nel mondo reale. Poich\u00e9 utilizziamo una metodologia ILP, possiamo anche supportare una serie di vincoli collaterali, che spesso svolgono un ruolo importante nella pratica dei mercati -LSB- 19 -RSB-. Possiamo anche essere favorevoli a forzare una parte dell'assegnazione, ad esempio dicendo: \"Questo adolescente gravemente malato deve ricevere, se possibile, un rene\". '' Il nostro lavoro ha trattato lo scambio di reni come un problema batch con informazioni complete -LRB- almeno nel breve periodo, molto probabilmente gli scambi di reni continueranno a funzionare in modalit\u00e0 batch ogni tanto -RRB-. Due importanti direzioni per il lavoro futuro consistono nell'affrontare esplicitamente sia gli aspetti online che quelli con informazioni limitate del problema.L\u2019aspetto online \u00e8 che i beneficiari e i donatori arriveranno nel sistema nel tempo, e potrebbe essere meglio non eseguire lo scambio miopemente ottimale adesso, ma piuttosto salvare parte del mercato attuale per abbinamenti successivi.", "keyphrases": ["mercato del baratto", "incontro", "gene di colonna", "reni", "trapianto", "caratteristica del mercato", "generatore di istanza", "approccio risolutivo", "formula del bordo", "formula ciclo"]}
{"file_name": "H-2", "text": "Espansione personalizzata delle query per il Web ABSTRACT L'ambiguit\u00e0 intrinseca delle query con keyphrases brevi richiede metodi avanzati per il recupero sul Web. In questo articolo proponiamo di migliorare tali query Web espandendole con termini raccolti dal repository di informazioni personali di ciascun utente, personalizzando cos\u00ec implicitamente l'output della ricerca. Introduciamo cinque tecniche generali per generare keyphrases di query aggiuntive analizzando i dati degli utenti a livelli di granularit\u00e0 crescenti, che vanno dall'analisi a livello di termini e composti fino alle statistiche di co-occorrenza globale, nonch\u00e9 all'utilizzo di thesauri esterni. La nostra ampia analisi empirica in quattro diversi scenari mostra che alcuni di questi approcci funzionano molto bene, soprattutto su query ambigue, producendo un forte aumento della qualit\u00e0 delle classifiche di output. Successivamente, spostiamo questo quadro di ricerca personalizzato un ulteriore passo avanti e proponiamo di rendere il processo di espansione adattivo alle varie caratteristiche di ciascuna query. Una serie separata di esperimenti indica che gli algoritmi adattivi apportano un ulteriore miglioramento statisticamente significativo rispetto al miglior approccio di espansione statica. 1. INTRODUZIONE La crescente popolarit\u00e0 dei motori di ricerca ha fatto s\u00ec che la semplice ricerca per keyphrases diventasse l'unica interfaccia utente ampiamente accettata per la ricerca di informazioni sul Web. Eppure le query relative a keyphrases sono * Parte di questo lavoro \u00e8 stata eseguita mentre l'autore stava visitando Yahoo! Ricerca, Barcellona, \u200b\u200bSpagna. intrinsecamente ambiguo. La query \"libro canonico\", ad esempio, copre diverse aree di interesse: religione, fotografia, letteratura e musica. Chiaramente, si preferirebbe che l'output della ricerca fosse allineato con l'argomento di interesse dell'utente -LRB- s -RRB-, piuttosto che visualizzare una selezione di URL popolari da ciascuna categoria. Gli studi hanno dimostrato che oltre l'80% degli utenti preferirebbe ricevere risultati di ricerca personalizzati -LSB- 33 -RSB- invece di quelli attualmente generici. L'espansione della query aiuta l'utente a formulare una query migliore, aggiungendo keyphrases aggiuntive alla richiesta di ricerca iniziale al fine di incapsularvi i suoi interessi, nonch\u00e9 di focalizzare di conseguenza l'output della ricerca Web. \u00c8 stato dimostrato che funziona molto bene su set di dati di grandi dimensioni, specialmente con query di input brevi -LRB- vedere ad esempio -LSB- 19, 3 -RSB- -RRB-. Questo \u00e8 esattamente lo scenario della ricerca sul Web! In questo articolo proponiamo di migliorare la riformulazione delle query Web sfruttando il Personal Information Repository dell'utente -LRB- PIR -RRB-, ovvero la raccolta personale di documenti di text, e-mail, pagine Web memorizzate nella cache, ecc. Diversi vantaggi sorgono durante lo spostamento Personalizzazione della ricerca Web fino al livello Desktop -LRB- tieni presente che con \"Desktop\" ci riferiamo a PIR e utilizziamo i due termini in modo intercambiabile -RRB-. Il primo \u00e8 ovviamente la qualit\u00e0 della personalizzazione: il desktop locale \u00e8 un ricco archivio di informazioni, che descrive accuratamente la maggior parte, se non tutti, gli interessi dell'utente.I nostri algoritmi espandono le query Web con keyphrases estratte dal PIR dell'utente, personalizzando cos\u00ec implicitamente l'output della ricerca. Dopo una discussione dei lavori precedenti nella Sezione 2, esamineremo innanzitutto l'analisi del context delle query del desktop locale nella Sezione 3.1.1. Proponiamo diverse tecniche basate su keyphrases, espressioni e riepiloghi per determinare i termini di espansione da quei documenti personali che meglio corrispondono alla query web. Nella Sezione 3.1.2 spostiamo la nostra analisi sulla raccolta globale del Desktop e investighiamo le espansioni basate su metriche di co-occorrenza e thesauri esterni. Gli esperimenti presentati nella Sezione 3.2 mostrano che molti di questi approcci funzionano molto bene, soprattutto su query ambigue, producendo miglioramenti NDCG -LSB- 15 -RSB- fino al 51,28%. Nella Sezione 4 spostiamo ulteriormente questo quadro algoritmico e proponiamo di rendere il processo di espansione adattivo al livello di chiarezza della query. Ci\u00f2 produce un ulteriore miglioramento dell'8,47% rispetto al miglior algoritmo precedentemente identificato. Concluderemo e discuteremo ulteriormente il lavoro nella Sezione 5. 2. LAVORO PRECEDENTE Questo documento riunisce due aree IR: personalizzazione della ricerca ed espansione automatica delle query. Esiste una grande quantit\u00e0 di algoritmi per entrambi i domini. In questa sezione presentiamo quindi un'analisi separata, introducendo prima alcuni approcci per personalizzare la ricerca, poich\u00e9 questo rappresenta l'obiettivo principale della nostra ricerca, e poi discutendo diverse tecniche di espansione delle query e la loro relazione con i nostri algoritmi. 2.1 Ricerca personalizzata La ricerca personalizzata comprende due componenti principali: -LRB- 1 -RRB- Profili utente e -LRB- 2 -RRB- L'algoritmo di ricerca vero e proprio. Questa sezione suddivide il background pertinente in base al focus di ciascun articolo in uno di questi elementi. Approcci focalizzati sul profilo utente. Sugiyama et al. -LSB- 32 -RSB- ha analizzato il comportamento di navigazione e ha generato profili utente come caratteristiche -LRB- termini -RRB- delle pagine visitate. Dopo aver inviato una nuova query, i risultati della ricerca sono stati classificati in base alla somiglianza tra ciascun URL e il profilo utente. Qiu e Cho -LSB- 26 -RSB- hanno utilizzato l'apprendimento automatico sulla cronologia dei clic passati dell'utente per determinare i vettori delle preferenze degli argomenti e quindi applicare il PageRank sensibile all'argomento -LSB- 13 -RSB-. La profilazione degli utenti basata sulla cronologia di navigazione ha il vantaggio di essere piuttosto semplice da ottenere ed elaborare. Questo \u00e8 probabilmente il motivo per cui viene utilizzato anche da diversi motori di ricerca industriali -LRB- ad esempio Yahoo! MyWeb2 -RRB-. Tuttavia, sicuramente non \u00e8 sufficiente per ottenere una visione approfondita degli interessi dell'utente. Inoltre, nessuno di questi ha studiato l\u2019applicazione adattiva della personalizzazione. Approcci focalizzati sull'algoritmo di personalizzazione. Haveliwala -LSB- 13 -RSB- ha calcolato un PageRank orientato all'argomento, in cui 16 vettori PageRank distorti su ciascuno degli argomenti principali di Open Directory sono stati inizialmente calcolati offline e quindi combinati in fase di esecuzione in base alla somiglianza tra l'utente query e ciascuno dei 16 argomenti. 2.2 Espansione automatica della query L'espansione automatica della query mira a ottenere una migliore formulazione della query dell'utente al fine di migliorarne il recupero. Si basa sullo sfruttamento di varie caratteristiche sociali o specifiche della raccolta per generare termini aggiuntivi, che vengono aggiunti alle keyphrases originali inserite prima di identificare i documenti corrispondenti restituiti come output. In questa sezione esaminiamo alcuni dei lavori rappresentativi di espansione delle query raggruppati in base alla fonte utilizzata per generare termini aggiuntivi: -LRB- 1 -RRB- Feedback sulla pertinenza, -LRB- 2 -RRB- Statistiche di co-occorrenza basate sulla raccolta e -LRB - 3 -RRB- Informazioni sul dizionario dei sinonimi. Alla fine della sezione vengono affrontati anche altri approcci. Tecniche di feedback sulla pertinenza. L'idea principale del Feedback sulla pertinenza -LRB- RF -RRB- \u00e8 che informazioni utili possono essere estratte dai documenti rilevanti restituiti per la query iniziale. I primi approcci sono stati manuali -LSB- 28 -RSB-, nel senso che era l'utente a scegliere i risultati rilevanti, quindi sono stati applicati vari metodi per estrarre nuovi termini, correlati alla query e ai documenti selezionati. Efthimiadis -LSB- 11 -RSB- ha presentato una revisione completa della letteratura e ha proposto diversi metodi semplici per estrarre tali nuove keyphrases in base alla frequenza dei termini, alla frequenza dei documenti, ecc.. Ne abbiamo utilizzati alcuni come ispirazione per le nostre tecniche specifiche per Desktop. Chang e Hsu -LSB- 5 -RSB- hanno chiesto agli utenti di scegliere cluster rilevanti, anzich\u00e9 documenti, riducendo cos\u00ec la quantit\u00e0 di interazione necessaria. \u00c8 stato anche dimostrato che RF viene automatizzato in modo efficace considerando i documenti in cima alla classifica come rilevanti -LSB- 37 -RSB- -LRB-, noto come Pseudo RF -RRB-. Lam e Jones -LSB- 21 -RSB- hanno utilizzato il riepilogo per estrarre frasi informative dai documenti di primo livello e aggiungerle alla query dell'utente. Infine, Yu et al. -LSB- 38 -RSB- ha selezionato i termini di espansione da segmenti di pagine Web basati sulla visione per far fronte ai molteplici argomenti in essi contenuti. Tecniche basate sulla co-occorrenza. \u00c8 stato dimostrato che i termini altamente ricorrenti con le keyphrases emesse aumentano la precisione quando aggiunti alla query -LSB- 17 -RSB-. Abbiamo anche studiato tre di questi approcci per identificare le keyphrases rilevanti per le query dal ricco, ma piuttosto complesso repository di informazioni personali. Tecniche basate sul thesaurus. Un metodo ampiamente esplorato consiste nell'espandere la query dell'utente con nuovi termini, il cui significato \u00e8 strettamente correlato alle keyphrases immesse. Proprio come per i metodi di co-occorrenza, gli esperimenti iniziali con questo approccio sono stati controversi, riportando miglioramenti o addirittura riduzioni nella qualit\u00e0 dell'output -LSB- 36 -RSB-. Utilizziamo anche termini di espansione basati su WordNet. Tuttavia, basiamo questo processo sull'analisi della relazione a livello di desktop tra la query originale e le nuove keyphrases proposte. Altre tecniche. Esistono molti altri tentativi per estrarre termini di espansione. Sebbene ortogonale al nostro approccio,due lavori sono molto rilevanti per l'ambiente Web: Cui et al. -LSB- 8 -RSB- ha generato correlazioni di parole utilizzando la probabilit\u00e0 che i termini di query appaiano in ciascun documento, calcolata sui log dei motori di ricerca. Kraft e Zien -LSB- 19 -RSB- hanno dimostrato che l'anchor text \u00e8 molto simile alle query degli utenti e quindi lo hanno sfruttato per acquisire keyphrases aggiuntive. 5. CONCLUSIONI E ULTERIORI LAVORI In questo articolo abbiamo proposto di espandere le query di ricerca sul Web sfruttando il Personal Information Repository dell'utente al fine di estrarre automaticamente ulteriori keyphrases correlate sia alla query stessa che agli interessi dell'utente, personalizzando l'output della ricerca. In questo context, il documento include i seguenti contributi: \u2022 Abbiamo proposto cinque tecniche per determinare i termini di espansione dai documenti personali. Ciascuno di essi produce keyphrases di query aggiuntive analizzando il desktop dell'utente a livelli di granularit\u00e0 crescenti, che vanno dall'analisi a livello di termini ed espressioni fino alle statistiche globali di co-occorrenza e ai thesauri esterni. Figura 1: Guadagno NDCG relativo -LRB- in % -RRB- per ciascun algoritmo nel complesso, nonch\u00e9 separato per categoria di query. \u2022 Abbiamo fornito un'analisi empirica approfondita di diverse varianti dei nostri approcci, in quattro diversi scenari. Abbiamo dimostrato che alcuni di questi approcci funzionano molto bene, producendo miglioramenti NDCG fino al 51,28%. \u2022 Abbiamo spostato ulteriormente questo quadro di ricerca personalizzato e abbiamo proposto di rendere il processo di espansione adattabile alle caratteristiche di ciascuna query, concentrandosi fortemente sul suo livello di chiarezza. \u2022 Nell'ambito di una serie separata di esperimenti, abbiamo dimostrato che i nostri algoritmi adattivi forniscono un ulteriore miglioramento dell'8,47% rispetto all'approccio migliore precedentemente identificato. Stiamo attualmente eseguendo indagini sulla dipendenza tra le varie funzionalit\u00e0 di query e il numero ottimale di termini di espansione. Stiamo anche analizzando altri tipi di approcci per identificare suggerimenti di espansione delle query, come l'applicazione dell'analisi semantica latente sui dati desktop. Infine, stiamo progettando una serie di combinazioni pi\u00f9 complesse di questi parametri per fornire una maggiore adattivit\u00e0 ai nostri algoritmi.Ciascuno di essi produce keyphrases di query aggiuntive analizzando il desktop dell'utente a livelli di granularit\u00e0 crescenti, che vanno dall'analisi a livello di termini ed espressioni fino alle statistiche globali di co-occorrenza e ai thesauri esterni. Figura 1: Guadagno NDCG relativo -LRB- in % -RRB- per ciascun algoritmo nel complesso, nonch\u00e9 separato per categoria di query. \u2022 Abbiamo fornito un'analisi empirica approfondita di diverse varianti dei nostri approcci, in quattro diversi scenari. Abbiamo dimostrato che alcuni di questi approcci funzionano molto bene, producendo miglioramenti NDCG fino al 51,28%. \u2022 Abbiamo spostato ulteriormente questo quadro di ricerca personalizzato e abbiamo proposto di rendere il processo di espansione adattabile alle caratteristiche di ciascuna query, concentrandosi fortemente sul suo livello di chiarezza. \u2022 Nell'ambito di una serie separata di esperimenti, abbiamo dimostrato che i nostri algoritmi adattivi forniscono un ulteriore miglioramento dell'8,47% rispetto all'approccio migliore precedentemente identificato. Stiamo attualmente eseguendo indagini sulla dipendenza tra le varie funzionalit\u00e0 di query e il numero ottimale di termini di espansione. Stiamo anche analizzando altri tipi di approcci per identificare suggerimenti di espansione delle query, come l'applicazione dell'analisi semantica latente sui dati desktop. Infine, stiamo progettando una serie di combinazioni pi\u00f9 complesse di questi parametri per fornire una maggiore adattivit\u00e0 ai nostri algoritmi.Ciascuno di essi produce keyphrases di query aggiuntive analizzando il desktop dell'utente a livelli di granularit\u00e0 crescenti, che vanno dall'analisi a livello di termini ed espressioni fino alle statistiche globali di co-occorrenza e ai thesauri esterni. Figura 1: Guadagno NDCG relativo -LRB- in % -RRB- per ciascun algoritmo nel complesso, nonch\u00e9 separato per categoria di query. \u2022 Abbiamo fornito un'analisi empirica approfondita di diverse varianti dei nostri approcci, in quattro diversi scenari. Abbiamo dimostrato che alcuni di questi approcci funzionano molto bene, producendo miglioramenti NDCG fino al 51,28%. \u2022 Abbiamo spostato ulteriormente questo quadro di ricerca personalizzato e abbiamo proposto di rendere il processo di espansione adattabile alle caratteristiche di ciascuna query, concentrandosi fortemente sul suo livello di chiarezza. \u2022 Nell'ambito di una serie separata di esperimenti, abbiamo dimostrato che i nostri algoritmi adattivi forniscono un ulteriore miglioramento dell'8,47% rispetto all'approccio migliore precedentemente identificato. Stiamo attualmente eseguendo indagini sulla dipendenza tra le varie funzionalit\u00e0 di query e il numero ottimale di termini di espansione. Stiamo anche analizzando altri tipi di approcci per identificare suggerimenti di espansione delle query, come l'applicazione dell'analisi semantica latente sui dati del desktop. Infine, stiamo progettando una serie di combinazioni pi\u00f9 complesse di questi parametri per fornire una maggiore adattivit\u00e0 ai nostri algoritmi.", "keyphrases": ["parola chiave breve queri", "recupero web", "domanda web", "la persona informa i repository", "risultato della ricerca", "aggiungi la parola chiave queri", "livello granulare", "analisi a livello di termini e composti", "Statista globale della co-occorrenza", "dizionario dei sinonimi esterno", "estende l'analisi empirica", "domanda ambigua", "qualit\u00e0", "rango di uscita", "quadro di ricerca delle persone", "espande il processo", "varie caratteristiche di ogni domanda", "adattare l'algoritmo", "improvvisazione significativa", "approccio di espansione statica"]}
{"file_name": "J-30", "text": "Implementazione con uno spazio d'azione limitato ABSTRACT Mentre la progettazione tradizionale del meccanismo presuppone tipicamente l'isomorfismo tra il tipo degli agenti e gli spazi d'azione, in molte situazioni gli agenti affrontano rigide restrizioni sul loro spazio d'azione a causa, ad esempio, di ragioni tecniche, comportamentali o normative. Elaboriamo un quadro generale per lo studio della progettazione di meccanismi in ambienti a parametro singolo con spazi di azione ristretti. Il nostro contributo \u00e8 triplice. In primo luogo, caratterizziamo condizioni sufficienti in cui la regola di scelta sociale teoricamente ottimale per l\u2019informazione pu\u00f2 essere implementata in strategie dominanti e dimostriamo che qualsiasi regola di scelta sociale multilineare \u00e8 implementabile nella strategia dominante senza costi aggiuntivi. In secondo luogo, identifichiamo le condizioni necessarie per l'ottimalit\u00e0 dei meccanismi limitati dall'azione e caratterizziamo completamente i meccanismi e le strategie ottimali nei giochi con due giocatori e due alternative. Infine, dimostriamo che per qualsiasi regola di scelta sociale multilineare, il meccanismo ottimale con k azioni comporta una perdita attesa di O -LRB- k21 -RRB- rispetto ai meccanismi ottimali con spazi di azione illimitati. I nostri risultati si applicano a vari contesti economici e computazionali e dimostriamo la loro applicabilit\u00e0 ai giochi di segnalazione, ai modelli di bene pubblico e al routing nelle reti. 1. INTRODUZIONE La progettazione dei meccanismi \u00e8 un sottocampo della teoria dei giochi che studia come progettare regole di giochi che portino a risultati desiderabili, quando i giocatori sono razionali. In un'impostazione standard, i giocatori conservano alcune informazioni private - i loro \"tipi\" - e scelgono le \"azioni\" dai loro spazi azione per massimizzare le loro utilit\u00e0. Il pianificatore sociale desidera implementare una funzione di scelta sociale, che mappa ogni possibile stato del mondo -LRB- cio\u00e8 un profilo dei tipi di giocatori -RRB- verso un'unica alternativa. Ad esempio, un governo che desidera intraprendere un progetto di bene pubblico -LRB- ad esempio, costruendo un ponte -RRB- solo se il beneficio totale per i giocatori supera il suo costo. Gran parte della letteratura sulla progettazione dei meccanismi limita l'attenzione ai meccanismi di rivelazione diretta, in cui lo spazio d'azione di un giocatore \u00e8 identico al suo spazio tipo. Questa attenzione \u00e8 dovuta al principio di rivelazione che asserisce che se qualche meccanismo raggiunge un certo risultato in un equilibrio, lo stesso risultato pu\u00f2 essere raggiunto in un equilibrio veritiero \u2013 un equilibrio in cui ciascun agente riporta semplicemente il suo tipo privato -LSB- 15 -RSB -. Tuttavia, in molti ambienti, i meccanismi di rivelazione diretta non sono praticabili poich\u00e9 le azioni a disposizione dei giocatori hanno un potere espressivo limitato. Consideriamo, ad esempio, il ben studiato modello di \"screening\", in cui una compagnia assicurativa desidera vendere diversi tipi di polizze a diversi conducenti in base al loro livello di cautela, che rappresenta le loro informazioni private. Ci sono varie ragioni per restrizioni cos\u00ec rigide sugli spazi di azione.Gli acquirenti in tali ambienti devono affrontare solo due azioni: acquistare o non acquistare, sebbene possano avere un numero infinito di possibili valori per l'articolo. In molti contesti simili, i giocatori potrebbero anche essere riluttanti a rivelare la propria tipologia precisa, ma disposti a rivelare informazioni parziali su di loro. Ad esempio, gli agenti in genere non saranno disposti a rivelare la propria tipologia, anche se sarebbe vantaggioso per loro nel breve periodo, poich\u00e9 potrebbe danneggiarli nelle transazioni future. Gli agenti potrebbero anche non fidarsi del meccanismo per mantenere private le loro valutazioni -LSB- 16 -RSB-, o addirittura non conoscerne il tipo esatto mentre calcolarlo potrebbe essere costoso -LSB- 12 -RSB-. Consideriamo ad esempio un modello di bene pubblico: un pianificatore sociale deve decidere se costruire un ponte. I due giocatori nel gioco hanno alcuni benefici privati \u200b\u200b\u03b81, \u03b82 \u2208 -LSB- 0, 1 -RSB- derivanti dall'uso di questo bridge. Il pianificatore sociale mira a costruire il ponte solo se la somma di questi benefici supera il costo di costruzione del ponte. Il pianificatore sociale non pu\u00f2 accedere ai dati privati \u200b\u200bdei giocatori, ma pu\u00f2 solo venirne a conoscenza dalle azioni dei giocatori. Quando \u00e8 consentita la rivelazione diretta, il pianificatore sociale pu\u00f2 far funzionare il noto meccanismo VCG, in cui i giocatori hanno incentivi a riportare i loro dati reali; quindi, il pianificatore pu\u00f2 ottenere le esatte informazioni private dei giocatori e costruire il ponte solo quando dovrebbe essere costruito. Supponiamo ora che i giocatori non possano inviare tutti i loro dati segreti, ma possano solo scegliere un'azione tra due possibili azioni -LRB-, ad esempio `` 0 '' o `` 1 '' -RRB-. Ora, evidentemente, il pianificatore sociale non potr\u00e0 pi\u00f9 costruire il ponte sempre secondo la sua funzione oggettiva, a causa della limitata espressivit\u00e0 dei messaggi degli attori. In questo lavoro si cerca di analizzare cosa \u00e8 possibile ottenere in presenza di tali restrizioni. Le restrizioni sullo spazio d'azione, per modelli specifici, sono state studiate in diversi articoli precedenti. Hanno studiato aste a singolo oggetto in cui agli offerenti \u00e8 consentito inviare messaggi con dimensioni fortemente limitate. Hanno caratterizzato i meccanismi ottimali in base a questa restrizione e hanno dimostrato che \u00e8 possibile ottenere risultati quasi ottimali anche con limitazioni molto rigide sullo spazio di azione. Il nostro lavoro generalizza i principali risultati di Blumrosen et al. a un quadro generale di progettazione del meccanismo che pu\u00f2 essere applicato a una moltitudine di modelli. Un'impostazione di progettazione di un meccanismo standard \u00e8 composta da agenti con informazioni private -LRB- i loro \"tipi\" -RRB-, e un pianificatore sociale, che desidera implementare una funzione di scelta sociale, c -- una funzione che mappa qualsiasi profilo del tipi di agenti in un'alternativa scelta. Un risultato classico in questo context dice che sotto una certa assunzione di monotonia sulle preferenze degli agenti - l'ipotesi del `` single-crossing '' -LRB- vedi definizione sotto -RRB- -- una funzione di scelta sociale \u00e8 implementabile nelle strategie dominanti se e solo se \u00e8 monotono nelle tipologie dei giocatori. Tuttavia,in ambienti con spazi di azione ristretti, il pianificatore sociale in genere non pu\u00f2 implementare ogni funzione di scelta sociale a causa dei vincoli informativi intrinseci. Cio\u00e8, per alcune realizzazioni dei tipi di giocatori, la decisione del pianificatore sociale sar\u00e0 incompatibile con la funzione di scelta sociale c. Per misurare quantitativamente quanto i meccanismi di azione limitata possano approssimare le funzioni originali di scelta sociale, seguiamo l\u2019ipotesi standard secondo cui la funzione di scelta sociale deriva da una funzione di valore sociale, g, che assegna un valore reale per ogni alternativa A e realizzazione delle tipologie dei giocatori. La funzione di scelta sociale c sceglier\u00e0 quindi un'alternativa che massimizza la funzione di valore sociale, dato il tipo \u2192 \u2212 \u03b8 = -LRB- \u03b81,. . Si osservi che la funzione del valore sociale non \u00e8 necessariamente la funzione del benessere sociale: la funzione del benessere sociale \u00e8 un caso speciale di g in cui g \u00e8 definito come la somma delle valutazioni dei giocatori per l'alternativa scelta. Di seguito sono riportati alcuni semplici esempi di funzioni di valore sociale: \u2022 Beni pubblici. Un governo desidera costruire un ponte solo se la somma dei benefici che gli agenti ne traggono supera il costo di costruzione C. Le funzioni di valore sociale in un gioco a 2 giocatori saranno quindi: g -LRB- \u03b81, \u03b82, `` build '' -RRB- = \u03b81 + \u03b82-C e g -LRB- \u03b81, \u03b82, `` non costruire '' -RRB- = 0. \u2022 Routing nelle reti. Consideriamo una rete composta da due collegamenti in parallelo. Ogni collegamento ha una probabilit\u00e0 segreta pi di trasferire con successo un messaggio. Un mittente desidera inviare il suo messaggio attraverso la rete solo se la probabilit\u00e0 di successo \u00e8 maggiore, diciamo, del 90% - la probabilit\u00e0 nota in una rete alternativa. \u2022 Aste per singoli articoli. Considera un'asta a 2 giocatori, in cui il banditore desidera assegnare l'oggetto al giocatore che lo valuta di pi\u00f9. La funzione di scelta sociale \u00e8 data da: g -LRB- \u03b81, \u03b82, ``vince il giocatore 1'' -RRB- = \u03b81 e per la seconda alternativa \u00e8 g -LRB- \u03b81, \u03b82, ``vince il giocatore 2'' - RRB- = \u03b82. 1.1 Il nostro contributo In questo articolo presentiamo un quadro generale per lo studio della progettazione di meccanismi in ambienti con un numero limitato di azioni. Assumiamo un modello bayesiano in cui i giocatori hanno tipi privati \u200b\u200bunidimensionali, distribuiti indipendentemente su un intervallo reale. La domanda principale che ci poniamo \u00e8: quando agli agenti \u00e8 consentito utilizzare solo k azioni diverse, quali meccanismi raggiungono il valore sociale atteso ottimale? Tieni presente che questa domanda \u00e8 in realt\u00e0 composta da due domande separate. La prima domanda \u00e8 una questione di teoria dell'informazione: qual \u00e8 il risultato ottimale ottenibile quando i giocatori possono rivelare informazioni solo utilizzando queste k azioni -LRB- ricordando che il loro spazio dei tipi pu\u00f2 essere continuo -RRB-. L'altra domanda riguarda considerazioni di teoria dei giochi: qual \u00e8 il miglior risultato ottenibile con k azioni, dove questo risultato dovrebbe essere raggiunto in un equilibrio di strategia dominante.Queste domande sollevano la questione del \u201cprezzo di implementazione\u201d: il risultato ottimale della teoria dell\u2019informazione pu\u00f2 sempre essere implementato in un equilibrio di strategia dominante? E in caso contrario, in che misura il requisito della strategia dominante degrada il risultato ottimale? Il nostro primo contributo \u00e8 la caratterizzazione di condizioni sufficienti per implementare la regola ottimale di scelta sociale della teoria dell\u2019informazione nelle strategie dominanti. Mostriamo che per la famiglia di funzioni di valore sociale multilineari -LRB- il Teorema: Data qualsiasi funzione di valore sociale multilineare a incrocio singolo, e per qualsiasi numero di alternative e giocatori, la regola di scelta sociale che \u00e8 teoricamente ottimale dal punto di vista informativo \u00e8 implementabile in dominanti strategie. Le funzioni multilineari di valore sociale catturano molti modelli importanti e ben studiati e includono, ad esempio, l'esempio di routing fornito sopra e qualsiasi funzione di benessere sociale in cui le valutazioni dei giocatori sono lineari nei loro tipi -LRB- come i beni pubblici e aste -RRB-. L\u2019implementabilit\u00e0 dei meccanismi di informazione teoricamente ottimali ci consente di utilizzare una routine standard nel Mechanism Design e determinare prima la regola di scelta sociale ottimale, quindi calcolare i pagamenti appropriati che garantiscono la compatibilit\u00e0 degli incentivi. Per mostrare questo risultato, dimostriamo un utile lemma che fornisce un'altra caratterizzazione per le funzioni di scelta sociale il cui \"prezzo di implementazione\" \u00e8 zero. Mostriamo che per qualsiasi funzione di scelta sociale, la compatibilit\u00e0 degli incentivi nei meccanismi limitati all\u2019azione \u00e8 equivalente alla propriet\u00e0 che il valore sociale atteso ottimale viene raggiunto con strategie non decrescenti -LRB- o strategie con soglia -RRB-.1 In altre parole, questo lemma implica che si possa sempre implementare, con strategie dominanti, la migliore regola di scelta sociale ottenibile con strategie non decrescenti. Il nostro secondo contributo consiste nel caratterizzare i meccanismi ottimali legati all'azione. Identifichiamo alcune condizioni necessarie per l'ottimalit\u00e0 dei meccanismi in generale e, utilizzando queste condizioni, caratterizziamo completamente i meccanismi ottimali in ambienti con due giocatori e due alternative. Completiamo la caratterizzazione dei meccanismi ottimali con la descrizione delle strategie ottimali - strategie che sono \"massimizzatrici reciproche\". Poich\u00e9 i pagamenti nell\u2019implementazione di una strategia dominante sono definiti in modo univoco da un\u2019allocazione monotona e da un profilo di strategie, ci\u00f2 definisce anche i pagamenti nel meccanismo. Diamo una prova intuitiva dell'ottimalit\u00e0 di tali strategie, generalizzando il concetto di strategie ottimali `` mutuamente centrate '' da -LSB- 4 -RSB-. Sorprendentemente, a differenza delle aste ottimali in -LSB- 4 -RSB-, per alcune funzioni di valore sociale non banali, il meccanismo \u201cdiagonale\u201d ottimale potrebbe non utilizzare tutte le k azioni disponibili. Teorema: per qualsiasi funzione di valore sociale multilineare a incrocio singolo su due alternative,il meccanismo di azione k a 2 giocatori ottimale dal punto di vista informativo \u00e8 diagonale e le strategie dominanti ottimali sono reciprocamente massimizzatrici. Sembra essere pi\u00f9 difficile ottenere una caratterizzazione completa del meccanismo ottimale di azione limitata per ambienti multi-giocatore o multi-alternativi. A sostegno di questa affermazione, osserviamo che il numero di meccanismi che soddisfano le condizioni necessarie di cui sopra sta crescendo esponenzialmente nel numero di giocatori. 1La restrizione a strategie non decrescenti \u00e8 molto comune in letteratura. Un risultato notevole di Athey -LSB- 1 -RSB- mostra che quando una strategia non decrescente \u00e8 la risposta migliore per qualsiasi altro profilo di strategie non decrescenti, deve esistere un equilibrio bayesiano-Nash puro. Il nostro prossimo risultato confronta il valore sociale atteso nei meccanismi di k-azione con il valore sociale atteso ottimale quando lo spazio di azione non \u00e8 limitato. Per qualsiasi numero di attori o alternative, e per qualsiasi profilo di funzioni di distribuzione indipendenti, costruiamo meccanismi che sono quasi ottimali \u2013 fino a una differenza additiva di O -LRB- k21 -RRB-. Questo risultato si ottiene nelle strategie dominanti. Teorema: Per qualsiasi funzione di valore sociale multilineare, il meccanismo di azione k ottimale comporta una perdita sociale attesa di O -LRB- k21 -RRB-. Si noti che ci sono funzioni di scelta sociale che possono essere implementate con k azioni senza alcuna perdita -LRB-, ad esempio la regola \"scegli sempre l'alternativa A\" -RRB-. Tuttavia, sappiamo che in alcuni contesti -LRB- ad esempio, aste -LSB- 5 -RSB- -RRB- la perdita ottimale pu\u00f2 essere proporzionale a 1k2, quindi un limite superiore generale migliore \u00e8 impossibile. Infine, presentiamo i nostri risultati nel context di diverse applicazioni naturali. Per prima cosa diamo una soluzione esplicita per un gioco del bene pubblico con k-azioni. Questa \u00e8 un'applicazione naturale nel nostro context poich\u00e9 i livelli di istruzione sono spesso discreti -LRB- ad esempio BA, MA e PhD -RRB-. Quest'ultimo esempio illustra come i nostri risultati si applicano a contesti in cui l'obiettivo del pianificatore sociale non \u00e8 la massimizzazione del benessere -LRB- n\u00e9 sue varianti come i \"massimizzatori affini\" -RRB-. Il resto del documento \u00e8 organizzato come segue: il nostro modello e le nostre notazioni sono descritti nella Sezione 2. Descriviamo quindi i nostri risultati generali riguardanti l'implementazione in ambienti multi-giocatore e multi-alternativi nella Sezione 3, inclusa l'analisi asintotica del valore sociale perdita. Nella Sezione 4, caratterizziamo completamente i meccanismi ottimali per ambienti a 2 giocatori con due alternative. Nella Sezione 5 concludiamo applicando i nostri risultati generali a diversi modelli ben studiati. 5. ESEMPI I nostri risultati si applicano a una variet\u00e0 di contesti economici, computazionali e di rete. In questa sezione, dimostriamo l'applicabilit\u00e0 dei nostri risultati a modelli di bene pubblico, giochi di segnalazione e applicazioni di routing. 5.1 Applicazione 1: Beni pubblici Il modello del bene pubblico si occupa di un pianificatore sociale -LRB- ad esempio, il governo -RRB- che deve decidere se fornire un bene pubblico, come la costruzione di un ponte.Sia S\u00ec e No denotino le rispettive alternative di costruire e non costruire il ponte. v = v1,..., vn \u00e8 il vettore dei tipi dei giocatori: i valori che ottengono utilizzando il bridge. La decisione che massimizza il benessere sociale \u00e8 quella di costruire il ponte se e solo se P viene costruito, il benessere sociale \u00e8 P i vi \u00e8 maggiore del suo costo, indicato con C. L'utilit\u00e0 del giocatore i con il pagamento pi \u00e8 ui = vi - - pi greco se il ponte \u00e8 costruito e 0 altrimenti. \u00c8 noto che senza alcuna restrizione nello spazio d'azione, \u00e8 possibile indurre una rivelazione veritiera mediante i meccanismi VCG, quindi \u00e8 possibile raggiungere la piena efficienza. Ovviamente, quando l'insieme di azioni \u00e8 limitato a k azioni, non possiamo raggiungere la piena efficienza a causa dei vincoli informativi. Quindi, il meccanismo di azione teoricamente ottimale dell\u2019informazione \u00e8 implementabile nelle strategie dominanti. Inoltre, come suggerisce il Teorema 3, nel gioco del bene pubblico a k-azioni per 2 giocatori, possiamo caratterizzare completamente i meccanismi ottimali. Nella dimostrazione del Teorema 3, abbiamo visto che quando per entrambi i giocatori g -LRB- \u03b8i, \u03b8i, A -RRB- = g -LRB- \u03b8i, \u03b8i, B -RRB-, il meccanismo \u00e8 non degenere rispetto ad entrambi giocatori.6 Questa condizione vale chiaramente qui -LRB- 1 + 0 -- C = 0 + 1 -- C -RRB-, quindi i meccanismi ottimali utilizzeranno tutte le k azioni. 1. Allocazione: Costruisci il ponte se j '' b1 + b2 > k. Strategie: Strategie di soglia basate sui vettori \u2192 -- x, -- \u2192 y dove per ogni 1 < i < k-1, 2. Allocazione: Costruisci il ponte se j '' b1 + b2 > k -- 1. Strategie: Strategie di soglia basate sui vettori \u2192 -- x, -- \u2192 y dove per ogni 1 < i < k-1 : Ricordiamo che definiamo i meccanismi ottimali mediante il loro schema di allocazione e mediante le strategie ottimali per i giocatori. \u00c8 noto che lo schema di assegnazione in meccanismi monotoni definisce in modo univoco i pagamenti che garantiscono la compatibilit\u00e0 degli incentivi. Nei giochi di bene pubblico, questi pagamenti soddisfano la regola secondo cui un giocatore paga il suo valore pi\u00f9 basso per il quale viene costruito il ponte, quando l'azione dell'altro giocatore \u00e8 fissa. Pertanto, i pagamenti per i giocatori 1 e 2 che riportano le azioni b1 e b2 sono i seguenti: nel meccanismo 1 della Proposizione 3, p1 = xb2 e p2 = yb1 ; nel meccanismo 2 dalla Proposizione 3, p1 = xb2 -- 1 e p2 = yb1 -- 1. Mostriamo ora un esempio pi\u00f9 specifico che presuppone distribuzioni uniformi. L'esempio mostra come il meccanismo ottimale sia determinato dal costo C: per bassi costi, il meccanismo di tipo 1 \u00e8 ottimale, e per costi elevati il \u200b\u200bmeccanismo ottimale \u00e8 di tipo 2. Un'ulteriore caratteristica interessante dei meccanismi ottimali nell'esempio \u00e8 che sono simmetrici rispetto ai giocatori. Ci\u00f2 si contrappone ai meccanismi ottimali del modello d'asta -LSB- 5 -RSB- che sono asimmetrici -LRB- anche quando i valori dei giocatori provengono da distribuzioni identiche -RRB-. Figura 2: Meccanismi ottimali in un gioco di beni pubblici a 2 giocatori, 2 alternative e 2 azioni, quando i tipi sono distribuiti uniformemente in -LSB- 0,1 -RSB-. Il meccanismo a sinistra \u00e8 ottimale quando C < 1 e l'altro \u00e8 ottimale quando C > 1. ESEMPIO 1. Supponiamo che le tipologie di entrambi i giocatori siano uniformemente distribuite su -LSB- 0, 1 -RSB-. La Figura 2 illustra i meccanismi ottimali per k = 2, e mostra come sia lo schema di allocazione che i pagamenti dipendano dal costo di costruzione C. Quindi, i meccanismi di massimizzazione del benessere sono: 5.2 Applicazione 2: Segnalazione Studiamo ora un modello di segnalazione nel travaglio mercati. In questo modello, la tipologia di ciascun lavoratore, \u03b8i \u2208 -LSB- \u03b8, \u03b8 -RSB-, descrive il livello di produttivit\u00e0 del lavoratore. L\u2019impresa vuole prendere le decisioni di assunzione secondo una funzione decisionale f -LRB- \u2212 \u2192 \u03b8 -RRB-. Ad esempio, l\u2019impresa potrebbe voler assumere il lavoratore pi\u00f9 produttivo -LRB- come nel modello dell\u2019asta -RRB-, o assumere un gruppo di lavoratori solo se la loro somma di produttivit\u00e0 \u00e8 maggiore di una soglia -LRB- simile al bene pubblico modello -RRB-. Tuttavia, la produttivit\u00e0 del lavoratore \u00e8 invisibile all'impresa; l'impresa osserva solo il livello di istruzione del lavoratore e che dovrebbe fornire segnali sul suo livello di produttivit\u00e0. Si noti che qui si presuppone che l'acquisizione dell'istruzione, a qualsiasi livello, non influisca sulla produttivit\u00e0 del lavoratore, ma solo segnali sulle competenze del lavoratore. Una componente principale di questo modello \u00e8 il fatto che poich\u00e9 il lavoratore \u00e8 pi\u00f9 produttivo, \u00e8 pi\u00f9 facile per lui acquisire un'istruzione di alto livello. Inoltre, il costo dell\u2019acquisizione dell\u2019istruzione aumenta con il livello di istruzione. Pi\u00f9 formalmente, una funzione continua C -LRB- e, \u03b8 -RRB- descrive il costo che un lavoratore deve sostenere per acquisire ciascun livello di istruzione in funzione della sua produttivit\u00e0. Un'azione per un lavoratore in questo gioco \u00e8 il livello di istruzione che sceglie di acquisire. Nei modelli standard, questo spazio d'azione \u00e8 continuo, e quindi esiste un \u201cequilibrio completamente separante\u201d -LRB- nelle condizioni di incrocio singolo sulla funzione di costo -RRB-. Esiste cio\u00e8 un equilibrio in cui ogni tipologia \u00e8 mappata in un diverso livello di istruzione; pertanto, l'impresa pu\u00f2 indurre gli esatti livelli di produttivit\u00e0 dei lavoratori tramite questo meccanismo di segnalazione. Tuttavia, \u00e8 difficile immaginare un mondo con un continuum di livelli di istruzione. Di solito accade che ci siano solo diversi livelli di istruzione distinti -LRB- ad esempio, BSc, MSc, PhD -RRB-. Con k livelli di istruzione, l\u2019impresa potrebbe non essere in grado di seguire esattamente la funzione decisionale f. Per ottenere il miglior risultato nelle k azioni, l\u2019impresa potrebbe volere che i lavoratori giochino secondo specifiche strategie di soglia. Si scopre che la condizione standard, la condizione del single-crossing sulla funzione di costo, \u00e8 sufficiente per garantire che queste strategie di soglia siano dominanti per i giocatori. COROLLARIO 4. Consideriamo una funzione decisionale multilineare fe una funzione di costo a incrocio singolo per i giocatori. Con k livelli di istruzione, l\u2019impresa pu\u00f2 implementare nelle strategie dominanti una funzione decisionale che comporta una perdita di O -LRB- k21 -RRB- rispetto alla funzione decisionale f. 5.3 Applicazione 3: Routing Nel nostro ultimo esempio, mostriamo l'applicabilit\u00e0 dei nostri risultati al routing in reti con perdite. In tali sistemi, il mittente deve decidere attraverso quale rete trasmettere il suo messaggio. In questo esempio, ci concentriamo sulle reti a percorso parallelo. I bordi di queste reti sono controllati da diversi agenti egoisti e ciascun bordo appare solo in una delle reti. Supponiamo che il mittente, che desidera inviare un messaggio dalla sorgente al sink, conosca la topologia di ciascuna rete, ma la probabilit\u00e0 di successo su ciascun collegamento, pi greco, corrisponde alle informazioni private del collegamento. Il problema del mittente \u00e8 decidere se inviare un messaggio attraverso la rete N1 o attraverso una rete alternativa N2. Ovviamente il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.In tali sistemi, il mittente deve decidere attraverso quale rete trasmettere il suo messaggio. In questo esempio, ci concentriamo sulle reti a percorso parallelo. I bordi di queste reti sono controllati da diversi agenti egoisti e ciascun bordo appare solo in una delle reti. Supponiamo che il mittente, che desidera inviare un messaggio dalla sorgente al sink, conosca la topologia di ciascuna rete, ma la probabilit\u00e0 di successo su ciascun collegamento, pi greco, corrisponde alle informazioni private del collegamento. Il problema del mittente \u00e8 decidere se inviare un messaggio attraverso la rete N1 o attraverso una rete alternativa N2. Ovviamente il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.In tali sistemi, il mittente deve decidere attraverso quale rete trasmettere il suo messaggio. In questo esempio, ci concentriamo sulle reti a percorso parallelo. I bordi di queste reti sono controllati da diversi agenti egoisti e ciascun bordo appare solo in una delle reti. Supponiamo che il mittente, che desidera inviare un messaggio dalla sorgente al sink, conosca la topologia di ciascuna rete, ma la probabilit\u00e0 di successo su ciascun collegamento, pi greco, corrisponde alle informazioni private del collegamento. Il problema del mittente \u00e8 decidere se inviare un messaggio attraverso la rete N1 o attraverso una rete alternativa N2. Ovviamente il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.e ciascun bordo appare solo in una delle reti. Supponiamo che il mittente, che desidera inviare un messaggio dalla sorgente al sink, conosca la topologia di ciascuna rete, ma la probabilit\u00e0 di successo su ciascun collegamento, pi greco, corrisponde alle informazioni private del collegamento. Il problema del mittente \u00e8 decidere se inviare un messaggio attraverso la rete N1 o attraverso una rete alternativa N2. Ovviamente il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.e ciascun bordo appare solo in una delle reti. Supponiamo che il mittente, che desidera inviare un messaggio dalla sorgente al sink, conosca la topologia di ciascuna rete, ma la probabilit\u00e0 di successo su ciascun collegamento, pi greco, corrisponde alle informazioni private del collegamento. Il problema del mittente \u00e8 decidere se inviare un messaggio attraverso la rete N1 o attraverso una rete alternativa N2. Ovviamente il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.il mittente desidera inviare il messaggio tramite N1 solo se la probabilit\u00e0 totale di successo in N1 \u00e8 maggiore della probabilit\u00e0 di successo in N2. Sia f N -LRB- \u2212 \u2192 p -RRB- la probabilit\u00e0 di successo nella rete N con un vettore di probabilit\u00e0 di successo \u2192 \u2212 p. La funzione di scelta sociale in questo esempio \u00e8 quindi: c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figura 3: Un esempio di rete a percorso parallelo, in cui ciascun collegamento ha una probabilit\u00e0 pi greco per il successo della trasmissione. Mostriamo che la probabilit\u00e0 complessiva di successo in tali reti \u00e8 multilineare in pi greco, e quindi la funzione di scelta sociale ottimale dell'azione k \u00e8 implementabile nella strategia dominante. In questo esempio, assumiamo che ogni agente abbia una funzione di valutazione singlecrossing sulle alternative. Cio\u00e8, ogni giocatore desidera che il messaggio venga inviato attraverso la sua rete, e il suo beneficio sia correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.e il suo beneficio \u00e8 correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.e il suo beneficio \u00e8 correlato positivamente con i suoi dati segreti -LRB-, ad esempio, la valutazione del giocatore i potrebbe essere esattamente pi -RRB-. Vorremmo sottolineare che il pianificatore sociale in questo esempio -LRB- il mittente -RRB- non mira a massimizzare il benessere sociale. Cio\u00e8, il valore sociale non \u00e8 la somma dei tipi dei giocatori n\u00e9 alcuna somma ponderata dei tipi -LRB- ``massimizzatore affine' -RRB-. La probabilit\u00e0 di successo di inviare un messaggio attraverso una rete a percorso parallelo \u00e8 multilineare, poich\u00e9 pu\u00f2 essere espressa dalla seguente formula multilineare -LRB- dove P denota l'insieme di tutti i percorsi tra la sorgente e il sink -RRB-: Si noti che per per ogni collegamento i, la derivata parziale in pi della probabilit\u00e0 di successo scritta nell'equazione 3 \u00e8 positiva. In tutte le altre reti, che non contengono il collegamento i, la derivata parziale \u00e8 chiaramente nulla. Pertanto, la funzione del valore sociale \u00e8 un incrocio singolo e possono essere applicati i nostri risultati generali. COROLLARIO 5. Per qualsiasi funzione di scelta sociale che massimizza la probabilit\u00e0 di successo su reti a cammini paralleli, la funzione di scelta sociale k-azione informativamente ottimale \u00e8 implementabile -LRB- per qualsiasi k -RRB-. Riconoscimento. Il lavoro del secondo autore \u00e8 sostenuto anche dalla Lady Davis Trust Fellowship.", "keyphrases": ["spazio d'azione delimitato", "strumento", "strategie di dominio", "funzione di scelta sociale", "funzione decisiva", "condizione a croce singola", "funzione multilineare", "meccanico ottimale", "meccanica legata all'azione", "probabile successo"]}
{"file_name": "C-19", "text": "Interfaccia di servizio: una nuova astrazione per l'implementazione e la composizione di protocolli * ABSTRACT In questo articolo confrontiamo due approcci alla progettazione di strutture di protocollo: strumenti per implementare protocolli di rete modulari. L'approccio pi\u00f9 comune utilizza gli eventi come astrazione principale per un'interazione locale tra i moduli del protocollo. Noi sosteniamo che un approccio alternativo, basato sull'astrazione del servizio, sia pi\u00f9 adatto per esprimere protocolli modulari. Facilita inoltre funzionalit\u00e0 avanzate nella progettazione dei protocolli, come l'aggiornamento dinamico dei protocolli distribuiti. Descriviamo quindi un'implementazione sperimentale di un framework di protocolli basato su servizi in Java. 1. INTRODUZIONE Permettono di implementare protocolli complessi scomponendoli in pi\u00f9 moduli che cooperano tra loro. Questo approccio facilita il riutilizzo del codice e la personalizzazione dei protocolli distribuiti per soddisfare le esigenze di diverse applicazioni. Inoltre, i moduli di protocollo possono essere collegati dinamicamente al sistema. Tutte queste caratteristiche dei framework di protocollo li rendono un'interessante tecnologia abilitante per l'implementazione di sistemi adattabili -LSB- 14 -RSB- - un'importante classe di applicazioni. La maggior parte dei framework di protocollo si basano su eventi -LRB- tutti i framework sopra citati si basano su questa astrazione -RRB-. Gli eventi vengono utilizzati per la comunicazione asincrona tra diversi moduli sulla stessa macchina. Ad esempio, la composizione dei moduli pu\u00f2 richiedere connettori per instradare gli eventi, il che introduce un onere per un compositore di protocollo -LSB-4 -RSB-. Strutture di protocollo come Appia ed Eva estendono l'approccio basato sugli eventi con i canali. Tuttavia, a nostro avviso, questa soluzione non \u00e8 soddisfacente poich\u00e9 la composizione di stack di protocolli complessi diventa pi\u00f9 difficile. In questo articolo proponiamo un nuovo approccio per la costruzione di protocolli modulari, basato sull'astrazione del servizio. Confrontiamo questo nuovo approccio con l'approccio comune basato sugli eventi. Mostriamo che le strutture di protocollo basate sui servizi presentano diversi vantaggi, ad esempio consentono una composizione del protocollo abbastanza semplice, un'implementazione chiara e un migliore supporto della sostituzione dinamica dei protocolli distribuiti. Per convalidare le nostre affermazioni, abbiamo implementato SAMOA, un framework di protocollo sperimentale basato esclusivamente sull'approccio basato sui servizi alla composizione e all'implementazione dei moduli. Il framework ci ha permesso di confrontare le implementazioni basate su servizi ed eventi di un middleware di comunicazione di gruppo adattivo. La sezione 2 definisce le nozioni generali. La sezione 3 presenta le principali caratteristiche dei framework basati sugli eventi e le caratteristiche distinte per ciascun framework. La sezione 4 descrive il nostro nuovo approccio, basato sull'astrazione del servizio. La sezione 5 discute i vantaggi di un framework di protocolli basato sui servizi rispetto a un framework di protocolli basato sugli eventi. La descrizione della nostra implementazione sperimentale \u00e8 presentata nella Sezione 6. Infine, concludiamo nella Sezione 7. 7. CONCLUSIONE Nel documento,abbiamo proposto un nuovo approccio alla composizione del protocollo che si basa sulla nozione di Service Interface, invece che su eventi. Riteniamo che il framework basato sui servizi presenti numerosi vantaggi rispetto ai framework basati sugli eventi. Un'implementazione del prototipo ci ha permesso di convalidare le nostre idee.", "keyphrases": ["quadro del protocollo", "algoritmo di distribuzione", "sistema di distribuzione", "interfaccia di servizio", "rete", "comun", "struttura basata sugli eventi", "pila", "modulo", "richiesta", "replica"]}
{"file_name": "I-21", "text": "Interazioni tra barriere di mercato e reti di comunicazione nei sistemi di marketing ABSTRACT Indaghiamo un quadro in cui gli agenti cercano prodotti soddisfacenti utilizzando i riferimenti di altri agenti. Il nostro modello di meccanismo per la trasmissione del passaparola e dei conseguenti effetti comportamentali si basa sull'integrazione di un modulo che governa il comportamento locale degli agenti con un modulo che governa la struttura e la funzione della rete sottostante di agenti. Il comportamento locale incorpora un modello di scelta soddisfacente, un insieme di regole che governano le interazioni tra agenti, compreso l\u2019apprendimento dell\u2019affidabilit\u00e0 di altri agenti nel tempo, e vincoli esterni sul comportamento che possono essere imposti da barriere di mercato o costi di cambiamento. Il comportamento locale avviene su un substrato di rete attraverso il quale gli agenti si scambiano informazioni positive e negative sui prodotti. Usiamo varie distribuzioni di grado che determinano l\u2019estensione della connettivit\u00e0 e incorporiamo sia gli effetti del piccolo mondo che la nozione di attaccamento preferenziale nei nostri modelli di rete. Confrontiamo l'efficacia dei sistemi di riferimento su varie strutture di rete per compiti di scelta facili e difficili e valutiamo come questa efficacia cambia con l'imposizione di barriere di mercato. 1. INTRODUZIONE Il comportamento di defezione, cio\u00e8 il motivo per cui le persone potrebbero smettere di utilizzare un particolare prodotto o servizio, dipende in gran parte dall'affinit\u00e0 psicologica o dalla soddisfazione che provano verso il prodotto attualmente utilizzato -LSB- 14 -RSB- e dalla disponibilit\u00e0 di prodotti pi\u00f9 attraenti. alternative -LSB- 17 -RSB-. Tuttavia, in molti casi la decisione se abbandonare o meno dipende anche da vari vincoli esterni posti al comportamento di cambiamento, sia dalla struttura del mercato, sia dagli stessi fornitori -LRB- sotto forma di contratti formali o informali -RRB-, o altri cosiddetti \"costi di trasferimento\" o barriere di mercato -LSB- 12, 5 -RSB-. La caratteristica chiave di tutti questi casi \u00e8 che la misura in cui l\u2019affinit\u00e0 psicologica gioca un ruolo nel processo decisionale effettivo \u00e8 vincolata dalle barriere del mercato, cos\u00ec che agli agenti viene impedito di perseguire quelle linee di azione che sarebbero pi\u00f9 soddisfacenti in un mercato non vincolato. Mentre il livello di soddisfazione con un prodotto attualmente utilizzato sar\u00e0 in gran parte una funzione delle proprie esperienze del prodotto durante il periodo di utilizzo, \u00e8 probabile che la conoscenza di eventuali alternative potenzialmente pi\u00f9 soddisfacenti venga acquisita aumentando le informazioni ottenute dai dati personali. esperienze con informazioni sulle esperienze degli altri raccolte dalla comunicazione casuale del passaparola. Inoltre, esiste un\u2019importante relazione tra le barriere del mercato e la comunicazione tramite passaparola. In presenza di barriere di mercato, gli agenti economici vincolati e intrappolati in relazioni di prodotto insoddisfacenti tenderanno a diffondere queste informazioni ad altri agenti. In assenza di tali barriere,gli agenti sono liberi di disertare prodotti insoddisfacenti e la comunicazione tramite passaparola tenderebbe quindi ad essere di tipo positivo. Poich\u00e9 l\u2019imposizione di almeno alcune forme di barriere di mercato \u00e8 spesso una decisione strategica presa dai fornitori di prodotti, queste relazioni possono essere fondamentali per il successo di un particolare fornitore. Inoltre, la relazione tra le barriere del mercato e la comunicazione tramite passaparola pu\u00f2 essere reciproca. Anche la struttura e la funzione della rete attraverso la quale viene condotta la comunicazione del passaparola, e in particolare il modo in cui la rete cambia in risposta all\u2019imposizione delle barriere di mercato, svolgono un ruolo nel determinare quali barriere di mercato siano pi\u00f9 efficaci. Un modello basato su agenti consente un'indagine a livello del singolo decisore, a livello del singolo decisore, a livello 2. BACKGROUND 2.1 Comunicazione tramite passaparola Il ruolo della comunicazione tramite passaparola sul comportamento di sistemi complessi \u00e8 stato studiato in sia modelli analitici che di simulazione. Le indagini basate sulla simulazione del passaparola -LSB- 6, 13 -RSB- si sono concentrate sullo sviluppo di strategie per garantire che un sistema raggiunga un livello di equilibrio in cui tutti gli agenti siano soddisfatti, in gran parte apprendendo l'efficacia dei riferimenti degli altri o variando il grado di inerzia del comportamento individuale. Il quadro di simulazione consente una modellazione dell'ambiente pi\u00f9 complessa rispetto ai modelli analitici, in cui i riferimenti sono casuali e sono disponibili solo due scelte, e il lavoro in -LSB- 6 -RSB- in particolare \u00e8 uno stretto antecedente del lavoro presentato in questo articolo, il nostro contributo principale \u00e8 stato quello di includere la struttura della rete e i vincoli imposti dalle barriere del mercato come effetti aggiuntivi. 2.2 Barriere di mercato La misura in cui le barriere di mercato influiscono sul comportamento dei sistemi attira l'attenzione soprattutto degli economisti interessati a come le barriere distorcono la concorrenza e degli operatori di marketing interessati a come le barriere distorcono le scelte dei consumatori. Un'utile tipologia di barriere di mercato distingue le barriere \"transazionali\" associate al costo monetario del cambiamento -LRB- ad esempio nei servizi finanziari -RRB-, le barriere \"di apprendimento\" associate alla decisione di sostituire prodotti esistenti ben noti, e le barriere \"contrattuali\" che impongono vincoli legali per la durata del contratto -LSB- 12 -RSB-. Una diversa tipologia -LSB-5 -RSB- introduce l'ulteriore aspetto delle barriere \u201crelazionali\u201d derivanti dalle relazioni personali che possono intrecciarsi con l'uso di un particolare prodotto. Generalmente esistono poche prove empiriche sulla relazione tra la creazione di barriere al cambiamento e il mantenimento di una base di clienti e, per quanto ne sappiamo, nessun lavoro precedente utilizzava modelli basati su agenti per generare risultati empirici. Burnham et al.-LSB- 5 -RSB- rilevano che le barriere di mercato percepite rappresentano quasi il doppio della varianza nell'intenzione di restare con un prodotto rispetto a quella spiegata dalla soddisfazione per il prodotto -LRB- rispettivamente 30% e 16% -RRB-, e che cos\u00ec- Le cosiddette barriere relazionali sono considerevolmente pi\u00f9 influenti delle barriere transazionali o di apprendimento. Inoltre, hanno scoperto che i consumatori percepiscono che i costi di cambiamento esistono anche in mercati fluidi e dove le barriere sembrerebbero deboli. In poche parole, le barriere del mercato sembrano svolgere un ruolo maggiore in ci\u00f2 che fanno le persone rispetto alla soddisfazione; e la loro presenza potrebbe essere pi\u00f9 pervasiva di quanto si pensi generalmente. 5. CONCLUSIONI E LAVORI CORRELATI Il comportamento d'acquisto in molti mercati avviene su un substrato di reti di comunicazione passaparola attraverso le quali gli agenti si scambiano informazioni sui prodotti e sulle loro simpatie e antipatie. Comprendere i modi in cui i flussi di comunicazione del passaparola influenzano il comportamento aggregato del mercato richiede di studiare sia le propriet\u00e0 strutturali sottostanti della rete sia le regole locali che governano il comportamento degli agenti sulla rete quando prendono decisioni di acquisto e quando interagiscono con altri operatori. agenti. Queste regole locali sono spesso vincolate dalla natura di un particolare mercato, oppure imposte da fornitori strategici o consuetudini sociali. La corretta modellizzazione di un meccanismo per la trasmissione del passaparola e dei conseguenti effetti comportamentali richiede quindi la considerazione di una serie di componenti complessi e interagenti: reti di comunicazione, credibilit\u00e0 della fonte, processi di apprendimento, assuefazione e memoria, vincoli esterni sul comportamento, teorie del trasferimento di informazioni e del comportamento adattivo. In questo articolo abbiamo tentato di affrontare alcuni di questi problemi in un modo che rifletta il modo in cui gli agenti potrebbero agire nel mondo reale. \u00c8 la scoperta finale che probabilmente sar\u00e0 pi\u00f9 sorprendente e praticamente rilevante per il campo delle ricerche di marketing, e suggerisce che potrebbe non essere sempre nel miglior interesse di un leader di mercato imporre barriere che impediscano ai clienti di andarsene. Nelle reti scarsamente connesse, l\u2019effetto delle barriere sulle quote di mercato \u00e8 lieve. Al contrario, in reti ben connesse, il passaparola negativo pu\u00f2 impedire agli agenti di provare un prodotto che altrimenti avrebbero potuto trovare soddisfacente, e questo pu\u00f2 infliggere un danno significativo alla quota di mercato. I prodotti con una piccola quota di mercato -LRB- che, nel context delle nostre simulazioni, \u00e8 generalmente dovuta al prodotto che offre scarse prestazioni -RRB- sono relativamente non influenzati dal passaparola negativo, poich\u00e9 la maggior parte delle prove di prodotto probabilmente non saranno soddisfacenti in ogni caso. La modellazione basata su agenti fornisce un modo naturale per iniziare a indagare i tipi di dinamiche che si verificano nei sistemi di marketing. Naturalmente l'utilit\u00e0 dei risultati dipende in gran parte dalla qualit\u00e0 della modellazione dei due `moduli' comprendenti la struttura della rete e il comportamento locale. Lato rete,il lavoro futuro potrebbe indagare la relazione tra le distribuzioni dei gradi, il modo in cui le connessioni vengono create e distrutte nel tempo, se l\u2019attaccamento preferenziale \u00e8 influente e la misura in cui l\u2019identit\u00e0 sociale informa la struttura della rete, il tutto in reti pi\u00f9 ampie di agenti pi\u00f9 eterogenei. Dal punto di vista comportamentale, si potrebbe considerare l\u2019adattamento delle soglie di soddisfazione nel corso della comunicazione, le risposte ai cambiamenti sistematici nelle prestazioni del prodotto nel tempo, l\u2019integrazione di varie fonti di informazione e le diverse strutture delle barriere di mercato. Tutte queste aree offrono opportunit\u00e0 affascinanti per introdurre realt\u00e0 psicologiche nei modelli dei sistemi di marketing e per osservare il comportamento risultante del sistema con descrizioni di scenari sempre pi\u00f9 realistiche.", "keyphrases": ["sistema di riferimento", "comportamento di acquisto", "comunicazione tramite passaparola", "sistema di mercato", "comportamento difettoso", "affine allo psicologo", "cambiare comportamento", "modello basato su agenti", "psicologo sociale", "barriera del mercato", "scelta del consumo", "costo del cambio"]}
{"file_name": "C-34", "text": "Ricerche sullo schema di creazione di chiavi a coppie per reti di sensori distribuite ABSTRACT Gli schemi di sicurezza di creazione di chiavi a coppie, che consentono ai sensori di comunicare tra loro in modo sicuro, svolgono un ruolo fondamentale nella ricerca sui problemi di sicurezza nelle reti di sensori wireless. Viene presentato un nuovo tipo di modello di distribuzione delle reti di sensori distribuite in cluster e, sulla base del quale, un innovativo modello di ipercubo gerarchico - H -LRB- k, u, m, v, n -RRB- e la relazione di mappatura tra le reti di sensori distribuite in cluster e vengono proposti gli H -LRB- k, u, m, v, n -RRB-. Utilizzando le propriet\u00e0 interessanti del modello H -LRB- k, u, m, v, n -RRB-, vengono progettati un nuovo quadro generale per la predistribuzione delle chiavi a coppie e un nuovo algoritmo per la creazione di chiavi a coppie, che combina l'idea di KDC -LRB- Centro di distribuzione delle chiavi -RRB- e schemi di pool polinomiale. Inoltre, le prestazioni operative dell'algoritmo di creazione delle chiavi a coppie recentemente proposto vengono attentamente controllate. L'analisi teorica e le cifre sperimentali mostrano che il nuovo algoritmo ha prestazioni migliori e offre maggiori possibilit\u00e0 al sensore di stabilire chiavi a coppie, rispetto ai precedenti lavori correlati. 1. INTRODUZIONE La comunicazione di sicurezza \u00e8 un requisito importante in molte applicazioni di reti di sensori, quindi vengono utilizzate chiavi segrete condivise tra i nodi comunicanti per crittografare i dati. Essendo uno dei servizi di sicurezza pi\u00f9 fondamentali, la creazione di chiavi a coppie consente ai nodi sensore di comunicare in modo sicuro tra loro utilizzando tecniche crittografiche. Tuttavia, a causa delle capacit\u00e0 computazionali limitate dei nodi sensore, dell'energia della batteria e della memoria disponibile, non \u00e8 possibile per loro utilizzare le tradizionali tecniche di creazione di chiavi a coppie come la crittografia a chiave pubblica e il centro di distribuzione delle chiavi -LRB- KDC -RRB-. Recentemente sono stati sviluppati diversi approcci alternativi per eseguire la creazione di chiavi a coppie su reti di sensori con risorse limitate senza coinvolgere l'uso della crittografia tradizionale -LSB- 14 -RSB-. Eschenauer e Gligor hanno proposto uno schema probabilistico di base di predistribuzione delle chiavi per la creazione di chiavi a coppie -LSB- 1 -RSB-. Nello schema, ciascun nodo sensore sceglie casualmente un set di chiavi da un pool di chiavi prima dell'implementazione in modo che due nodi sensore qualsiasi abbiano una certa probabilit\u00e0 di condividere almeno una chiave comune. Chan et al. ha ulteriormente esteso questa idea e presentato due schemi di pre-distribuzione delle chiavi: uno schema di pre-distribuzione delle chiavi q-composite e uno schema di chiavi a coppie casuali. Lo schema q-composito richiede che due sensori qualsiasi condividano almeno q chiavi pre-distribuite. Lo schema casuale seleziona casualmente una coppia di sensori e assegna a ciascuna coppia una chiave casuale univoca -LSB- 2 -RSB-. Sulla base di tale quadro, hanno presentato due schemi di pre-distribuzione delle chiavi a coppie: uno schema di assegnazione di sottoinsiemi casuali e uno schema basato su griglia. In questi schemi viene utilizzato un pool polinomiale, invece di utilizzare un pool di chiavi nelle tecniche precedenti.Lo schema di assegnazione del sottoinsieme casuale assegna a ciascun nodo sensore i segreti generati da un sottoinsieme casuale di polinomi nel pool polinomiale. Lo schema basato sulla griglia associa i polinomi alle righe e alle colonne di una griglia artificiale, assegna ciascun nodo sensore a una coordinata univoca nella griglia e fornisce al nodo i segreti generati dai corrispondenti polinomi di riga e colonna. Sulla base di questa griglia, ciascun nodo sensore pu\u00f2 quindi identificare se pu\u00f2 stabilire direttamente una chiave a coppia con un altro nodo e, in caso contrario, quali nodi intermedi pu\u00f2 contattare per stabilire indirettamente la chiave a coppia. Un approccio simile a quegli schemi descritti da Liu et al \u00e8 stato sviluppato in modo indipendente da Du et al. -LSB-5 -RSB-. Piuttosto che sullo schema di Blundo, il loro approccio si basa sullo schema di Blom -LSB- 6 -RSB-. Tutti gli schemi di cui sopra migliorano la sicurezza rispetto allo schema di pre-distribuzione delle chiavi probabilistiche di base. Tuttavia, il problema della creazione di chiavi a coppie nelle reti di sensori non \u00e8 ancora ben risolto. Per gli schemi probabilistici di base e di predistribuzione delle chiavi q-composite, all'aumentare del numero di nodi compromessi, la frazione di chiavi a coppie interessate aumenta rapidamente. Di conseguenza, un piccolo numero di nodi compromessi pu\u00f2 influenzare una grande frazione di chiavi a coppie -LSB- 3 -RSB-. Sebbene lo schema di chiavi casuali a coppie non soffra del problema di sicurezza di cui sopra, comporta un elevato sovraccarico di memoria, che aumenta linearmente con il numero di nodi nella rete se il livello di sicurezza viene mantenuto costante -LSB- 2 -RSB- -LSB -4 -RSB-. Per lo schema di assegnazione di sottoinsiemi casuali, soffre di maggiori spese generali di comunicazione e calcolo. Nel 2004, Liu ha proposto un nuovo schema di predistribuzione delle chiavi a coppie basato sull'ipercubo -LSB- 7 -RSB-, che estende lo schema basato sulla griglia da una griglia bidimensionale a un ipercubo multidimensionale. L\u2019analisi mostra che lo schema basato sull\u2019ipercubo mantiene alcune propriet\u00e0 interessanti dello schema basato sulla griglia, inclusa la garanzia di stabilire chiavi a coppie e la resilienza ai compromessi dei nodi. Inoltre, quando \u00e8 richiesta una sicurezza perfetta contro la compromissione dei nodi, lo schema basato sull\u2019ipercubo pu\u00f2 supportare una rete pi\u00f9 grande aggiungendo pi\u00f9 dimensioni invece di aumentare il sovraccarico di archiviazione sui nodi dei sensori. Sebbene lo schema basato sull'ipercubo -LRB- consideriamo che lo schema basato sulla griglia sia un caso speciale di schema basato sull'ipercubo -RRB- ha molte propriet\u00e0 interessanti, richiede che due nodi qualsiasi nelle reti di sensori possano comunicare direttamente tra loro. Questa forte assunzione non \u00e8 praticabile nella maggior parte delle attuali applicazioni delle reti di sensori. In questo articolo presentiamo una sorta di nuovo modello di distribuzione basato su cluster di reti di sensori e per il quale proponiamo un nuovo schema di pre-distribuzione delle chiavi a coppie. Sulla base della topologia, proponiamo un nuovo modello di ipercubo gerarchico basato sulla distribuzione dei cluster per stabilire la chiave a coppie. Sviluppiamo una sorta di nuovo algoritmo per la creazione di chiavi a coppie con il nostro modello di ipercubo gerarchico.La struttura di questo documento \u00e8 organizzata come segue: Nella sezione 3 viene presentato un nuovo modello di distribuzione delle reti di sensori distribuite in cluster. Nella sezione 4 viene proposto un nuovo modello di Ipercubo Gerarchico. Nella sezione 5 viene discussa la relazione di mappatura tra la rete di sensori distribuita nei cluster e il modello dell'ipercubo gerarchico. Nelle sezioni 6 e 7, vengono progettati nuovi algoritmi per la creazione di chiavi a coppie basati sul modello dell'ipercubo gerarchico e vengono descritte analisi dettagliate. Infine, la sezione 8 presenta una conclusione. 2. Definizione PRELIMINARE 1 -LRB- Predistribuzione delle chiavi -RRB-: La procedura, utilizzata per codificare i corrispondenti algoritmi di crittografia e decrittografia nei nodi sensore prima della distribuzione, \u00e8 chiamata Predistribuzione delle chiavi. Definizione 2 -LRB- Chiave a coppia -RRB-: Per due nodi A e B qualsiasi, se hanno una chiave comune E, allora la chiave E \u00e8 chiamata chiave a coppia tra di loro. 8. CONCLUSIONE Viene proposto un nuovo modello di ipercubo gerarchico denominato H -LRB- k, u, m, v, n -RRB-, che pu\u00f2 essere utilizzato per la predistribuzione di chiavi a coppie per reti di sensori distribuite in cluster. Inoltre, sulla base del modello H -LRB- k, u, m, v, n -RRB-, vengono progettati rispettivamente uno schema innovativo di predistribuzione delle chiavi a coppie e un algoritmo, combinando le buone propriet\u00e0 degli schemi di crittografia Polynomial Key e Key Pool. Quindi, il tradizionale algoritmo di predistribuzione delle chiavi a coppie basato sul modello dell\u2019ipercubo -LSB- 7 -RSB- \u00e8 solo un caso speciale del nuovo algoritmo proposto in questo articolo. Analisi teoriche e sperimentali mostrano che l'algoritmo appena proposto \u00e8 un efficiente algoritmo di creazione di chiavi a coppie adatto per le reti di sensori distribuite in cluster.combinando le buone propriet\u00e0 degli schemi di crittografia Polynomial Key e Key Pool. Quindi, il tradizionale algoritmo di predistribuzione delle chiavi a coppie basato sul modello dell\u2019ipercubo -LSB- 7 -RSB- \u00e8 solo un caso speciale del nuovo algoritmo proposto in questo articolo. Analisi teoriche e sperimentali mostrano che l'algoritmo appena proposto \u00e8 un efficiente algoritmo di creazione di chiavi a coppie adatto per le reti di sensori distribuite in cluster.combinando le buone propriet\u00e0 degli schemi di crittografia Polynomial Key e Key Pool. Quindi, il tradizionale algoritmo di predistribuzione delle chiavi a coppie basato sul modello dell\u2019ipercubo -LSB- 7 -RSB- \u00e8 solo un caso speciale del nuovo algoritmo proposto in questo articolo. Analisi teoriche e sperimentali mostrano che l'algoritmo appena proposto \u00e8 un efficiente algoritmo di creazione di chiavi a coppie adatto per le reti di sensori distribuite in cluster.", "keyphrases": ["rete di sensori", "piscina kei", "kei predistribuire", "modello dell\u2019ipercubo gerarchico", "sicuro", "Pairwis Kei stabilisce l'algoritmo", "modello di distribuzione basato su cluster", "polinomi kei", "crittografare", "codice del nodo", "elevata tolleranza agli errori"]}
{"file_name": "H-32", "text": "Nuggets interessanti e il loro impatto sulle risposte alle domande definitive ABSTRACT Gli approcci attuali per identificare le frasi di definizione nel context delle risposte alle domande implicano principalmente l'uso di modelli linguistici o sintattici per identificare nuggets informativi. Ci\u00f2 non \u00e8 sufficiente in quanto non affrontano il fattore di novit\u00e0 che anche una pepita di definizione deve possedere. Questo articolo propone di colmare questa carenza costruendo un \u201cmodello di interesse umano\u201d partendo dalla conoscenza esterna. Si spera che un tale modello permetta il calcolo dell'interesse umano nella frase rispetto all'argomento. Confrontiamo e contrapponiamo il nostro modello con gli attuali modelli di risposta alle domande di definizione per dimostrare che l'interesse gioca un fattore importante nella risposta alle domande di definizione. 1. RISPOSTA ALLA DOMANDA DEFINITIVA La risposta alla domanda di definizione \u00e8 stata introdotta per la prima volta nell'attivit\u00e0 principale del tracciamento delle risposte alle domande della TExt Retrieval Conference nel 2003. Le domande di definizione, chiamate anche Altre domande negli ultimi anni, sono definite come segue. Dato un argomento X, il compito di un sistema di garanzia della qualit\u00e0 \u00e8 simile a rispondere alla domanda \"Cos'\u00e8 X?\" '' . Il sistema di QA definitorio consiste nel cercare in un corpus di notizie e restituire una serie di risposte che descrivono al meglio l'argomento della domanda. Ogni risposta dovrebbe essere un insieme unico e specifico dell'argomento che costituisce un aspetto nella definizione dell'argomento della domanda. 1.1 I due aspetti dei nugget di argomenti Ufficialmente, i nugget di risposte specifiche per argomento o semplicemente i nugget di argomenti sono descritti come \"nugget informativi\". Ogni pepita informativa \u00e8 un frammento di frase che descrive alcune informazioni concrete sull'argomento. Dall'osservazione della serie di risposte per le domande di definizione dal TREC dal 2003 al 2005, sembra che un numero significativo di nugget di argomenti non possa essere semplicemente descritto come nuggets informativi. Piuttosto, a questi argomenti \u00e8 associata una qualit\u00e0 simile a una curiosit\u00e0. In genere, si tratta di informazioni fuori dall'ordinario su un argomento che possono suscitare l'interesse di un lettore umano. Per questo motivo, abbiamo deciso di definire le nugget di risposta che possono suscitare l'interesse umano come \u201cnuggets interessanti\u201d. In sostanza, pepite interessanti rispondono alle domande \"Per cosa \u00e8 famoso X?\" '' , ``Cosa definisce X? '' . Ora abbiamo due prospettive molto diverse su ci\u00f2 che costituisce una risposta alle domande sulla definizione. Una risposta pu\u00f2 essere costituita da alcune importanti informazioni fattuali sull'argomento o da qualche aspetto nuovo e interessante dell'argomento. Questa dualit\u00e0 di informativit\u00e0 e interesse pu\u00f2 essere chiaramente osservata nelle cinque risposte vitali per un argomento TREC 2005 di \"George Foreman\". Alcune risposte sono pi\u00f9 informative mentre altre sono di natura pi\u00f9 interessante. Nuggets informativi - \u00c8 diventato il campione del mondo pi\u00f9 anziano nella storia della boxe. Nuggets interessanti - Ritornato alla boxe dopo 10 anni di pausa. Come visto qui,le pepite interessanti hanno qualche fattore sorpresa o qualit\u00e0 unica che le rende interessanti per i lettori umani. 1.2 Identificazione delle pepite interessanti Poich\u00e9 la descrizione ufficiale originale delle definizioni prevede l'identificazione delle pepite informative, la maggior parte della ricerca si \u00e8 concentrata interamente sull'identificazione delle pepite informative. In questo articolo, ci concentreremo sull'esplorazione delle propriet\u00e0 delle pepite interessanti e sullo sviluppo di metodi per identificare tali pepite interessanti. Un sistema di risposta alle domande definitorie del \"modello dell'interesse umano\" viene sviluppato con l'accento sull'identificazione delle pepite interessanti al fine di valutare l'impatto delle pepite interessanti sulle prestazioni di un sistema di risposta alle domande definitive. Abbiamo ulteriormente sperimentato combinando il modello dell'interesse umano con un sistema di risposta alle domande definitorio basato su modelli lessicali al fine di catturare informazioni sia informative che interessanti. 2. LAVORI CORRELATI Attualmente esistono due metodi generali per la risposta alle domande definitive. Il metodo pi\u00f9 comune che utilizza un approccio basato su modelli lessicali \u00e8 stato proposto per la prima volta da Blair-Goldensohn et al. -LSB- 1 -RSB- e Xu et al. -LSB-14 -RSB-. Entrambi i gruppi utilizzavano prevalentemente modelli come copule e appositivi, nonch\u00e9 modelli lessicosintattici realizzati manualmente per identificare frasi che contengono pepite informative. Ad esempio, Xu et al. hanno utilizzato 40 \"modelli strutturati\" definiti manualmente nel loro sistema di risposta alle domande di definizione del 2003. Da allora, nel tentativo di catturare una classe pi\u00f9 ampia di pepite informative, sono stati creati molti di questi sistemi di crescente complessit\u00e0. Un recente sistema di Harabagiu et al. -LSB- 6 -RSB- ha creato un sistema di risposta alle domande di definizione che combina l'uso di 150 modelli positivi e negativi definiti manualmente, relazioni di entit\u00e0 denominate e modelli di estrazione di informazioni appositamente predisposti per 33 domini di destinazione. Come si pu\u00f2 immaginare, si tratta di un approccio ad alta intensit\u00e0 di conoscenza che richiede che un linguista esperto definisca manualmente tutti i possibili modelli lessicali o sintattici necessari per identificare tipi specifici di informazioni. Invece di codificare manualmente i modelli, le risposte alle precedenti valutazioni delle risposte alle domande di definizione sono state convertite in modelli generici e un modello probabilistico \u00e8 addestrato per identificare tali modelli nelle frasi. \u00c8 stato dimostrato che tale approccio basato su modelli lessicalosintattici \u00e8 efficace nell'identificare elementi informativi fattuali come la data di nascita di una persona o il nome dell'amministratore delegato di un'azienda. Tuttavia, questi modelli sono applicabili a livello globale a tutti gli argomenti o a un insieme specifico di entit\u00e0 come musicisti o organizzazioni. Ci\u00f2 \u00e8 in diretto contrasto con le pepite interessanti che sono altamente specifiche per singoli argomenti e non per un insieme di entit\u00e0. Ad esempio, le pepite interessanti per George Foreman sono specifiche solo per George Foreman e nessun altro pugile o essere umano. La specificit\u00e0 dell'argomento o la pertinenza dell'argomento sono quindi un criterio importante che aiuta a identificare le pepite interessanti.Ci\u00f2 porta all'esplorazione del secondo approccio basato sulla pertinenza che \u00e8 stato utilizzato nella risposta alle domande definitive. Prevalentemente, questo approccio \u00e8 stato utilizzato come metodo di backup per identificare frasi di definizione quando il metodo primario dei modelli lessicalosintattici non \u00e8 riuscito a trovare un numero sufficiente di nuggets informativi -LSB- 1 -RSB-. Un approccio simile \u00e8 stato utilizzato anche come sistema di base per TREC 2003 -LSB- 14 -RSB-. Pi\u00f9 recentemente, Chen et al. -LSB- 3 -RSB- ha adattato un modello linguistico bi-gram o bi-term per la risposta alle domande definitiva. In generale, l'approccio basato sulla pertinenza richiede un \"corpus di definizione\" che contenga documenti altamente rilevanti per l'argomento. Il sistema di base del TREC 2003 utilizza semplicemente le parole dell'argomento come corpus di definizione. Blair-Goldensohn et al. -LSB- 1 -RSB- utilizza un machine learning per includere nel corpus definitorio frasi che potrebbero essere definitive. Chen et al. -LSB- 3 -RSB- raccoglie snippet da Google per costruire il suo corpus di definizione. Dal corpus di definizione, viene costruito un vettore del centroide di definizione oppure viene selezionato un insieme di parole del centroide. Questo vettore centroide o insieme di parole centroide \u00e8 considerato altamente indicativo dell'argomento. I sistemi possono quindi utilizzare questo centroide per identificare le risposte di definizione utilizzando una variet\u00e0 di parametri di distanza da confrontare con le frasi trovate nell'insieme di documenti recuperati per l'argomento. BlairGoldensohn et al. -LSB- 1 -RSB- utilizza la somiglianza del coseno per classificare le frasi in base alla \"centralit\u00e0\". Come descritto qui, l'approccio basato sulla pertinenza \u00e8 altamente specifico per i singoli argomenti a causa della sua dipendenza da un corpus di definizioni specifico dell'argomento. Tuttavia, se le singole frasi vengono visualizzate come un documento, gli approcci basati sulla pertinenza utilizzano essenzialmente le parole centroidi specifiche dell'argomento raccolte come una forma di recupero del documento con espansione automatizzata delle query per identificare frasi fortemente pertinenti. Pertanto tali metodi identificano frasi rilevanti e non frasi contenenti pepite di definizione. Tuttavia, il sistema di riferimento TREC 2003 -LSB- 14 -RSB- ha sovraperformato tutti gli altri sistemi tranne uno. Il modello linguistico bi-termine -LSB- 3 -RSB- \u00e8 in grado di riportare risultati altamente competitivi rispetto ai risultati all'avanguardia utilizzando questo approccio basato sul recupero. Al TREC 2006, un semplice modello di somma ponderata di tutti i termini con termini ponderati utilizzando esclusivamente snippet di Google ha sovraperformato tutti gli altri sistemi con un margine significativo -LSB- 7 -RSB-. Riteniamo che le informazioni interessanti spesso si presentino sotto forma di curiosit\u00e0, novit\u00e0 o fatti rari sull'argomento che tendono a coincidere fortemente con la menzione diretta delle keyphrases dell'argomento. Ci\u00f2 potrebbe spiegare perch\u00e9 il metodo basato sulla pertinenza pu\u00f2 funzionare in modo competitivo nella risposta alle domande di definizione. Tuttavia, il semplice confronto con un singolo vettore centroide o un insieme di parole centroide pu\u00f2 aver enfatizzato eccessivamente la rilevanza dell'argomento e ha identificato solo pepite di definizione interessanti in modo indiretto. Ancora,i metodi di recupero basati sulla pertinenza possono essere utilizzati come punto di partenza per identificare pepite interessanti. Descriveremo come espandere tali metodi per identificare pepite interessanti nella sezione successiva. 7. CONCLUSIONE Questo articolo ha presentato una nuova prospettiva per rispondere a domande di definizione attraverso l'identificazione di pepite interessanti. Le pepite interessanti sono informazioni insolite sull'argomento che possono evocare la curiosit\u00e0 di un lettore umano. La nozione di \u201clettore umano medio\u201d \u00e8 una considerazione importante nel nostro approccio. Questo \u00e8 molto diverso dall'approccio del modello lessico-sintattico in cui il context di un lettore umano non viene nemmeno considerato quando si trovano risposte alle domande di definizione. Utilizzando questa prospettiva, abbiamo dimostrato che utilizzando una combinazione di un corpus esterno accuratamente selezionato, confrontandolo con pi\u00f9 centroidi e prendendo in considerazione termini rari ma altamente specifici dell'argomento, possiamo costruire un modulo di risposta alle domande definitive che \u00e8 pi\u00f9 focalizzato sull'identificazione delle pepite che sono di interesse per gli esseri umani. I risultati sperimentali hanno dimostrato che questo approccio pu\u00f2 superare significativamente le prestazioni dei sistemi di risposta alle domande di definizione all'avanguardia. Abbiamo inoltre dimostrato che sono necessari almeno due diversi tipi di serie di risposte per formare un insieme pi\u00f9 completo di risposte definitive. Ci\u00f2 che sembra essere un buon insieme di risposte alle definizioni sono alcune informazioni generali che forniscono una rapida panoramica informativa mescolata ad alcuni aspetti nuovi o interessanti sull'argomento. Pertanto riteniamo che un buon sistema di risposta alle domande sulla definizione dovrebbe raccogliere tipi di pepite sia informativi che interessanti al fine di fornire una copertura di definizione completa su tutti gli aspetti importanti dell'argomento. In effetti, ci\u00f2 \u00e8 naturale poich\u00e9 i due modelli sono stati progettati per identificare due tipi molto diversi di risposte di definizione utilizzando tipi di caratteristiche molto diversi. Di conseguenza, attualmente siamo in grado di realizzare solo un sistema ibrido che abbia lo stesso livello di prestazioni del modello dell\u2019interesse umano da noi proposto. Abbiamo affrontato il problema della risposta alle domande di definizione da una nuova prospettiva, con l'idea che il fattore di interesse gioca un ruolo nell'identificazione delle risposte di definizione. Sebbene i metodi che abbiamo utilizzato siano semplici, si sono dimostrati sperimentalmente efficaci. Il nostro approccio pu\u00f2 anche fornire alcune informazioni su alcune anomalie nelle precedenti prove di risposta alle domande sulla definizione. Ad esempio, il sistema di massima definizione della recente valutazione TREC 2006 \u00e8 stato in grado di sovraperformare significativamente tutti gli altri sistemi utilizzando probabilit\u00e0 unigrammi relativamente semplici estratte dagli snippet di Google. Sospettiamo che sia il principale contributore alle prestazioni del sistema Tabella 3: Argomenti TREC 2005 raggruppati per tipo di entit\u00e0 Nel nostro lavoro futuro, cercheremo di migliorare ulteriormente il sistema combinato incorporando pi\u00f9 prove a supporto di risposte di definizione corrette o filtrandole ovviamente risposte sbagliate.", "keyphrases": ["noi del linguista", "conoscenza esterna", "calcolo dell'interesse umano", "nuovo corpo", "argomento della domanda", "informare la pepita", "frammento di frase", "lettore umano", "interesse", "pepita di interesse", "qualit\u00e0 uniche", "fattore sorpresa", "modello lessicale", "lavoro manuale", "sistema di base"]}
{"file_name": "I-4", "text": "Coordinamento a meta-livello per risolvere catene di negoziazione in sistemi multi-agente semi-cooperativi ABSTRACT Una catena di negoziazione si forma quando pi\u00f9 negoziazioni correlate vengono distribuite su pi\u00f9 agenti. Per ordinare e strutturare adeguatamente le negoziazioni che si verificano nella catena in modo da ottimizzare l'utilit\u00e0 attesa, presentiamo un'estensione a un quadro di negoziazione simultanea ad agente singolo. Questo lavoro \u00e8 rivolto a sistemi multi-agente semi-cooperativi, in cui ogni agente ha i propri obiettivi e lavora per massimizzare la propria utilit\u00e0 locale; tuttavia, la prestazione di ogni singolo agente \u00e8 strettamente correlata alla cooperazione degli altri agenti e alla prestazione complessiva del sistema. Introduciamo una fase di pre-negoziazione che consente agli agenti di trasferire informazioni a livello meta. Utilizzando queste informazioni, l'agente pu\u00f2 costruire un modello pi\u00f9 accurato della negoziazione in termini di modellazione del rapporto tra flessibilit\u00e0 e probabilit\u00e0 di successo. Questo modello pi\u00f9 accurato aiuta l'agente nella scelta di una soluzione negoziale migliore nel context della catena negoziale globale. L'agente pu\u00f2 anche utilizzare queste informazioni per allocare il tempo appropriato per ciascuna negoziazione, quindi per trovare un buon ordine di tutte le negoziazioni correlate. I dati sperimentali mostrano che questi meccanismi migliorano in modo significativo le prestazioni complessive degli agenti e del sistema. 1. INTRODUZIONE Una negoziazione sofisticata per l'allocazione di compiti e risorse \u00e8 cruciale per la prossima generazione di applicazioni di sistemi multi-agente -LRB-MAS-RRB-. Gruppi di agenti devono negoziare in modo efficiente su pi\u00f9 questioni correlate contemporaneamente in un ambiente complesso e distribuito in cui esistono scadenze entro le quali le negoziazioni devono essere completate. Si tratta di un\u2019importante area di ricerca in cui \u00e8 stato fatto pochissimo lavoro. Questo lavoro \u00e8 rivolto a sistemi multi-agente semi-cooperativi, in cui ogni agente ha i propri obiettivi e lavora per massimizzare la propria utilit\u00e0 locale; tuttavia, la prestazione di ogni singolo agente \u00e8 strettamente correlata alla cooperazione degli altri agenti e alla prestazione complessiva del sistema. Non esiste un unico obiettivo globale in tali sistemi, sia perch\u00e9 ogni agente rappresenta un\u2019organizzazione/utente differente, sia perch\u00e9 \u00e8 difficile/impossibile progettare un unico obiettivo globale. Questo problema sorge a causa di molteplici attivit\u00e0 simultanee, vincoli di risorse e incertezze, e quindi nessun agente ha conoscenze o risorse computazionali sufficienti per determinare cosa \u00e8 meglio per l'intero sistema -LSB- 11 -RSB-. Per portare a termine i compiti che arrivano continuamente nell'organizzazione virtuale, la cooperazione e il trasferimento delle sottoattivit\u00e0 sono necessari e preferiti. Non esiste un unico obiettivo globale poich\u00e9 ciascun agente pu\u00f2 essere coinvolto in pi\u00f9 organizzazioni virtuali. Nel frattempo, le prestazioni di ogni singolo agente sono strettamente correlate alla cooperazione degli altri agenti e alle prestazioni complessive dell'organizzazione virtuale. La negoziazione in tali sistemi non \u00e8 un gioco a somma zero,un accordo che aumenti le utilit\u00e0 di entrambi gli agenti pu\u00f2 essere trovato attraverso una negoziazione efficiente. Inoltre, ci sono pi\u00f9 incontri tra gli agenti poich\u00e9 arrivano continuamente nuove attivit\u00e0. In tali trattative, il prezzo pu\u00f2 essere importante o meno, poich\u00e9 pu\u00f2 essere fissato in base a un contratto a lungo termine. Anche altri fattori come la qualit\u00e0 e i tempi di consegna sono importanti. I meccanismi di reputazione nel sistema rendono l\u2019imbroglio non attraente da un punto di vista a lungo termine a causa dei molteplici incontri tra gli agenti. Un\u2019altra importante differenza tra questo lavoro e altri lavori sulla negoziazione \u00e8 che la negoziazione, qui, non \u00e8 vista come un processo autonomo. Si tratta piuttosto di una parte dell'attivit\u00e0 dell'agente che \u00e8 strettamente interconnessa con la pianificazione, la programmazione e l'esecuzione delle attivit\u00e0 dell'agente, che possono riguardare anche altre negoziazioni. Sulla base di questo riconoscimento, questo lavoro sulla negoziazione si preoccupa pi\u00f9 del processo decisionale del meta-livello nella negoziazione piuttosto che dei protocolli o dei linguaggi di base. si svolgano le trattative. Queste macro-strategie sono diverse da quelle micro-strategie che dirigono il filo della negoziazione individuale, ad esempio se l'agente dovrebbe concedere e quanto dovrebbe concedere, ecc. -LSB- 3 -RSB-. In questo articolo estendiamo un modello di negoziazione multi-collegata -LSB- 10 -RSB- da una prospettiva a singolo agente a una prospettiva multi-agente, in modo che un gruppo di agenti coinvolti in catene di negoziazioni interconnesse possa trovare una macro negoziazione quasi ottimale strategie per portare avanti le negoziazioni. La sezione 2 descrive il processo di negoziazione di base ed esamina brevemente il modello di negoziazione multi-collegata di un singolo agente. La sezione 3 introduce uno scenario complesso relativo alla catena di approvvigionamento. La sezione 4 descrive dettagliatamente come risolvere i problemi che emergono nella catena di negoziazione. La sezione 5 riporta il lavoro sperimentale. La sezione 6 discute il lavoro correlato e la sezione 7 presenta conclusioni e aree di lavoro futuro. 2. BACKGROUND DELLA NEGOZIAZIONE MULTI-CONNESSA Questo processo pu\u00f2 andare avanti e indietro finch\u00e9 non viene raggiunto un accordo o gli agenti decidono di interromperlo. Se viene raggiunto un accordo e un agente non pu\u00f2 mantenere l'impegno, deve pagare all'altra parte una penale di disimpegno come specificato nell'impegno. Una negoziazione inizia con una proposta, che annuncia che un'attivit\u00e0 -LRB- t -RRB- deve essere eseguita include i seguenti attributi: 1. scadenza -LRB- dl -RRB-: l'ultima ora di fine dell'attivit\u00e0; l'attivit\u00e0 deve essere terminata entro la scadenza dl. 3. requisito minimo di qualit\u00e0 -LRB- minq -RRB-: l'attivit\u00e0 deve essere completata con un raggiungimento di qualit\u00e0 non inferiore a minq. 4. ricompensa regolare -LRB- r -RRB-: se l'attivit\u00e0 viene completata come richiesto dal contratto, l'agente appaltatore ricever\u00e0 la ricompensa r. 5. Tasso di ricompensa per fine anticipata -LRB- e -RRB-: se l'agente appaltatore riesce a terminare l'attivit\u00e0 prima del dl, ricever\u00e0 la ricompensa extra per fine anticipata proporzionale a questa tariffa. 6.Il problema della negoziazione multi-connessa ha due dimensioni: le negoziazioni e i soggetti delle negoziazioni. Le trattative sono interconnesse e gli argomenti sono interconnessi; anche gli attributi delle negoziazioni e gli attributi dei soggetti sono correlati. Questa complessit\u00e0 bidimensionale di interrelazioni lo distingue dal classico problema di gestione del progetto o problema di pianificazione, in cui tutte le attivit\u00e0 da pianificare sono attivit\u00e0 locali e non \u00e8 necessaria alcuna negoziazione. 1. durata della negoziazione -LRB- \u03b4 -LRB- v -RRB- -RRB-: il tempo massimo consentito per il completamento della negoziazione v, sia che si arrivi a una proposta concordata -LRB- successo -RRB- o che non si raggiunga alcun accordo -LRB- fallimento - RRB-. 2. ora di inizio della negoziazione -LRB- \u03b1 -LRB- v -RRB- -RRB- : l'ora di inizio della negoziazione v. \u03b1 -LRB- v -RRB- \u00e8 un attributo che deve essere deciso dall'agente. 3. scadenza della negoziazione -LRB- e -LRB- v -RRB- -RRB-: la negoziazione v deve essere completata entro tale scadenza e -LRB- v -RRB-. La negoziazione non \u00e8 pi\u00f9 valida dopo il tempo e -LRB- v -RRB-, che equivale ad un esito di fallimento di tale negoziazione. 4. Dipende da una serie di attributi, inclusi entrambi gli attributi-in-negotiation -LRB- ovvero ricompensa, flessibilit\u00e0, ecc. -RRB- e attributi-of-negotiation -LRB- ovvero ora di inizio negoziazione, scadenza negoziazione, ecc. - RRB-. Un agente coinvolto in molteplici processi di negoziazione correlati deve ragionare su come gestire queste negoziazioni in termini di ordinamento e di scelta dei valori appropriati per le caratteristiche. Questo \u00e8 il problema della negoziazione multi-connessa -LSB- 10 -RSB- : \u03c1 -LRB- v -RRB- -RRB-, che descrive la relazione tra la negoziazione v e i suoi figli. La relazione AND associata a una negoziazione v significa che il raggiungimento con successo dell'impegno su v richiede che tutti i suoi nodi figli abbiano risultati positivi. La relazione OR associata a una negoziazione v significa che il successo dell'impegno su v richiede che almeno un nodo figlio abbia successo, dove pi\u00f9 nodi figli rappresentano alternative per raggiungere lo stesso obiettivo. Il problema della negoziazione multi-linkata \u00e8 un problema di ottimizzazione locale. Risolvere un problema di negoziazione multi-connessa significa trovare una soluzione di negoziazione -LRB- 0, \u03d5 -RRB- con utilit\u00e0 attesa ottimizzata Elf -LRB- 0, \u03d5 -RRB-, che \u00e8 definita come: Un ordinamento di negoziazione 0 definisce un ordinamento parziale ordine di tutte le questioni negoziali. Un'assegnazione di funzionalit\u00e0 \u03d5 \u00e8 una funzione di mappatura che assegna un valore a ciascun attributo che deve essere deciso nella negoziazione. Un risultato di negoziazione \u03c7 per un insieme di negoziazioni -LCB- vj 1, -LRB- j = 1,..., n -RRB- specifica il risultato di ciascuna negoziazione, successo o fallimento. Ci sono un totale di 2n risultati diversi per n negoziazioni: -LCB- chii1, -LRB- i = 1,..., 2n -RRB-. P -LRB- \u03c7i, \u03d5 -RRB- denota la probabilit\u00e0 del risultato \u03c7i data l'assegnazione della caratteristica \u03d5, che viene calcolata in base alla probabilit\u00e0 di successo di ciascuna negoziazione. La Sesta Int..Conf. congiunta Figura 1: Uno scenario di catena di negoziazione complessa Un algoritmo di ricerca euristica -LSB-10 -RSB- \u00e8 stato sviluppato per risolvere il problema di negoziazione multi-connessa del singolo agente che produce soluzioni quasi ottimali. Questo algoritmo viene utilizzato come nucleo del processo decisionale per ogni singolo agente nello scenario della catena di negoziazione. Nel resto del documento presentiamo il nostro lavoro su come migliorare la soluzione locale di un singolo agente nel context della catena negoziale globale. 6. LAVORI CORRELATI Fatima, Wooldridge e Jennings -LSB- 1 -RSB- hanno studiato le molteplici questioni nella negoziazione in termini di agenda e procedura di negoziazione. Tuttavia, questo lavoro \u00e8 limitato poich\u00e9 coinvolge solo la prospettiva di un singolo agente senza alcuna comprensione del fatto che l'agente possa far parte di una catena di negoziazione. Mailler e Lesser -LSB- 4 -RSB- hanno presentato un approccio a un problema di allocazione di risorse distribuite in cui si verifica lo scenario della catena di negoziazione. Modella il problema della negoziazione come un problema di ottimizzazione dei vincoli distribuiti -LRB- DCOP -RRB- e viene utilizzato un meccanismo di mediazione cooperativa per centralizzare parti rilevanti del DCOP. Nel nostro lavoro la negoziazione coinvolge questioni pi\u00f9 complicate come ricompensa, penalit\u00e0 e utilit\u00e0; inoltre, adottiamo un approccio distributivo in cui non \u00e8 necessario alcun controllo centralizzato. Un approccio parzialmente centralizzato basato su mediatori \u00e8 stato applicato al coordinamento e alla pianificazione di reti di attivit\u00e0 complesse -LSB- 8 -RSB-, che \u00e8 diverso dal nostro lavoro poich\u00e9 il sistema \u00e8 un sistema cooperativo completo e l'utilit\u00e0 individuale del singolo agente non \u00e8 interessata affatto. Un'asta combinatoria -LSB- 2, 9 -RSB- potrebbe essere un altro approccio per risolvere il problema della catena di negoziazione. Tuttavia, in un'asta combinatoria, l'agente non ragiona sull'ordine delle trattative. Ci\u00f2 porterebbe a un problema simile a quelli di cui abbiamo discusso quando viene utilizzata la politica della stessa scadenza. 7. CONCLUSIONE E LAVORO FUTURO In questo documento, abbiamo risolto i problemi della catena di negoziazione estendendo il nostro modello di negoziazione multi-collegata dalla prospettiva di un singolo agente a pi\u00f9 agenti. Invece di risolvere il problema della catena negoziale con un approccio centralizzato, adottiamo un approccio distribuito in cui ciascun agente ha un modello locale esteso e un processo decisionale. Abbiamo introdotto una fase di pre-negoziazione che consente agli agenti di trasferire informazioni a livello meta su questioni di negoziazione correlate. Utilizzando queste informazioni, l'agente pu\u00f2 costruire un modello pi\u00f9 accurato della negoziazione in termini di modellazione del rapporto tra flessibilit\u00e0 e probabilit\u00e0 di successo. Questo modello pi\u00f9 accurato aiuta l'agente nella scelta della soluzione negoziale appropriata. I dati sperimentali mostrano che questi meccanismi migliorano significativamente le prestazioni complessive dell'agente e del sistema. Nella futura estensione di questo lavoro, vorremmo sviluppare meccanismi per verificare quanto siano affidabili gli agenti.Uno scenario di catena di negoziazione complessa Un algoritmo di ricerca euristica -LSB- 10 -RSB- \u00e8 stato sviluppato per risolvere il problema di negoziazione multi-connessa del singolo agente che produce soluzioni quasi ottimali. Questo algoritmo viene utilizzato come nucleo del processo decisionale per ogni singolo agente nello scenario della catena di negoziazione. Nel resto del documento presentiamo il nostro lavoro su come migliorare la soluzione locale di un singolo agente nel context della catena negoziale globale. 6. LAVORI CORRELATI Fatima, Wooldridge e Jennings -LSB- 1 -RSB- hanno studiato le molteplici questioni nella negoziazione in termini di agenda e procedura di negoziazione. Tuttavia, questo lavoro \u00e8 limitato poich\u00e9 coinvolge solo la prospettiva di un singolo agente senza alcuna comprensione del fatto che l'agente possa far parte di una catena di negoziazione. Mailler e Lesser -LSB- 4 -RSB- hanno presentato un approccio a un problema di allocazione di risorse distribuite in cui si verifica lo scenario della catena di negoziazione. Modella il problema della negoziazione come un problema di ottimizzazione dei vincoli distribuiti -LRB- DCOP -RRB- e viene utilizzato un meccanismo di mediazione cooperativa per centralizzare parti rilevanti del DCOP. Nel nostro lavoro la negoziazione coinvolge questioni pi\u00f9 complicate come ricompensa, penalit\u00e0 e utilit\u00e0; inoltre, adottiamo un approccio distributivo in cui non \u00e8 necessario alcun controllo centralizzato. Un approccio parzialmente centralizzato basato su mediatori \u00e8 stato applicato al coordinamento e alla pianificazione di reti di attivit\u00e0 complesse -LSB- 8 -RSB-, che \u00e8 diverso dal nostro lavoro poich\u00e9 il sistema \u00e8 un sistema cooperativo completo e l'utilit\u00e0 individuale del singolo agente non \u00e8 interessata affatto. Un'asta combinatoria -LSB- 2, 9 -RSB- potrebbe essere un altro approccio per risolvere il problema della catena di negoziazione. Tuttavia, in un'asta combinatoria, l'agente non ragiona sull'ordine delle trattative. Ci\u00f2 porterebbe a un problema simile a quelli di cui abbiamo discusso quando viene utilizzata la politica della stessa scadenza. 7. CONCLUSIONE E LAVORO FUTURO In questo documento, abbiamo risolto i problemi della catena di negoziazione estendendo il nostro modello di negoziazione multi-collegata dalla prospettiva di un singolo agente a pi\u00f9 agenti. Invece di risolvere il problema della catena negoziale con un approccio centralizzato, adottiamo un approccio distribuito in cui ciascun agente ha un modello locale esteso e un processo decisionale. Abbiamo introdotto una fase di pre-negoziazione che consente agli agenti di trasferire informazioni a livello meta su questioni di negoziazione correlate. Utilizzando queste informazioni, l'agente pu\u00f2 costruire un modello pi\u00f9 accurato della negoziazione in termini di modellazione del rapporto tra flessibilit\u00e0 e probabilit\u00e0 di successo. Questo modello pi\u00f9 accurato aiuta l'agente nella scelta della soluzione negoziale appropriata. I dati sperimentali mostrano che questi meccanismi migliorano significativamente le prestazioni complessive dell'agente e del sistema. Nella futura estensione di questo lavoro, vorremmo sviluppare meccanismi per verificare quanto siano affidabili gli agenti.Uno scenario di catena di negoziazione complessa Un algoritmo di ricerca euristica -LSB- 10 -RSB- \u00e8 stato sviluppato per risolvere il problema di negoziazione multi-connessa del singolo agente che produce soluzioni quasi ottimali. Questo algoritmo viene utilizzato come nucleo del processo decisionale per ogni singolo agente nello scenario della catena di negoziazione. Nel resto del documento presentiamo il nostro lavoro su come migliorare la soluzione locale di un singolo agente nel context della catena negoziale globale. 6. LAVORI CORRELATI Fatima, Wooldridge e Jennings -LSB- 1 -RSB- hanno studiato le molteplici questioni nella negoziazione in termini di agenda e procedura di negoziazione. Tuttavia, questo lavoro \u00e8 limitato poich\u00e9 coinvolge solo la prospettiva di un singolo agente senza alcuna comprensione del fatto che l'agente possa far parte di una catena di negoziazione. Mailler e Lesser -LSB- 4 -RSB- hanno presentato un approccio a un problema di allocazione di risorse distribuite in cui si verifica lo scenario della catena di negoziazione. Modella il problema della negoziazione come un problema di ottimizzazione dei vincoli distribuiti -LRB- DCOP -RRB- e viene utilizzato un meccanismo di mediazione cooperativa per centralizzare parti rilevanti del DCOP. Nel nostro lavoro la negoziazione coinvolge questioni pi\u00f9 complicate come ricompensa, penalit\u00e0 e utilit\u00e0; inoltre, adottiamo un approccio distributivo in cui non \u00e8 necessario alcun controllo centralizzato. Un approccio parzialmente centralizzato basato su mediatori \u00e8 stato applicato al coordinamento e alla pianificazione di reti di attivit\u00e0 complesse -LSB- 8 -RSB-, che \u00e8 diverso dal nostro lavoro poich\u00e9 il sistema \u00e8 un sistema cooperativo completo e l'utilit\u00e0 individuale del singolo agente non \u00e8 interessata affatto. Un'asta combinatoria -LSB- 2, 9 -RSB- potrebbe essere un altro approccio per risolvere il problema della catena di negoziazione. Tuttavia, in un'asta combinatoria, l'agente non ragiona sull'ordine delle trattative. Ci\u00f2 porterebbe a un problema simile a quelli di cui abbiamo discusso quando viene utilizzata la politica della stessa scadenza. 7. CONCLUSIONE E LAVORO FUTURO In questo documento, abbiamo risolto i problemi della catena di negoziazione estendendo il nostro modello di negoziazione multi-collegata dalla prospettiva di un singolo agente a pi\u00f9 agenti. Invece di risolvere il problema della catena negoziale con un approccio centralizzato, adottiamo un approccio distribuito in cui ciascun agente ha un modello locale esteso e un processo decisionale. Abbiamo introdotto una fase di pre-negoziazione che consente agli agenti di trasferire informazioni a livello meta su questioni di negoziazione correlate. Utilizzando queste informazioni, l'agente pu\u00f2 costruire un modello pi\u00f9 accurato della negoziazione in termini di modellazione del rapporto tra flessibilit\u00e0 e probabilit\u00e0 di successo. Questo modello pi\u00f9 accurato aiuta l'agente nella scelta della soluzione negoziale appropriata. I dati sperimentali mostrano che questi meccanismi migliorano significativamente le prestazioni complessive dell'agente e del sistema. Nella futura estensione di questo lavoro, vorremmo sviluppare meccanismi per verificare quanto siano affidabili gli agenti.", "keyphrases": ["agente multiplo", "quadro negoziale", "catena negoziale", "sistema multi-agente semi-cooper", "pre-negoziazione", "negoziazione multi-link", "agente", "insieme di distribuzione", "attivit\u00e0 concorrenti multiple", "organo virtuale", "trasferimento dell'attivit\u00e0 secondaria", "meccanico di reputazione", "scenario complesso della catena di fornitura"]}
{"file_name": "I-14", "text": "Un algoritmo di ricerca distribuita basato sull'apprendimento per rinforzo per sistemi gerarchici di recupero delle informazioni peer-to-peer ABSTRACT Le strategie di routing esistenti dominanti impiegate nei sistemi di recupero delle informazioni basati su peerto-peer -LRB- P2P -RRB- -LRB- IR -RRB- sono basate sulla similarit\u00e0 approcci. In questi approcci, gli agenti dipendono dalla somiglianza del contenuto tra le query in entrata e gli agenti diretti vicini per dirigere le sessioni di ricerca distribuite. Tuttavia, tale euristica \u00e8 miope in quanto gli agenti vicini potrebbero non essere collegati ad agenti pi\u00f9 rilevanti. In questo articolo viene sviluppato un approccio basato sull'apprendimento per rinforzo online per sfruttare le caratteristiche dinamiche di runtime dei sistemi IR P2P rappresentate dalle informazioni sulle sessioni di ricerca passate. Nello specifico, gli agenti mantengono stime sulle capacit\u00e0 degli agenti a valle di fornire documenti rilevanti per le query in arrivo. Queste stime vengono aggiornate gradualmente imparando dalle informazioni di feedback restituite dalle precedenti sessioni di ricerca. Sulla base di queste informazioni, gli agenti derivano le politiche di routing corrispondenti. Successivamente, questi agenti instradano le query in base alle policy apprese e aggiornano le stime in base alle nuove policy di instradamento. I risultati sperimentali dimostrano che l'algoritmo di apprendimento migliora considerevolmente le prestazioni di instradamento su due set di raccolte di test che sono stati utilizzati in una variet\u00e0 di studi IR distribuiti. 1. INTRODUZIONE Negli ultimi anni \u00e8 cresciuto l\u2019interesse nello studio di come controllare i processi di ricerca nei sistemi peer-to-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- -LSB- 6, 13, 14, 15 -RSB-. In questa linea di ricerca, uno dei problemi principali che preoccupa i ricercatori \u00e8 quello di indirizzare in modo efficiente le richieste degli utenti nella rete agli agenti che sono in possesso dei documenti appropriati. In assenza di informazioni globali, le strategie dominanti per affrontare questo problema sono approcci basati sulla somiglianza dei contenuti -LSB- 6, 13, 14, 15 -RSB-. Sebbene la somiglianza dei contenuti tra query e nodi locali sembri essere un indicatore attendibile del numero di documenti rilevanti che risiedono su ciascun nodo, questi approcci sono limitati da una serie di fattori. In secondo luogo, gli approcci basati sulla similarit\u00e0 non tengono conto delle caratteristiche di runtime dei sistemi IR P2P, inclusi i parametri ambientali, l\u2019utilizzo della larghezza di banda e le informazioni storiche delle sessioni di ricerca passate, che forniscono informazioni preziose per gli algoritmi di instradamento delle query. In questo articolo sviluppiamo un approccio IR basato sull'apprendimento per rinforzo per migliorare le prestazioni degli algoritmi di ricerca IR distribuiti. Gli agenti possono acquisire strategie di ricerca migliori raccogliendo e analizzando le informazioni di feedback delle sessioni di ricerca precedenti. In particolare, gli agenti mantengono stime, vale a dire l'utilit\u00e0 attesa, sulle capacit\u00e0 degli agenti a valle di fornire documenti rilevanti per tipi specifici di query in arrivo.Queste stime vengono aggiornate gradualmente imparando dalle informazioni di feedback restituite dalle precedenti sessioni di ricerca. Sulla base delle informazioni aggiornate sull'utilit\u00e0 prevista, gli agenti derivano le politiche di instradamento corrispondenti. Successivamente, questi agenti instradano le query in base alle politiche apprese e aggiornano le stime sull'utilit\u00e0 prevista in base alle nuove politiche di instradamento. Questo processo viene condotto in modo iterativo. L'obiettivo dell'algoritmo di apprendimento, anche se consuma una certa larghezza di banda della rete, \u00e8 quello di abbreviare il tempo di instradamento in modo che vengano elaborate pi\u00f9 query per unit\u00e0 di tempo e allo stesso tempo trovino documenti pi\u00f9 rilevanti. Ci\u00f2 contrasta con gli approcci basati sulla somiglianza dei contenuti in cui operazioni simili vengono ripetute per ogni query in arrivo e il tempo di elaborazione rimane sostanzialmente costante nel tempo. Un altro modo di vedere questo articolo \u00e8 che il nostro approccio di base alla ricerca IR distribuita \u00e8 quello di costruire una rete sovrapposta gerarchica -LRB- organizzazione di agenti -RRB- basata sulla misura della similarit\u00e0 dei contenuti tra le raccolte di documenti degli agenti in modo bottom-up. Nel lavoro passato, abbiamo dimostrato che questa organizzazione migliora in modo significativo le prestazioni di ricerca. Lo scopo dell'apprendimento per rinforzo \u00e8 adattare le decisioni di routing degli agenti alle situazioni dinamiche della rete e imparare dalle sessioni di ricerca passate. Nello specifico, i contributi di questo documento includono: -LRB- 1 -RRB- un approccio basato sull'apprendimento di rinforzo per gli agenti per acquisire politiche di instradamento soddisfacenti basate sulle stime del contributo potenziale dei loro agenti vicini; -LRB- 2 -RRB- due strategie per accelerare il processo di apprendimento. Il resto di questo articolo \u00e8 organizzato come segue: La Sezione 2 esamina i sistemi gerarchici di condivisione dei contenuti e l'algoritmo di ricerca a due fasi basato su tale topologia. La sezione 3 descrive un approccio basato sull'apprendimento per rinforzo per dirigere il processo di instradamento; La sezione 4 descrive in dettaglio le impostazioni sperimentali e analizza i risultati. La sezione 5 discute gli studi correlati e la sezione 6 conclude il documento. 5. Lavoro correlato Il problema dell'instradamento del contenuto differisce dall'instradamento a livello di rete nelle reti di comunicazione a commutazione di pacchetto in quanto l'instradamento basato sul contenuto avviene nelle reti a livello di applicazione. Inoltre, gli agenti di destinazione nei nostri algoritmi di instradamento dei contenuti sono molteplici e gli indirizzi non sono noti nel processo di instradamento. I problemi di routing a livello IP sono stati attaccati dalla prospettiva dell'apprendimento per rinforzo -LSB- 2, 5, 11, 12 -RSB-. In letteratura esistono due classi principali di algoritmi di instradamento di pacchetti distribuiti e adattativi: algoritmi del vettore di distanza e algoritmi dello stato del collegamento. Sebbene questa linea di studi presenti una certa somiglianza con il nostro lavoro, si \u00e8 concentrata principalmente sulle reti di comunicazione a commutazione di pacchetto. Ogni agente mantiene stime, probabilistiche o deterministiche, sulla distanza da una determinata destinazione attraverso i suoi vicini. Una variante delle tecniche Q-Learning viene utilizzata al Sixth Intl..Conf. congiunta aggiornare le stime per convergere alle distanze reali. \u00c8 stato scoperto che la propriet\u00e0 della localit\u00e0 \u00e8 una caratteristica importante dei sistemi di recupero delle informazioni negli studi di modellazione degli utenti -LSB- 3 -RSB-. L'approccio basato sull'apprendimento \u00e8 percepito come pi\u00f9 vantaggioso per i sistemi di recupero delle informazioni distribuiti reali che mostrano propriet\u00e0 di localit\u00e0. Questo perch\u00e9 il traffico e i modelli di query degli utenti possono ridurre lo spazio degli stati e accelerare il processo di apprendimento. Il lavoro correlato per sfruttare questa propriet\u00e0 include -LSB- 7 -RSB-, in cui gli autori hanno tentato di affrontare questo problema mediante tecniche di modellazione dell'utente. 6. CONCLUSIONI In questo articolo viene sviluppato un approccio basato sull'apprendimento per rinforzo per migliorare le prestazioni degli algoritmi di ricerca IR distribuiti. In particolare, gli agenti mantengono stime, vale a dire l'utilit\u00e0 attesa, sulla capacit\u00e0 degli agenti a valle di fornire documenti rilevanti per le query in arrivo. Queste stime vengono aggiornate gradualmente imparando dalle informazioni di feedback restituite dalle precedenti sessioni di ricerca. Sulla base delle informazioni aggiornate sull'utilit\u00e0 prevista, gli agenti modificano le proprie politiche di instradamento. Successivamente, questi agenti instradano le query in base alle politiche apprese e aggiornano le stime sull'utilit\u00e0 prevista in base alle nuove politiche di instradamento. Gli esperimenti su due diversi set di dati IR distribuiti illustrano che l'approccio dell'apprendimento per rinforzo migliora considerevolmente l'utilit\u00e0 cumulativa nel tempo.", "keyphrases": ["sistema di recupero delle informazioni peer-to-peer", "rafforzare l'apprendimento", "algoritmo di ricerca distribuita", "decisione di rotta", "util", "rete", "imparare l'algoritmo", "rotta politica", "queri"]}
{"file_name": "C-23", "text": "Implementazione di un meccanismo di regolazione dinamica con selezione efficiente delle repliche in ambienti con griglia di dati ABSTRACT L'architettura di co-allocazione \u00e8 stata sviluppata per consentire il download parallelo di set di dati da pi\u00f9 server. Diverse strategie di co-allocazione sono state accoppiate e utilizzate per sfruttare le differenze di velocit\u00e0 tra vari collegamenti client-server e per affrontare le fluttuazioni di velocit\u00e0 dinamiche dividendo i file in pi\u00f9 blocchi di uguali dimensioni. Tuttavia, uno dei maggiori ostacoli, ovvero il tempo di inattivit\u00e0 dei server pi\u00f9 veloci che devono attendere che il server pi\u00f9 lento consegni il blocco finale, rende importante ridurre le differenze nei tempi di finitura tra i server di replica. In questo articolo, proponiamo uno schema di coallocazione dinamico, ovvero lo schema di co-allocazione di aggiustamento ricorsivo, per migliorare le prestazioni del trasferimento dei dati nelle griglie di dati. Il nostro approccio riduce il tempo di inattivit\u00e0 trascorso in attesa del server pi\u00f9 lento e diminuisce il tempo di completamento del trasferimento dei dati. Forniamo inoltre uno schema efficace per ridurre i costi di riassemblaggio dei blocchi di dati. 1. INTRODUZIONE Le griglie di dati aggregano risorse distribuite per risolvere problemi di gestione di set di dati di grandi dimensioni. La maggior parte delle applicazioni Data Grid vengono eseguite simultaneamente e accedono a un gran numero di file di dati nell'ambiente Grid. Alcune applicazioni scientifiche ad alta intensit\u00e0 di dati, come la fisica delle alte energie, le applicazioni bioinformatiche e gli osservatori astrofisici virtuali, comportano enormi quantit\u00e0 di dati che richiedono sistemi di gestione dei file di dati per replicare i file e gestire i trasferimenti di dati e l\u2019accesso distribuito ai dati. Il download di set di dati di grandi dimensioni da diverse posizioni di replica pu\u00f2 comportare prestazioni diverse, poich\u00e9 i siti di replica possono avere architetture, caricamenti di sistema e connettivit\u00e0 di rete diversi. Un modo per migliorare la velocit\u00e0 di download \u00e8 determinare le migliori posizioni di replica utilizzando le tecniche di selezione della replica -LSB- 19 -RSB-. Questo metodo seleziona i server migliori per fornire velocit\u00e0 di trasferimento ottimali poich\u00e9 la qualit\u00e0 della larghezza di banda pu\u00f2 variare in modo imprevedibile a causa della natura di condivisione di Internet. Un altro modo \u00e8 utilizzare la tecnologia di co-allocazione -LSB- 17 -RSB- per scaricare i dati. La co-allocazione dei trasferimenti di dati consente ai client di scaricare dati da pi\u00f9 posizioni stabilendo pi\u00f9 connessioni in parallelo. Diverse strategie di co-allocazione sono state fornite nel lavoro precedente -LSB- 17 -RSB-. Rimane uno svantaggio del tempo di inattivit\u00e0 poich\u00e9 i server pi\u00f9 veloci devono attendere che il server pi\u00f9 lento consegni il suo blocco finale. Pertanto, \u00e8 importante ridurre le differenze nei tempi di finitura tra i server di replica. In questo articolo, proponiamo uno schema di co-allocazione dinamico basato sull'architettura di trasferimento dati Grid di co-allocazione chiamata schema di co-allocazione RecursiveAdjustment che riduce il tempo di inattivit\u00e0 trascorso in attesa del server pi\u00f9 lento e migliora le prestazioni di trasferimento dei dati -LSB- 24 -RSB- . I risultati sperimentali mostrano che il nostro approccio \u00e8 superiore ai metodi precedenti e ha ottenuto le migliori prestazioni complessive.Discuteremo anche dei costi di combinazione e forniremo uno schema efficace per ridurli. La revisione di fondo e gli studi correlati sono presentati nella Sezione 2 e l'architettura di co-allocazione e il lavoro correlato sono introdotti nella Sezione 3. Nella Sezione 4, proponiamo un efficiente servizio di selezione delle repliche. I nostri approcci di ricerca sono delineati nella Sezione 5, mentre i risultati sperimentali e una valutazione delle prestazioni del nostro schema sono presentati nella Sezione 6. La Sezione 7 conclude questo documento di ricerca. 2. BACKGROUND 2.1 Griglia di dati Le griglie di dati -LSB- 1, 2, 16 -RSB- federano molte risorse di archiviazione. Grandi raccolte di dati misurati o calcolati stanno emergendo come risorse importanti in molte applicazioni ad alta intensit\u00e0 di dati. 2.1.1 Gestione delle repliche La gestione delle repliche comporta la creazione o la rimozione di repliche in un sito della griglia dati -LSB- 19 -RSB-. In altre parole, il ruolo di un gestore di replica \u00e8 creare o eliminare repliche all'interno di sistemi di archiviazione specifici. Nella maggior parte dei casi, queste repliche sono copie esatte dei file originali, create solo per sfruttare determinati vantaggi in termini di prestazioni. Un gestore di replica in genere mantiene un catalogo di replica contenente gli indirizzi dei siti di replica e le istanze di file. Il servizio di gestione della replica si occupa di gestire la replica di copie complete e parziali di set di dati, definiti come raccolte di file. Il servizio di gestione delle repliche \u00e8 solo uno dei componenti di un ambiente Data Grid che fornisce supporto per applicazioni ad alte prestazioni e ad uso intensivo di dati. Una replica o una posizione \u00e8 un sottoinsieme di una raccolta archiviata su un particolare sistema di archiviazione fisico. Potrebbero esserci pi\u00f9 sottoinsiemi eventualmente sovrapposti di una raccolta archiviata su pi\u00f9 sistemi di archiviazione in una griglia dati. Questi sistemi di storage a griglia possono utilizzare una variet\u00e0 di tecnologie di storage sottostanti e protocolli di spostamento dei dati, che sono indipendenti dalla gestione delle repliche. 2.1.2 Catalogo di replica Come accennato in precedenza, lo scopo del catalogo di replica \u00e8 fornire mappature tra nomi logici per file o raccolte e una o pi\u00f9 copie degli oggetti sui sistemi di archiviazione fisica. Il catalogo di replica include voci facoltative che descrivono singoli file logici. I file logici sono entit\u00e0 con nomi univoci a livello globale che possono avere una o pi\u00f9 istanze fisiche. Facoltativamente, il catalogo pu\u00f2 contenere una voce di file logico nel catalogo di replica per ogni file logico in una raccolta. Una griglia dati pu\u00f2 contenere pi\u00f9 cataloghi di replica. Ad esempio, una comunit\u00e0 di ricercatori interessati a un particolare argomento di ricerca potrebbe mantenere un catalogo di repliche per una raccolta di set di dati di reciproco interesse. \u00c8 possibile creare gerarchie di cataloghi di replica per imporre una struttura simile a directory su raccolte logiche correlate. Inoltre, il gestore delle repliche pu\u00f2 eseguire il controllo degli accessi su interi cataloghi nonch\u00e9 su singoli file logici. 2.1.3 Selezione della replica Lo scopo della selezione della replica -LSB- 16 -RSB- \u00e8 selezionare una replica tra i siti che costituiscono una griglia di dati -LSB- 19 -RSB-.I criteri di selezione dipendono dalle caratteristiche della domanda. Utilizzando questo meccanismo, gli utenti del Data Grid possono gestire facilmente le repliche dei set di dati nei propri siti, con prestazioni migliori. Molti sforzi precedenti sono stati dedicati al problema della selezione delle repliche. Il processo comune di selezione delle repliche consiste di tre fasi: preparazione dei dati, preelaborazione e previsione. Quindi, le applicazioni possono selezionare una replica in base ai suoi attributi specifici. La selezione delle repliche \u00e8 importante per le applicazioni ad uso intensivo di dati e pu\u00f2 garantire la trasparenza della posizione. Quando un utente richiede l'accesso a un set di dati, il sistema determina un modo appropriato per consegnare la replica all'utente. 2.2 Globus Toolkit e GridFTP Il progetto Globus -LSB- 9, 11, 16 -RSB- fornisce strumenti software chiamati collettivamente The Globus Toolkit che semplificano la creazione di griglie computazionali e applicazioni basate su griglie. Molte organizzazioni utilizzano Globus Toolkit per creare griglie computazionali per supportare le loro applicazioni. La composizione del Globus Toolkit pu\u00f2 essere descritta come tre pilastri: gestione delle risorse, servizi informativi e gestione dei dati. GRAM implementa un protocollo di gestione delle risorse, MDS implementa un protocollo di servizi di informazione e GridFTP implementa un protocollo di trasferimento dati. L'alleanza Globus ha proposto un protocollo comune di trasferimento e accesso ai dati chiamato GridFTP che fornisce un movimento di dati sicuro ed efficiente negli ambienti Grid -LSB- 3 -RSB-. Questo protocollo, che estende il protocollo FTP standard, fornisce un superset delle funzionalit\u00e0 offerte dai vari sistemi di storage Grid attualmente in uso. Per risolvere i problemi emergenti, la comunit\u00e0 Data Grid cerca di sviluppare un meccanismo di trasporto dati sicuro ed efficiente e servizi di gestione delle repliche. GridFTP \u00e8 un protocollo di trasporto dati affidabile, sicuro ed efficiente sviluppato come parte del progetto Globus. Esiste un'altra tecnologia chiave del progetto Globus, chiamata catalogo di replica -LSB- 16 -RSB- che viene utilizzato per registrare e gestire copie complete e parziali di set di dati. Il catalogo di replica contiene le informazioni di mappatura da un file logico o da una raccolta a uno o pi\u00f9 file fisici. 2.3 Servizio Meteorologico di Rete Il Servizio Meteorologico di Rete -LRB- NWS -RRB- -LSB- 22 -RSB- \u00e8 un sistema di monitoraggio generalizzato e distribuito per produrre previsioni di prestazione a breve termine basate su misurazioni storiche delle prestazioni. L'obiettivo del sistema \u00e8 caratterizzare e prevedere dinamicamente le prestazioni fornibili a livello applicativo da un insieme di risorse di rete e computazionali. 2.4 Utilit\u00e0 Sysstat Le utilit\u00e0 Sysstat -LSB- 15 -RSB- sono una raccolta di strumenti di monitoraggio delle prestazioni per il sistema operativo Linux. Il pacchetto Sysstat incorpora i comandi sar, mpstat e iostat. Il comando sar raccoglie e segnala informazioni sull'attivit\u00e0 del sistema, che possono anche essere salvate in un file di attivit\u00e0 del sistema per ispezioni future.Il comando iostat riporta le statistiche della CPU e le statistiche di I/O per dispositivi e dischi tty. 7. CONCLUSIONI L'architettura di co-allocazione fornisce un agente coordinato per l'assegnazione dei blocchi di dati. Un lavoro precedente ha dimostrato che lo schema di coallocazione dinamica porta a miglioramenti delle prestazioni. Tuttavia, non \u00e8 in grado di gestire il tempo di inattivit\u00e0 dei server pi\u00f9 veloci, che devono attendere che il server pi\u00f9 lento consegni il blocco finale. Abbiamo proposto lo schema di co-allocazione di aggiustamento ricorsivo per migliorare le prestazioni di trasferimento dei dati utilizzando l'architettura di co-allocazione in -LSB- 17 -RSB-. In questo approccio, i carichi di lavoro dei server di replica selezionati vengono continuamente adattati durante i trasferimenti di dati e forniamo una funzione che consente agli utenti di definire una soglia di blocco finale, in base al loro ambiente di griglia dati. I risultati sperimentali mostrano l'efficacia della nostra tecnica proposta nel migliorare il tempo di trasferimento e nel ridurre il tempo di inattivit\u00e0 complessivo trascorso in attesa del server pi\u00f9 lento. Abbiamo anche discusso il costo della ricombinazione e fornito uno schema efficace per ridurlo.", "keyphrases": ["distribuire risorse", "applicazione griglia dati", "replica", "co-alloc", "set di dati di grandi dimensioni", "protocollo di gestione delle risorse", "replica", "strategie di co-allocazione", "server", "eseguire"]}
{"file_name": "J-28", "text": "Aste multi-unit\u00e0 approssimativamente a prova di strategia e trattabili ABSTRACT Presentiamo un meccanismo d'asta approssimativamente efficiente e approssimativamente a prova di strategia per un problema di allocazione multi-unit\u00e0 di un singolo bene. Il linguaggio delle offerte nelle nostre aste consente curve costanti a tratti con decrescita marginale. Innanzitutto, sviluppiamo uno schema di approssimazione in tempo completamente polinomiale per il problema dell'allocazione multi-unit\u00e0, che calcola un'approssimazione -LRB- 1 + e -RRB- nel tempo peggiore T = O -LRB- n3/e -RRB-, date n offerte ciascuna con un numero costante di pezzi. In secondo luogo, incorporiamo questo schema di approssimazione all'interno di un meccanismo Vickrey-Clarke-Groves -LRB- VCG -RRB- e calcoliamo i pagamenti a n agenti per un costo asintotico di O -LRB- T log n -RRB-. Il massimo guadagno possibile derivante dalla manipolazione per un offerente nello schema combinato \u00e8 limitato da e / -LRB- 1 + e -RRB- V, dove V \u00e8 il surplus totale nel risultato efficiente. 1. INTRODUZIONE In questo articolo presentiamo uno schema di approssimazione completamente polinomiale per il problema dell'asta multi-unit\u00e0 a bene singolo. Il nostro schema \u00e8 sia approssimativamente efficiente che approssimativamente a prova di strategia. Le impostazioni delle aste considerate nel nostro articolo sono motivate dalle recenti tendenze del commercio elettronico; ad esempio, le aziende utilizzano sempre pi\u00f9 le aste per il loro approvvigionamento strategico. Consideriamo sia una variazione dell'asta inversa che una variazione dell'asta a termine e proponiamo un linguaggio di offerta compatto ed espressivo che consente curve costanti a tratti con decrescita marginale. Nell'asta inversa, consideriamo un singolo acquirente con una domanda di M unit\u00e0 di un bene e n fornitori, ciascuno con una funzione di costo marginale decrescente e costante a tratti. Inoltre, ciascun fornitore pu\u00f2 anche esprimere un limite superiore, o vincolo di capacit\u00e0, sul numero di unit\u00e0 che pu\u00f2 fornire. I modelli a variazione inversa, ad esempio, un'asta di approvvigionamento per ottenere materie prime o altri servizi -LRB- ad esempio circuiti stampati, alimentatori, cartucce toner -RRB-, con lotti di dimensioni flessibili. Nell'asta a termine, consideriamo un singolo venditore con M unit\u00e0 di un bene e n acquirenti, ciascuno con una funzione di valutazione costante a tratti decrescente marginale. Un acquirente pu\u00f2 anche esprimere un limite inferiore, o dimensione minima del lotto, sul numero di unit\u00e0 richieste. I modelli di variazione a termine, ad esempio, un'asta per vendere l'inventario in eccesso in lotti di dimensioni flessibili. Consideriamo la complessit\u00e0 computazionale dell'implementazione del meccanismo Vickrey-Clarke-Groves -LSB- 22, 5, 11 -RSB- per il problema dell'asta multiunit\u00e0. Il meccanismo Vickrey-Clarke-Groves -LRB- VCG -RRB- ha una serie di propriet\u00e0 economiche interessanti in questo context, inclusa la resistenza alla strategia, tale che le offerte veritiere sono una strategia dominante per gli acquirenti nell'asta a termine e per i venditori nell'asta inversa, e efficienza allocativa, tale che il risultato massimizzi il surplus totale del sistema. Tuttavia, come discusso nella Sezione 2,l'applicazione dell'approccio basato su VCG \u00e8 limitata, nella direzione opposta, ai casi in cui i pagamenti totali ai venditori sono inferiori al valore del risultato per l'acquirente. In caso contrario, in questi casi o l'asta deve svolgersi in perdita oppure non ci si pu\u00f2 aspettare che l'acquirente scelga volontariamente di partecipare. Questo \u00e8 un esempio del problema del deficit di bilancio che spesso si verifica nella progettazione di meccanismi efficienti -LSB- 17 -RSB-. Il problema computazionale \u00e8 interessante, perch\u00e9 anche con curve di offerta marginalmente decrescenti, il problema di allocazione sottostante risulta essere -LRB- debolmente -RRB- intrattabile. Ad esempio, il classico zaino 0/1 \u00e8 un caso speciale di questo problema.1 Modelliamo il modello 1Tuttavia, il problema pu\u00f2 essere risolto facilmente con uno schema avido se rimuoviamo tutti i vincoli di capacit\u00e0 dal venditore e tutti i problemi di allocazione come un nuovo e interessante generalizzazione del classico problema dello zaino, e sviluppare uno schema di approssimazione in tempo completamente polinomiale, calcolando un'approssimazione -LRB- 1 + ~ -RRB- - nel tempo peggiore T = O -LRB- n3 / \u03b5 -RRB-, dove ciascuna offerta ha un numero fisso di pezzi costanti a tratti. Dato questo schema, un calcolo semplice dei pagamenti VCG a tutti gli n agenti richiede tempo O -LRB- nT -RRB-. Questo limite superiore tende a 1 all\u2019aumentare del numero di venditori. Il meccanismo VCG approssimativo \u00e8 -LRB- \u03b5 1 + \u03b5 -RRB- - a prova di strategia per un'approssimazione entro -LRB- 1 + ~ -RRB- dell'allocazione ottimale. Ci\u00f2 significa che un offerente pu\u00f2 guadagnare al massimo -LRB- \u03b5 1 + \u03b5 -RRB- V da un'offerta non veritiera, dove V \u00e8 il surplus totale derivante dall'allocazione efficiente. La sezione 2 definisce formalmente le aste a termine e quelle inverse e definisce i meccanismi VCG. Dimostriamo anche le nostre affermazioni sulla \u03b5-strategyproofness. La Sezione 3 fornisce la formulazione a zaino generalizzata per i problemi di allocazione multi-unit\u00e0 e introduce lo schema di approssimazione temporale completamente polinomiale. La sezione 4 definisce lo schema di ravvicinamento dei pagamenti nel meccanismo VCG. La sezione 5 conclude. 1.1 Lavoro correlato Negli ultimi anni c'\u00e8 stato un notevole interesse nella caratterizzazione di casi speciali tempo-polinomiali o approssimabili del problema generale di allocazione combinatoria, in cui sono presenti pi\u00f9 elementi diversi. Il problema di allocazione combinatoria -LRB- CAP -RRB- \u00e8 sia NP-completo che inapprossimabile -LRB- es -LSB- 6 -RSB- -RRB-. Identifichiamo un problema di allocazione non banale ma approssimabile con un linguaggio di offerta esclusivo o espressivo: all'acquirente nella nostra impostazione \u00e8 consentito accettare al massimo un punto sulla curva di offerta. L\u2019idea di utilizzare approssimazioni all\u2019interno dei meccanismi, pur mantenendo la completa impermeabilit\u00e0 alla strategia o la dominanza \u03b5, ha ricevuto una certa attenzione in precedenza. Ad esempio, Lehmann et al. -LSB- 15 -RSB- propone un'approssimazione greedy e a prova di strategia a un problema di asta combinatoria single-minded. Vincoli di dimensione minima del lotto Feigen da parte degli acquirenti.baum & Shenker -LSB- 8 -RSB- hanno definito il concetto di approssimazioni strategicamente fedeli e hanno proposto lo studio delle approssimazioni come una direzione importante per la progettazione di meccanismi algoritmici. Eso et al. -LSB- 7 -RSB- hanno studiato un problema di approvvigionamento simile, ma per un diverso modello di sconto sul volume. Questo lavoro precedente formula il problema come un programma lineare intero misto generale e fornisce alcuni risultati empirici su dati simulati. Kalagnanam et al. -LSB- 12 -RSB- affronta le aste doppie, in cui pi\u00f9 acquirenti e venditori commerciano un bene divisibile. Anche il focus di questo articolo \u00e8 diverso: indaga i prezzi di equilibrio utilizzando le curve di domanda e offerta, mentre il nostro focus \u00e8 sulla progettazione di meccanismi efficienti. Ausubel -LSB- 1 -RSB- ha proposto un'asta multi-unit\u00e0 a prezzo ascendente per acquirenti con valori marginalmente decrescenti -LSB- 1 -RSB-, con un'interpretazione come algoritmo primordiale-duale -LSB- 2 -RSB-. 5. CONCLUSIONI Abbiamo presentato uno schema di approssimazione in tempo completamente polinomiale per il problema dell'asta multi-unit\u00e0 per un singolo bene, utilizzando un linguaggio di offerta costante a tratti decrescente marginale. Il nostro schema \u00e8 sia approssimativamente efficiente che approssimativamente a prova di strategia all'interno di qualsiasi fattore specificato \u03b5 > 0. Come tale \u00e8 un esempio di risultato di dominanza \u03b5 trattabile computazionalmente, nonch\u00e9 un esempio di problema di allocazione non banale ma approssimabile. \u00c8 particolarmente interessante il fatto che siamo in grado di calcolare i pagamenti ad n agenti in un meccanismo basato su VCG nel caso peggiore O -LRB- T log n -RRB-, dove T \u00e8 la complessit\u00e0 temporale per calcolare la soluzione ad un singolo problema di allocazione.\u00c8 particolarmente interessante il fatto che siamo in grado di calcolare i pagamenti ad n agenti in un meccanismo basato su VCG nel caso peggiore O -LRB- T log n -RRB-, dove T \u00e8 la complessit\u00e0 temporale per calcolare la soluzione ad un singolo problema di allocazione.\u00c8 particolarmente interessante il fatto che siamo in grado di calcolare i pagamenti ad n agenti in un meccanismo basato su VCG nel caso peggiore O -LRB- T log n -RRB-, dove T \u00e8 la complessit\u00e0 temporale per calcolare la soluzione ad un singolo problema di allocazione.", "keyphrases": ["meccanismo d'asta approx-effici e approssimativamente a prova di strategia", "problema di allocazione multi-unit\u00e0 a singolo bene", "Schema di approssimazione fulli polinomi-tempo", "vickrei-clark-grove", "asta a termine", "asta al ribasso", "equilibrio", "margine-diminuzione pezzo con curva costante", "lingua dell'offerta", "programma dinamico"]}
{"file_name": "C-40", "text": "Edge Indexing in una griglia per ambienti virtuali altamente dinamici \u2217 ABSTRACT I sistemi applicativi emergenti basati su giochi come Second Life1 forniscono ambienti virtuali 3D in cui pi\u00f9 utenti interagiscono tra loro in tempo reale. Sono pieni di contenuti virtuali autonomi e mutevoli che vengono continuamente aumentati dagli utenti. Per rendere i sistemi altamente scalabili ed estensibili dinamicamente, sono solitamente costruiti su una divisione del sottospazio della griglia basata su client-server in cui i mondi virtuali sono suddivisi in sottomondi gestibili. In ogni sottomondo, l'utente riceve continuamente aggiornamenti rilevanti sulla geometria degli oggetti in movimento da server connessi in remoto e li visualizza secondo il suo punto di vista, invece di recuperarli da un supporto di memorizzazione locale. In tali sistemi, la determinazione dell'insieme di oggetti visibili dal punto di vista dell'utente \u00e8 uno dei fattori principali che influenzano la velocit\u00e0 effettiva e la scalabilit\u00e0 del server. Nello specifico, eseguire test di visibilit\u00e0 in tempo reale in ambienti virtuali estremamente dinamici \u00e8 un compito molto impegnativo poich\u00e9 milioni di oggetti e milioni di utenti attivi si muovono e interagiscono. Riconosciamo che le sfide descritte sono strettamente correlate a un problema di database spaziale, e quindi mappiamo gli oggetti geometrici in movimento nello spazio virtuale su un insieme di oggetti multidimensionali in un database spaziale modellando ciascun avatar sia come oggetto spaziale che come oggetto interrogazione in movimento. Sfortunatamente, i metodi di indicizzazione spaziale esistenti non sono adatti a questo tipo di nuovi ambienti. L'obiettivo principale di questo articolo \u00e8 presentare un'efficiente struttura di indice spaziale che riduca al minimo lo scoppio inaspettato di oggetti e supporti la determinazione della visibilit\u00e0 in tempo reale altamente scalabile. Scopriremo quindi molte propriet\u00e0 utili di questa struttura e confronteremo la struttura dell'indice con vari metodi di indicizzazione spaziale in termini di qualit\u00e0 delle query, throughput del sistema e utilizzo delle risorse. Ci aspettiamo che il nostro approccio getti le basi per i framework virtuali di prossima generazione che potrebbero fondersi nei servizi web esistenti nel prossimo futuro. \u2217 Questa ricerca \u00e8 stata finanziata in parte dalle sovvenzioni NSF EEC9529152 -LRB- IMSC ERC -RRB- e IIS-0534761 e dalle donazioni di apparecchiature da parte di Intel Corporation, Hewlett-Packard, Sun Microsystems e Raptor Networks Technology. Categorie e descrittori di soggetto: C. 2.4 -LSB- Computer -- Com 1. INTRODUZIONE Recentemente, i giochi online multigiocatore di massa -LRB- MMOG -RRB- sono stati studiati come struttura per ambienti virtuali di prossima generazione. In questo articolo ci concentreremo principalmente sui primi due requisiti. L'estensibilit\u00e0 dinamica consente agli utenti del gioco normale di distribuire i propri contenuti creati. Questo \u00e8 un concetto potente, ma sfortunatamente il contenuto creato dall'utente tende a creare squilibri nella complessit\u00e0 della scena esistente, causando problemi di prestazioni a livello di sistema. Un altro requisito importante \u00e8 la scalabilit\u00e0.Suddividendo attentamente il mondo in pi\u00f9 sottomondi o replicando i mondi in posizioni geograficamente disperse, \u00e8 possibile supportare un numero enorme di utenti simultanei. Second Life -LSB- 4 -RSB- \u00e8 il primo sistema MMOG distribuito con successo che soddisfa entrambi i requisiti. Ma riconosciamo che questi requisiti valgono anche per i nuovi ambienti virtuali. Figura 1: Lo scoppio dell'oggetto si \u00e8 verificato mentre l'utente avanzava -LRB- screenshot da Second Life -RRB- dove \u0394 = 2 secondi. utilizza un modello di streaming di oggetti 3D basato su client/server -LSB- 5 -RSB-. In questo modello, un server trasmette continuamente sia eventi di aggiornamento che dati di geometria a ogni utente connesso. Di conseguenza, questo ambiente di gioco estensibile ha accelerato la distribuzione di contenuti creati dagli utenti e offre agli utenti la libert\u00e0 illimitata di perseguire un'esperienza di navigazione nel suo spazio. Una delle operazioni principali nelle applicazioni MMOG che trasmettono oggetti 3D \u00e8 calcolare accuratamente tutti gli oggetti visibili all'utente. L'approccio tradizionale alla determinazione della visibilit\u00e0, tuttavia, presenta un problema di scostamento degli oggetti. Ad esempio, una casa al di fuori del campo visibile dell'utente non viene disegnata al tempo t, illustrato nella Figura 1 -LRB- a -RRB-. Mentre l'utente avanza, la casa apparir\u00e0 improvvisamente al tempo -LRB- t + \u0394 -RRB- come mostrato nella Figura 1 -LRB- b -RRB-. Il calcolo della visibilit\u00e0 per ciascun utente non solo deve essere accurato, ma anche veloce. Questa sfida \u00e8 illustrata dal fatto che il numero massimo di utenti simultanei per server di Second Life \u00e8 ancora un ordine di grandezza inferiore a quello dei mondi stazionari. Per affrontare queste sfide, proponiamo un metodo che identifica gli oggetti visibili pi\u00f9 rilevanti da un dato database geometrico -LRB- modello di visualizzazione -RRB- e quindi propone un metodo di indicizzazione veloce che calcola gli oggetti visibili per ciascun utente -LRB- indicizzazione spaziale -RRB-. I nostri due nuovi metodi rappresentano i principali contributi di questo lavoro. La sezione 2 presenta il lavoro correlato. La sezione 3 descrive il nostro nuovo metodo di visualizzazione. Nella Sezione 4 presentiamo le ipotesi sulla nostra applicazione target e introduciamo un nuovo metodo di indicizzazione spaziale progettato per supportare calcoli di visibilit\u00e0 in tempo reale. Discutiamo anche i suoi problemi di ottimizzazione. La Sezione 5 riporta l'analisi quantitativa e la Sezione 6 presenta i risultati preliminari dei nostri esperimenti basati sulla simulazione. Infine, concludiamo e affrontiamo le future direzioni di ricerca nella Sezione 7. 2. LAVORI CORRELATI La determinazione della visibilit\u00e0 \u00e8 stata ampiamente esplorata nel campo della grafica 3D. Sono stati proposti vari algoritmi di rendering locale per eliminare gli oggetti non necessari prima del rendering o in qualsiasi fase della pipeline di rendering. Tuttavia, questi algoritmi presuppongono che tutti gli oggetti visibili candidati siano stati archiviati localmente. Se gli oggetti di destinazione sono archiviati su server remoti, i client ricevono gli elementi geometrici necessari per il rendering dai database del server. Tuttavia,questi algoritmi di ottimizzazione online non riescono a risolvere i problemi di prestazioni del server in ambienti altamente affollati. D'altra parte, il nostro modello di calcolo della visibilit\u00e0, rappresentativo di questa categoria, si basa su diverse ipotesi sulla rappresentazione dei dati delle entit\u00e0 virtuali. Nell'area grafica, c'\u00e8 stato poco lavoro sul supporto dei calcoli di visibilit\u00e0 in tempo reale per un numero enorme di oggetti e utenti in movimento. Qui riconosciamo che tali problemi legati alla grafica hanno una somiglianza molto stretta con i problemi dei database spaziali. Recentemente, numerose pubblicazioni hanno affrontato il problema della scalabilit\u00e0 su come supportare un numero elevato di oggetti e query in ambienti altamente dinamici. Per supportare aggiornamenti frequenti, sono state studiate in profondit\u00e0 due politiche di partizionamento: indicizzazione spaziale basata su -LRB- 1 -RRB- R-tree e indicizzazione spaziale basata su -LRB- 2 -RRB- grid. Il modello di partizionamento basato sulla griglia \u00e8 un caso speciale di partizionamento fisso. Recentemente \u00e8 stato riscoperto perch\u00e9 pu\u00f2 essere efficace in ambienti altamente dinamici. Q-Index -LSB- 13, 11 -RSB- \u00e8 uno dei primi lavori che riscopre l'utilit\u00e0 del partizionamento dello spazio basato su griglia per ambienti emergenti di oggetti in movimento. A differenza dei tradizionali metodi di indicizzazione spaziale che costruiscono un indice sugli oggetti in movimento, costruisce un indice sulle query a intervallo continuo, presupponendo che le query si spostino raramente mentre gli oggetti si muovono liberamente. L'idea di base dell'albero Q+R -LSB- 14 -RSB- \u00e8 quella di separare le strutture di indicizzazione per oggetti quasi stazionari e oggetti in movimento: gli oggetti in movimento veloce sono indicizzati in un Quadtree e gli oggetti quasi stazionari sono memorizzati in un R\u2217 - albero. SINA -LSB- 10 -RSB- \u00e8 stato proposto per fornire valutazioni efficienti delle query per qualsiasi combinazione di oggetti stazionari/in movimento e query stazionarie/in movimento. Nello specifico, questo approccio rileva in modo efficiente solo gli aggiornamenti di oggetti -LRB- positivi -RRB- o non pi\u00f9 rilevanti -LRB- negativi -RRB-. A differenza di altri metodi di indicizzazione spaziale che si concentrano sulla riduzione del costo di valutazione delle query, Hu et al. -LSB- 12 -RSB- ha proposto un quadro generale che riduce al minimo i costi di comunicazione per gli aggiornamenti sulla posizione mantenendo un'area rettangolare chiamata regione sicura attorno agli oggetti in movimento. Finch\u00e9 un oggetto risiede in questa regione, \u00e8 garantito che tutti i risultati della query siano validi nel sistema. Se gli oggetti si spostano fuori dalla loro regione, le richieste di aggiornamento della posizione dovrebbero essere consegnate al server del database e le query interessate verranno rivalutate al volo. Il nostro metodo di indicizzazione \u00e8 molto simile agli approcci di cui sopra. La differenza principale \u00e8 che noi ci concentriamo maggiormente sulla determinazione della visibilit\u00e0 in tempo reale, mentre altri assumono vincoli temporali sciolti. 6. VALUTAZIONE Questa sezione presenta due configurazioni di simulazione e i relativi risultati prestazionali. La sezione 6.1 esamina se il nostro nuovo approccio di visualizzazione \u00e8 superiore ai modelli di visualizzazione esistenti, nonostante la sua maggiore complessit\u00e0 di indicizzazione. Sezione 6.2 illustra il grado di praticit\u00e0 e scalabilit\u00e0 del nostro metodo di indicizzazione progettato per il nostro nuovo modello di visualizzazione. 6.1 Giustificazione del modello di visualizzazione avviato dall'oggetto 6.1.1 Metriche di valutazione P \u00e8 il rapporto tra gli elementi rilevanti recuperati e tutti gli elementi recuperati. Un valore inferiore di P implica che il set di risultati della query contiene un numero elevato di oggetti non necessari che non devono essere recapitati a un client. Un valore P pi\u00f9 elevato indica un carico di traffico di rete maggiore del necessario. R \u00e8 il rapporto tra gli elementi rilevanti recuperati e tutti gli elementi rilevanti. Un valore R pi\u00f9 basso significa che vengono ignorati pi\u00f9 oggetti che dovrebbero essere riconosciuti. Dalla misura R, possiamo stimare quantitativamente il verificarsi dello scoppio degli oggetti. Oltre alle metriche P e R, utilizziamo una metrica standardizzata di valutazione delle query a valore singolo che combina P e R, denominata E -- Measure -LSB- 15 -RSB-. La misura E -- \u00e8 definita come: Se \u03b2 \u00e8 inferiore a 1, P diventa pi\u00f9 importante. Altrimenti, R influenzer\u00e0 la misura E in modo significativo. Un valore E-measure inferiore implica che il modello di visualizzazione testato ha una qualit\u00e0 superiore. Il miglior valore di misura E \u00e8 zero, dove i valori migliori per P e R sono entrambi. 6.1.2 Impostazione della simulazione Abbiamo testato quattro schemi di elaborazione delle query, che utilizzano un modello di visualizzazione avviato dall'utente o avviato dall'oggetto: \u2022 Calcolo della visibilit\u00e0 avviato dall'utente \u2013 RQ \u2013 OP: Query sulla regione \u2013 Punto oggetto \u2022 Oggetto calcolo della visibilit\u00e0 orientato alla visibilit\u00e0 -- PQ-OR : Interrogazione punto -- Regione oggetto -- RQ-OR : Interrogazione regione -- Regione oggetto -- ACQ-OR : Interrogazione cella approssimativa -- Regione oggetto RQ -- OP \u00e8 il tipico schema di calcolo che raccoglie tutti gli oggetti la cui posizione \u00e8 all'interno di un'AOI definita dall'utente. PQ -- OR raccoglie un insieme di oggetti la cui AOI interseca un dato punto utente, formalmente -LCB- o | qP \u2208 oR -RCB-. RQ -- OR, uno schema di calcolo immaginario, \u00e8 la combinazione di RQ -- OP e PQ -- OR dove l'AOI di un oggetto si interseca con quello di un utente, -LCB- o | oR \u2229 qR = ~ \u2205 -RCB-. Infine, ACQ - OR, un modello di calcolo della visibilit\u00e0 approssimativo, \u00e8 uno schema speciale progettato per il partizionamento dello spazio basato su griglia, che \u00e8 la nostra metodologia di valutazione delle celle da noi scelta per l'indicizzazione dei bordi. Se uno spazio virtuale \u00e8 partizionato in celle affiancate e un punto utente appartiene a una delle celle, ACQ -- OR ricerca gli oggetti i cui calcoli AOI Tabella 5: P e R di diversi schemi di determinazione della visibilit\u00e0. Tabella 6: Il tempo trascorso misurato -LRB- secondi -RRB- di 100.000 oggetti in movimento e 10.000 utenti in movimento in un ambiente in movimento lento si intersecherebbe con la regione della cella della griglia corrispondente. Identifica qualsiasi oggetto o che soddisfa la condizione cR n oR _ ~ 0 dove la cella c soddisfa anche qP E cR. Il nostro programma di simulazione ha popolato 100.000 entit\u00e0 oggetto e 10.000 entit\u00e0 utente in uno spazio unitario 2D, -LSB- 0, 1 -RRB- x -LSB- 0, 1 -RRB-. Le entit\u00e0 popolate sono collocate uniformemente nello spazio unitario.Il programma esegue test di intersezione tra tutte le entit\u00e0 utente e tutte le entit\u00e0 oggetto in modo esaustivo e calcola i valori di misura P, R ed E -LRB- mostrati nella Tabella 5 -RRB-. 6.1.3 Risultati sperimentali Distribuzione delle misure P e R: La Figura 7 mostra la distribuzione di P e R per RQ \u2013 OP. Possiamo osservare che P e R sono pi\u00f9 o meno inversamente proporzionali tra loro quando si varia l'intervallo AOI di un utente. Una lunghezza laterale minore comporta una maggiore precisione ma una minore completezza. Ad esempio, il 5% della lunghezza del lato di un'AOI utente rileva tutti gli oggetti la cui lunghezza del lato dell'AOI \u00e8 almeno del 5%. Pertanto, \u00e8 garantito che ogni oggetto recuperato da RQ - OP venga sottoposto a rendering sul client. Ma RQ - OP non \u00e8 in grado di rilevare gli oggetti al di fuori dell'AOI dell'utente, soffrendo quindi di troppi oggetti mancanti che dovrebbero essere renderizzati. Allo stesso modo, l'utente la cui AOI \u00e8 pi\u00f9 ampia di qualsiasi altra AOI non pu\u00f2 perdere alcun oggetto di cui dovrebbe essere eseguito il rendering, ma rileva troppi oggetti non necessari. Per eliminare qualsiasi problema di scoppio degli oggetti, la lunghezza del lato di qualsiasi AOI deve essere maggiore o uguale alla distanza massima visibile di qualsiasi oggetto nel sistema, il che potrebbe comportare un significativo degrado del sistema. Distribuzione delle misure elettroniche: la Figura 8 rivela due tendenze. Innanzitutto, i valori di precisione di RQ -- OP si trovano tra quelli di ACQ -- OR -LRB- griglia 100 x 100 -RRB- e RQ -- OR. In secondo luogo, la curva di tendenza del grafico della misura Precisione -- a -- E -- di RQ -- OR mostra somiglianza con quella di ACQ -- OR. Effetto delle diverse dimensioni della griglia: la Figura 9 mostra la differenza statistica di E - valori di misura di sette diversi schemi di partizionamento della griglia -LRB- utilizzando ACQ - OR -RRB- e un modello RQ - OP. Utilizziamo un diagramma a scatola e a baffi per mostrare sia i valori mediani che le varianze delle distribuzioni delle misure E e i valori anomali di ciascuno schema. Tracciamo anche il valore mediano delle misure RQ - OP E - -LRB- linea verde -RRB- a scopo di confronto. Sebbene gli schemi ACQ - OR presentino alcuni valori anomali, i loro valori di misura E sono fortemente concentrati attorno ai valori mediani, quindi sono meno sensibili all'oggetto AOI. Come previsto, il partizionamento a grana fine della griglia ha mostrato un valore di misura E inferiore. Lo schema RQ-OP ha mostrato una variazione di qualit\u00e0 pi\u00f9 ampia rispetto ad altri schemi, il che \u00e8 in gran parte attribuibile alle diverse lunghezze del lato utente. Man mano che la misura R diventa pi\u00f9 importante, la qualit\u00e0 della query di ACQ -- OR viene migliorata in modo pi\u00f9 evidente rispetto a quella di RQ -- OP. Dalla Figura 9, lo schema a griglia 20x20 aveva una misura E migliore. Tabella 7: Tempo trascorso misurato -LRB- secondi -RRB- di 100.000 oggetti in movimento e 10.000 utenti in movimento in un ambiente altamente dinamico -LRB- valore in un ambiente con priorit\u00e0 rispetto a un ambiente con pari priorit\u00e0. Di conseguenza, possiamo approssimativamente anticipare che almeno il partizionamento delle celle della griglia 20x20 recupera una qualit\u00e0 superiore di insiemi visibili rispetto a RQ - OP. 6.2 Valutazione dell'Edge Indexing In questa sezione,presentiamo i risultati preliminari delle simulazioni che esaminano l'applicabilit\u00e0 della nostra implementazione dell'indicizzazione dei bordi. Per stimare il grado di supporto in tempo reale del nostro metodo di indicizzazione, abbiamo utilizzato il tempo totale trascorso per aggiornare tutte le entit\u00e0 in movimento e calcolare i set visibili per ogni cella. Abbiamo anche sperimentato diverse politiche di partizionamento della griglia e le abbiamo confrontate con soluzioni di ricerca esaustive. 6.2.1 Configurazione della simulazione Abbiamo implementato algoritmi di indicizzazione dei bordi in C e abbiamo eseguito gli esperimenti su un processore Itanium a 64 bit e 900 MHz con 8 GB di memoria. Abbiamo implementato un meccanismo di tabella hash generalizzato per memorizzare strutture di nodi e bordi. 6.2.2 Risultati sperimentali Costo del monitoraggio periodico: le tabelle 6 e 7 mostrano i numeri delle prestazioni di diversi metodi di indicizzazione dei bordi variando v. Anche la velocit\u00e0 di movimento delle entit\u00e0 \u00e8 stata assegnata uniformemente tra 0 e v. Tuttavia, il metodo a due tabelle ha mostrato un tempo di valutazione leggermente superiore rispetto ai due metodi a tabella singola a causa della rimozione sequenziale del token. La Tabella 7 ha esemplificato il tempo trascorso tra gli aggiornamenti dell'indice e le valutazioni delle celle in un ambiente altamente dinamico in cui coesistono oggetti che si muovono lentamente e oggetti che si muovono dinamicamente. Rispetto ai risultati mostrati nella Tabella 6, l'approccio a due tabelle ha prodotto numeri di prestazioni simili indipendentemente dagli ambienti di movimento sottostanti. Tuttavia, il miglioramento delle prestazioni ottenuto dalla politica incrementale della singola tabella \u00e8 inferiore rispetto a quello dell'ambiente in lento movimento. Effetto delle diverse dimensioni della griglia: il numero di aggiornamenti di oggetti e valutazioni di celle che possono essere supportati in un determinato periodo di tempo \u00e8 un parametro prestazionale importante per quantificare la velocit\u00e0 effettiva del sistema. In questa sezione, valutiamo i risultati delle prestazioni di tre diversi modelli di calcolo della visibilit\u00e0: due metodi di ricerca esaustiva guidata dal calcolo; e un metodo di indicizzazione dei bordi della tabella a due con diverse dimensioni della griglia. Figura 7: Distribuzione di P e R misurata da RQ \u2013 OP. Figura 8: E - misura il valore in funzione di Figura 9: E - misura il valore in funzione di ACQ - Schema di partizionamento della griglia QR quando Figura 10: Tempo totale trascorso di diversi schemi di indicizzazione. I metodi di ricerca esaustivi non mantengono alcun risultato intermedio. Calcolano semplicemente se un dato punto utente si trova all'interno di un dato oggetto AOI. Possono tollerare il comportamento imprevedibile del movimento degli oggetti. La Figura 10 rivela la differenza di prestazioni tra le soluzioni esaustive e i metodi a due tabelle, una differenza fino a due ordini di grandezza. Come mostrato nella Sezione 5, il tempo totale trascorso tra gli aggiornamenti degli oggetti e le valutazioni delle celle \u00e8 lineare rispetto alla lunghezza media del lato dell'oggetto AOI. Poich\u00e9 la lunghezza del lato \u00e8 rappresentata dalle unit\u00e0 di cella, un aumento del numero di celle aumenta proporzionalmente la lunghezza del lato. La Figura 10 illustra che i risultati della simulazione misurati corrispondono approssimativamente al miglioramento delle prestazioni previsto calcolato dall'analisi. 7.CONCLUSIONE E LAVORO FUTURO Per supportare l'estensibilit\u00e0 e la scalabilit\u00e0 dinamiche in ambienti altamente dinamici, abbiamo proposto un nuovo paradigma di visualizzazione, il modello di visualizzazione avviato dall'oggetto, e il suo efficiente metodo di indicizzazione, l'edge indexing. Rispetto al modello di visualizzazione tradizionale, il nostro nuovo modello di visualizzazione promette di eliminare qualsiasi problema di object popping che pu\u00f2 essere facilmente osservato negli ambienti virtuali esistenti a scapito di una maggiore complessit\u00e0 di indicizzazione. Il nostro modello di indicizzazione dei bordi, tuttavia, pu\u00f2 superare tale complessit\u00e0 di indicizzazione pi\u00f9 elevata indicizzando le estensioni spaziali a livello di bordo, non a livello di nodo, in un sottomondo partizionato a griglia ed \u00e8 stato convalidato attraverso analisi e simulazioni quantitative. Tuttavia, per ora la nostra indicizzazione dei bordi mantiene ancora una complessit\u00e0 maggiore, anche in un dominio bidimensionale. Attualmente stiamo sviluppando un altro metodo di indicizzazione dei bordi per rendere costante la complessit\u00e0 dell'indicizzazione. Una volta che la complessit\u00e0 dell'indicizzazione diventa costante, prevediamo di indicizzare le estensioni spaziali 3D e i dati geometrici multi-risoluzione. Ci aspettiamo che la nostra edge indexing possa contribuire al successo dell'implementazione degli ambienti di gioco di prossima generazione.", "keyphrases": ["indice del bordo", "ambiente virtuale dinamico", "applicazione base di gioco", "contenuto virtuale mutevole", "database spaziali", "metodo dell'indice spaziale", "test visibile in tempo reale", "modello di visualizzazione object-initi", "oggetto pop", "Estensione spaziale 3D"]}
{"file_name": "C-86", "text": "Affrontare il comportamento strategico in un allocatore di risorse microeconomiche implementato ABSTRACT Mentre i sistemi basati sul mercato sono stati a lungo proposti come soluzioni per l'allocazione distribuita delle risorse, pochi sono stati implementati per l'uso produttivo in sistemi informatici reali. A tal fine, presentiamo la nostra esperienza iniziale utilizzando Mirage, un sistema di allocazione delle risorse microeconomiche basato su un'asta combinatoria ripetuta. Mirage assegna tempo su un banco di prova di rete di sensori wireless a 148 nodi molto utilizzato. In particolare, ci concentriamo sul comportamento strategico degli utenti osservato nell'arco di un periodo di quattro mesi in cui sono state assegnate 312.148 ore nodo in 11 progetti di ricerca. Sulla base di questi risultati, presentiamo una serie di sfide chiave per i sistemi di allocazione delle risorse basati sul mercato basati su aste combinatorie ripetute. Infine, proponiamo miglioramenti all'attuale schema d'asta del sistema per mitigare le strategie osservate fino ad oggi e commentiamo anche alcuni passi iniziali verso la costruzione di un'asta combinatoria ripetuta approssimativamente a prova di strategia. 1. INTRODUZIONE I sistemi basati sul mercato sono stati a lungo proposti come soluzioni per l'allocazione delle risorse in sistemi distribuiti, comprese le griglie computazionali -LSB- 2, 20 -RSB-, banchi di prova per reti ad ampia area -LSB- 9 -RSB- e peer-to- sistemi peer -LSB- 17 -RSB-. Tuttavia, mentre i fondamenti teorici degli schemi basati sul mercato hanno fatto passi da gigante negli ultimi anni, l\u2019integrazione pratica dei meccanismi basati sul mercato nei sistemi informatici reali e le osservazioni empiriche di tali sistemi sotto carichi di lavoro reali sono rimasti un obiettivo sfuggente. A tal fine, abbiamo progettato, implementato e distribuito un sistema di allocazione delle risorse microeconomiche chiamato Mirage -LSB- 3 -RSB- per programmare il tempo del banco di prova su una rete di sensori wireless a 148 nodi -LRB- SensorNet -RRB- presso Intel Research. Il sistema, che utilizza un'asta combinatoria ripetuta -LSB- 5, 14 -RSB- per pianificare le allocazioni, \u00e8 in uso in produzione da oltre quattro mesi e ad oggi ha programmato oltre 312.148 ore nodo in 11 progetti di ricerca. Nel progettare e implementare Mirage, avevamo tre obiettivi principali. Innanzitutto, volevamo verificare se uno schema di allocazione delle risorse basato sul mercato fosse davvero necessario. Un problema economico esiste solo quando le risorse sono scarse. Pertanto, un obiettivo chiave era quello di misurare innanzitutto sia la contesa delle risorse sia la gamma di valutazioni sottostanti che gli utenti attribuiscono alle risorse durante i periodi di scarsit\u00e0 di risorse. In secondo luogo, volevamo osservare come si sarebbero effettivamente comportati gli utenti in un ambiente basato sul mercato. Con Mirage, volevamo osservare fino a che punto la razionalit\u00e0 reggesse e in che modo gli utenti avrebbero tentato di creare strategie e ingannare il sistema. Infine, volevamo identificare quali altri problemi pratici sarebbero emersi nell\u2019implementazione di un sistema basato sul mercato. In questo articolo riportiamo brevemente il nostro primo obiettivo concentrandoci principalmente sul secondo. Nell'implementazione di Mirage,abbiamo preso la decisione iniziale di basare il sistema su un'asta combinatoria ripetuta nota per non essere a prova di strategia. Cio\u00e8, gli utenti egoisti potrebbero tentare di aumentare il proprio guadagno personale, a scapito degli altri, non rivelando il loro vero valore al sistema. Abbiamo preso questa decisione principalmente perch\u00e9 la progettazione di un meccanismo a prova di strategia rimane un problema aperto e impegnativo e volevamo implementare un sistema funzionante e acquisire esperienza con utenti reali per raggiungere i nostri tre obiettivi in \u200b\u200bmodo tempestivo. L\u2019implementazione di un meccanismo non a prova di strategia ha avuto anche il vantaggio di testare la razionalit\u00e0 e vedere come e in che misura gli utenti avrebbero tentato di ingannare il sistema. Il contributo chiave di questo documento \u00e8 un'analisi di tale comportamento strategico osservato nell'arco di un periodo di quattro mesi e ha proposto perfezionamenti per mitigare tale comportamento nel percorso verso la costruzione di un'asta combinatoria ripetuta approssimativamente a prova di strategia. Il resto di questo documento \u00e8 organizzato come segue. Nella sezione 2 presentiamo una panoramica di Mirage, comprese osservazioni di alto livello sull'utilizzo in un periodo di quattro mesi. Nella Sezione 3 esaminiamo il comportamento strategico degli utenti, concentrandoci sui quattro tipi principali di strategie impiegate dagli utenti nel sistema. Sulla base di questi risultati, la Sezione 4 presenta una serie di sfide chiave per i sistemi di allocazione delle risorse basati sul mercato basati su aste combinatorie ripetute. Come primo passo per affrontare alcune di queste sfide, descriviamo i perfezionamenti dell'attuale schema d'asta di Mirage che mitigano le strategie osservate fino ad oggi e commentiamo anche alcuni passi iniziali verso la costruzione di un'asta combinatoria ripetuta approssimativamente a prova di strategia per Mirage. Infine, nella Sezione 5, concludiamo il lavoro. 5. CONCLUSIONE Nonostante inizialmente avesse utilizzato un'asta combinatoria ripetuta nota per non essere a prova di strategia, Mirage si \u00e8 dimostrato molto promettente come veicolo per l'allocazione del banco di prova SensorNet. Per realizzare pienamente questi vantaggi, tuttavia, \u00e8 necessario affrontare i problemi chiave nella progettazione di meccanismi a prova di strategia e nell\u2019ottimizzazione combinatoria. La natura temporale delle risorse computazionali e le richieste di risorse combinatorie delle applicazioni distribuite aggiungono un ulteriore livello di complessit\u00e0.Il resto di questo documento \u00e8 organizzato come segue. Nella sezione 2 presentiamo una panoramica di Mirage, comprese osservazioni di alto livello sull'utilizzo in un periodo di quattro mesi. Nella Sezione 3 esaminiamo il comportamento strategico degli utenti, concentrandoci sui quattro tipi principali di strategie impiegate dagli utenti nel sistema. Sulla base di questi risultati, la Sezione 4 presenta una serie di sfide chiave per i sistemi di allocazione delle risorse basati sul mercato basati su aste combinatorie ripetute. Come primo passo per affrontare alcune di queste sfide, descriviamo i perfezionamenti dell'attuale schema d'asta di Mirage che mitigano le strategie osservate fino ad oggi e commentiamo anche alcuni passi iniziali verso la costruzione di un'asta combinatoria ripetuta approssimativamente a prova di strategia per Mirage. Infine, nella Sezione 5, concludiamo il lavoro. 5. CONCLUSIONE Nonostante inizialmente avesse utilizzato un'asta combinatoria ripetuta nota per non essere a prova di strategia, Mirage si \u00e8 dimostrato molto promettente come veicolo per l'allocazione del banco di prova SensorNet. Per realizzare pienamente questi vantaggi, tuttavia, \u00e8 necessario affrontare i problemi chiave nella progettazione di meccanismi a prova di strategia e nell\u2019ottimizzazione combinatoria. La natura temporale delle risorse computazionali e le richieste di risorse combinatorie delle applicazioni distribuite aggiungono un ulteriore livello di complessit\u00e0.Il resto di questo documento \u00e8 organizzato come segue. Nella sezione 2 presentiamo una panoramica di Mirage, comprese osservazioni di alto livello sull'utilizzo in un periodo di quattro mesi. Nella Sezione 3 esaminiamo il comportamento strategico degli utenti, concentrandoci sui quattro tipi principali di strategie impiegate dagli utenti nel sistema. Sulla base di questi risultati, la Sezione 4 presenta una serie di sfide chiave per i sistemi di allocazione delle risorse basati sul mercato basati su aste combinatorie ripetute. Come primo passo per affrontare alcune di queste sfide, descriviamo i perfezionamenti dell'attuale schema d'asta di Mirage che mitigano le strategie osservate fino ad oggi e commentiamo anche alcuni passi iniziali verso la costruzione di un'asta combinatoria ripetuta approssimativamente a prova di strategia per Mirage. Infine, nella Sezione 5, concludiamo il lavoro. 5. CONCLUSIONE Nonostante inizialmente avesse utilizzato un'asta combinatoria ripetuta nota per non essere a prova di strategia, Mirage si \u00e8 dimostrato molto promettente come veicolo per l'allocazione del banco di prova SensorNet. Per realizzare pienamente questi vantaggi, tuttavia, \u00e8 necessario affrontare i problemi chiave nella progettazione di meccanismi a prova di strategia e nell\u2019ottimizzazione combinatoria. La natura temporale delle risorse computazionali e le richieste di risorse combinatorie delle applicazioni distribuite aggiungono un ulteriore livello di complessit\u00e0.", "keyphrases": ["sistema di allocazione delle risorse", "asta combinatoria", "sistema basato sul mercato", "sistema di distribuzione", "comportamento strategico", "razione", "schema basato sull'asta", "sistema mirag", "test della rete sensorib", "prezzo nodo-ora", "spese generali utilizzabili", "pianificazione batch", "distribuire applic"]}