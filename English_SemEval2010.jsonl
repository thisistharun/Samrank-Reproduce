{"file_name": "C-38", "text": "A Framework for Architecting Peer-to-Peer Receiver-driven Overlays ABSTRACT This paper presents a simple and scalable framework for architecting peer-to-peer overlays called Peer-to-peer Receiverdriven Overlay -LRB- or PRO -RRB-. PRO is designed for non-interactive streaming applications and its primary design goal is to maximize delivered bandwidth -LRB- and thus delivered quality -RRB- to peers with heterogeneous and asymmetric bandwidth. To achieve this goal, PRO adopts a receiver-driven approach where each receiver -LRB- or participating peer -RRB- -LRB- i -RRB- independently discovers other peers in the overlay through gossiping, and -LRB- ii -RRB- selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth. Participating peers form an unstructured overlay which is inherently robust to high churn rate. Furthermore, each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long-term changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers dynamically converge to an efficient overlay structure. 1. INTRODUCTION ing heterogeneity and asymmetry of bandwidth connectivity among participating peers -LSB- 19 -RSB-. Coping with bandwidth variations, heterogeneity and asymmetry are particularly important in design of peer-to-peer overlay for streaming applications because delivered quality to each peer is directly determined by its bandwidth connectivity to -LRB- other peer -LRB- s -RRB- on -RRB- the overlay. This paper presents a simple framework for architecting Peer-to-peer Receiver-driven Overlay, called PRO. The main design philosophy in PRO is that each peer should be allowed to independently and selfishly determine the best way to connect to the overlay in order to maximize its own delivered quality. Toward this end, each peer can connect to the overlay topology at multiple points -LRB- i.e., receive content through multiple parent peers -RRB-. Therefore, participating peers form an unstructured overlay that can gracefully cope with high churn rate -LSB- 5 -RSB-. Furthermore, having multiple parent peers accommodates bandwidth heterogeneity and asymmetry while improves resiliency against dynamics of peer participation. PRO consists of two key components : -LRB- i -RRB- Gossip-based Peer Discovery : Each peer periodically exchanges message -LRB- i.e., gossips -RRB- with other known peers to progressively learn about a subset of participating peers in the overlay that are likely to be good parents. -LRB- ii -RRB- Receiver-driven Parent Selection : Given the collected information about other participating peers by gossiping mechanism, each peer -LRB- or receiver -RRB- gradually improves its own delivered quality by dynamically selecting a proper subset of parent peers that collectively maximize provided bandwidth to the receiver. Since the available bandwidth from different participating peers to a receiver -LRB- and possible correlation among them -RRB- can be measured only at that receiver, a receiver-driven approach is the natural solution to maximize available bandwidth to heterogeneous peers. Furthermore, the available bandwidth from parent peers serves as an implicit signal for a receiver to detect and react to changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers leads to an efficient overlay that maximizes delivered quality to each peer. PRO incorporates several damping functions to ensure stability of the overlay despite uncoordinated actions by different peers. PRO is part of a larger architecture that we have developed for peer-to-peer streaming. Thus, PRO and PALS are both receiver-driven but complement each other. More specifically, PRO determines a proper subset of parent peers that collectively maximize delivered bandwidth to each receiver whereas PALS coordinates `` in-time '' streaming of different segments of multimedia content from these parents despite unpredictable variations in their available bandwidth. This division of functionality provides a great deal of flexibility because it decouples overlay construction from delivery mechanism. In this paper, we primarily focus on the overlay construction mechanism, or PRO. The rest of this paper is organized as follows : In Section 2, we revisit the problem of overlay construction for peerto-peer streaming and identify its two key components and explore their design space. We present our proposed framework in Section 3. In Sections 4 and 5, the key components of our framework are described in further detail. Finally, Section 6 concludes the paper and presents our future plans. 6. CONCLUSIONS AND FUTURE WORK In this paper, we presented a simple receiver-driven framework for architecting peer-to-pee overlay structures called PRO. PRO allows each peer to selfishly and independently determine the best way to connect to the overlay to maximize its performance. Therefore, PRO should be able to maximize delivered quality to peers with heterogeneous and asymmetric bandwidth connectivity. Both peer discovery and peer selection in this framework are scalable. Furthermore, PRO uses congestion controlled bandwidth as an implicit signal to detect shared bottleneck among existing parents as well as changes in network or overlay conditions to properly reshape the structure. We described the basic framework and its key components, and sketched our strawman solutions. This is a starting point for our work on PRO. We are currently evaluating various aspects of this framework via simulation, and exploring the design space of key components. We are also prototyping this framework to conduct real-world experiments on the Planet-Lab in a near future.", "keyphrases": ["peer-to-peer stream", "congest control", "receiv-driven approach", "receiv-driven overlai", "distribut system", "design", "measur", "effici overlai structur", "pro", "proper subset of parent peer", "gossip-base peer discoveri", "receiv-driven parent select"]}
{"file_name": "H-17", "text": "Pruning Policies for Two-Tiered Inverted Index with Correctness Guarantee ABSTRACT The Web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information. In order to cope with the vast amounts of query loads, search engines prune their index to keep documents that are likely to be returned as top results, and use this pruned index to compute the first batches of results. While this approach can improve performance by reducing the size of the index, if we compute the top results only from the pruned index we may notice a significant degradation in the result quality : if a document should be in the top results but was not included in the pruned index, it will be placed behind the results computed from the pruned index. Given the fierce competition in the online search market, this phenomenon is clearly undesirable. In this paper, we study how we can avoid any degradation of result quality due to the pruning-based performance optimization, while still realizing most of its benefit. Our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top-matching pages are always placed at the top search results, even though we are computing the first batch from the pruned index most of the time. We also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 130 million Web pages. 1. INTRODUCTION According to a recent study -LSB- 13 -RSB-, it is estimated that the \u2217 Work done while author was at UCLA Computer Science Department. \u2020 This work is partially supported by NSF grants, IIS-0534784, IIS0347993, and CNS-0626702. Due to this immense amount of available information, the users are becoming more and more dependent on the Web search engines for locating relevant information on the Web. Typically, the Web search engines, similar to other information retrieval applications, utilize a data structure called inverted index. An inverted index provides for the efficient retrieval of the documents -LRB- or Web pages -RRB- that contain a particular keyword. In most cases, a query that the user issues may have thousands or even millions of matching documents. In order to avoid overwhelming the users with a huge amount of results, the search engines present the results in batches of 10 to 20 relevant documents. The user then looks through the first batch of results and, if she does n't find the answer she is looking for, she may potentially request to view the next batch or decide to issue a new query. A recent study -LSB- 16 -RSB- indicated that approximately 80 % of the users examine at most the first 3 batches of the results. That is, 80 % of the users typically view at most 30 to 60 results for every query that they issue to a search engine. At the same time, given the size of the Web, the inverted index that the search engines maintain can grow very large. One natural solution to this problem is to create a small index on a subset of the documents that are likely to be returned as the top results -LRB- by using, for example, the pruning techniques in -LSB- 7, 20 -RSB- -RRB- and compute the first batch of answers using the pruned index. While this approach has been shown to give significant improvement in performance, it also leads to noticeable degradation in the quality of the search results, because the top answers are computed only from the pruned index -LSB- 7, 20 -RSB-. That is, even if a page should be placed as the top-matching page according to a search engine 's ranking metric, the page may be placed behind the ones contained in the pruned index if the page did not become part of the pruned index for various reasons -LSB- 7, 20 -RSB-. Given the fierce competition among search engines today this degradation is clearly undesirable and needs to be addressed if possible. In this paper, we study how we can avoid any degradation of search quality due to the above performance optimization while still realizing most of its benefit. That is, we present a number of simple -LRB- yet important -RRB- changes in the pruning techniques for creating the pruned index. Our main contribution is a new answer computation algorithm that guarantees that the top-matching pages -LRB- according to the search-engine 's ranking metric -RRB- are always placed at the top of search results, even though we are computing the first batch of answers from the pruned index most of the time. These enhanced pruning techniques and answer-computation algorithms are explored in the context of the cluster architecture commonly employed by today 's search engines. Finally, we study and present how search engines can minimize the operational cost of answering queries while providing high quality search results. Figure 1 : -LRB- a -RRB- Search engine replicates its full index IF to in crease query-answering capacity. -LRB- b -RRB- In the 1st tier, small pindexes IP handle most of the queries. When IP can not answer a query, it is redirected to the 2nd tier, where the full index IF is used to compute the answer. 6. RELATED WORK -LSB- 3, 30 -RSB- provide a good overview of inverted indexing in Web search engines and IR systems. Experimental studies and analyses of various partitioning schemes for an inverted index are presented in -LSB- 6, 23, 33 -RSB-. The pruning algorithms that we have presented in this paper are independent of the partitioning scheme used. However, -LSB- 1, 5, 7, 27 -RSB- do not consider any query-independent quality -LRB- such as PageRank -RRB- in the ranking function. -LSB- 32 -RSB- presents a generic framework for computing approximate top-k answers with some probabilistic bounds on the quality of results. Our work essentially extends -LSB- 1, 2, 4, 7, 20, 27, 31 -RSB- by proposing mechanisms for providing the correctness guarantee to the computed top-k results. Search engines use various methods of caching as a means of reducing the cost associated with queries -LSB- 18, 19, 21, 31 -RSB-. This thread of work is also orthogonal to ours because a caching scheme may operate on top of our p-index in order to minimize the answer computation cost. The exact ranking functions employed by current search engines are closely guarded secrets. In general, however, the rankings are based on query-dependent relevance and queryindependent document `` quality. '' Similarly, there are a number of works that measure the `` quality '' of the documents, typically as captured through link-based analysis -LSB- 17, 28, 26 -RSB-. Since our work does not assume a particular form of ranking function, it is complementary to this body of work. There has been a great body of work on top-k result calculation. 7. CONCLUDING REMARKS Web search engines typically prune their large-scale inverted indexes in order to scale to enormous query loads. While this approach may improve performance, by computing the top results from a pruned index we may notice a significant degradation in the result quality. In this paper, we provided a framework for new pruning techniques and answer computation algorithms that guarantee that the top matching pages are always placed at the top of search results in the correct order. We studied two pruning techniques, namely keyword-based and document-based pruning as well as their combination. Our experimental results demonstrated that our algorithms can effectively be used to prune an inverted index without degradation in the quality of results. In particular, a keyword-pruned index can guarantee 73 % of the queries with a size of 30 % of the full index, while a document-pruned index can guarantee 68 % of the queries with the same size. When we combine the two pruning algorithms we can guarantee 60 % of the queries with an index size of 16 %. It is our hope that our work will help search engines develop better, faster and more efficient indexes and thus provide for a better user search experience on the Web.", "keyphrases": ["web search engin", "larg-scale invert index", "queri load", "prune index", "onlin search market", "degrad of result qualiti", "prune-base perform optim", "prune techniqu", "result comput algorithm", "top-match page", "top search result", "optim size"]}
{"file_name": "J-25", "text": "Betting Boolean-Style : A Framework for Trading in Securities Based on Logical Formulas ABSTRACT We develop a framework for trading in compound securities : financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic. Buying or selling securities -- which can be thought of as betting on or against a particular future outcome -- allows agents both to hedge risk and to profit -LRB- in expectation -RRB- on subjective predictions. A compound securities market allows agents to place bets on arbitrary boolean combinations of events, enabling them to more closely achieve their optimal risk exposure, and enabling the market as a whole to more closely achieve the social optimum. The tradeoff for allowing such expressivity is in the complexity of the agents ' and auctioneer 's optimization problems. We develop and motivate the concept of a compound securities market, presenting the framework through a series of formal definitions and examples. We then analyze in detail the auctioneer 's matching problem. We show that, with n events, the matching problem is co-NP-complete in the divisible case and \u03a3p2-complete in the indivisible case. We show that the latter hardness result holds even under severe language restrictions on bids. With log n events, the problem is polynomial in the divisible case and NP-complete in the indivisible case. We briefly discuss matching algorithms and tractable special cases. 1. INTRODUCTION Securities markets effectively allow traders to place bets on the outcomes of uncertain future propositions. The economic value of securities markets is two-fold. First, they allow traders to hedge risk, or to insure against undesirable outcomes. For example, the owner of a stock might buy a put option -LRB- the right to sell the stock at a particular price -RRB- in order to insure against a stock downturn. Second, securities markets allow traders to speculate, or to obtain a subjective expected profit when market prices do not reflect their assessment of the likelihood of future outcomes. For example, a trader might buy a call option if he believes that the likelihood is high that the price of the underlying stock will go up, regardless of risk exposure to changes in the stock price. Because traders stand to earn a profit if they can make effective probability assessments, often prices in financial markets yield very accurate aggregate forecasts of future events -LSB- 10, 29, 27, 28 -RSB-. Real securities markets have complex payoff structures with various triggers. However, these can all be modeled as collections of more basic or atomic Arrow-Debreu securities -LSB- 1, 8, 20 -RSB-. One unit of one Arrow-Debreu security pays off one dollar if and only if -LRB- iff -RRB- a corresponding binary event occurs ; it pays nothing if the event does not occur. So, for example, one unit of a security denoted -LRB- Acme100 -RRB- might pay $ 1 iff Acme 's stock is above $ 100 on January 4, 2004. An Acme stock option as it would be defined on a finan cial exchange can be though of as a portfolio of such atomic securities.1 In this paper, we develop and analyze a framework for trading in compound securities markets with payoffs contingent on arbitrary logical combinations of events, including conditionals. For example, given binary events A, B, and C, one trader might bid to buy three units of a security denoted -LRB- A n B \u00af V C -RRB- that pays off $ 1 iff the compound event A n B \u00af V C occurs for thirty cents each. Given a set of such bids, the auctioneer faces a complex matching problem to decide which bids are accepted for how many units at what price. Typically, the auctioneer seeks to take on no risk of its own, only matching up agreeable trades among the bidders, but we also consider alternative formulations where the auctioneer acts as a market maker willing to accept some risk. We examine the computational complexity of the auctioneer 's matching problem. Let the length of the description of all the available securities be O -LRB- n -RRB-. With n events, the matching problem is co-NP-complete in the divisible case and Ep2-complete in the indivisible case. This Ep2-complete hardness holds even when the bidding language is significantly restricted. With log n events, the problem is polynomial in the divisible case and NP-complete in the indivisible case. Section 2 presents some necessary background information, motivation, and related work. Section 3 formally describes our framework for compound securities, and defines the auctioneer 's matching problem. Section 4 briefly discusses natural algorithms for solving the matching problem. Section 5 proves our central computational complexity results. Section 6 discusses the possibility of tractable special cases. Section 7 concludes with a summary and some ideas of future directions. 2. PRELIMINARIES 2.1 Background and notation In this simple world there are four possible future states -- all possible combinations of the binary events ' outcomes : struck n acme100, struck n acme100, struck n acme100, struck n acme100. Hedging risk can be thought of as an action of moving money between various possible future states. For example, insur1Technically, an option is a portfolio of infinitely many atomic securities, though it can be approximately modeled with a finite number. ing one 's house transfers money from future states where struck is not true to states where it is. Selling a security denoted -LRB- acme100 -RRB- -- that pays off $ 1 iff the event acme100 occurs -- transfers money from future states where Acme 's price is above $ 100 on January 4 to states where it 's not. Speculating is also an act of transferring money between future states, though usually associated with maximizing expected return rather than reducing risk. For example, betting on a football team moves money from the `` team loses '' state to the `` team wins '' state. All possible future outcomes form a state space \u2126, consisting of mutually exclusive and exhaustive states \u03c9 E \u2126. Often a more natural way to think of possible future outcomes is as an event space A of linearly independent events A E A that may overlap arbitrarily. So in our toy example struck n acme100 is one of the four disjoint states, while struck is one of the two events. Note that a set of n linearly independent events defines a state space \u2126 of size 2 '' consisting of all possible combinations of event outcomes. Conversely, any state space \u2126 can be factored into -LSB- log l\u2126ll events. Suppose that A exhaustively covers all meaningful future outcomes -LRB- i.e., covers all eventualities that agents may wish to hedge against and/or speculate upon -RRB-. Then the existence of 2 '' linearly independent securities -- called a complete market -- allows agents to distribute their wealth arbitrarily across future states.2 An agent may create any hedge or speculation it desires. Under classical conditions, agents trading in a complete market form an equilibrium where risk is allocated Pareto optimally. If the market is incomplete, meaning it consists of fewer than 2 '' linearly independent securities, then in general agents can not construct arbitrary hedges and equilibrium allocations may be nonoptimal -LSB- 1, 8, 19, 20 -RSB-. In real-world settings, the number of meaningful events n is large and thus the number of securities required for completeness is intractable. No truly complete market exists or will ever exist. One motivation behind compound securities markets is to provide a mechanism that supports the most transfer of risk using the least number of transactions possible. Compound securities allow a high degree of expressivity in constructing bids. The tradeoff for increased expressivity is increased computational complexity, from both the bidder 's and auctioneer 's point of view. 2.2 Related work The quest to reduce the number of financial instruments required to support an optimal allocation of risk dates to Arrow 's original work -LSB- 1 -RSB-. The requirement stated above of `` only '' 2 '' linearly-independent securities is itself a reduction from the most straightforward formulation. In an economy with k standard goods, the most straightforward complete market contains k \u2022 2 '' securities, each paying off in one good under one state realization. Arrow -LSB- 1 -RSB- showed that a market where securities and goods are essentially separated, with 2 '' securities paying off in a single numeraire good plus k spot markets in the standard goods, is also complete. For our purposes, we need consider only the securities market. 2By linearly independent securities, we mean that the vectors of payoffs in all future states of these securities are linearly independent. Varian -LSB- 34 -RSB- shows that a complete market can be constructed using fewer than 2n securities, replacing the missing securities with options. Still, the number of linearly independent financial instruments -- securities plus options -- must be 2n to guarantee completeness. The authors show that in some cases the market can be structured and `` compacted '' in analogy to Bayesian network representations of joint probability distributions -LSB- 23 -RSB-. They show that, if all agents ' risk-neutral independencies agree with the independencies encoded in the market structure, then the market is operationally complete. For collections of agents all with constant absolute risk aversion, agreement on Markov independencies is sufficient. Bossaerts, Fine, and Ledyard -LSB- 2 -RSB- develop a mechanism they call combined-value trading -LRB- CVT -RRB- that allows traders to order an arbitrary portfolio of securities in one bid, rather than breaking up the order into a sequence of bids on individual securities. If the portfolio order is accepted, all of the implied trades on individual securities are executed simultaneously, thus eliminating so-called execution risk that prices will change in the middle of a planned sequence of orders. The authors conduct laboratory experiments showing that, even in thin markets where ordinary sequential trading breaks down, CVT supports efficient pricing and allocation. Note that CVT differs significantly from compound securities trading. CVT allows instantaneous trading of any linear combination of securities, while compound securities allow more expressive securities that can encode nonlinear boolean combinations of events. For example, CVT may allow an agent to order securities -LRB- A -RRB- and -LRB- B -RRB- in a bundle that pays off as a linear combination of A and B,3 but CVT wo n't allow the construction of a compound security -LRB- A n B -RRB- that pays off $ 1 iff both A and B occur, or a compound security -LRB- AIB -RRB-. Combinatorial auctions allow bidders to place distinct values on all possible bundles of goods rather than just on individual goods. Compound securities differ from combinatorial auctions in concept and complexity. Compound securities allow bidders to construct an arbitrary bet on any of the 22n possible compound events expressible as logical functions of the n base events, conditional on any other of the 22n compound events. Agents optimize based on their own subjective probabilities and risk attitude -LRB- and in general, their beliefs about other agents ' beliefs and utilities, ad infinitum -RRB-. The central auctioneer problem is identifying arbitrage opportunities : that is, to match bets together without taking on any risk. Combinatorial auctions, on the other hand, allow bids on any of the 2n bundles of n goods. uncertainty -- and thus risk -- is not considered. The central auctioneer problem is to maximize social welfare. Also note that the problems lie in different complexity classes. While clearing a combinatorial auction is polynomial in the divisible case and NP-complete in the indivisible case, matching in a compound securities market is NP-complete in the divisible case and Ep2-complete in the indivisible case. In fact, even the problem of deciding whether two bids on compound securities match, even in the divisible case, is NP-complete -LRB- see Section 5.2 -RRB-. There is a sense in which it is possible to translate our matching problem for compound securities into an analogous problem for clearing two-sided combinatorial exchanges -LSB- 31 -RSB- of exponential size. Specifically, if we regard payoff in a particular state as a good, then compound securities can be viewed as bundles of -LRB- fractional quantities of -RRB- such goods. The material balance constraint facing the combinatorial auctioneer corresponds to a restriction that the compoundsecurity auctioneer be disallowed from assuming any risk. Note that this translation is not at all useful for addressing the compound-security matching problem, as the resulting combinatorial exchange has an exponential number of goods. Hanson -LSB- 15 -RSB- develops a market mechanism called a market scoring rule that is especially well suited for allowing bets on a combinatorial number of outcomes. The mechanism maintains a joint probability distribution over all 2n states, either explicitly or implicitly using a Bayesian network or other compact representation. In the limit of a single trader, the mechanism behaves like a scoring rule, suitable for polling a single agent for its probability distribution. In the limit of many traders, it produces a combined estimate. Since the market essentially always has a complete set of posted prices for all possible outcomes, the mechanism avoids the problem of thin markets, or illiquidity, that necessarily plagues any market containing an exponential number of alternative investments. Bids for compound securities can be thought of as expressions of probabilistic inequalities : for example, a bid to buy -LRB- A n B -RRB- at price 0.3 is a statement that the probability of A n B is greater than 0.3. If a set of single-unit bids correspond to a set of inconsistent probabilistic inequalities, then there is a match. We address these issues below.", "keyphrases": ["combinatori bet", "effect probabl assess", "arbitrari logic combin", "compound secur", "bayesian network", "combin-valu trade", "approxim algorithm", "payoff vector", "tractabl case", "base secur"]}
{"file_name": "J-15", "text": "Generalized Value Decomposition and Structured Multiattribute Auctions ABSTRACT Multiattribute auction mechanisms generally either remain agnostic about traders ' preferences, or presume highly restrictive forms, such as full additivity. Real preferences often exhibit dependencies among attributes, yet may possess some structure that can be usefully exploited to streamline communication and simplify operation of a multiattribute auction. We develop such a structure using the theory of measurable value functions, a cardinal utility representation based on an underlying order over preference differences. A set of local conditional independence relations over such differences supports a generalized additive preference representation, which decomposes utility across overlapping clusters of related attributes. We introduce an iterative auction mechanism that maintains prices on local clusters of attributes rather than the full space of joint configurations. When traders ' preferences are consistent with the auction 's generalized additive structure, the mechanism produces approximately optimal allocations, at approximate VCG prices. 1. INTRODUCTION Multiattribute trading mechanisms extend traditional, price-only mechanisms by facilitating the negotiation over a set of predefined attributes representing various non-price aspects of the deal. Rather than negotiating over a fully defined good or service, a multiattribute mechanism delays commitment to specific configurations until the most promising candidates are identified. For example, a procurement department of a company may use a multiattribute auction to select a supplier of hard drives. In order to account for traders ' preferences, the auction mechanism must extract evaluative information over a complex domain of multidimensional configurations. Constructing and communicating a complete preference specification can be a severe burden for even a moderate number of attributes, therefore practical multiattribute auctions must either accommodate partial specifications, or support compact expression of preferences assuming some simplified form. By far the most popular multiattribute form to adopt is the simplest : an additive representation where overall value is a linear combination of values associated with each attribute. For example, several recent proposals for iterative multiattribute auctions -LSB- 2, 3, 8, 19 -RSB- require additive preference representations. Such additivity reduces the complexity of preference specification exponentially -LRB- compared to the general discrete case -RRB-, but precludes expression of any interdependencies among the attributes. In practice, however, interdependencies among natural attributes are quite common. In such cases an additive value function may not be able to provide even a reasonable approximation of real preferences. On the other hand, fully general models are intractable, and it is reasonable to expect multiattribute preferences to exhibit some structure. Our goal, therefore, is to identify the subtler yet more widely applicable structured representations, and exploit these properties of preferences in trading mechanisms. We propose an iterative auction mechanism based on just such a flexible preference structure. Our approach is inspired by the design of an iterative multiattribute procurement auction for additive preferences, due to Parkes and Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propose two types of iterative auctions : the first -LRB- NLD -RRB- makes no assumptions about traders ' preferences, and lets sellers bid on the full multidimensional attribute space. Because NLD maintains an exponential price structure, it is suitable only for small domains. The other auction -LRB- AD -RRB- assumes additive buyer valuation and seller cost functions. It collects sell bids per attribute level and for a single discount term. The price of a configuration is defined as the sum of the prices of the chosen attribute levels minus the discount. The auction we propose also supports compact price spaces, albeit for levels of clusters of attributes rather than singletons. Given its roots in multiattribute utility theory -LSB- 13 -RSB-, the GAI condition is defined with respect to the expected utility function. To apply it for modeling values for certain outcomes, therefore, requires a reinterpretation for preference under certainty. To this end, we exploit the fact that auction outcomes are associated with continuous prices, which provide a natural scale for assessing magnitude of preference. We first lay out a representation framework for preferences that captures, in addition to simple orderings among attribute configuration values, the difference in the willingness to pay -LRB- wtp -RRB- for each. Next, we build a direct, formally justified link from preference statements over priced outcomes to a generalized additive decomposition of the wtp function. After laying out this infrastructure, we employ this representation tool for the development of a multiattribute iterative auction mechanism that allows traders to express their complex preferences in GAI format. We then study the auction 's allocational, computational, and practical properties. In Section 2 we present essential background on our representation framework, the measurable value function -LRB- MVF -RRB-. Section 3 develops new multiattribute structures for MVF, supporting generalized additive decompositions. Next, we show the applicability of the theoretical framework to preferences in trading. The rest of the paper is devoted to the proposed auction mechanism.", "keyphrases": ["auction", "multiattribut auction", "prefer handl", "theori of measur valu function", "iter auction mechan", "mvf", "gau", "gai base auction"]}
{"file_name": "I-7", "text": "Commitment and Extortion * ABSTRACT Making commitments, e.g., through promises and threats, enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents. Which commitments a player can make with credibility depends on the circumstances. In some, a player can only commit to the performance of an action, in others, he can commit himself conditionally on the actions of the other players. Some situations even allow for commitments on commitments or for commitments to randomized actions. We explore the formal properties of these types of -LRB- conditional -RRB- commitment and their interrelationships. So as to preclude inconsistencies among conditional commitments, we assume an order in which the players make their commitments. Central to our analyses is the notion of an extortion, which we define, for a given order of the players, as a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. On this basis, we investigate for different commitment types whether it is advantageous to commit earlier rather than later, and how the outcomes obtained through extortions relate to backward induction and Pareto efficiency. 1. INTRODUCTION On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent 's expected utility in situations of interactive decision making. From this perspective, the formal model of a game in strategic form only outlines the strategic features of an interactive situation. Apart from merely choosing and performing an action from a set of actions, there may also be other courses open to an agent. E.g., the strategic lie of the land may be such that a promise, a threat, or a combination of both would be more conductive to his ends. Likewise, a threat only succeeds in deterring an agent if the latter can be made to believe that the threatener is bound to execute the threat, should it be ignored. In this sense, promises and threats essentially involve a commitment on the part of the one who makes them, thus purposely restricting his freedom of choice. Promises and threats epitomize one of the fundamental and at first sight perhaps most surprising phenomena in game theory : it may occur that a player can improve his strategic position by limiting his own freedom of action. By commitments we will understand such limitations of one 's action space. Action itself could be seen as the ultimate commitment. Performing a particular action means doing so to the exclusion of all other actions. Commitments come in different forms and it may depend on the circumstances which ones can and which ones can not credibly be made. Besides simply committing to the performance of an action, an agent might make his commitment conditional on the actions of other agents, as, e.g., the kidnapper does, when he promises to set free a hostage on receiving a ransom, while threatening to cut off another toe, otherwise. Some situations even allow for commitments on commitments or for commitments to randomized actions. By focusing on the selection of actions rather than on commitments, it might seem that the conception of game theory as mere interactive decision theory is too narrow. In this respect, Schelling 's view might seem to evince a more comprehensive understanding of what game theory tries to accomplish. One might object, that commitments could be seen as the actions of a larger game. -LSB-... -RSB- What we want is a theory that systematizes the study of the various universal ingredients that make up the move-structure of games ; too abstract a model will miss them. -LSB- 9, pp. 156-7 -RSB- Our concern is with these commitment tactics, be it that our analysis is confined to situations in which the players can commit in a given order and where we assume the commitments the players can make are given. Despite Schelling 's warning for too abstract a framework, our approach will be based on the formal notion of an extortion, which we will propose in Section 4 as a uniform tactic for a comprehensive class of situations in which commitments can be made sequentially. On this basis we tackle such issues as the usefulness of certain types of commitment in different situations -LRB- strategic games -RRB- or whether it is better to commit early rather than late. We also provide a framework for the assessment of more general game theoretic matters like the relationship of extortions to backward induction or Pareto efficiency. For example, commitments have been argued to be of importance for interacting software agents as well as for mechanism design. In the former setting, the inability to re-program a software agent on the fly can be seen as a commitment to its specification and thus exploited to strengthen its strategic position in a multiagent setting. A mechanism, on the other hand, could be seen as a set of commitments that steers the players ' behavior in a certain desired way -LRB- see, e.g., -LSB- 2 -RSB- -RRB-. These games analyze situations in which a leader commits to a pure or mixed strategy, and a number of followers, who then act simultaneously. After briefly discussing related work in Section 2, we present the formal game theoretic framework, in which we define the notions of a commitment type as well as conditional and unconditional commitments -LRB- Section 3 -RRB-. In Section 4 we propose the generic concept of an extortion, which for each commitment type captures the idea of an optimal commitment profile. Section 5 briefly reviews some other commitment types, such as inductive, mixed and mixed conditional commitments. 2. RELATED WORK Commitment is a central concept in game theory. The possibility to make commitments distinguishes cooperative from noncooperative game theory -LSB- 4, 6 -RSB-. Leadership games, as mentioned in the introduction, analyze commitments to pure or mixed strategies in what is essentially a two-player setting -LSB- 15, 16 -RSB-. Informally, Schelling -LSB- 9 -RSB- has emphasized the importance of promises, threats and the like for a proper understanding of social interaction. On a more formal level, threats have also figured in bargaining theory. Nash 's threat game -LSB- 5 -RSB- and Harsanyi 's rational threats -LSB- 3 -RSB- are two important early examples. Also, commitments have played a significant role in the theory of equilibrium selection -LRB- see, e.g., -LSB- 13 -RSB-. Over the last few years, game theory has become almost indispensable as a research tool for computer science and -LRB- multi -RRB- agent research. Commitments have by no means gone unnoticed -LRB- see, Figure 1 : Committing to a dominated strategy can be advantageous. e.g., -LSB- 1, 11 -RSB- -RRB-. Recently, also the strategic aspects of commitments have attracted the attention of computer scientists. Thus, Conitzer and Sandholm -LSB- 2 -RSB- have studied the computational complexity of computing the optimal strategy to commit to in normal form and Bayesian games. Sandholm and Lesser -LSB- 8 -RSB- employ levelled commitments for the design of multiagent systems in which contractual agreements are not fully binding. Another connection between commitments and computer science has been pointed out by Samet -LSB- 7 -RSB- and Tennenholtz -LSB- 12 -RSB-. Their point of departure is the observation that programs can be used to formulate commitments that are conditional on the programs of other systems. Our approach is similar to the Stackleberg setting in that we assume an order in which the players commit. We, however, consider a number of different commitment types, among which conditional commitments, and propose a generic solution concept. 6. SUMMARY AND CONCLUSION In some situations agents can strengthen their strategic position by committing themselves to a particular course of action. There are various types of commitment, e.g., pure, mixed and conditional. Which type of commitment an agent is in a position in to make essentially depends on the situation under consideration. If the agents commit in a particular order, there is a tactic common to making commitments of any type, which we have formalized by means the concept of an extortion. This generic concept of extortion can be analyzed in abstracto. Moreover, on its basis the various commitment types can be compared formally and systematically. We have seen that the type of commitment an agent can make has a profound impact on what an agent can achieve in a gamelike situation. In some situations a player is much helped if he is in a position to commit conditionally, whereas in others mixed commitments would be more profitable. This raises the question as to the characteristic formal features of the situations in which it is advantageous for a player to be able to make commitments of a particular type. Another issue which we leave for future research is the computational complexity of finding an extortion for the different commitment types.", "keyphrases": ["commit", "credibl", "game theori", "decis make", "strateg posit", "freedom of action", "multiag system", "distribut comput", "electron market", "extort", "stackleberg set", "optim condit commit", "sequenti commit type", "induct hypothesi", "pareto effici", "pareto effici condit extort"]}
{"file_name": "J-10", "text": "Understanding User Behavior in Online Feedback Reporting ABSTRACT Online reviews have become increasingly popular as a way to judge the quality of various products and services. Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult. In this paper, we investigate underlying factors that influence user behavior when reporting feedback. We look at two sources of information besides numerical ratings : linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports. We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature. Second, we show that a user 's rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews. Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias. Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website. 1. MOTIVATIONS riously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation. Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product. In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews. Under these circumstances, using the arithmetic mean to predict quality -LRB- as most forums actually do -RRB- gives the typical user an estimator with high variance that is often false. Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users. Hu et al. -LSB- 12 -RSB- propose the `` Brag-and-Moan Model '' where users rate only if their utility of the product -LRB- drawn from a normal distribution -RRB- falls outside a median interval. The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product. In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback. Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users. The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings. We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature. Intuitively, lengthy comments reveal the importance of the feature to the user. Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature. Second we investigate the relationship between a review Figure 1 : The TripAdvisor page displaying reviews for a popular Boston hotel. Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it. A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts. One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users. By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service. The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service -LSB- 17, 18, 16, 21 -RSB- which will later reflect in the user 's rating. We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected. Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews. Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews -LRB- i.e., those with lengthy comments of that feature -RRB- is considered. The second leads to an algorithm that outputs a more precise estimate of the real quality.", "keyphrases": ["onlin review", "reput mechan", "featur-by-featur estim of qualiti", "absenc of clear incent", "util of the product", "brag-and-moan model", "rate", "great probabl bi-modal", "u-shape distribut", "semant orient of product evalu", "correl", "larg span of time"]}
{"file_name": "C-1", "text": "Scalable Grid Service Discovery Based on UDDI * ABSTRACT Efficient discovery of grid services is essential for the success of grid computing. The standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids Even though UDDI has been the de facto industry standard for web-services discovery, imposed requirements of tight-replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage. With the advent of grid computing the scalability issue of UDDI will become a roadblock that will prevent its deployment in grids. In this paper we present our distributed web-service discovery architecture, called DUDE -LRB- Distributed UDDI Deployment Engine -RRB-. DUDE leverages DHT -LRB- Distributed Hash Tables -RRB- as a rendezvous mechanism between multiple UDDI registries. DUDE enables consumers to query multiple registries, still at the same time allowing organizations to have autonomous control over their registries. . Based on preliminary prototype on PlanetLab, we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues. Furthermore, The DUDE architecture for scalable distribution can be applied beyond UDDI to any Grid Service Discovery mechanism. 1. INTRODUCTION Efficient discovery of grid services is essential for the success of grid computing. discovery mechanisms to be deployed in grids. Grid discovery services provide the ability to monitor and discover resources and services on grids. They provide the ability to query and subscribe to resource/service information. The state of the data needs to be maintained in a soft state so that the most recent information is always available. The information gathered needs to be provided to variety of systems for the purpose of either utilizing the grid or proving summary information. However, the fundamental problem is the need to be scalable to handle huge amounts of data from multiple sources. The web services community has addressed the need for service discovery, before grids were anticipated, via an industry standard called UDDI. However, even though UDDI has been the de facto industry standard for web-services discovery, imposed requirements of tight-replication among registries and lack of autonomous control, among other things has severely hindered its widespread deployment and usage -LSB- 7 -RSB-. With the advent of grid computing the scalability issue with UDDI will become a roadblock that will prevent its deployment in grids. This paper tackles the scalability issue and a way to find services across multiple registries in UDDI by developing a distributed web services discovery architecture. Distributing UDDI functionality can be achieved in multiple ways and perhaps using different distributed computing infrastructure/platforms -LRB- e.g., CORBA, DCE, etc. -RRB-. In this paper we explore how Distributed Hash Table -LRB- DHT -RRB- technology can be leveraged to develop a scalable distributed web services discovery architecture. A DHT is a peer-to-peer -LRB- P2P -RRB- distributed system that forms a structured overlay allowing more efficient routing than the underlying network. The first motivating factor is the inherent simplicity of the put/get abstraction that DHTs provide, which makes it easy to rapidly build applications on top of DHTs. Other distributed computing platforms/middleware while providing more functionality have much higher overhead and complexity. The second motivating factor stems from the fact that DHTs are relatively new tool for building distributed applications and we would like to test its potential by applying it to the problem of distributing UDDI. In the next section, we provide a brief overview of grid information services, UDDI and its limitations, which is followed by an overview of DHTs in Section 3. Section 4 describes our proposed architecture with details on use cases. In Section 5, we Article 2 describe our current implementation, followed by our findings in Section 6. Section 7 discusses the related work in this area and Section 8 contains our concluding remarks. 2. BACKGROUND 2.1 Grid Service Discovery Grid computing is based on standards which use web services technology. In the architecture presented in -LSB- 6 -RSB-, the service discovery function is assigned to a specialized Grid service called Registry. Its basic function makes it similar to UDDI registry. To attain scalability, Index services from different Globus containers can register with each other in a hierarchical fashion to aggregate data. Specifically, this approach is not a good match for systems that try to exploit the convergence of grid and peer-to-peer computing -LSB- 5 -RSB-. 2.2 UDDI Beyond grid computing, the problem of service discovery needs to be addressed more generally in the web services community. Again, scalability is a major concern since millions of buyers looking for specific services need to find all the potential sellers of the service who can meet their needs. Although there are different ways of doing this, the web services standards committees address this requirement through a specification called UDDI -LRB- Universal Description, Discovery, and Integration -RRB-. A UDDI registry enables a business to enter three types of information in a UDDI registry -- white pages, yellow pages and green pages. UDDI 's intent is to function as a registry for services just as the yellow pages is a registry for businesses. Just like in Yellow pages, companies register themselves and their services under different categories. In UDDI, White Pages are a listing of the business entities. Green pages represent the technical information that is necessary to invoke a given service. Thus, by browsing a UDDI registry, a developer should be able to locate a service and a company and find out how to invoke the service. When UDDI was initially offered, it provided a lot of potential. However, today we find that UDDI has not been widely deployed in the Internet. In fact, the only known uses of UDDI are what are known as private UDDI registries within an enterprise 's boundaries. The readers can refer to -LSB- 7 -RSB- for a recent article that discusses the shortcomings of UDDI and the properties of an ideal service registry. Improvement of the UDDI standard is continuing in full force and UDDI version 3 -LRB- V3 -RRB- was recently approved as an OASIS Standard. However, UDDI today has issues that have not been addressed, such as scalability and autonomy of individual registries. UDDI V3 provides larger support for multi-registry environments based on portability of keys By allowing keys to be re-registered in multiple registries, the ability to link registries in various topologies is effectively enabled. However, no normative description of these topologies is provided in the UDDI specification at this point. The improvements within UDDI V3 that allow support for multi-registry environments are significant and open the possibility for additional research around how multiregistry environments may be deployed. A recommended deployment scenario proposed by the UDDI V3.0.2 Specification is to use the UDDI Business Registries as root registries, and it is possible to enable this using our solution. 2.3 Distributed Hash Tables A Distributed Hash Table -LRB- DHT -RRB- is a peer-to-peer -LRB- P2P -RRB- distributed system that forms a structured overlay allowing more efficient routing than the underlying network. It maintains a collection of key-value pairs on the nodes participating in this graph structure. For our deployment, a key is the hash of a keyword from a service name or description. There will be multiple values for this key, one for each service containing the keyword. Just like any other hash table data structure, it provides a simple interface consisting of put -LRB- -RRB- and get -LRB- -RRB- operations. This has to be done with robustness because of the transient nature of nodes in P2P systems. The DHT keys are obtained from a large identifier space. A hash function, such as MD5 or SHA-1, is applied to an object name to obtain its DHT key. Nodes in a DHT are also mapped into the same identifier space by applying the hash function to their identifier, such as IP address and port number, or public key. The identifier space is assigned to the nodes in a distributed and deterministic fashion, so that routing and lookup can be performed efficiently. The nodes of a DHT maintain links to some of the other nodes in the DHT. The pattern of these links is known as the DHT 's geometry. For example, in the Bamboo DHT -LSB- 11 -RSB-, and in the Pastry DHT -LSB- 8 -RSB- on which Bamboo is based, nodes maintain links to neighboring nodes and to other distant nodes found in a routing table. The routing table allows efficient overlay routing. To attain consistent routing or lookup, a DHT key must be routed to the node with the numerically closest identifier. For details of how the routing tables are constructed and maintained, the reader is referred to -LSB- 8, 11 -RSB-. 5. RELATED WORK A framework for QoS-based service discovery in grids has been proposed in -LSB- 18 -RSB-. UDDIe, an extended UDDI registry for publishing and discovering services based on QoS parameters, is proposed in -LSB- 19 -RSB-. Our work is complementary since we focus on how to federate the UDDI registries and address the scalability issue with UDDI. The DUDE proxy can publish the service properties supported by UDDIe in the DHT and support range queries using techniques proposed for such queries on DHTs. Then we can deliver the scalability benefits of our current solution to both UDDI and UDDIe registries. Discovering services meeting QoS and price requirements has been studied in the context of a grid economy, so that grid schedulers can use various market models such as commodity markets and auctions. The Grid Market Directory -LSB- 20 -RSB- was proposed for this purpose. Resource and request descriptions are expressed in RDF Schema, a semantic markup language. Matchmaking rules are expressed in TRIPLE, a language based on Horn Logic. Although our current implementation focuses on UDDI version 2, in future we will consider semantic extensions to UDDI, WS-Discovery -LSB- 16 -RSB- and other Grid computing standards such as Monitoring and Discovery Service -LRB- MDS -RRB- -LSB- 10 -RSB-. So the simplest extension of our work could involve using the DHT to do an initial syntax-based search to identify the local registries that need to be contacted. The convergence of grid and P2P computing has been explored in -LSB- 5 -RSB-. A federated UDDI service -LSB- 4 -RSB- has been built on top of the PlanetP -LSB- 3 -RSB- publish-subscribe system for unstructured P2P communities. The focus of this work has been on the manageability of the federated service. The UDDI service is treated as an application Article 2 service to be managed in their framework. So they do not address the issue of scalability in UDDI, and instead use simple replication. In -LSB- 21 -RSB-, the authors describe a UDDI extension -LRB- UX -RRB- system that launches a federated query only if locally found results are not adequate. While the UX Server is positioned as an intermediary similarly to the UDDI Proxy described in our DUDE framework, it focuses more on the QoS framework and does not attempt to implement a seamless federation mechanism such as our DHT based approach. In -LSB- 22 -RSB- D2HT describes a discovery framework built on top of DHT. However, we have chosen to use UDDI on top of DHT. 6. CONCLUSIONS AND FUTURE WORK In this paper, we have described a distributed architecture to support large scale discovery of web-services. Our architecture will enable organizations to maintain autonomous control over their UDDI registries and at the same time allowing clients to query multiple registries simultaneously. Based on initial prototype testing, we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues. The paper has solved the scalability issues with UDDI but does not preclude the application of this approach to other service discovery mechanisms. An example of another service discovery mechanism that could benefit from such an approach is Globus Toolkit 's MDS. Furthermore, we plan to investigate other aspects of grid service discovery that extend this work. In addition, we plan to revisit the service APIs for a Grid Service Discovery solution leveraging the available solutions and specifications as well as the work presented in this paper.", "keyphrases": ["grid servic discoveri", "uddi", "distribut web-servic discoveri architectur", "dht base uddi registri hierarchi", "deploy issu", "bamboo dht code", "case-insensit search", "queri", "longest avail prefix", "qo-base servic discoveri", "autonom control", "uddi registri", "scalabl issu", "soft state"]}
{"file_name": "J-22", "text": "Betting on Permutations ABSTRACT We consider a permutation betting scenario, where people wager on the final ordering of n candidates : for example, the outcome of a horse race. We examine the auctioneer problem of risklessly matching up wagers or, equivalently, finding arbitrage opportunities among the proposed wagers. Requiring bidders to explicitly list the orderings that they 'd like to bet on is both unnatural and intractable, because the number of orderings is n! and the number of subsets of orderings is 2n! . We propose two expressive betting languages that seem natural for bidders, and examine the computational complexity of the auctioneer problem in each case. Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering, for example, `` horse A will finish in positions 4, 9, or 13-21 '', or that a position will be taken by some subset of candidates, for example `` horse A, B, or D will finish in position 2 ''. For subset betting, we show that the auctioneer problem can be solved in polynomial time if orders are divisible. Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate, for example `` horse A will beat horse B ''. We prove that the auctioneer problem becomes NP-hard for pair betting. We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time. We also show that a natural greedy algorithm gives a poor approximation for indivisible orders. 1. INTRODUCTION Buying or selling a financial security in effect is a wager on the security 's value. For example, buying a stock is a bet that the stock 's value is greater than its current price. Each trader evaluates his expected profit to decide the quantity to buy or sell according to his own information and subjective probability assessment. The collective interaction of all bets leads to an equilibrium that reflects an aggregation of all the traders ' information and beliefs. Consider buying a security at price fifty-two cents, that pays $ 1 if and only if a Democrat wins the 2008 US Presidential election. In this case of an event-contingent security, the price -- the market 's value of the security -- corresponds directly to the estimated probability of the event. Almost all existing financial and betting exchanges pair up bilateral trading partners. For example, one trader willing to accept an x dollar loss if a Democrat does not win in return for a y dollar profit if a Democrat wins is matched up with a second trader willing to accept the opposite. However in many scenarios, even if no bilateral agreements exist among traders, multilateral agreements may be possible. We propose an exchange where traders have considerable flexibility to naturally and succinctly express their wagers, and examine the computational complexity of the auctioneer 's resulting matching problem of identifying bilateral and multilateral agreements. In particular, we focus on a setting where traders bet on the outcome of a competition among n candidates. For example, suppose that there are n candidates in an election -LRB- or n horses in a race, etc. -RRB- and thus n! possible orderings of candidates after the final vote tally. As we shall see, the matching problem can be set up as a linear or integer program, depending on whether orders are divisible or indivisible, respectively. Attempting to reduce the problem to a bilateral matching problem by explicitly creating n! securities, one for each possible final ordering, is both cumbersome for the traders and computationally infeasible even for modest sized n. Moreover, traders ' attention would be spread among n! independent choices, making the likelihood of two traders converging at the same time and place seem remote. There is a tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We want to offer traders the most expressive bidding language possible while maintaining computational feasibility. We explore two bidding languages that seem natural from a trader perspective. Subset betting, described in Section 3.2, allows traders to bet on which positions in the ranking a candidate will fall, for example `` candidate D will finish in position 1, 3-5, or 10 ''. Symetrically, traders can also bet on which candidates will fall in a particular position. In Section 4, we derive a polynomial-time algorithm for matching -LRB- divisible -RRB- subset bets. Pair betting, described in Section 3.3, allows traders to bet on the final ranking of any two candidates, for example `` candidate D will defeat candidate R ''. In Section 5, we show that optimal matching of -LRB- divisible or indivisible -RRB- pair bets is NP-hard, via a reduction from the unweighted minimum feedback arc set problem. We also provide a polynomiallyverifiable sufficient condition for the existence of a pairbetting match and show that a greedy algorithm offers poor approximation for indivisible pair bets. 2. BACKGROUND AND RELATED WORK We consider permutation betting, or betting on the outcome of a competition among n candidates. The final outcome or state s E S is an ordinal ranking of the n candidates. For example, the candidates could be horses in a race and the outcome the list of horses in increasing order of their finishing times. The state space S contains all n! mutually exclusive and exhaustive permutations of candidates. In practice at the racetrack, each of these different types of bets are processed in separate pools or groups. Instead, we describe a central exchange where all bets on the outcome are processed together, thus aggregating liquidity and ensuring that informational inference happens automatically. Ideally, we 'd like to allow traders to bet on any property of the final ordering they like, stated in exactly the language they prefer. In practice, allowing too flexible a language creates a computational burden for the auctioneer attempting to match willing traders. We explore the tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We consider a framework where people propose to buy securities that pay $ 1 if and only if some property of the final ordering is true. Traders state the price they are willing to pay per share and the number of shares they would like to purchase. A divisible order permits the trader to receive fewer shares than requested, as long as the price constraint is met ; an indivisible order is an all-or-nothing order. securities, one for every state s E S -LRB- or in fact any set of n! linearly independent securities -RRB-. This is the so-called complete Arrow-Debreu securities market -LSB- 1 -RSB- for our setting. In practice, traders do not want to deal with low-level specification of complete orderings : people think more naturally in terms of high-level properties of orderings. Moreover, operating n! securities is infeasible in practice from a computational point of view as n grows. A very simple bidding language might allow traders to bet only on who wins the competition, as is done in the `` win '' pool at racetracks. The corresponding matching problem is polynomial, however the language is not very expressive. A trader who believes that A will defeat B, but that neither will win outright can not usefully impart his information to the market. The price space of the market reveals the collective estimates of win probabilities but nothing else. Our goal is to find languages that are as expressive and intuitive as possible and reveal as much information as possible, while maintaining computational feasibility. Our work is in direct analogy to work by Fortnow et. Whereas we explore permutation combinatorics, Fortnow et. al. explore Boolean combinatorics. The authors consider a state space of the 2n possible outcomes of n binary variables. Traders express bets in Boolean logic. The authors show that divisible matching is co-NP-complete and indivisible matching is p2-complete. Hanson -LSB- 9 -RSB- describes a market scoring rule mechanism which can allow betting on combinatorial number of outcomes. The market starts with a joint probability distribution across all outcomes. It works like a sequential version of a scoring rule. Any trader can change the probability distribution as long as he agrees to pay the most recent trader according to the scoring rule. The market maker pays the last trader. Hence, he bears risk and may incur loss. Market scoring rule mechanisms have a nice property that the worst-case loss of the market maker is bounded. However, the computational aspects on how to operate the mechanism have not been fully explored. Our mechanisms have an auctioneer who does not bear any risk and only matches orders. Combinatorial auctions allow bidders to place distinct values on bundles of goods rather than just on individual goods. Uncertainty and risk are typically not considered and the central auctioneer problem is to maximize social welfare. Our mechanisms allow traders to construct bets for an event with n! outcomes. Uncertainty and risk are considered and the auctioneer problem is to explore arbitrage opportunities and risklessly match up wagers. 6. CONCLUSION We consider a permutation betting scenario, where traders wager on the final ordering of n candidates. While it is unnatural and intractable to allow traders to bet directly on the n! different final orderings, we propose two expressive betting languages, subset betting and pair betting. In a subset betting market, traders can bet either on a subset of positions that a candidate stands or on a subset of candidates who occupy a specific position in the final ordering. Pair betting allows traders bet on whether one given candidate ranks higher than another given candidate. We examine the auctioneer problem of matching orders without incurring risk. We find that in a subset betting market an auctioneer can find the optimal set and quantity of orders to accept such that his worst-case profit is maximized in polynomial time if orders are divisible. The complexity changes dramatically for pair betting. We prove that the optimal matching problem for the auctioneer is NP-hard for pair betting with both indivisible and divisible orders via reductions to the minimum feedback arc set problem. We identify a sufficient condition for the existence of a match, which can be verified in polynomial time. A natural greedy algorithm has been shown to give poor approximation for indivisible pair betting. Interesting open questions for our permutation betting include the computational complexity of optimal indivisible matching for subset betting and the necessary condition for the existence of a match in pair betting markets. We are interested in further exploring better approximation algorithms for pair betting markets.", "keyphrases": ["permut bet", "subset bet", "bilater trade partner", "polynomi-time algorithm", "inform aggreg", "permut combinator", "pair-bet market", "bipartit graph", "minimum feedback", "greedi algorithm", "complex polynomi transform"]}
{"file_name": "I-31", "text": "Reasoning about Judgment and Preference Aggregation \u25e6 ABSTRACT Agents that must reach agreements with other agents need to reason about how their preferences, judgments, and beliefs might be aggregated with those of others by the social choice mechanisms that govern their interactions. The recently emerging field of judgment aggregation studies aggregation from a logical perspective, and considers how multiple sets of logical formulae can be aggregated to a single consistent set. As a special case, judgment aggregation can be seen to subsume classical preference aggregation. We present a modal logic that is intended to support reasoning about judgment aggregation scenarios -LRB- and hence, as a special case, about preference aggregation -RRB- : the logical language is interpreted directly in judgment aggregation rules. We present a sound and complete axiomatisation of such rules. We show that the logic can express aggregation rules such as majority voting ; rule properties such as independence ; and results such as the discursive paradox, Arrow 's theorem and Condorcet 's paradox -- which are derivable as formal theorems of the logic. The logic is parameterised in such a way that it can be used as a general framework for comparing the logical properties of different types of aggregation -- including classical preference aggregation. 1. INTRODUCTION In this paper, we are interested in knowledge representation formalisms for systems in which agents need to aggregate their pref erences, judgments, beliefs, etc.. For example, an agent may need to reason about majority voting in a group he is a member of. Preference aggregation -- combining individuals ' preference relations over some set of alternatives into a preference relation which represents the joint preferences of the group by so-called social welfare functions -- has been extensively studied in social choice theory -LSB- 2 -RSB-. The recently emerging field of judgment aggregation studies aggregation from a logical perspective, and discusses how, given a consistent set of logical formulae for each agent, representing the agent 's beliefs or judgments, we can aggregate these to a single consistent set of formulae. A variety of judgment aggregation rules have been developed to this end. As a special case, judgment aggregation can be seen to subsume preference aggregation -LSB- 5 -RSB-. In this paper we present a logic, called Judgment Aggregation Logic -LRB- jal -RRB-, for reasoning about judgment aggregation. The formulae of the logic are interpreted as statements about judgment aggregation rules, and we give a sound and complete axiomatisation of all such rules. The axiomatisation is parameterised in such a way that we can instantiate it to get a range of different judgment aggregation logics. For example, one instance is an axiomatisation, in our language, of all social welfare functions -- thus we get a logic of classical preference aggregation as well. And this is one of the main contributions of this paper : we identify the logical properties of judgment aggregation, and we can compare the logical properties of different classes of judgment aggregation -- and of general judgment aggregation and preference aggregation in particular. Of course, a logic is only interesting as long as it is expressive. One of the goals of this paper is to investigate the representational and logical capabilities an agent needs for judgment and preference aggregation ; that is, what kind of logical language might be used to represent and reason about judgment aggregation? An agent 's knowledge representation language should be able to express : common aggregation rules such as majority voting ; commonly discussed properties of judgment aggregation rules and social welfare functions such as independence ; paradoxes commonly used to illustrate judgment aggregation and preference aggregation, viz. the discursive paradox and Condorcet 's paradox respectively ; and other important properties such as Arrow 's theorem. From this example it seems that a formal language for SWFs should be able to express : \u2022 Properties of preference relations for different agents, and properties of several different preference relations for the same agent in the same formula. \u2022 Comparison of different preference relations. \u2022 The preference relation resulting from applying a SWF to other preference relations. From these points it might seem that such a language would be rather complex -LRB- in particular, these requirements seem to rule out a standard propositional modal logic -RRB-. In the next section we review the basics of judgment aggregation as well as preference aggregation, and mention some commonly discussed properties of judgment aggregation rules and social welfare functions. Formulae of JAL are interpreted directly by, and thus represent properties of, judgment aggregation rules. In Section 4 we demonstrate that the logic can express commonly discussed properties of judgment aggregation rules, such as the discursive paradox. We give a sound and complete axiomatisation of the logic in Section 5, under the assumption that the agenda the agents make judgments over is finite. As mentioned above, preference aggregation can be seen as a special case of judgment aggregation, and in Section 6 we introduce an alternative interpretation of JAL formulae directly in social welfare functions. We obtain a sound and complete axiomatisation of the logic for preference aggregation as well. Sections 7 and 8 discusses related work and concludes. 7. RELATED WORK Formal logics related to social choice have focused mostly on the logical representation of preferences when the set of alternatives is large and on the computation properties of computing aggregated preferences for a given representation -LSB- 6, 7, 8 -RSB-. A notable and recent exception is a logical framework for judgment aggregation developed by Marc Pauly in -LSB- 10 -RSB-, in order to be able to characterise the logical relationships between different judgment aggregation rules. The modal logic arrow logic -LSB- 11 -RSB- is designed to reason about any object that can be graphically represented as an arrow, and has various modal operators for expressing properties of and relationships between these arrows. In the preference aggregation logic jal -LRB- LK -RRB- we interpreted formulae in pairs of alternatives -- which can be seen as arrows. Thus, -LRB- at least -RRB- the preference aggregation variant of our logic is related to arrow logic. However, while the modal operators of arrow logic can express properties of preference relations such as transitivity, they can not directly express most of the properties we have discussed in this paper. Nevertheless, the relationship to arrow logic could be investigated further in future work. In particular, arrow logics are usually proven complete wrt. an algebra. This could mean that it might be possible to use such algebras as the underlying structure to represent individual and collective preferences. Then, changing the preference profile takes us from one algebra to another, and a SWF determines the collective preference, in each of the algebras. 8. DISCUSSION We have presented a sound and complete logic jal for representing and reasoning about judgment aggregation. jal is expressive : it can express judgment aggregation rules such as majority voting ; complicated properties such as independence ; and important results such as the discursive paradox, Arrow 's theorem and Condorcet 's paradox. We argue that these results show exactly which logical capabilities an agent needs in order to be able to reason about judgment aggregation. It is perhaps surprising that a relatively simple language provides these capabilities. The axiomatisation describes the logical principles of judgment aggregation, and can also be instantiated to reason about specific instances of judgment aggregation, such as classical Arrovian preference aggregation. Thus our framework sheds light on the differences between the logical principles behind general judgment aggregation on the one hand and classical preference aggregation on the other. In future work it would be interesting to relax the completeness and consistency requirements of judgment sets, and try to characterise these in the logical language, as properties of general judgment sets, instead.", "keyphrases": ["knowledg represent formal", "social welfar function", "complet axiomatis", "syntax and semant of jal", "discurs paradox", "judgment aggreg rule", "arrow's theorem", "express", "non-dictatorship", "unanim", "prefer aggreg", "arrow logic", "jal"]}
{"file_name": "C-27", "text": "A High-Accuracy, Low-Cost Localization System for Wireless Sensor Networks ABSTRACT The problem of localization of wireless sensor nodes has long been regarded as very difficult to solve, when considering the realities of real world environments. In this paper, we formally describe, design, implement and evaluate a novel localization system, called Spotlight. Our system uses the spatio-temporal properties of well controlled events in the network -LRB- e.g., light -RRB-, to obtain the locations of sensor nodes. We demonstrate that a high accuracy in localization can be achieved without the aid of expensive hardware on the sensor nodes, as required by other localization systems. We evaluate the performance of our system in deployments of Mica2 and XSM motes. Through performance evaluations of a real system deployed outdoors, we obtain a 20cm localization error. A sensor network, with any number of nodes, deployed in a 2500m2 area, can be localized in under 10 minutes, using a device that costs less than $ 1000. To the best of our knowledge, this is the first report of a sub-meter localization error, obtained in an outdoor environment, without equipping the wireless sensor nodes with specialized ranging hardware. 1. INTRODUCTION Recently, wireless sensor network systems have been used in many promising applications including military surveillance, habitat monitoring, wildlife tracking etc. -LSB- 12 -RSB- -LSB- 22 -RSB- -LSB- 33 -RSB- -LSB- 36 -RSB-. While many middleware services, to support these applications, have been designed and implemented successfully, localization - finding the position of sensor nodes - remains one of the most difficult research challenges to be solved practically. An on-board GPS -LSB- 23 -RSB- is a typical high-end solution, which requires sophisticated hardware to achieve high resolution time synchronization with satellites. The constraints on power and cost for tiny sensor nodes preclude this as a viable solution. Other solutions require per node devices that can perform ranging among neighboring nodes. The difficulties of these approaches are twofold. First, under constraints of form factor and power supply, the effective ranges of such devices are very limited. For example the effective range of the ultrasonic transducers used in the Cricket system is less than 2 meters when the sender and receiver are not facing each other -LSB- 26 -RSB-. Second, since most sensor nodes are static, i.e. the location is not expected to change, it is not cost-effective to equip these sensors with special circuitry just for a one-time localization. To overcome these limitations, many range-free localization schemes have been proposed. Most of these schemes estimate the location of sensor nodes by exploiting the radio connectivity information among neighboring nodes. These approaches eliminate the need of high-cost specialized hardware, at the cost of a less accurate localization. In addition, the radio propagation characteristics vary over time and are environment dependent, thus imposing high calibration costs for the range-free localizations schemes. Our answer to this challenge is a localization system called Spotlight. This system employs an asymmetric architecture, in which sensor nodes do not need any additional hardware, other than what they currently have. All the sophisticated hardware and computation reside on a single Spotlight device. The Spotlight device uses a steerable laser light source, illuminating the sensor nodes placed within a known terrain. At the same time, since only a single sophisticated device is needed to localize the whole network, the amortized cost is much smaller than the cost to add hardware components to the individual sensors. 2. RELATED WORK The localization problem is a fundamental research problem in many domains. The reported localization errors are on the order of tens of centimeters, when using specialized ranging hardware, i.e. laser range finder or ultrasound. Due to the high cost and non-negligible form factor of the ranging hardware, these solutions can not be simply applied to sensor networks. The RSSI has been an attractive solution for estimating the distance between the sender and the receiver. The RADAR system -LSB- 2 -RSB- uses the RSSI to build a centralized repository of signal strengths at various positions with respect to a set of beacon nodes. The location of a mobile user is estimated within a few meters. In a similar approach, MoteTrack -LSB- 17 -RSB- distributes the reference RSSI values to the beacon nodes. Solutions that use RSSI and do not require beacon nodes have also been proposed -LSB- 5 -RSB- -LSB- 14 -RSB- -LSB- 24 -RSB- -LSB- 26 -RSB- -LSB- 29 -RSB-. They all share the idea of using a mobile beacon. The sensor nodes that receive the beacons, apply different algorithms for inferring their location. In -LSB- 29 -RSB-, Sichitiu proposes a solution in which the nodes that receive the beacon construct, based on the RSSI value, a constraint on their position estimate. In -LSB- 24 -RSB-, Pathirana et al. formulate the localization problem as an on-line estimation in a nonlinear dynamic system and proposes a Robust Extended Kalman Filter for solving it. Elnahrawy -LSB- 8 -RSB- provides strong evidence of inherent limitations of localization accuracy using RSSI, in indoor environments. A more precise ranging technique uses the time difference between a radio signal and an acoustic wave, to obtain pair wise distances between sensor nodes. This approach produces smaller localization errors, at the cost of additional hardware. The Cricket location-support system -LSB- 25 -RSB- can achieve a location granularity of tens of centimeters with short range ultrasound transceivers. AHLoS, proposed by Savvides et al. -LSB- 27 -RSB-, employs Time of Arrival -LRB- ToA -RRB- ranging techniques that require extensive hardware and solving relatively large nonlinear systems of equations. In -LSB- 30 -RSB-, Simon et al. implement a distributed system -LRB- using acoustic ranging -RRB- which locates a sniper in an urban terrain. Acoustic ranging for localization is also used by Kwon et al. -LSB- 15 -RSB-. The reported errors in localization vary from 2.2 m to 9.5 m, depending on the type -LRB- centralized vs. distributed -RRB- of the Least Square Scaling algorithm used. For wireless sensor networks ranging is a difficult option. However, the high localization accuracy, achievable by these schemes is very desirable. To overcome the challenges posed by the range-based localization schemes, when applied to sensor networks, a different approach has been proposed and evaluated in the past. This approach is called range-free and it attempts to obtain location information from the proximity to a set of known beacon nodes. Bulusu et al. propose in -LSB- 4 -RSB- a localization scheme, called Centroid, in which each node localizes itself to the centroid of its proximate beacon nodes. The Global Coordinate System -LSB- 20 -RSB-, developed at MIT, uses apriori knowledge of the node density in the network, to estimate the average hop distance. The DV - * family of localization schemes -LSB- 21 -RSB-, uses the hop count from known beacon nodes to the nodes in the network to infer the distance. The majority of range-free localization schemes have been evaluated in simulations, or controlled environments. Langendoen and Reijers present a detailed, comparative study of several localization schemes in -LSB- 16 -RSB-. To the best of our knowledge, Spotlight is the first range-free localization scheme that works very well in an outdoor environment. Our system requires a line of sight between a single device and the sensor nodes, and the map of the terrain where the sensor field is located. The Spotlight system has a long effective range -LRB- 1000 's meters -RRB- and does not require any infrastructure or additional hardware for sensor nodes. The Spotlight system combines the advantages and does not suffer from the disadvantages of the two localization classes. 7. CONCLUSIONS AND FUTURE WORK In this paper we presented the design, implementation and evaluation of a localization system for wireless sensor networks, called Spotlight. Our localization solution does not require any additional hardware for the sensor nodes, other than what already exists. All the complexity of the system is encapsulated into a single Spotlight device. Our localization system is reusable, i.e. the costs can be amortized through several deployments, and its performance is not affected by the number of sensor nodes in the network. Our experimental results, obtained from a real system deployed outdoors, show that the localization error is less than 20cm. This error is currently state of art, even for range-based localization systems and it is 75 % smaller than the error obtained when using GPS devices or when the manual deployment of sensor nodes is a feasible option -LSB- 31 -RSB-. As future work, we would like to explore the self-calibration and self-tuning of the Spotlight system. The accuracy of the system can be further improved if the distribution of the event, instead of a single timestamp, is reported. A generalization could be obtained by reformulating the problem as an angular estimation problem that provides the building blocks for more general localization techniques.", "keyphrases": ["wireless sensor network", "local", "rang-base local", "rang-free scheme", "transmiss", "perform", "accuraci", "local error", "sensor network", "spotlight system", "local techniqu", "distribut"]}
{"file_name": "C-17", "text": "Deployment Issues of a VoIP Conferencing System in a Virtual Conferencing Environment ABSTRACT Real-time services have been supported by and large on circuitswitched networks. Recent trends favour services ported on packet-switched networks. For audio conferencing, we need to consider many issues -- scalability, quality of the conference application, floor control and load on the clients/servers -- to name a few. In this paper, we describe an audio service framework designed to provide a Virtual Conferencing Environment -LRB- VCE -RRB-. The system is designed to accommodate a large number of end users speaking at the same time and spread across the Internet. The framework is based on Conference Servers -LSB- 14 -RSB-, which facilitate the audio handling, while we exploit the SIP capabilities for signaling purposes. Client selection is based on a recent quantifier called `` Loudness Number '' that helps mimic a physical face-to-face conference. We deal with deployment issues of the proposed solution both in terms of scalability and interactivity, while explaining the techniques we use to reduce the traffic. We have implemented a Conference Server -LRB- CS -RRB- application on a campus-wide network at our Institute. 1. INTRODUCTION Today 's Internet uses the IP protocol suite that was primarily designed for the transport of data and provides best effort data delivery. Delay-constraints and characteristics separate traditional data on the one hand from voice & video applications on the other. Hence, as progressively time-sensitive voice and video applications are deployed on the Internet, the inadequacy of the Internet is exposed. Further, we seek to port telephone services on the Internet. Among them, virtual conference -LRB- teleconference -RRB- facility is at the cutting edge. Audio and video conferencing on Internet are popular -LSB- 25 -RSB- for the several advantages they inhere -LSB- 3,6 -RSB-. Clearly, the bandwidth required for a teleconference over the Internet increases rapidly with the number of participants ; reducing bandwidth without compromising audio quality is a challenge in Internet Telephony. There is plenty of discussion amongst HCI and CSCW community on the use of Ethnomethodology for design of CSCW applications. The basic approach is to provide larger bandwidth, more facilities and more advanced control mechanisms, looking forward to better quality of interaction. This approach ignores the functional utility of the environment that is used for collaboration. Thus, the need is to take an approach that considers both aspects -- the technical and the functional. In this work, we do not discuss video conferencing ; its inclusion does not significantly benefit conference quality -LSB- 4 -RSB-. Our focus is on virtual audio environments. We first outline the challenges encountered in virtual audio conferences. Then we look into the motivations followed by relevant literature. In Section 5, we explain the architecture of our system. Section 6 comprises description of the various algorithms used in our setup. We address deployment issues. A discussion on performance follows. We conclude taking alongside some implementation issues. 4. RELATED WORK The SIP standard defined in RFC 3261 -LSB- 22 -RSB- and in later extensions such as -LSB- 21 -RSB- does not offer conference control services such as floor control or voting and does not prescribe how a Fig. 1. Conference example -- 3 domains containing the necessary entities so that the conference can take place. conference is to be managed. However SIP can be used to initiate a session that uses some other conference control protocol. The core SIP specification supports many models for conferencing -LSB- 26, 23 -RSB-. In the server-based models, a server mixes media streams, whereas in a server-less conference, mixing is done at the end systems. SDP -LSB- 7 -RSB- can be used to define media capabilities and provide other information about the conference. We shall now consider a few conference models in SIP that have been proposed recently -LSB- 23 -RSB-. First, let us look into server-less models. In End-System Mixing, only one client -LRB- SIP UA -RRB- handles the signaling and media mixing for all the others, which is clearly not scalable and causes problems when that particular client leaves the conference. This leads to an increasing number of hops for the remote leaves and is not scalable. Another option would be to use multicast for conferencing but multicast is not enabled over Internet and only possible on a LAN presently. Among server-based models, in a Dial-In Conference, UAs connect to a central server that handles all the mixing. This model is not scalable as it is limited by the processing power of the server and bandwidth of the network. Adhoc Centralized Conferences and Dial-Out Conference Servers have similar mechanisms and problems. Hybrid models involving centralized signaling and distributed media, with the latter using unicast or multicast, raise scalability problems as before. However an advantage is that the conference control can be a third party solution. Loss of spatialism when they mix and the bandwidth increase when they do not are open problems. A related study -LSB- 19 -RSB- by the same author proposes conferencing architecture for Collaborative Virtual Environments -LRB- CVEs -RRB- but does not provide the scalability angle without the availability of multicasting. With the limitations of proposed conferencing systems in mind, we will now detail our proposal. 9. CONCLUSION In this paper, we have presented a discussion on a voice-only virtual conferencing environment. We have argued that the distributed nature of deployment here makes it scalable. Interactivity is achieved by adapting a recent stream selection scheme based on Loudness Number. Thus, there is significantly effective utilization of bandwidth. These render impromptu speech in a virtual teleconference over VoIP a reality, as in a real face-to-face conference. The traffic in the WAN -LRB- Internet -RRB- is upper-bounded by the square of the number of domains, -- further reduced by using heuristic algorithms -- which is far below the total number of clients in the conference. This is due to the use of a Conference Server local to each domain. VAD techniques help further traffic reduction. Using SIP standard for signaling makes this solution highly interoperable. We have implemented a CS application on a campus-wide network. We believe this new generation of virtual conferencing environments will gain more popularity in the future as their ease of deployment is assured thanks to readily available technologies and scalable frameworks.", "keyphrases": ["voip conferenc system", "packet-switch network", "audio servic framework", "virtual conferenc environ", "confer server", "loud number", "partial mix", "voic activ detect", "suffici of three simultan speaker", "vad techniqu"]}
{"file_name": "H-30", "text": "Latent Concept Expansion Using Markov Random Fields ABSTRACT Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to improve retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based on the Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expansion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our technique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demonstrates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation. 1. INTRODUCTION possibly a longer narrative. A great deal of information is lost during the process of translating from the information need to the actual query. For this reason, there has been a strong interest in query expansion techniques. Such techniques are used to augment the original query to produce a representation that better reflects the underlying information need. Query expansion techniques have been well studied for various models in the past and have shown to significantly improve effectiveness in both the relevance feedback and pseudo-relevance feedback setting -LSB- 12, 21, 28, 29 -RSB-. The MRF model generalizes the unigram, bigram, and other various dependence models -LSB- 14 -RSB-. Most past term dependence models have failed to show consistent, significant improvements over unigram baselines, with few exceptions -LSB- 8 -RSB-. Until now, the model has been solely used for ranking documents in response to a given query. In this work, we show how the model can be extended and used for query expansion using a technique that we call latent concept expansion -LRB- LCE -RRB-. There are three primary contributions of our work. First, LCE provides a mechanism for combining term dependence with query expansion. Previous query expansion techniques are based on bag of words models. Therefore, by performing query expansion using the MRF model, we are able to study the dynamics between term dependence and query expansion. Next, as we will show, the MRF model allows arbitrary features to be used within the model. Query expansion techniques in the past have implicitly only made use of term occurrence features. By using more robust feature sets, it is possible to produce better expansion terms that discriminate between relevant and non-relevant documents better. Finally, our proposed approach seamlessly provides a mechanism for generating both single and multi-term concepts. Most previous techniques, by default, generate terms independently. There have been several approaches that make use of generalized concepts, however such approaches were somewhat heuristic and done outside of the model -LSB- 19, 28 -RSB-. Our approach is both formally motivated and a natural extension of the underlying model. In Section 2 we describe related query expansion approaches. Section 3 provides an overview of the MRF model and details our proposed latent concept expansion technique. In Section 4 we evaluate our proposed model and analyze the results. 2. RELATED WORK One of the classic and most widely used approaches to query expansion is the Rocchio algorithm -LSB- 21 -RSB-. Rocchio 's approach, which was developed within the vector space model, reweights the original query vector by moving the weights towards the set of relevant or pseudo-relevant documents and away from the non-relevant documents. Unfortunately, it is not possible to formally apply Rocchio 's approach to a statistical retrieval model, such as language modeling for information retrieval. A number of formalized query expansion techniques have been developed for the language modeling framework, including Zhai and Lafferty 's model-based feedback and Lavrenko and Croft 's relevance models -LSB- 12, 29 -RSB-. Both approaches attempt to use pseudo-relevant or relevant documents to estimate a better query model. Model-based feedback finds the model that best describes the relevant documents while taking a background -LRB- noise -RRB- model into consideration. This separates the content model from the background model. The content model is then interpolated with the original query model to form the expanded query. The other technique, relevance models, is more closely related to our work. Therefore, we go into the details of the model. Much like model-based feedback, relevance models estimate an improved query model. The only difference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant documents. Instead, they model a more generalized notion of relevance, as we now show. Given a query Q, a relevance model is a multinomial distribution, P -LRB- \u00b7 | Q -RRB-, that encodes the likelihood of each term given the query as evidence. It is computed as : where RQ is the set of documents that are relevant or pseudorelevant to query Q. These mild assumptions make computing the Bayesian posterior more practical. After the model is estimated, documents are ranked by clipping the relevance model by choosing the k most likely terms from P -LRB- \u00b7 | Q -RRB-. This clipped distribution is then interpolated with with the original, maximum likelihood query model -LSB- 1 -RSB-. This can be thought of as expanding the original query by k weighted terms. Throughout the remainder of this work, we refer to this instantiation of relevance models as RM3. There has been relatively little work done in the area of query expansion in the context of dependence models -LSB- 9 -RSB-. However, there have been several attempts to expand using multi-term concepts. Xu and Croft 's local context analysis -LRB- LCA -RRB- method combined passage-level retrieval with concept expansion, where concepts were single terms and phrases -LSB- 28 -RSB-. Expansion concepts were chosen and weighted using a metric based on co-occurrence statistics. Papka and Allan investigate using relevance feedback to perform multi-term concept expansion for document routing -LSB- 19 -RSB-. Results showed that combining single term and large window multi-term concepts significantly improved effectiveness. 5. CONCLUSIONS In this paper we proposed a robust query expansion technique called latent concept expansion. The technique was shown to be a natural extension of the Markov random field model for information retrieval and a generalization of relevance models. We showed that the technique can be used to produce high quality, well formed, topically relevant multi-term expansion concepts. The concepts generated can be used in an alternative query suggestion module. We also showed that the model is highly effective. In fact, it achieves significant improvements in mean average precision over relevance models across a selection of TREC data sets. It was also shown the MRF model itself, without any query expansion, outperforms relevance models on large web data sets. Finally, we reiterated the importance of choosing expansion terms that model relevance, rather than the relevant documents and showed how LCE captures both syntactic and query-side semantic dependencies. Future work will look at incorporating document-side dependencies, as well.", "keyphrases": ["robust queri expans techniqu", "languag model queri expans techniqu", "relev feedback", "pseudo-relev feedback", "inform retriev", "languag model approach", "web search", "queri expans", "mrf", "rocchio algorithm", "languag model framework", "rm3", "document rout", "ad-hoc retriev", "mrf model", "relev distribut"]}
{"file_name": "I-10", "text": "SMILE : Sound Multi-agent Incremental LEarning ;--RRB- * ABSTRACT This article deals with the problem of collaborative learning in a multi-agent system. Here each agent can update incrementally its beliefs B -LRB- the concept representation -RRB- so that it is in a way kept consistent with the whole set of information K -LRB- the examples -RRB- that he has received from the environment or other agents. We extend this notion of consistency -LRB- or soundness -RRB- to the whole MAS and discuss how to obtain that, at any moment, a same consistent concept representation is present in each agent. The corresponding protocol is applied to supervised concept learning. The resulting method SMILE -LRB- standing for Sound Multiagent Incremental LEarning -RRB- is described and experimented here. Surprisingly some difficult boolean formulas are better learned, given the same learning set, by a Multi agent system than by a single agent. 1. INTRODUCTION This article deals with the problem of collaborative concept learning in a multi-agent system. -LSB- 6 -RSB- introduces a characterisation of learning in multi-agent system according to the level of awareness of the agents. At level 1, agents learn * The primary author of this paper is a student. in the system without taking into account the presence of other agents, except through the modification brought upon the environment by their action. Level 2 implies direct interaction between the agents as they can exchange messages to improve their learning. We focus in this paper on level 2, studying direct interaction between agents involved in a learning process. Each agent is assumed to be able to learn incrementally from the data he receives, meaning that each agent can update his belief set B to keep it consistent with the whole set of information K that he has received from the environment or from other agents. Moreover, we suppose that at least a part Bc of the beliefs of each agent is common to all agents and must stay that way. Therefore, an update of this common set Bc by agent r must provoke an update of Bc for the whole community of agents. It leads us to define what is the mas-consistency of an agent with respect to the community. The update process of the community beliefs when one of its members gets new information can then be defined as the consistency maintenance process ensuring that every agent in the community will stay masconsistent. This mas-consistency maintenance process of an agent getting new information gives him the role of a learner and implies communication with other agents acting as critics. However, agents are not specialised and can in turn be learners or critics, none of them being kept to a specific role. Pieces of information are distributed among the agents, but can be redundant. There is no central memory. The work described here has its origin in a former work concerning learning in an intentional multi-agent system using a BDI formalism -LSB- 6 -RSB-. In that work, agents had plans, each of them being associated with a context defining in which conditions it can be triggered. Plans -LRB- each of them having its own context -RRB- were common to the whole set of agents in the community. Agents had to adapt their plan contexts depending on the failure or success of executed plans, using a learning mechanism and asking other agents for examples -LRB- plans successes or failures -RRB-. However this work lacked a collective learning protocol enabling a real autonomy of the multi-agent system. The study of such a protocol is the ob ject of the present paper. In section 2 we formally define the mas-consistency of an update mechanism for the whole MAS and we propose a generic update mechanism proved to be mas consistent. In section 3 we describe SMILE, an incremental multi agent concept learner applying our mas consistent update mechanism to collaborative concept learning. Section 4 describes various experiments on SMILE and discusses various issues including how the accuracy and the simplicity of the current hypothesis vary when comparing single agent learning and mas learning. In section 5 we briefly present some related works and then conclude in section 6 by discussing further investigations on mas consistent learning. 5. RELATED WORKS Since 96 -LSB- 15 -RSB-, various work have been performed on learning in MAS, but rather few on concept learning. In -LSB- 11 -RSB- the MAS performs a form of ensemble learning in which the agents are lazy learners -LRB- no explicit representation is maintained -RRB- and sell useless examples to other agents. In -LSB- 10 -RSB- each agent observes all the examples but only perceive a part of their representation. In mutual online concept learning -LSB- 14 -RSB- the agents converge to a unique hypothesis, but each agent produces examples from its own concept representation, thus resulting in a kind of synchronization rather than in pure concept learning. 6. CONCLUSION We have presented here and experimented a protocol for MAS online concept learning. Nevertheless, our framework is open, i.e., the agents can leave the system or enter it while the consistency mechanism is preserved. For instance if we introduce a timeout mechanism, even when a critic agent crashes or omits to answer, the consistency with the other critics -LRB- within the remaining agents -RRB- is entailed. Further work concerns first coupling induction and abduction in order to perform collaborative concept learning when examples are only partially observed by each agent, and second, investigating partial memory learning : how learning is preserved whenever one agent or the whole MAS forgets some selected examples.", "keyphrases": ["multi-agent learn", "collabor concept learn", "learn process", "knowledg", "ma-consist", "increment learn", "agent", "updat mechan", "synchron"]}
{"file_name": "H-31", "text": "A Study of Poisson Query Generation Model for Information Retrieval ABSTRACT Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods. We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling. We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing. 1. INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks -LSB- 21, 28, 14, 4 -RSB-. We can then rank documents based on the likelihood of generating the query. Virtually all the existing query generation language models are based on either multinomial distribution -LSB- 19, 6, 28 -RSB- or multivariate Bernoulli distribution -LSB- 21, 18 -RSB-. The multinomial distribution is especially popular and also shown to be quite effective. Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1. In this paper, we propose and study a new family of query generation models based on the Poisson distribution. In this new family of models, we model the frequency of each term independently with a Poisson distribution. To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model. In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing. As in the existing work on multinomial language models, smoothing is critical for this new family of models. We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multi nomial distributions. We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing. In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model. We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model. This advantage is seen for both one-stage and two-stage smoothing. Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula. This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter. In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions. In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval. We then design empirical experiments to compare the two families of language models in Section 4. We discuss the related work in 5 and conclude in 6. 5. RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution. Language models have been shown to be effective for many retrieval tasks -LSB- 21, 28, 14, 4 -RSB-. The most popular and fundamental one is the query-generation language model -LSB- 21, 13 -RSB-. All existing query generation language models are based on either multinomial distribution -LSB- 19, 6, 28, 13 -RSB- or multivariate Bernoulli distribution -LSB- 21, 17, 18 -RSB-. We introduce a new family of language models, based on Poisson distribution. Poisson distribution has been previously studied in the document generation models -LSB- 16, 22, 3, 24 -RSB-, leading to the development of one of the most effective retrieval formula BM25 -LSB- 23 -RSB-. -LSB- 24 -RSB- studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial. However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. -LSB- 26 -RSB- introduces a way to empirically search for an exponential model for the documents. Poisson mixtures -LSB- 3 -RSB- such as 2-Poisson -LSB- 22 -RSB-, Negative multinomial, and Katz 's KMixture -LSB- 9 -RSB- has shown to be effective to model and retrieve documents. Once again, none of this work explores Poisson distribution in the query generation framework. Language model smoothing -LSB- 2, 28, 29 -RSB- and background structures -LSB- 15, 10, 25, 27 -RSB- have been studied with multinomial language models. -LSB- 7 -RSB- analytically shows that term specific smoothing could be useful. We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6. CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution. We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing. We compare the new models with the popular multinomial retrieval models both analytically and experimentally. Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences. In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing. We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models. Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model. Our work opens up many interesting directions for further exploration in this new family of models. Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.", "keyphrases": ["multinomi distribut", "queri gener probabilist model", "poisson distribut", "two-stage smooth", "multivari bernoullu distribut", "speech recognit", "term frequenc", "perterm smooth", "new term-depend smooth algorithm", "vocabulari set", "homogen poisson process", "singl pseudo term"]}
{"file_name": "I-35", "text": "Distributed Norm Management in Regulated Multi-Agent Systems * ABSTRACT Norms are widely recognised as a means of coordinating multi-agent systems. The distributed management of norms is a challenging issue and we observe a lack of truly distributed computational realisations of normative models. In order to regulate the behaviour of autonomous agents that take part in multiple, related activities, we propose a normative model, the Normative Structure -LRB- NS -RRB-, an artifact that is based on the propagation of normative positions -LRB- obligations, prohibitions, permissions -RRB-, as consequences of agents ' actions. Within a NS, conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents ' actions. However, ensuring conflict-freedom of a NS at design time is computationally intractable. We show this by formalising the notion of conflict, providing a mapping of NSs into Coloured Petri Nets and borrowing well-known theoretical results from that field. Since online conflict resolution is required, we present a tractable algorithm to be employed distributedly. We then demonstrate that this algorithm is paramount for the distributed enactment of a NS. 1. INTRODUCTION A fundamental feature of open, regulated multi-agent systems in which autonomous agents interact, is that participating agents are meant to comply with the conventions of the system. Norms can be used to model such conventions and hence as a means to regulate the observable behaviour of agents -LSB- 6, 29 -RSB-. There are many contributions on the subject of norms from sociologists, philosophers and logicians -LRB- e.g., -LSB- 15, 28 -RSB- -RRB-. However, there are very few proposals for computational realisations of normative models -- the way norms can be integrated in the design and execution of MASs.. To our knowledge, no proposal truly supports the distributed enactment of normative environments. In our paper we approach that problem and propose means to handle conflicting commitments in open, regulated, multiagent systems in a distributed manner. The type of regulated MAS we envisage consists of multiple, concurrent, related activities where agents interact. Each agent may concurrently participate in several activities, and change from one activity to another. An agent 's actions within an activity may have consequences in the form of normative positions -LRB- i.e. obligations, permissions, and prohibitions -RRB- -LSB- 26 -RSB- that may constrain its future behaviour. We assume that agents may choose not to fulfill all their obligations and hence may be sanctioned by the MAS. Notice that, when activities are distributed, normative positions must flow from the activities in which they are generated to those in which they take effect. Since in an open, regulated MAS one can not embed normative aspects into the agents ' design, we adopt the view that the MAS should be supplemented with a separate set of norms that further regulates the behaviour of participating agents. In order to model the separation of concerns between the coordination level -LRB- agents ' interactions -RRB- and the normative level -LRB- propagation of normative positions -RRB-, we propose an artifact called the Normative Structure -LRB- NS -RRB-. Within a NS conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents ' actions. For instance, an agent may be obliged and prohibited to do the very same action in an activity. However, ensuring conflict-freedom of a NS at design time is computationally intractable. We show this by formalising the notion of conflict, providing a mapping of NSs into Coloured Petri Nets -LRB- CPNs -RRB- and borrowing well-known theoretical results from the field of CPNs. We believe that online conflict detection and resolution is required. Hence, we present a tractable algorithm for conflict resolution. This algorithm is paramount for the distributed enactment of a NS. The paper is organised as follows. In Section 2 we detail a scenario to serve as an example throughout the paper. Next, in Section 3 we formally define the normative structure artifact. Further on, in Section 4 we formalise the notion of conflict to subsequently analyse the complexity of conflict detection in terms of CPNs in Section 5. Section 6 describes the computational management of NSs by describing their enactment and presenting an algorithm for conflict resolution. Finally, we comment on related work, draw conclusions and report on future work in Section 7. 7. RELATED WORK AND CONCLUSIONS Our contributions in this paper are three-fold. Firstly, we introduce an approach for the management of and reasoning about norms in a distributed manner. To our knowledge, there is little work published in this direction. In -LSB- 8, 21 -RSB-, two languages are presented for the distributed enforcement of norms in MAS. However, in both works, each agent has a local message interface that forwards legal messages according to a set of norms. Since these interfaces are local to each agent, norms can only be expressed in terms of actions of that agent. This is a serious disadvantage, e.g. when one needs to activate an obligation to one agent due to a certain message of another one. The second contribution is the proposal of a normative structure. The notion is fruitful because it allows the separation of normative and procedural concerns. The normative structure we propose makes evident the similarity between the propagation of normative positions and the propagation 642 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- of tokens in Coloured Petri Nets. That similarity readily suggests a mapping between the two, and gives grounds to a convenient analytical treatment of the normative structure, in general, and the complexity of conflict detection, in particular. In -LSB- 5 -RSB-, conversations are first designed and analysed at the level of CPNs and thereafter translated into protocols. Lin et al. -LSB- 20 -RSB- map conversation schemata to CPNs. To our knowledge, the use of this representation in the support of conflict detection in regulated MAS has not been reported elsewhere. Finally, we present a distributed mechanism to resolve normative conflicts. Sartor -LSB- 25 -RSB- treats normative conflicts from the point of view of legal theory and suggests a way to order the norms involved. His idea is implemented in -LSB- 12 -RSB- but requires a central resource for norm maintenance. The approach to conflict detection and resolution is an adaptation and extension of the work on instantiation graphs reported in -LSB- 17 -RSB- and a related algorithm in -LSB- 27 -RSB-. These three contributions we present in this paper open many possibilities for future work. We expect such coupling will endow electronic institutions with a more flexible -- and more expressive -- normative environment. On the theoretical side, we intend to use analysis techniques of CPNs in order to characterise classes of CPNs -LRB- e.g., acyclic, symmetric, etc. -RRB- corresponding to families of Normative Structures that are susceptible to tractable offline conflict detection. The combination of these techniques along with our online conflict resolution mechanisms is intended to endow MAS designers with the ability to incorporate norms into their systems in a principled way.", "keyphrases": ["algorithm", "activ", "scenario", "norm posit", "protocol", "norm scene", "norm transit rule", "norm structur", "bi-partit graph", "prohibit", "permiss overlap", "token", "conflict"]}
{"file_name": "I-33", "text": "A Formal Road from Institutional Norms to Organizational Structures ABSTRACT Up to now, the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics. In order to develop systems actually implementing institutions and organizations, formal methods should take the place of heuristic ones. The paper presents a formal semantics for the notion of institution and its components -LRB- abstract and concrete norms, empowerment of agents, roles -RRB- and defines a formal relation between institutions and organizational structures. As a result, it is shown how institutional norms can be refined to constructs -- organizational structures -- which are closer to an implemented system. It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification. 1. INTRODUCTION The opportunity of a `` technology transfer '' from the field of organizational and social theory to distributed AI and multiagent systems -LRB- MASs -RRB- has long been advocated -LRB- -LSB- 8 -RSB- -RRB-. In MASs the application of the organizational and institutional metaphors to system design has proven to be useful for the development of methodologies and tools. In many cases, however, the application of these conceptual apparatuses amounts to mere heuristics guiding the high level design of the systems. treated formally, that is, once notions such as norm, role, structure, etc. obtain a formal semantics. Aim of the present paper is to fill this gap with respect to the notion of institution providing formal foundations for the application of the institutional metaphor and for its relation to the organizational one. The main result of the paper consists in showing how abstract constraints -LRB- institutions -RRB- can be step by step refined to concrete structural descriptions -LRB- organizational structures -RRB- of the to-be-implemented system, bridging thus the gap between abstract norms and concrete system specifications. Concretely, in Section 2, a logical framework is presented which provides a formal semantics for the notions of institution, norm, role, and which supports the account of key features of institutions such as the translation of abstract norms into concrete and implementable ones, the institutional empowerment of agents, and some aspects of the design of norm enforcement. In Section 3 the framework is extended to deal with the notion of the infrastructure of an institution. The extended framework is then studied in relation to the formalism for representing organizational structures presented in -LSB- 11 -RSB-. In Section 4 some conclusions follow. 4. CONCLUSIONS The paper aimed at providing a comprehensive formal analysis of the institutional metaphor and its relation to the organizational one. The predominant formal tool has been description logic. TBoxes has been used to represent the specifications of institutions -LRB- Definition 3 -RRB- and their infrastructures -LRB- Definition 6 -RRB-, providing therefore a transition system semantics for a number of institutional notions -LRB- Examples 1-7 -RRB-. Multi-graphs has then been used to represent the specification of organizational structures -LRB- Definition 6 -RRB-. The last result presented concerned the definition of a formal correspondence between institution and organization specifications -LRB- Definition 7 -RRB-, which provides a formal way for switching between the two paradigms. All in all, these results deliver a way for relating abstract system specifications -LRB- i.e., institutions as sets of norms -RRB- to specifications that are closer to an implemented system -LRB- i.e., organizational structures -RRB-.", "keyphrases": ["formal method", "institut norm", "abstract constraint", "formal for repres organiz structur", "entiti", "properti", "descript logic", "dynam logic", "terminolog axiom", "role", "infrastructur"]}
{"file_name": "C-36", "text": "Encryption-Enforced Access Control in Dynamic Multi-Domain Publish/Subscribe Networks ABSTRACT Publish/subscribe systems provide an efficient, event-based, wide-area distributed communications infrastructure. Large scale publish/subscribe systems are likely to employ components of the event transport network owned by cooperating, but independent organisations. As the number of participants in the network increases, security becomes an increasing concern. This paper extends previous work to present and evaluate a secure multi-domain publish/subscribe infrastructure that supports and enforces fine-grained access control over the individual attributes of event types. Key refresh allows us to ensure forward and backward security when event brokers join and leave the network. We demonstrate that the time and space overheads can be minimised by careful consideration of encryption techniques, and by the use of caching to decrease unnecessary decryptions. We show that our approach has a smaller overall communication overhead than existing approaches for achieving the same degree of control over security in publish/subscribe networks. 1. INTRODUCTION Publish/subscribe is well suited as a communication mechanism for building Internet-scale distributed event-driven applications. of participants comes from its decoupling of publishers and subscribers by placing an asynchronous event delivery service between them. In truly Internet-scale publish/subscribe systems, the event delivery service will include a large set of interconnected broker nodes spanning a wide geographic -LRB- and thus network -RRB- area. While the communication capabilities of publish/subscribe systems are well proved, spanning multiple administrative domains is likely to require addressing security considerations. As security and access control are almost the antithesis of decoupling, relatively little publish/subscribe research has focused on security so far. Our overall research aim is to develop Internet-scale publish/subscribe networks that provide secure, efficient delivery of events, fault-tolerance and self-healing in the delivery infrastructure, and a convenient event interface. In -LSB- 12 -RSB- Pesonen et al. propose a multi-domain, capabilitybased access control architecture for publish/subscribe systems. The architecture provides a mechanism for authorising event clients to publish and subscribe to event types. The privileges of the client are checked by the local broker that the client connects to in order to access the publish / subscribe system. The approach implements access control at the edge of the broker network and assumes that all brokers can be trusted to enforce the access control policies correctly. Any malicious, compromised or unauthorised broker is free to read and write any events that pass through it on their way from the publishers to the subscribers. We propose enforcing access control within the broker network by encrypting event content, and that policy dictate controls over the necessary encryption keys. With encrypted event content only those brokers that are authorised to ac cess the encryption keys are able to access the event content -LRB- i.e. publish, subscribe to, or filter -RRB-. We effectively move the enforcement of access control from the brokers to the encryption key managers. We expect that access control would need to be enforced in a multi-domain publish/subscribe system when multiple organisations form a shared publish/subscribe system yet run multiple independent applications. Access control might also be needed when a single organisation consists of multiple sub-domains that deliver confidential data over the organisation-wide publish/subscribe system. Both cases require access control because event delivery in a dynamic publish/subscribe infrastructure based on a shared broker network may well lead to events being routed through unauthorised domains along their paths from publishers to subscribers. There are two particular benefits to sharing the publish / subscribe infrastructure, both of which relate to the broker network. First, sharing brokers will create a physically larger network that will provide greater geographic reach. Second, increasing the inter-connectivity of brokers will allow the publish/subscribe system to provide higher faulttolerance. Figure 1 shows the multi-domain publish/subscribe network we use as an example throughout this paper. This domain contains a set of CCTV cameras that publish information about the movements of vehicles around the London area. We have included Detective Smith as a subscriber in this domain. Congestion Charge Service Domain. The charges that are levied on the vehicles that have passed through the London Congestion Charge zone each day are issued by systems within this domain. The source numberplate recognition data comes from the cameras in the Metropolitan Police Domain. The fact that the CCS are only authorised to read a subset of the vehicle event data will exercise some of the key features of the enforceable publish/subscribe system access control presented in this paper. PITO Domain. It is the event type owner in this particular scenario. Encryption protects the confidentiality of events should they be transported through unauthorised domains. However encrypting whole events means unauthorised brokers can not make efficient routing decisions. Our approach is to apply encryption to the individual attributes of events. This way our multi-domain access control policy works at a finer granularity -- publishers and subscribers may be authorised access to a subset of the available attributes. In cases where non-encrypted events are used for routing, we can reduce the total number of events sent through the system without revealing the values of sensitive attributes. We thus preserve the privacy of motorists while still allowing the CCS to do its job using the shared publish/subscribe infrastructure. The detective gets a court order that authorises her to subscribe to numberplate events of the specific numberplate related to her case. Current publish/subscribe access control systems enforce security at the edge of the broker network where clients connect to it. However this approach will often not be acceptable in Internet-scale systems. We propose enforcing security within the broker network as well as at the edges that event clients connect to, by encrypting event content. Publications will be encrypted with their event type specific encryption keys. By controlling access to the encryption keys, we can control access to the event types. The proposed approach allows event brokers to route events even when they have access only to a subset of the potential encryption keys. We introduce decentralised publish/subscribe systems and relevant cryptography in Section 2. In Section 3 we present our model for encrypting event content on both the event and the attribute level. Section 4 discusses managing encryption keys in multi-domain publish/subscribe systems. Finally Section 6 discusses related work in securing publish/subscribe systems and Section 7 provides concluding remarks. 2. BACKGROUND In this section we provide a brief introduction to decentralised publish/subscribe systems. We indicate our assumptions about multi-domain publish/subscribe systems, and describe how these assumptions influence the developments we have made from our previously published work. 2.1 Decentralised Publish/Subscribe Systems A publish/subscribe system includes publishers, subscribers, and an event service. Publishers publish events, subscribers subscribe to events of interest to them, and the event service is responsible for delivering published events to all subscribers whose interests match the given event. The event service in a decentralised publish/subscribe system is distributed over a number of broker nodes. Together these brokers form a network that is responsible for maintaining the necessary routing paths from publishers to subscribers. Clients -LRB- publishers and subscribers -RRB- connect to a local broker, which is fully trusted by the client. In our discussion we refer to the client hosting brokers as publisher hosting brokers -LRB- PHB -RRB- or subscriber hosting brokers -LRB- SHB -RRB- depending on whether the connected client is a publisher or Figure 1 : An overall view of our multi-domain publish/subscribe deployment a subscriber, respectively. A local broker is usually either part of the same domain as the client, or it is owned by a service provider trusted by the client. A broker network can have a static topology -LRB- e.g. Siena -LSB- 3 -RSB- and Gryphon -LSB- 14 -RSB- -RRB- or a dynamic topology -LRB- e.g. Scribe -LSB- 4 -RSB- and Hermes -LSB- 13 -RSB- -RRB-. Our proposed approach will work in both cases. A static topology enables the system administrator to build trusted domains and in that way improve the efficiency of routing by avoiding unnecessary encryptions -LRB- see Sect. Our work is based on the Hermes system. Hermes is a content-based publish/subscribe middleware that includes strong event type support. In other words, each publication is an instance of a particular predefined event type. Publications are type checked at the local broker of each publisher. Our attribute level encryption scheme assumes that events are typed. Hermes uses a structured overlay network as a transport and therefore has a dynamic topology. A Hermes publication consists of an event type identifier and a set of attribute value pairs. The type identifier is the SHA-1 hash of the name of the event type. It is used to route the publication through the event broker network. It conveniently hides the type of the publication, i.e. brokers are prevented from seeing which events are flowing through them unless they are aware of the specific event type name and identifier. 2.2 Secure Event Types Pesonen et al. introduced secure event types in -LSB- 11 -RSB-, which can have their integrity and authenticity confirmed by checking their digital signatures. A useful side effect of secure event types are their globally unique event type and attribute names. These names can be referred to by access control policies. In this paper we use the secure name of the event type or attribute to refer to the encryption key used to encrypt the event or attribute. 2.3 Capability-Based Access Control Pesonen et al. proposed a capability-based access control architecture for multi-domain publish/subscribe systems in -LSB- 12 -RSB-. The model treats event types as resources that publishers, subscribers, and event brokers want to access. The event type owner is responsible for managing access control for an event type by issuing Simple Public Key Infrastructure -LRB- SPKI -RRB- authorisation certificates that grant the holder access to the specified event type. For example, authorised publishers will have been issued an authorisation certificate that specifies that the publisher, identified by public key, is authorised to publish instances of the event type specified in the certificate. We leverage the above mentioned access control mechanism in this paper by controlling access to encryption keys using the same authorisation certificates. That is, a publisher who is authorised to publish a given event type, is also authorised to access the encryption keys used to protect events of that type. 4. 2.4 Threat model The goal of the proposed mechanism is to enforce access control for authorised participants in the system. In our case the first level of access control is applied when the participant tries to join the publish/subscribe network. Unauthorised event brokers are not allowed to join the broker network. Similarly unauthorised event clients are not allowed to connect to an event broker. All the connections in the broker network between event brokers and event clients utilise Transport Layer Security -LRB- TLS -RRB- -LSB- 5 -RSB- in order to prevent unauthorised access on the transport layer. The architecture of the publish/subscribe system means that event clients must connect to event brokers in order to be able to access the publish/subscribe system. Thus we assume that these clients are not a threat. The event client relies completely on the local event broker for access to the broker network. Therefore the event client is unable to access any events without the assistance of the local broker. The brokers on the other hand are able to analyse all events in the system that pass through them. A broker can analyse both the event traffic as well as the number and names of attributes that are populated in an event -LRB- in the case of attribute level encryption -RRB-. There are viable approaches to preventing traffic analysis by inserting random events into the event stream in order to produce a uniform traffic pattern. 6. RELATED WORK Wang et al. have categorised the various security issues that need to be addressed in publish/subscribe systems in the future in -LSB- 20 -RSB-. The paper is a comprehensive overview of security issues in publish/subscribe systems and as such tries to draw attention to the issues rather than providing solutions. Bacon et al. in -LSB- 1 -RSB- examine the use of role-based access control in multi-domain, distributed publish/subscribe systems. Opyrchal and Prakash address the problem of event confidentiality at the last link between the subscriber and the SHB in -LSB- 10 -RSB-. They correctly state that a secure group communication approach is infeasible in an environment like publish/subscribe that has highly dynamic group memberships. We assume in our work that the SHB is powerful enough to man Figure 8 : Hop Counts When Emulating Attribute Encryption age a TLS secured connection for each local subscriber. Both Srivatsa et al. -LSB- 19 -RSB- and Raiciu et al. -LSB- 16 -RSB- present mechanisms for protecting the confidentiality of messages in decentralised publish/subscribe infrastructures. Compared to our work both papers aim to provide the means for protecting the integrity and confidentiality of messages whereas the goal for our work is to enforce access control inside the broker network. Raiciu et al. assume in their work that none of the brokers in the network are trusted and therefore all events are encrypted from publisher to subscriber and that all matching is based on encrypted events. In contrast, we assume that some of the brokers on the path of a publication are trusted to access that publication and are therefore able to implement event matching. We also assume that the publisher and subscriber hosting brokers are always trusted to access the publication. Finally, Fiege et al. address the related topic of event visibility in -LSB- 6 -RSB-. While the work concentrated on using scopes as mechanism for structuring large-scale event-based systems, the notion of event visibility does resonate with access control to some extent. 7. CONCLUSIONS Event content encryption can be used to enforce an access control policy while events are in transit in the broker network of a multi-domain publish/subscribe system. Attribute level encryption can be implemented in order to enforce fine-grained access control policies. In addition to providing attribute-level access control, attribute encryption enables partially authorised brokers to implement contentbased routing based on the attributes that are accessible to them.", "keyphrases": ["secur publish/subscrib system", "distribut access control", "multipl administr domain", "attribut encrypt", "multi-domain", "overal commun overhead", "distribut system-distribut applic", "perform", "encrypt", "congest charg servic"]}
{"file_name": "C-29", "text": "Implementation and Performance Evaluation of CONFLEX-G : Grid-enabled Molecular Conformational Space Search Program with OmniRPC ABSTRACT CONFLEX-G is the grid-enabled version of a molecular conformational space search program called CONFLEX. We have implemented CONFLEX-G using a grid RPC system called OmniRPC. In this paper, we report the performance of CONFLEX-G in a grid testbed of several geographically distributed PC clusters. In order to explore many conformation of large bio-molecules, CONFLEX-G generates trial structures of the molecules and allocates jobs to optimize a trial structure with a reliable molecular mechanics method in the grid. OmniRPC provides a restricted persistence model to support the parametric search applications. In this model, when the initialization procedure is defined in the RPC module, the module is automatically initialized at the time of invocation by calling the initialization procedure. This can eliminate unnecessary communication and initialization at each call in CONFLEX-G. CONFLEXG can achieve performance comparable to CONFLEX MPI and can exploit more computing resources by allowing the use of a cluster of multiple clusters in the grid. The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times in the case of the 1BL1 molecule, where the molecule consists of a large number of atoms, and each trial structure optimization requires significant time. The load imbalance of the optimization time of the trial structure may also cause performance degradation. 1. INTRODUCTION Recently, the concept of the computational grid has begun to attract significant interest in the field of high-performance network computing. CONFLEX is one of the most efficient and reliable conformational space search programs -LSB- 1 -RSB-. We have applied this program to parallelization using global computing. The performance of the parallelized CONFLEX enables exploration of the lower-energy region of the conformational space of small peptides within an available elapsed time using a local PC cluster. Since trial structure optimization in CONFLEX is calculated via molecular mechanics, conformational space search can be performed quickly compared to that using molecular orbital calculation. Although the parallelized version of CONFLEX was used to calculate in parallel the structure optimization, which takes up over 90 % of the processing in the molecular conformation search, sufficient improvement in the speedup could not be achieved by this method alone. This requires the vast computer resources of a grid computing environment. In this paper, we describe CONFLEX-G, a grid-enabled molecular conformational search program, using OmniRPC and report its performance in a grid of several PC clusters which are geographically distributed. The prototype CONFLEX-G allocates calculation trial structures optimization, which is a very time-consuming task, to worker nodes in the grid environment in order to obtain high throughput. In addition, we compare the performance of CONFLEX-G in a local PC cluster to that in a grid testbed. OmniRPC -LSB- 2, 3, 4 -RSB- is a thread-safe implementation of Ninf RPC -LSB- 5, 6 -RSB- which is a Grid RPC facility for grid environment computing. Several systems adopt the concept of the RPC as the basic model for grid environment computing, including Ninf-G -LSB- 7 -RSB-, NetSolve -LSB- 8 -RSB- and CORBA -LSB- 9 -RSB-. The RPCstyle system provides an easy-to-use, intuitive programming interface, allowing users of the grid system to easily create grid-enabled applications. In order to support parallel programming, an RPC client can issue asynchronous call requests to a different remote computer to exploit networkwide parallelism via OmniRPC. In this paper, we propose the OmniRPC persistence model to a Grid RPC system and demonstrate its effectiveness. In order to support a typical application for a grid environment, such as a parametric search application, in which the same function is executed with different input parameters on the same data set. In the current GridRPC system -LSB- 10 -RSB-, the data set by the previous call can not be used by subsequent calls. This paper demonstrates that CONFLEX-G is able to exploit the huge computer resources of a grid environment and search large-scale molecular conformers. We demonstrate CONFLEX-G on our grid testbed using the actual protein as a sample molecule. The OmniRPC facility of the automatic initializable module -LRB- AIM -RRB- allows the system to efficiently calculate numerous conformers. Furthermore, by using OmniRPC, the user can grid-parallelize the existing application, and move from the cluster to the grid environment without modifying program code and compiling the program. In addition, the user can easily build a private grid environment. An overview Figure 1 : Algorithm of conformational space search in the original CONFLEX. of the CONFLEX system is presented in Section2, and the implementation and design of CONFLEX-G are described in Section 3. We report experimental results obtained using CONFLEX-G and discuss its performance in Section 4. In Section 6, we present conclusions and discuss subjects for future study. 5. RELATED WORK Recently, an algorithm has been developed that solves the problems of parallelization and communication in poorly connected processors to be used for simulation. This has allowed us to simulate folding for the first time and to directly examine folding related diseases. SETI@home[14] is a program to search for alien life by analyzing radio telescope signals using Fourier transform radio telescope data from telescopes from different sites. SETI@home tackles immensely parallel problems, in which calculation can easily be divided among several computers. Radio telescope data chunks can easily be assigned to different computers. However, the skills and effort required to develop a grid application may not be required for OmniRPC. Nimrod/G -LSB- 15 -RSB- is a tool for distributed parametric modeling and implements a parallel task farm for simulations that require several varying input parameters. Nimrod has been applied to applications including bio-informatics, operations research, and molecular modeling for drug design. NetSolve -LSB- 8 -RSB- is an RPC facility similar to OmniRPC and Ninf, providing a similar programming interface and automatic load balancing mechanism. Matsuoka et al. -LSB- 16 -RSB- has also discussed several design issues related to grid RPC systems. 6. CONCLUSIONS AND FUTURE WORK We have designed and implemented CONFLEX-G using OmniRPC. We reported its performance in a grid testbed of several geographically distributed PC clusters. In order to explore the conformation of large bio-molecules, CONFLEXG was used to generate trial structures of the molecules, and allocate jobs to optimize them by molecular mechanics in the grid. OmniRPC provides a restricted persistence model so that the module is automatically initialized at invocation by calling the initialization procedure. This can eliminate unnecessary communication and the initialization at each call in CONFLEX-G. CONFLEX-G can achieves performance comparable to CONFLEX MPI and exploits more computing resources by allowing the use of multiple PC clusters in the grid. The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times for the 1BL1 molecule, where the molecule consists of a large number of atoms and each trial structure optimization requires a great deal of time. The load imbalance of the trial structure optimizations may cause performance degradation. We need to refine the algorithm used to generate the trial structure in order to improve the load balance optimization for trial structures in CONFLEX. Future studies will include development of deployment tools and an examination of fault tolerance. In the current OmniRPC, the registration of an execution program to remote hosts and deployments of worker programs are manually set. Deployment tools will be required as the number of remote hosts is increased. In grid environments in which the environment changes dynamically, it is also necessary to support fault tolerance. This feature is especially important in large-scale applications which require lengthy calculation in a grid environment. We plan to refine the conformational optimization algorithm in CONFLEX to explore the conformation space search of larger bio-molecules such HIV protease using up to 1000 workers in a grid environment.", "keyphrases": ["conflex-g", "omnirpc", "conform space search", "bio-molecul", "rpc modul", "initi procedur", "mpu", "pc cluster", "grid comput", "grid rpc system", "molecular mechan", "automat initializ modul"]}
{"file_name": "C-9", "text": "EDAS : Providing an Environment for Decentralized Adaptive Services ABSTRACT As the idea of virtualisation of compute power, storage and bandwidth becomes more and more important, grid computing evolves and is applied to a rising number of applications. The environment for decentralized adaptive services -LRB- EDAS -RRB- provides a grid-like infrastructure for user-accessed, longterm services -LRB- e.g. webserver, source-code repository etc. -RRB-. It aims at supporting the autonomous execution and evolution of services in terms of scalability and resource-aware distribution. EDAS offers flexible service models based on distributed mobile objects ranging from a traditional clientserver scenario to a fully peer-to-peer based approach. Automatic, dynamic resource management allows optimized use of available resources while minimizing the administrative complexity. 1. INTRODUCTION Infrastructures for grid computing aim at virtualizing a group of computers, servers, and storage as one large computing system. Resource management is a key issue in such systems, needed for an efficient and automated distribution of tasks on the grid. Such grid infrastructures are often deployed at enterprise level, but projects like SETI@home -LSB- 1 -RSB- have demonstrated the feasibility of more decentralized grids as well. Current grid computing infrastructures do n't provide sufficient support for the execution of distributed, useraccessed, long-term services as they are designed to solve compute - or data-intensive tasks with a more or less fixed set of parameters. Instead an infrastructure for long-term services has to place services based on their current demand and their estimated future requirements. Migration however is expensive as the whole state of a service has to be transfered. Additionally a non-replicated service is not accessible during migration. Therefore the resource management has to avoid migration if possible. Furthermore a service concept has to be provided that evades overload in the first place, and secondly inhibits service unavailability if migration ca n't be avoided. EDAS -LSB- 2 -RSB- aims at providing a grid-like infrastructure for user-accessed, long-term services that allows the dynamic adaptation at run-time, provides a management infrastructure, and offers system-level support for scalability and fault tolerance. Nodes can dynamically join and leave the infrastructure, and all management tasks, especially the resource management, are decentralized. The environment is built upon our AspectIX -LSB- 3 -RSB- middleware infrastructure, which directly supports QoS-based, dynamic reconfiguration of services. The resource management focuses on the execution of services that have a long, potentially infinite, operating time. Theses services are organized in projects. Each project has a distributed execution scope called a service environment. Such an environment possibly spans multiple institutions. Each institution represents an administrative domain that can support a project with a fixed set of resources. Our approach supports the adaptive resource management of all projects in scope of an institution based on an algorithm inspired by the diffusive algorithms for decentralized loadbalancing -LSB- 4 -RSB-. It is not known how to optimally subdivide these resources for the services as the resource demand of services can change over time or even frequently fluctuate. To provide resources as needed, our approach automatically rededicates evenly free or not needed resources between service instances across projects and nodes. In cases where rededication is not possible, the migration of the demanding service is initiated. In a longterm-service grid infrastructure, active replication has various benefits : Replicas can join and leave the object group and therefore replicas can be migrated without service unavailability. Finally a certain amount of node crashes can be tolerated. Section 4 explains the self-managing and rededication concepts of distributed adaptive resource management. Section 5 describes the framework for decentralized adaptive services. Section 6 describes related work and finally Section 7 concludes the paper. 6. RELATED WORK Grid infrastructures like the Globus-Toolkit -LSB- 11 -RSB- provide services and mechanisms for distributed heterogeneous environments to combine resources on demand to solve resource consuming and compute intensive tasks. Due to this orientation they focus on different service models, provide no support for object mobility if even supporting a distributed object approach at all. But most important they follow a different resource management approach as they target the parallel execution of a large number of short and midterm tasks. Actively replicated objects are provided by Jgroup -LSB- 14 -RSB- based on RMI. On top of this basic middleware a replication management layer has been implemented called ARM -LSB- 15 -RSB-. JGroup focus on the active replication of objects but lacks support for more flexible services like EDAS does. ARM can be compared to EDAS but supports no resource aware distribution. Fog -LSB- 16 -RSB- and Globe -LSB- 17 -RSB- are basic middleware environments that support the fragmented object approach. Globe considers replication and caching. Both systems lack support for resource aware distribution. 7. CONCLUSION AND ONGOING WORK Based on the fragmented object model and the architecture of the EDAS environment, decentralized adaptive services can be easily designed, implemented and executed. As described the resource management can be decomposed in two main problems that have to be solved. Controlling and managing of resource limits including ensuring that the assigned resources are available -LRB- even in the context of node crashes -RRB- and the autonomous placement of services. For both problems we offer a solution, a currently implemented simulation environment will verify their feasibility. In a next step the resource management will be integrate in an already implemented prototype of the EDAS architecture. As described we have already an early implementation of the framework for the decentralized adaptive services. This framework has to be extended to smoothly interact with the resource management and the EDAS architecture. In a final step we need to implement some services that verify the usability of the whole EDAS project.", "keyphrases": ["decentr adapt servic", "resourc manag", "home environ", "infrastructur", "client", "long-term servic", "eda", "local limit", "global limit", "resourc", "node"]}
{"file_name": "H-14", "text": "Studying the Use of Popular Destinations to Enhance Web Search Interaction ABSTRACT We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity. 1. INTRODUCTION The problem of improving queries sent to Information Retrieval -LRB- IR -RRB- systems has been studied extensively in IR research -LSB- 4 -RSB- -LSB- 11 -RSB-. Alternative query formulations, known as query suggestions, can be offered to users following an initial query, allowing them to modify the specification of their needs provided to the system, leading to improved retrieval performance. Recent popularity of Web search engines has enabled query suggestions that draw upon the query reformulation behavior of many users to make query recommendations based on previous user interactions -LSB- 10 -RSB-. Leveraging the decision-making processes of many users for query reformulation has its roots in adaptive indexing -LSB- 8 -RSB-. However, interaction-based approaches to query suggestion may be less potent when the information need is exploratory, since a large proportion of user activity for such information needs may occur beyond search engine interactions. In cases where directed searching is only a fraction of users ' information-seeking behavior, the utility of other users ' clicks over the space of top-ranked results may be limited, as it does not cover the subsequent browsing behavior. At the same time, user navigation that follows search engine interactions provides implicit endorsement of Web resources preferred by users, which may be particularly valuable for exploratory search tasks. Thus, we propose exploiting a combination of past searching and browsing user behavior to enhance users ' Web search interactions. Browser plugins and proxy server logs provide access to the browsing patterns of users that transcend search engine interactions. In previous work, such data have been used to improve search result ranking by Agichtein et al. -LSB- 1 -RSB-. Radlinski and Joachims -LSB- 13 -RSB- have utilized such collective user intelligence to improve retrieval accuracy by using sequences of consecutive query reformulations, yet their approach does not consider users ' interactions beyond the search result page. In this paper, we present a user study of a technique that exploits the searching and browsing behavior of many users to suggest popular Web pages, referred to as destinations henceforth, in addition to the regular search results. The destinations may not be among the topranked results, may not contain the queried terms, or may not even be indexed by the search engine. Instead, they are pages at which other users end up frequently after submitting same or similar queries and then browsing away from initially clicked search results. We conjecture that destinations popular across a large number of users can capture the collective user experience for information needs, and our results support this hypothesis. In -LSB- 19 -RSB-, Wexelblat and Maes describe a system to support within-domain navigation based on the browse trails of other users. However, we are not aware of such principles being applied to Web search. Perhaps the nearest instantiation of teleportation is search engines ' offering of several within-domain shortcuts below the title of a search result. While these may be based on user behavior and possibly site structure, the user saves at most one click from this feature. In contrast, our proposed approach can transport users to locations many clicks beyond the search result, saving time and giving them a broader perspective on the available related information. The conducted user study investigates the effectiveness of including links to popular destinations as an additional interface feature on search engine result pages. We compare two variants of this approach against the suggestion of related queries and unaided Web search, and seek answers to questions on : -LRB- i -RRB- user preference and search effectiveness for known-item and exploratory search tasks, and -LRB- ii -RRB- the preferred distance between query and destination used to identify popular destinations from past behavior logs. The results indicate that suggesting popular destinations to users attempting exploratory tasks provides best results in key aspects of the information-seeking experience, while providing query refinement suggestions is most desirable for known-item tasks. In Section 2 we describe the extraction of search and browsing trails from user activity logs, and their use in identifying top destinations for new queries. Section 3 describes the design of the user study, while Sections 4 and 5 present the study findings and their discussion, respectively. 6. CONCLUSIONS We presented a novel approach for enhancing users ' Web search interaction by providing links to websites frequently visited by past searchers with similar information needs. A user study was conducted in which we evaluated the effectiveness of the proposed technique compared with a query refinement system and unaided Web search. Results of our study revealed that : -LRB- i -RRB- systems suggesting query refinements were preferred for known-item tasks, -LRB- ii -RRB- systems offering popular destinations were preferred for exploratory search tasks, and -LRB- iii -RRB- destinations should be mined from the end of query trails, not session trails. Overall, popular destination suggestions strategically influenced searches in a way not achievable by query suggestion approaches by offering a new way to resolve information problems, and enhance the informationseeking experience for many Web searchers.", "keyphrases": ["popular destin", "web search interact", "improv queri", "retriev perform", "relat queri", "inform-seek experi", "queri trail", "session trail", "lookup-base approach", "log-base evalu"]}
{"file_name": "I-20", "text": "Computing the Banzhaf Power Index in Network Flow Games ABSTRACT Preference aggregation is used in a variety of multiagent applications, and as a result, voting theory has become an important topic in multiagent system research. However, power indices -LRB- which reflect how much `` real power '' a voter has in a weighted voting system -RRB- have received relatively little attention, although they have long been studied in political science and economics. The Banzhaf power index is one of the most popular ; it is also well-defined for any simple coalitional game. In this paper, we examine the computational complexity of calculating the Banzhaf power index within a particular multiagent domain, a network flow game. Agents control the edges of a graph ; a coalition wins if it can send a flow of a given size from a source vertex to a target vertex. The relative power of each edge/agent reflects its significance in enabling such a flow, and in real-world networks could be used, for example, to allocate resources for maintaining parts of the network. We show that calculating the Banzhaf power index of each agent in this network flow domain is #P - complete. We also show that for some restricted network flow domains there exists a polynomial algorithm to calculate agents ' Banzhaf power indices. 1. INTRODUCTION What is the complexity of the process? Can complexity be used to guard against unwanted phenomena? Does complexity of computation prevent realistic implementation of a technique? The practical applications of voting among automated agents are already widespread. In fact, to see the generality of the -LRB- automated -RRB- voting scenario, consider modern web searching. In this paper, we consider a topic that has been less studied in the context of automated agent voting, namely power indices. A power index is a measure of the power that a subgroup, or equivalently a voter in a weighted voting environment, has over decisions of a larger group. The Banzhaf power index is one of the most popular measures of voting power, and although it has been used primarily for measuring power in weighted voting games, it is well-defined for any simple coalitional game. We look at some computational aspects of the Banzhaf power index in a specific environment, namely a network flow game. In this game, a coalition of agents wins if it can send a flow of size k from a source vertex s to a target vertex t, with the relative power of each edge reflecting its significance in allowing such a flow. We show that calculating the Banzhaf power index of each agent in this general network flow domain is #P - complete. nectivity games on bounded layer graphs -RRB-, there does exist a polynomial algorithm to calculate the Banzhaf power index of an agent. The paper proceeds as follows. In Section 2 we give some background concerning coalitional games and the Banzhaf power index, and in Section 3 we introduce our specific network flow game. In Section 4 we discuss the Banzhaf power index in network flow games, presenting our complexity result in the general case. In Section 5 we consider a restricted case of the network flow game, and present results. In Section 6 we discuss related work, and we conclude in Section 7. 6. RELATED WORK Measuring the power of individual players in coalitional games has been studied for many years. The most popular indices suggested for such measurement are the Banzhaf index -LSB- 1 -RSB- and the Shapley-Shubik index -LSB- 19 -RSB-. In his seminal paper, Shapley -LSB- 18 -RSB- considered coalitional games and the fair allocation of the utility gained by the grand coalition -LRB- the coalition of all agents -RRB- to its members. The Shapley-Shubik index -LSB- 19 -RSB- is the direct application of the Shapley value to simple coalitional games. The Banzhaf index emerged directly from the study of voting in decision-making bodies. The normalized Banzhaf index measures the proportion of coalitions in which a player is a swinger, out of all winning coalitions. This index is similar to the Banzhaf index discussed in Section 1, and is defined as : The Banzhaf index was mathematically analyzed in -LSB- 3 -RSB-, where it was shown that this normalization lacks certain desirable properties, and the more natural Banzhaf index is introduced. Both the Shapley-Shubik and the Banzhaf indices have been widely studied, and Straffin -LSB- 20 -RSB- has shown that each index reflects specific conditions in a voting body. -LSB- 11 -RSB- considers these two indices along with several others, and describes the axioms that characterize the different indices. The naive implementation of an algorithm for calculating the Banzhaf index of an agent i enumerates all coalitions containing i. There are 2n \u2212 1 such coalitions, so the performance is exponential in the number of agents. -LSB- 12 -RSB- contains a survey of algorithms for calculating power indices of weighted majority games. Deng and Papadimitriou -LSB- 2 -RSB- show that computing the Shapley value in weighted majority games is #P - complete, using a reduction from KNAPSACK. Since the Shapley value of any simple game has the same value as its Shapley-Shubik index, this shows that calculating the Shapley-Shubik index in weighted majority games is #Pcomplete. Matsui and Matsui -LSB- 13 -RSB- have shown that calculating both the Banzhaf and Shapley-Shubik indices in weighted voting games is NP-complete. The problem of computing power indices in simple games depends on the chosen representation of the game. Since the number of possible coalitions is exponential in the number of agents, calculating power indices in time polynomial in the number of agents can only be achieved in specific domains. In this paper, we have considered the network flow domain, where a coalition of agents must achieve a flow beyond a certain value. The network flow game we have defined is a simple game. -LSB- 10, 9 -RSB- have considered a similar network flow domain, where each agent controls an edge of a network flow graph. However, they introduced a non-simple game, where the value a coalition of agents achieves is the maximal total flow. They have shown that certain families of network flow games and similar games have nonempty cores. 7. CONCLUSIONS AND FUTURE DIRECTIONS We have considered network flow games, where a coalition of agents wins if it manages to send a flow of more than some value k between two vertices. We have assessed the relative power of each agent in this scenario using the Banzhaf index. This power index may be used to decide how to allocate maintenance resources in real-world networks, in order to maximize our ability to maintain a certain flow of information between two sites. Although the Banzhaf index theoretically allows us to measure the power of the agents in the network flow game, we have shown that the problem of calculating the Banzhaf index in this domain in #P - complete. Despite this discouraging result for the general network flow domain, we have also provided a more encouraging result for a restricted domain. In the case of connectivity games -LRB- where it is only required for a coalition to contain a path from the source to the destination -RRB- played on bounded layer graphs, it is possible to calculate the Banzhaf index of an agent in polynomial time. It remains an open problem to find ways to tractably approximate the Banzhaf index in the general network flow domain. It might also be possible to find other useful restricted domains where it is possible to exactly calculate the Banzhaf index. We have only considered the complexity of calculating the Banzhaf index ; it remains an open problem to find the complexity of calculating the Shapley-Shubik or other indices in the network flow domain. Finally, we believe that there are many additional interesting domains other than weighted voting games and network flow games, and it would be worthwhile to investigate the complexity of calculating the Banzhaf index or other power indices in such domains.", "keyphrases": ["prefer aggreg", "multiag applic", "vote theori", "banzhaf power index", "analysi of algorithm and problem complex", "social choic theori", "autom agent vote", "network flow game", "probabilist model", "connect game"]}
{"file_name": "J-7", "text": "The Role of Compatibility in the Diffusion of Technologies Through Social Networks ABSTRACT In many settings, competing technologies -- for example, operating systems, instant messenger systems, or document formats -- can be seen adopting a limited amount of compatibility with one another ; in other words, the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability. There are a range of reasons why this phenomenon occurs, many of which -- based on legal, social, or business considerations -- seem to defy concise mathematical models. Despite this, we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks, thus offering a basic explanation for this phenomenon in purely strategic terms. Our approach builds on work on the diffusion of innovations in the economics literature, which seeks to model how a new technology A might spread through a social network of individuals who are currently users of technology B. We consider several ways of capturing the compatibility of A and B, focusing primarily on a model in which users can choose to adopt A, adopt B, or -- at an extra cost -- adopt both A and B. We characterize how the ability of A to spread depends on both its quality relative to B, and also this additional cost of adopting both, and find some surprising non-monotonicity properties in the dependence on these parameters : in some cases, for one technology to survive the introduction of another, the cost of adopting both technologies must be balanced within a narrow, intermediate range. We also extend the framework to the case of multiple technologies, where we find that a simple This work has been supported in part by NSF grants CCF0325453, IIS-0329064, CNS-0403340, and BCS-0537606, a Google Research Grant, a Yahoo! Research Alliance Grant, the Institute for the Social Sciences at Cornell, and the John D. and Catherine T. MacArthur Foundation. model captures the phenomenon of two firms adopting a limited `` strategic alliance '' to defend against a new, third technology. 1. INTRODUCTION Diffusion and Networked Coordination Games. Such issues arise, for example, in the adoption of new technologies, the emergence of new social norms or organizational conventions, or the spread of human languages -LSB- 2, 14, 15, 16, 17 -RSB-. An active line of research in economics and mathematical sociology is concerned with modeling these types of diffusion processes as a coordination game played on a social network -LSB- 1, 5, 7, 13, 19 -RSB-. We begin by discussing one of the most basic game-theoretic diffusion models, proposed in an influential paper of Morris -LSB- 13 -RSB-, which will form the starting point for our work here. We describe it in terms of the following technology adoption scenario, though there are many other examples that would serve the same purpose. Note that A is the `` better '' technology if q < 21, in the sense that A-A payoffs would then exceed B-B payoffs, while A is the worse technology if q > 21. A number of qualitative insights can be derived from a diffusion model even at this level of simplicity. Specifically, consider a network G, and let all nodes initially play B. Now suppose a small number of nodes begin adopting strategy A instead. Compatibility, Interoperability, and Bilinguality. An important piece that is arguably missing from the basic game-theoretic models of diffusion, however, is a more detailed picture of what is happening at the coexistence boundary, where the basic form of the model posits nodes that adopt A linked to nodes that adopt B. In these motivating settings for the models, of course, one very often sees interface regions in which individuals essentially become `` bilingual. '' In the case of human language diffusion, this bilinguality is meant literally : geographic regions where there is substantial interaction with speakers of two different languages tend to have inhabitants who speak both. Taking this view, it is natural to ask how diffusion models behave when extended so that certain nodes can be bilingual in this very general sense, adopting both strategies at some cost to themselves. What might we learn from such an extension? To begin with, it has the potential to provide a valuable perspective on the question of compatibility and incompatibility that underpins competition among technology companies. There is a large literature on how compatibility among technologies affects competition between firms, and in particular how incompatibility may be a beneficial strategic decision for certain participants in a market -LSB- 3, 4, 8, 9, 12 -RSB-. While these existing models of compatibility capture network effects in the sense that the users in the market prefer to use technology that is more widespread, they do not capture the more finegrained network phenomenon represented by diffusion -- that each user is including its local view in the decision, based on what its own social network neighbors are doing. A diffusion model that incorporated such extensions could provide insight into the structure of boundaries in the network between technologies ; it could potentially offer a graph-theoretic basis for how incompatibility may benefit an existing technology, by strengthening these boundaries and preventing the incursion of a new, better technology. The present work : Diffusion with bilingual behavior. In this paper, we develop a set of diffusion models that incorporate notions of compatibility and bilinguality, and we find that some unexpected phenomena emerge even from very simple versions of the models. We begin with perhaps the simplest way of extending Morris 's model discussed above to incorporate bilingual behavior. Consider again the example of IM systems A and B, with the payoff structure as before, but now suppose that each node can adopt a third strategy, denoted AB, in which it decides to use both A and B. Finally, an adopter of AB pays a fixed-cost penalty of c -LRB- i.e. -- c is added to its total payoff -RRB- to represent the cost of having to maintain both technologies. Thus, in this model, there are two parameters that can be varied : the relative qualities of the two technologies -LRB- encoded by q -RRB-, and the cost of being bilingual, which reflects a type of incompatibility -LRB- encoded by c -RRB-. We also introduce one additional bit of notation that will be useful in the subsequent sections : we define r = c / \u0394, the fixed penalty for adopting AB, scaled so that it is a per-edge cost. In the Morris model, where the only strategic options are A and B, a key parameter is the contagion threshold of G, denoted q \u2217 -LRB- G -RRB- : this is the supremum of q for which A can become epidemic in G with parameter q in the payoff structure. A central result of -LSB- 13 -RSB- is that 21 is the maximum possible contagion threshold for any graph : supG q \u2217 -LRB- G -RRB- = 21. Indeed, there exist graphs in which the contagion threshold is as large as 21 -LRB- including the infinite line -- the unique infinite connected 2-regular graph -RRB- ; on the other hand, one can show there is no graph with a contagion threshold greater than Figure 1 : The region of the -LRB- q, r -RRB- plane for which technology A can become epidemic on the infinite line. Our Results. -LRB- We find analogous shapes that become even more complex for other simple infinite graph structures ; see for example Figures 3 and 4. -RRB- In particular, this means that for values of q close to but less than 21, strategy A can become epidemic on the infinite line if r is sufficiently small or sufficiently large, but not if r takes values in some intermediate interval. In other words, strategy B -LRB- which represents the worse technology, since q < 21 -RRB- will survive if and only if the cost of being bilingual is calibrated to lie in this middle interval. This is a reflection of limited compatibility -- that it may be in the interest of an incumbent technology to make it difficult but not too difficult to use a new technology -- and we find it surprising that it should emerge from a basic model on such a simple network structure. It is natural to ask whether there is a qualitative interpretation of how this arises from the model, and in fact it is not hard to give such an interpretation, as follows. When r is very small, it is cheap for nodes to adopt AB as a strategy, and so AB spreads through the whole network. Once AB is everywhere, the best-response updates cause all nodes to switch to A, since they get the same interaction benefits without paying the penalty of r. When r is very large, nodes at the interface, with one A neighbor and one B neighbor, will find it too expensive to choose AB, so they will choose A -LRB- the better technology -RRB-, and hence A will spread step-by-step through the network. When r takes an intermediate value, a node v at the interface, with one A neighbor and one B neighbor, will find it most beneficial to adopt AB as a strategy. Hence, this intermediate value of r allows a `` boundary '' of AB to form between the adopters of A and the adopters of B. But if it has the right balance in the value of r, then the adoptions of A come to a stop at a bilingual boundary where nodes adopt AB. Moving beyond specific graphs G, we find that this non-convexity holds in a much more general sense as well, by considering the general epidemic region \u03a9 = UG\u03a9 -LRB- G -RRB-. For any given value of \u0394, the region \u03a9 is a complicated union of bounded and unbounded polygons, and we do not have a simple closed-form description for it. However, we can show via a potential function argument that no point -LRB- q, r -RRB- with q > 21 belongs to \u03a9. Moreover, we can show the existence of a point -LRB- q, r -RRB- E ~ \u03a9 for which q < 21. On the other hand, consideration of the epidemic region for the infinite line shows that -LRB- 21, r -RRB- E \u03a9 for r = 0 and for r sufficiently large. Hence, neither \u03a9 nor its complement is convex in the positive quadrant. Finally, we also extend a characterization that Morris gave for the contagion threshold -LSB- 13 -RSB-, producing a somewhat more intricate characterization of the region \u03a9 -LRB- G -RRB-. In Morris 's setting, without an AB strategy, he showed that A can not become epidemic with parameter q if and only if every cofinite set of nodes contains a subset S that functions as a well-connected `` community '' : every node in S has at least a -LRB- 1 -- q -RRB- fraction of its neighbors in S. In other words, tightly-knit communities are the natural obstacles to diffusion in his setting. With the AB strategy as a further option, a more complex structure becomes the obstacle : we show that A can not become epidemic with parameters -LRB- q, r -RRB- if and only if every cofinite set contains a structure consisting of a tightly-knit community with a particular kind of `` interface '' of neighboring nodes. We show that such a structure allows nodes to adopt AB at the interface and B inside the community itself, preventing the further spread of A ; and conversely, this is the only way for the spread of A to be blocked. Further Extensions. Another way to model compatibility and interoperability in diffusion models is through the `` off-diagonal '' terms representing the payoff for interactions between a node adopting A and a node adopting B. Rather than setting these to 0, we can consider setting them to a value x < min -LRB- q, 1 -- q -RRB-. We find that for the case of two technologies, the model does not become more general, in that any such instance is equivalent, by a re-scaling of q and r, to one where x = 0. Moreover, using our characterization of the region \u03a9 -LRB- G -RRB- in terms of communities and interfaces, we show a monotonicty result : if A can become epidemic on a graph G with parameters -LRB- q, r, x -RRB-, and then x is increased, then A can still become epidemic with the new parameters. We also consider the effect of these off-diagonal terms in an extension to k > 2 competing technologies ; for technologies X and Y, let qX denote the payoff from an X-X interaction on an edge and qXY denote the payoff from an X-Y interaction on an edge. We consider a setting in which two technologies B and C, which initially coexist with qBC = 0, face the introduction of a third, better technology A at a finite set of nodes. We show an example in which B and C both survive in equilibrium if they set qBC in a particular range of values, but not if they set qBC too low or too high to lie in this range. Thus, in even in a basic diffusion model with three technologies, one finds cases in which two firms have an incentive to adopt a limited `` strategic alliance, '' partially increasing their interoperability to defend against a new entrant in the market. 6. LIMITED COMPATIBILITY We now consider some further ways of modeling compatibility and interoperability. We first consider two technologies, as in the previous sections, and introduce `` off-diagonal '' payoffs to capture a positive benefit in direct A-B interactions. We find that this is in fact no more general than the model with zero payoffs for A-B interactions. We then consider extensions to three technologies, identifying situations in which two coexisting incumbent technologies may or may not want to increases their mutual compatibility in the face of a new, third technology. Two technologies. A natural relaxation of the two-technology model is to introduce -LRB- small -RRB- positive payoffs for A-B interaction ; that is, cross-technology communication yields some lesser value to both agents. We can model this using a variable xAB representing the payoff gathered by an agent with technology A when her neighbor has technology B, and similarly, a variable xBA representing the payoff gathered by an agent with B when her neighbor has A. Here we consider the special case in which these `` off-diagonal '' entries are symmetric, i.e., xAB = xBA = x. We also assume that x < q < 1 -- q. We first show that the game with off-diagonal entries is equivalent to a game without these entries, under a simple re-scaling of q and r. Note that if we re-scale all payoffs by either an additive or a multiplicative constant, the behavior of the game is unaffected. Given a game with off-diagonal entries parameterized by q, r and x, consider subtracting x from all payoffs, and scaling up by a factor of 1 / -LRB- 1 -- 2x -RRB-. As can be seen by examining Table 1, the resulting payoffs are exactly those of a game without off-diagonal entries, parameterized by q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- and r ' = r / -LRB- 1 -- 2x -RRB-. Thus the addition of symmetric off-diagonal entries does not expand the class of games being considered. Table 1 represents the payoffs in the coordination game in terms of these parameters. Nevertheless, we can still ask how the addition of an off-diagonal entry might affect the outcome of any particular game. As the following example shows, increasing compatibility between two technologies can allow one technology that was not initially epidemic to become so. EXAMPLE 6.1. Consider the contagion game played on a thick line graph -LRB- see Section 3 -RRB- with r = 5/32 and q = 3/8. In this case, A is not epidemic, as can be seen by examining Figure 1, since 2r < q and q + r > 1/2. However, if we insert symmetric off-diagonal payoffs x = 1/4, we have a new game, equivalent to a game parameterized by r ' = 5/16 and q ' = 1/4. Since q ' < 1/2 and q ' < 2r ', A is epidemic in this game, and thus also in the game with limited compatibility. We now show that generally, if A is the superior technology -LRB- i.e., q < 1/2 -RRB-, adding a compatibility term x can only help A spread. THEOREM 6.2. Let G be a game without compatibility, parameterized by r and q on a particular network. Let G ' be that same game, but with an added symmetric compatibility term x. If A is epidemic for G, then A is epidemic for G '. PROOF. We will show that any blocking structure in G ' is also a blocking structure in G. By our characterization theorem, Theorem 4.6, this implies the desired result. We have that G ' is equivalent to a game without compatibility parameterized by q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- and r ' = r / -LRB- 1 -- 2x -RRB-. Consider a blocking structure -LRB- SB, SAB -RRB- for G '. Thus More than two technologies. Given the complex structure inherent in contagion games with two technologies, the understanding of contagion games with three or more technologies is largely open. Here we indicate some of the technical issues that come up with multiple technologies, through a series of initial results. The basic set-up we study is one in which two incumbent technologies B and C are initially coexisting, and a third technology A, superior to both, is introduced initially at a finite set of nodes. We first present a theorem stating that for any even \u0394, there is a contagion game on a \u0394 -- regular graph in which the two incumbent technologies B and C may find it beneficial to increase their compatibility so as to prevent getting wiped out by the new superior technology A. In particular, we consider a situation in which initially, two technologies B and C with zero compatibility are at a stable state. By a stable state, we mean that no finite perturbation of the current states can lead to an epidemic for either B or C. We also have a technology A that is superior to both B and C, and can become epidemic by forcing a single node to choose A. However, by increasing their compatibility, B and C can maintain their stability and resist an epidemic from A. Let qA denote the payoffs to two adjacent nodes that both choose technology A, and define qB and qC analogously. We will assume qA > qB > qC. We also assume that r, the cost of selecting additional technologies, is sufficiently large so as to ensure that nodes never adopt more than one technology. Finally, we consider a compatibility parameter qBC that represents the payoffs to two adjacent nodes when one selects B and the other selects C. Thus our contagion game is now described by five parameters -LRB- G, qA, qB, qC, qBC -RRB-. PROOF. -LRB- Sketch. -RRB- Given \u0394, define G by starting with an infinite grid and connecting each node to its nearest \u0394 -- 2 neighbors that are in the same row. The initial state s assigns strategy B to even rows and strategy C to odd rows. The first, third, and fourth claims in the theorem can be verified by checking the corresponding inequalities. The second claim follows from the first and the observation that the alternating rows contain any plausible epidemic from growing vertically. The above theorem shows that two technologies may both be able to survive the introduction of a new technology by increasing their level of compatibility with each other. As one might expect, Table 1 : The payoffs in the coordination game. Entry -LRB- x, y -RRB- in row i, column j indicates that the row player gets a payoff of x and the column player gets a payoff of y when the row player plays strategy i and the column player plays strategy j. there are cases when increased compatibility between two technologies helps one technology at the expense of the other. Surprisingly, however, there are also instances in which compatibility is in fact harmful to both parties ; the next example considers a fixed initial configuration with technologies A, B and C that is at equilibrium when qBC = 0. However, if this compatibility term is increased sufficiently, equilibrium is lost, and A becomes epidemic. EXAMPLE 6.4. Consider the union of an infinite two-dimensional grid graph with nodes u -LRB- x, y -RRB- and an infinite line graph with nodes v -LRB- y -RRB-. Add an edge between u -LRB- 1, y -RRB- and v -LRB- y -RRB- for all y. For this network, we consider the initial configuration in which all v -LRB- y -RRB- nodes select A, and node u -LRB- x, y -RRB- selects B if x < 0 and selects C otherwise. We now define the parameters of this game as follows. It is easily verified that for these values, the initial configuration given above is an equilibrium. However, now suppose we increase the coordination term, setting qBC = 0.9. This is not an equilibrium, since each node of the form u -LRB- 0, y -RRB- now has an incentive to switch from C -LRB- generating a payoff of 3.9 -RRB- to B -LRB- thereby generating a payoff of 3.95 -RRB-. However, once these nodes have adopted B, the best-response for each node of the form u -LRB- 1, y -RRB- is A -LRB- A generates a payoff of 4 where as B only generates a payoff of 3.95 -RRB-. From here, it is not hard to show that A spreads directly throughout the entire network.", "keyphrases": ["diffus process", "game-theoret diffus model", "strateg incompat", "bilingu", "limit compat", "interoper", "non-convex properti", "character", "morri's theorem", "contagion threshold", "contagion game", "potenti function"]}
{"file_name": "I-34", "text": "Resolving Conflict and Inconsistency in Norm-Regulated Virtual Organizations ABSTRACT Norm-governed virtual organizations define, govern and facilitate coordinated resource sharing and problem solving in societies of agents. With an explicit account of norms, openness in virtual organizations can be achieved : new components, designed by various parties, can be seamlessly accommodated. We focus on virtual organizations realised as multi-agent systems, in which human and software agents interact to achieve individual and global goals. However, any realistic account of norms should address their dynamic nature : norms will change as agents interact with each other and their environment. Due to the changing nature of norms or due to norms stemming from different virtual organizations, there will be situations when an action is simultaneously permitted and prohibited, that is, a conflict arises. Likewise, there will be situations when an action is both obliged and prohibited, that is, an inconsistency arises. We introduce an approach, based on first-order unification, to detect and resolve such conflicts and inconsistencies. In our proposed solution, we annotate a norm with the set of values their variables should not have in order to avoid a conflict or an inconsistency with another norm. Our approach neatly accommodates the domain-dependent interrelations among actions and the indirect conflicts/inconsistencies these may cause. More generally, we can capture a useful notion of inter-agent -LRB- and inter-role -RRB- delegation of actions and norms associated to them, and use it to address conflicts/inconsistencies caused by action delegation. We illustrate our approach with an e-Science example in which agents support Grid services. 1. INTRODUCTION Virtual organizations -LRB- VOs -RRB- facilitate coordinated resource sharing and problem solving involving various parties geographically remote -LSB- 9 -RSB-. VOs define and regulate interactions -LRB- thus facilitating coordination -RRB- among software and/or human agents that communicate to achieve individual and global goals -LSB- 16 -RSB-. VOs are realised as multi-agent systems and a most desirable feature of such systems is openness whereby new components designed by other parties are seamlessly accommodated. Norms regulate the observable behaviour of self-interested, heterogeneous software agents, designed by various parties who may not entirely trust each other -LSB- 3, 24 -RSB-. However, norm-regulated VOs may experience problems when norms assigned to their agents are in conflict -LRB- i.e., an action is simultaneously prohibited and permitted -RRB- or inconsistent -LRB- i.e., an action is simultaneously prohibited and obliged -RRB-. We propose a means to automatically detect and solve conflict and inconsistency in norm-regulated VOs. We make use of firstorder term unification -LSB- 8 -RSB- to find out if and how norms overlap in their influence -LRB- i.e., the agents and values of parameters in agents ' actions that norms may affect -RRB-. This allows for a fine-grained solution whereby the influence of conflicting or inconsistent norms is curtailed for particular sets of values. For instance, norms `` agent x is permitted to send bid -LRB- ag1, 20 -RRB- '' and `` agent ag2 is prohibited from doing send bid -LRB- y, z -RRB- '' -LRB- where x, y, z are variables and ag1, ag2, 20 are constants -RRB- are in conflict because their agents, actions and terms -LRB- within the actions -RRB- unify. We solve the conflict by annotating norms with sets of values their variables can not have, thus curtailing their influence. In our example, the conflict is avoided if we require that variable y can not be ag1 and that z can not be 20. In the next section we provide a minimalistic definition for norm-regulated VOs. In section 3 we formally define norm conflicts, and explain how they are detected and resolved. In section 4 we describe how the machinery of the previous section can be adapted to detect and resolve norm inconsistencies. In section 5 we describe how our curtailed norms are used in norm-aware agent societies. In section 6 we explain how our machinery can be used to detect and solve indirect conflicts/inconsistencies, that is, those caused via relationships among actions ; we extend and adapt the machinery to accommodate the delegation of norms. In section 7 we illustrate our approach with an example of norm-regulated software agents serving the Grid.", "keyphrases": ["virtual organ", "multi-agent system", "norm-regul vo", "agent", "norm conflict", "conflict prohibit", "norm inconsist", "extern agent", "governor agent"]}
{"file_name": "J-31", "text": "Computing the Optimal Strategy to Commit to \u2217 ABSTRACT In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously. The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models -LRB- with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position -RRB-. In this paper, we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. We give both positive results -LRB- efficient algorithms -RRB- and negative results -LRB- NP-hardness results -RRB-. 1. INTRODUCTION In multiagent systems with self-interested agents -LRB- including most economic settings -RRB-, the optimal action for one agent to take depends on the actions that the other agents take. To analyze how an agent should behave in such settings, the tools of game theory need to be applied. Typically, when a strategic setting is modeled in the framework of game theory, it is assumed that players choose their strategies simultaneously. This is especially true when the setting is modeled as a normal-form game, which only specifies each agent 's utility as a function of the vector of strategies that the agents choose, and does not provide any information on the order in which agents make their decisions and what the agents observe about earlier decisions by other agents. Given that the game is modeled in normal form, it is typically analyzed using the concept of Nash equilibrium. A Nash equilibrium specifies a strategy for each player, such that no player has an incentive to individually deviate from this profile of strategies. -LRB- Typically, the strategies are allowed to be mixed, that is, probability distributions over the original -LRB- pure -RRB- strategies. -RRB- A -LRB- mixed-strategy -RRB- Nash equilibrium is guaranteed to exist in finite games -LSB- 18 -RSB-, but one problem is that there may be multiple Nash equilibria. This leads to the equilibrium selection problem of how an agent can know which strategy to play if it does not know which equilibrium is to be played. When the setting is modeled as an extensive-form game, it is possible to specify that some players receive some information about actions taken by others earlier in the game before deciding on their action. Nevertheless, in general, the players do not know everything that happened earlier in the game. Because of this, these games are typically still analyzed using an equilibrium concept, where one specifies a mixed strategy for each player, and requires that each player 's strategy is a best response to the others ' strategies. -LRB- Typically an additional constraint on the strategies is now imposed to ensure that players do not play in a way that is irrational with respect to the information that they have received so far. This leads to refinements of Nash equilibrium such as subgame perfect and sequential equilibrium. -RRB- However, in many real-world settings, strategies are not selected in such a simultaneous manner. Oftentimes, one player -LRB- the leader -RRB- is able to commit to a strategy before another player -LRB- the follower -RRB-. This can be due to a variety of reasons. For example, one of the players may arrive at the site at which the game is to be played before another agent -LRB- e.g., in economic settings, one player may enter a market earlier and commit to a way of doing busi ness -RRB-. Such commitment power has a profound impact on how the game should be played. For example, the leader may be best off playing a strategy that is dominated in the normal-form representation of the game. In general, if commitment to mixed strategies is possible, then -LRB- under minor assumptions -RRB- it never hurts, and often helps, to commit to a strategy -LSB- 26 -RSB-. Being forced to commit to a pure strategy sometimes helps, and sometimes hurts -LRB- for example, committing to a pure strategy in rock-paper-scissors before the other player 's decision will naturally result in a loss -RRB-. In this paper, we will assume commitment is always forced ; if it is not, the player who has the choice of whether to commit can simply compare the commitment outcome to the non-commitment -LRB- simultaneous-move -RRB- outcome. Models of leadership are especially important in settings with multiple self-interested software agents. Once the code for an agent -LRB- or for a team of agents -RRB- is finalized and the agent is deployed, the agent is committed to playing the -LRB- possibly randomized -RRB- strategy that the code prescribes. Finally, there is also an implicit leadership situation in the field of mechanism design, in which one player -LRB- the designer -RRB- gets to choose the rules of the game that the remaining players then play. Indeed, the mechanism designer may benefit from committing to a choice that, if the -LRB- remaining -RRB- agents ' actions were fixed, would be suboptimal. However, the computation of the optimal strategy to commit to in a leadership situation has gone ignored. Theoretically, leadership situations can simply be thought of as an extensive-form game in which one player chooses a strategy -LRB- for the original game -RRB- first. The number of strategies in this extensive-form game, however, can be exceedingly large. For example, if the leader is able to commit to a mixed strategy in the original game, then every one of the -LRB- continuum of -RRB- mixed strategies constitutes a pure strategy in the extensive-form representation of the leadership situation. -LRB- We note that a commitment to a distribution is not the same as a distribution over commitments. -RRB- Moreover, if the original game is itself an extensive-form game, the number of strategies in the extensive-form representation of the leadership situation -LRB- which is a different extensive-form game -RRB- becomes even larger. Because of this, it is usually not computationally feasible to simply transform the original game into the extensive-form representation of the leadership situation ; instead, we have to analyze the game in its original representation. In this paper, we study how to compute the optimal strategy to commit to, both in normal-form games -LRB- Section 2 -RRB- and in Bayesian games, which are a special case of extensiveform games -LRB- Section 3 -RRB-. 4. CONCLUSIONS AND FUTURE RESEARCH In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. This requires some equilibrium notion -LRB- Nash equilibrium and its refinements -RRB-, and often leads to the equilibrium selection problem : it is unclear to each individual player according to which equilibrium she should play. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. For example, one agent may arrive at the -LRB- real or virtual -RRB- site of the game before the other, or, in the specific case of software agents, the code for one agent may be completed and committed before that of another agent. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously. Specifically, if commitment to mixed strategies is possible, then -LRB- optimal -RRB- commitment never hurts the leader, and often helps. The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models -LRB- with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position -RRB-. In this paper, we studied how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. For normal-form games, we showed that the optimal pure strategy to commit to can be found efficiently for any number of players. An optimal mixed strategy to commit to in a normal-form game can be found efficiently for two players using linear programming -LRB- and no more efficiently than that, in the sense that any linear program with a probability constraint can be encoded as such a problem -RRB-. -LRB- This is a generalization of the polynomial-time computability of minimax strategies in normal-form games. -RRB- The problem becomes NP-hard for three -LRB- or more -RRB- players. In Bayesian games, the problem of finding an optimal pure strategy to commit to is NP-hard even in two-player games in which the follower has only a single type, although two-player games in which the leader has only a single type can be solved efficiently. The problem of finding an optimal mixed strategy to commit to in a Bayesian game is NP-hard even in two-player games in which the leader has only a single type, although two-player games in which the follower has only a single type can be solved efficiently using a generalization of the linear progamming approach for normal-form games. The following two tables summarize these results. Results for commitment to mixed strategies. -LRB- With more than 2 players, the `` follower '' is the last player to commit, the `` leader '' is the first. -RRB- Future research can take a number of directions. We can also study the computation of optimal strategies to commit to in other1 concise representations of normal-form games -- for example, in graphical games -LSB- 10 -RSB- or local-effect/action graph games -LSB- 14, 1 -RSB-. For the cases where computing an optimal strategy to commit to is NP-hard, we can also study the computation of approximately optimal strategies to commit to. One may also study models in which multiple -LRB- but not all -RRB- players commit at the same time. Another interesting direction to pursue is to see if computing optimal mixed strategies to commit to can help us in, or otherwise shed light on, computing Nash equilibria. Often, optimal mixed strategies to commit to are also Nash equilibrium strategies -LRB- for example, in two-player zero-sum games this is always true -RRB-, although this is not always the case -LRB- for example, as we already pointed out, sometimes the optimal strategy to commit to is a strictly dominated strategy, which can never be a Nash equilibrium strategy -RRB-.", "keyphrases": ["optim strategi", "multiag system", "simultan manner", "stackelberg model", "leadership model", "pure strategi", "mix strategi", "normal-form game", "bayesian game", "nash equilibrium", "np-hard"]}
{"file_name": "H-13", "text": "The Influence of Caption Features on Clickthrough Patterns in Web Search ABSTRACT Web search engines present lists of captions, comprising title, snippet, and URL, to help users decide which search results to visit. Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation. In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions. The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms, the readability of the snippet, and the length of the URL shown in the caption, can significantly influence users ' Web search behavior. 1. INTRODUCTION The major commercial Web search engines all present their results in much the same way. Each search result is described by a brief caption, comprising the URL of the associated Web page, a title, and a brief summary -LRB- or `` snippet '' -RRB- describing the contents of the page. Often the snippet is extracted from the Web page itself, but it may also be taken from external sources, such as the human-generated summaries found in Web directories. Figure 1 shows a typical Web search, with captions for the top three results. While the three captions share the same The snippet of the third caption is nearly twice as long as that of the first, while the snippet is missing entirely from the second caption. The title of the third caption contains all of the query terms in order, while the titles of the first and second captions contain only two of the three terms. One of the query terms is repeated in the first caption. All of the query terms appear in the URL of the third caption, while none appear in the URL of the first caption. While these differences may seem minor, they may also have a substantial impact on user behavior. A principal motivation for providing a caption is to assist the user in determining the relevance of the associated page without actually having to click through to the result. In the case of a navigational query -- particularly when the destination is well known -- the URL alone may be sufficient to identify the desired page. But in the case of an informational query, the title and snippet may be necessary to guide the user in selecting a page for further study, and she may judge the relevance of a page on the basis of the caption alone. When this judgment is correct, it can speed the search process by allowing the user to avoid unwanted material. When it fails, the user may waste her time clicking through to an inappropriate result and scanning a page containing little or nothing of interest. Even worse, the user may be misled into skipping a page that contains desired information. All three of the results in figure 1 are relevant, with some limitations. The first result links to the main Yahoo Kids! homepage, but it is then necessary to follow a link in a menu to find the main page for games. Despite appearances, the second result links to a surprisingly large collection of online games, primarily with environmental themes. Unfortunately, these page characteristics are not entirely reflected in the captions. In this paper, we examine the influence of caption features on user 's Web search behavior, using clickthroughs extracted from search engines logs as our primary investigative tool. Understanding this influence may help to validate algorithms and guidelines for the improved generation of the Figure 1 : Top three results for the query : kids online games. captions themselves. In addition, these features can play a role in the process of inferring relevance judgments from user behavior -LSB- 1 -RSB-. By better understanding their influence, better judgments may result. Different caption generation algorithms might select snippets of different lengths from different areas of a page. Snippets may be generated in a query-independent fashion, providing a summary of the page as a whole, or in a querydependent fashion, providing a summary of how the page relates to the query terms. The correct choice of snippet may depend on aspects of both the query and the result page. For links that re-direct, it may be possible to display alternative URLs. Moreover, for pages listed in human-edited Web directories such as the Open Directory Project ', it may be possible to display alternative titles and snippets derived from these listings. When these alternative snippets, titles and URLs are available, the selection of an appropriate combination for display may be guided by their features. A snippet from a Web directory may consist of complete sentences and be less fragmentary than an extracted snippet. A title extracted from the body may provide greater coverage of the query terms. The work reported in this paper was undertaken in the context of the Windows Live search engine. The experiments reported in later sections are based on Windows Live query logs, result pages and relevance judgments collected as part of ongoing research into search engine performance -LSB- 1, 2 -RSB-. Nonetheless, given the similarity of caption formats across the major Web search engines we believe the results are applicable to these other engines. The query in ` www.dmoz.org figure 1 produces results with similar relevance on the other major search engines. This and other queries produce captions that exhibit similar variations. In addition, we believe our methodology may be generalized to other search applications when sufficient clickthrough data is available. 2. RELATED WORK While commercial Web search engines have followed similar approaches to caption display since their genesis, relatively little research has been published about methods for generating these captions and evaluating their impact on user behavior. Most research on the display of Web results has proposed substantial interface changes, rather than addressing details of the existing interfaces. 2.1 Display of Web results Varadarajan and Hristidis -LSB- 16 -RSB- are among the few who have attempted to improve directly upon the snippets generated by commercial search systems, without introducing additional changes to the interface. They generated snippets from spanning trees of document graphs and experimentally compared these snippets against the snippets generated for the same documents by the Google desktop search system and MSN desktop search system. They evaluated their method by asking users to compare snippets from the various sources. 6. CONCLUSIONS Clickthrough inversions form an appropriate tool for assessing the influence of caption features. Using clickthrough inversions, we have demonstrated that relatively simple caption features can significantly influence user behavior. To our knowledge, this is first methodology validated for assessing the quality of Web captions through implicit feedback. We also hope to directly address the goal of predicting relevance from clickthoughs and other information present in search engine logs.", "keyphrases": ["clickthrough pattern", "caption featur", "web search behavior", "human factor", "extract summar", "snippet", "queri log", "queri re-formul", "signific word", "clickthrough invers", "queri term match"]}
{"file_name": "I-5", "text": "Towards Self-organising Agent-based Resource Allocation in a Multi-Server Environment ABSTRACT Distributed applications require distributed techniques for efficient resource allocation. These techniques need to take into account the heterogeneity and potential unreliability of resources and resource consumers in a distributed environments. In this paper we propose a distributed algorithm that solves the resource allocation problem in distributed multiagent systems. Our solution is based on the self-organisation of agents, which does not require any facilitator or management layer. The resource allocation in the system is a purely emergent effect. We present results of the proposed resource allocation mechanism in the simulated static and dynamic multi-server environment. 1. INTRODUCTION In this sense, each agent is a resource consumer that acquires a certain amount of resources for the execution of its tasks. It is difficult for a central resource allocation mechanism to collect and manage the information about all shared resources and resource consumers to effectively perform the allocation of resources. Hence, distributed solutions of the resource allocation problem are required. Researchers have recognised these requirements -LSB- 10 -RSB- and proposed techniques for distributed resource allocation. A promising kind of such distributed approaches are based on economic market models -LSB- 4 -RSB-, inspired by principles of real stock markets. Even if those approaches are distributed, they usually require a facilitator for pricing, resource discovery and dispatching jobs to resources -LSB- 5, 9 -RSB-. Another mainly unsolved problem of those approaches is the fine-tuning of price and time, budget constraints to enable efficient resource allocation in large, dynamic systems -LSB- 22 -RSB-. In this paper we propose a distributed solution of the resource allocation problem based on self-organisation of the resource consumers in a system with limited resources. In our approach, agents dynamically allocate tasks to servers that provide a limited amount of resources. In our approach, agents select autonomously the execution platform for the task rather than ask a resource broker to do the allocation. All control needed for our algorithm is distributed among the agents in the system. They optimise the resource allocation process continuously over their lifetime to changes in the availability of shared resources by learning from past allocation decisions. The only information available to all agents are resource load and allocation success information from past resource allocations. Additional resource load information about servers is not disseminated. The proposed mechanism does not require a central controlling authority, resource management layer or introduce additional communication between agents to decide which task is allocated on which server. We demonstrate that this mechanism performs well dynamic systems with a large number of tasks and can easily be adapted to various system sizes. In addition, the overall system performance is not affected in case agents or servers fail or become unavailable. The proposed approach provides an easy way to implement distributed resource allocation and takes into account multi-agent system tendencies toward autonomy, heterogeneity and unreliability of resources and agents. This proposed technique can be easily supplemented by techniques for queuing or rejecting resource allocation requests of agents -LSB- 11 -RSB-. Such self-managing capabilities of software agents allow a reliable resource allocation even in an environment with unreliable resource providers. This can be achieved by the mutual interactions between agents by applying techniques from complex system theory. Selforganisation of all agents leads to a self-organisation of the 2. RELATED WORK Resource allocation is an important problem in the area of computer science. Generally speaking, resource allocation is a mechanism or policy for the efficient and effective management of the access to a limited resource or set of resources by its consumers. In the simplest case, resource consumers ask a central broker or dispatcher for available resources where the resource consumer will be allocated. The broker usually has full knowledge about all system resources. In those approaches, the resource consumer can not influence the allocation decision process. Load balancing -LSB- 3 -RSB- is a special case of the resource allocation problem using a broker that tries to be fair to all resources by balancing the system load equally among all resource providers. This mechanism works best in a homogeneous system. A simple distributed technique for resource management is capacity planning by refusing or queuing incoming agents to avoid resource overload -LSB- 11 -RSB-. From the resource owner perspective, this technique is important to prevent overload at the resource but it is not sufficient for effective resource allocation. This technique can only provide a good supplement for distributed resource allocation mechanisms. Those coordinators usually need to have global knowledge on the state of all system resources. An example of a dynamic resource allocation algorithm is the Cactus project -LSB- 1 -RSB- for the allocation of computational very expensive jobs. The value of distributed solutions for the resource allocation problem has been recognised by research -LSB- 10 -RSB-. Inspired by the principles in stock markets, economic market models have been developed for trading resources for the regulation of supply and demand in the grid. Users try to purchase cheap resources required to run the job while providers try to make as much profit as possible and operate the available resources at full capacity. A collection of different distributed resource allocation techniques based on market models is presented in Clearwater -LSB- 10 -RSB-. Buyya et al. developed a resource allocation framework based on the regulation of supply and demand -LSB- 4 -RSB- for Nimrod-G -LSB- 6 -RSB- with the main focus on job deadlines and budget constraints. The Agent based Resource Allocation Model -LRB- ARAM -RRB- for grids is designed to schedule computational expensive jobs using agents. Drawback of this model is the extensive use of message exchange between agents for periodic monitoring and information exchange within the hierarchical structure. Subtasks of a job migrate through the network until they find a resource that meets the price constraints. The job 's migration itinerary is determined by the resources in connecting them in different topologies -LSB- 17 -RSB-. The proposed mechanism in this paper eliminates the need of periodic information exchange about resource loads and does not need a connection topology between the resources. There has been considerable work on decentralised resource allocation techniques using game theory published over recent years. It is an ill-defined decision problem that assumes and models inductive reasoning. In this repetitive decision game, an odd number of agents have to choose between two resources based on past success information trying to allocate itself at the resource with the minority. Galstyan et al. -LSB- 14 -RSB- studied a variation with more than two resources, changing resource capacities and information from neighbour agents. They showed that agents can adapt effectively to changing capacities in this environment using a set of simple look-up tables -LRB- strategies -RRB- per agent. Another distributed technique that is employed for solving the resource allocation problem is based on reinforcement learning -LSB- 18 -RSB-. Similar to our approach, a set of agents compete for a limited number of resources based only on prior individual experience. In this paper, the system objective is to maximise system throughput while ensuring fairness to resources, measured as the average processing time per job unit. A resource allocation approach for sensor networks based on self-organisation techniques and reinforcement learning is presented in -LSB- 16 -RSB- with main focus on the optimisation of energy consumption of network nodes. We -LSB- 19 -RSB- proposed a self-organising load balancing approach for a single server with focus on optimising the communication costs of mobile agents. A mobile agent will reject a migration to a remote agent server, if it expects the destination server to be already overloaded by other agents or server tasks. Agents make their decisions themselves based on forecasts of the server utilisation. In this paper a solution for a multi-server environment is presented without consideration of communication or migration costs. 6. CONCLUSIONS AND FUTURE WORK In this paper a self-organising distributed resource allocation technique for multi-agent systems was presented. We enable agents to select the execution platform for their tasks themselves before each execution at run-time. In our approach the agents compete for an allocation at one of the Figure 5 : Results of experiment 2 in a dynamic server environment averaged over 100 repetitions. available shared resource. Agents sense their server environment and adopt their action to compete more efficient in the new created environment. This process is adaptive and has a strong feedback as allocation decisions influence indirectly decisions of other agents. The resource allocation is a purely emergent effect. Our mechanism demonstrates that resource allocation can be done by the effective competition of individual and autonomous agents. Neither do they need coordination or information from a higher authority nor is an additional direct communication between agents required. This mechanism was inspired by inductive reasoning and bounded rationality principles which enables the agents ' adaptation of their strategies to compete effectively in a dynamic environment. In the case of a server becomes unavailable, the agents can adapt quickly to this new situation by exploring new resources or remain at the home server if an allocation is not possible. Especially in dynamic and scalable environments such as grid systems, a robust and distributed mechanism for resource allocation is required. Our self-organising resource allocation approach was evaluated with a number of simulation experiments in a dynamic environment of agents and server resources. The presented results for this new approach for strategic migration optimisation are very promising and justify further investigation in a real multi-agent system environment. It is a distributed, scalable and easy-to-understand policy for the regulation of supply and demand of resources. All control is implemented in the agents. A simple decision mechanism based on different beliefs of the agent creates an emergent behaviour that leads to effective resource allocation. This approach can be easily extended or supported by resource balancing/queuing mechanisms provided by resources. Our approach adapts to changes in the environment but it is not evolutionary. There is no discovery of new strategies by the agents. investigated in the future. In the near future we will investigate if an automatic adaptation of the decay rate of historical information our algorithm is possible and can improve the resource allocation performance. A large number of shared resources requires older historical information to avoid a too frequently resources exploration. In contrast, a dynamic environment with varying capacities requires more up-to-date information to make more reliable predictions. We are aware of the long learning phase in environments with a large number of shared resources known by each agent. In the case that more resources are requested by agents than shared resources are provided by all servers, all agents will randomly explore all known servers. This process of acquiring resource load information about all servers can take a long time in the case that no not enough shared resources for all tasks are provided. In this situation, it is difficult for an agent to efficiently gather historical information about all remote servers. This issue needs more investigation in the future.", "keyphrases": ["multi-agent system", "agent", "resourc alloc", "distribut algorithm", "dynam alloc task", "network of server", "server utilis", "adapt process", "competit", "predictor"]}
{"file_name": "J-14", "text": "Computing Good Nash Equilibria in Graphical Games * ABSTRACT This paper addresses the problem of fair equilibrium selection in graphical games. Our approach is based on the data structure called the best response policy, which was proposed by Kearns et al. -LSB- 13 -RSB- as a way to represent all Nash equilibria of a graphical game. In -LSB- 9 -RSB-, it was shown that the best response policy has polynomial size as long as the underlying graph is a path. In this paper, we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants. Another attractive solution concept is a Nash equilibrium that maximizes the social welfare. We show that, while exactly computing the latter is infeasible -LRB- we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree -RRB-, there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size. These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria. 1. INTRODUCTION This is the intuition behind graphical games, which were introduced by Kearns, Littman and Singh in -LSB- 13 -RSB- as a compact representation scheme for games with many players. In an n-player graphical game, each player is associated with a vertex of an underlying graph G, and the payoffs of each player depend on his action as well as on the actions of his neighbors in the graph. If the maximum degree of G is \u0394, and each player has two actions available to him, then the game can be represented using n2\u0394 +1 numbers. In contrast, we need n2n numbers to represent a general n-player 2-action game, which is only practical for small values of n. For graphical games with constant \u0394, the size of the game is linear in n. One of the most natural problems for a graphical game is that of finding a Nash equilibrium, the existence of which follows from Nash 's celebrated theorem -LRB- as graphical games are just a special case of n-player games -RRB-. The first attempt to tackle this problem was made in -LSB- 13 -RSB-, where the authors consider graphical games with two actions per player in which the underlying graph is a boundeddegree tree. They propose a generic algorithm for finding Nash equilibria that can be specialized in two ways : an exponential-time algorithm for finding an -LRB- exact -RRB- Nash equilibrium, and a fully polynomial time approximation scheme -LRB- FPTAS -RRB- for finding an approximation to a Nash equilibrium. For any e > 0 this algorithm outputs an e-Nash equilibrium, which is a strategy profile in which no player can improve his payoff by more than e by unilaterally changing his strategy. While e-Nash equilibria are often easier to compute than exact Nash equilibria, this solution concept has several drawbacks. First, the players may be sensitive to a small loss in payoffs, so the strategy profile that is an e-Nash equilibrium will not be stable. Second, the strategy profiles that are close to being Nash equilibria may be much better with respect to the properties under consideration than exact Nash equilibria. Therefore, the -LRB- approximation to the -RRB- value of the best solution that corresponds to an e-Nash equilibrium may not be indicative of what can be achieved under an exact Nash equilibrium. This is especially important if the purpose of the approximate solution is to provide a good benchmark for a system of selfish agents, as the benchmark implied by an e-Nash equilibrium may be unrealistic. For these reasons, in this paper we focus on the problem of computing exact Nash equilibria. Building on ideas of -LSB- 14 -RSB-, Elkind et al. -LSB- 9 -RSB- showed how to find an -LRB- exact -RRB- Nash equilibrium in polynomial time when the underlying By contrast, finding a Nash equilibrium in a general degree-bounded graph appears to be computationally intractable : it has been shown -LRB- see -LSB- 5, 12, 7 -RSB- -RRB- to be complete for the complexity class PPAD. -LSB- 9 -RSB- extends this hardness result to the case in which the underlying graph has bounded pathwidth. A graphical game may not have a unique Nash equilibrium, indeed it may have exponentially many. Moreover, some Nash equilibria are more desirable than others. Rather than having an algorithm which merely finds some Nash equilibrium, we would like to have algorithms for finding Nash equilibria with various sociallydesirable properties, such as maximizing overall payoff or distributing profit fairly. A useful property of the data structure of -LSB- 13 -RSB- is that it simultaneously represents the set of all Nash equilibria of the underlying game. If this representation has polynomial size -LRB- as is the case for paths, as shown in -LSB- 9 -RSB- -RRB-, one may hope to extract from it a Nash equilibrium with the desired properties. In fact, in -LSB- 13 -RSB- the authors mention that this is indeed possible if one is interested in finding an -LRB- approximate -RRB- a-Nash equilibrium. The goal of this paper is to extend this to exact Nash equilibria. 1.1 Our Results In this paper, we study n-player 2-action graphical games on bounded-degree trees for which the data structure of -LSB- 13 -RSB- has size poly -LRB- n -RRB-. We focus on the problem of finding exact Nash equilibria with certain socially-desirable properties. In particular, we show how to find a Nash equilibrium that -LRB- nearly -RRB- maximizes the social welfare, i.e., the sum of the players ' payoffs, and we show how to find a Nash equilibrium that -LRB- nearly -RRB- satisfies prescribed payoff bounds for all players. Graphical games on bounded-degree trees have a simple algebraic structure. One attractive feature, which follows from -LSB- 13 -RSB-, is that every such game has a Nash equilibrium in which the strategy of every player is a rational number. Section 3 studies the algebraic structure of those Nash equilibria that maximize social welfare. We show -LRB- Theorems 1 and 2 -RRB- that, surprisingly, the set of Nash equilibria that maximize social welfare is more complex. It seems to be a novel feature of the setting we consider here, that an optimal Nash equilibrium is hard to represent, in a situation where it is easy to find and represent a Nash equilibrium. As the social welfare-maximizing Nash equilibrium may be hard to represent efficiently, we have to settle for an approximation. However, the crucial difference between our approach and that of previous papers -LSB- 13, 16, 19 -RSB- is that we require our algorithm to output an exact Nash equilibrium, though not necessarily the optimal one with respect to our criteria. In Section 4, we describe an algorithm that satisfies this requirement. Namely, we propose an algorithm that for any e > 0 finds a Nash equilibrium whose total payoff is within a of optimal. More pre1A related result in a different context was obtained by Datta -LSB- 8 -RSB-, who shows that n-player 2-action games are universal in the sense that any real algebraic variety can be represented as the set of totally mixed Nash equilibria of such games. We show -LRB- Section 4.1 -RRB- that under some restrictions on the payoff matrices, the algorithm can be transformed into a -LRB- truly -RRB- polynomial-time algorithm that outputs a Nash equilibrium whose total payoff is within a 1 \u2212 e factor from the optimal. In Section 5, we consider the problem of finding a Nash equilibrium in which the expected payoff of each player Vi exceeds a prescribed threshold Ti. Using the idea from Section 4 we give -LRB- Theorem 5 -RRB- a fully polynomial time approximation scheme for this problem. The running time of the algorithm is bounded by a polynomial in n, Pmax, and E. If the instance has a Nash equilibrium satisfying the prescribed thresholds then the algorithm constructs a Nash equilibrium in which the expected payoff of each player Vi is at least Ti \u2212 E. In Section 6, we introduce other natural criteria for selecting a `` good '' Nash equilibrium and we show that the algorithms described in the two previous sections can be used as building blocks in finding Nash equilibria that satisfy these criteria. In particular, in Section 6.1 we show how to find a Nash equilibrium that approximates the maximum social welfare, while guaranteeing that each individual payoff is close to a prescribed threshold. In Section 6.2 we show how to find a Nash equilibrium that -LRB- nearly -RRB- maximizes the minimum individual payoff. Finally, in Section 6.3 we show how to find a Nash equilibrium in which the individual payoffs of the players are close to each other. 1.2 Related Work Our approximation scheme -LRB- Theorem 3 and Theorem 4 -RRB- shows a contrast between the games that we study and two-player n-action games, for which the corresponding problems are usually intractable. For two-player n-action games, the problem of finding Nash equilibria with special properties is typically NP-hard. In particular, this is the case for Nash equilibria that maximize the social welfare -LSB- 11, 6 -RSB-. Moreover, it is likely to be intractable even to approximate such equilibria. In particular, Chen, Deng and Teng -LSB- 4 -RSB- show that there exists some e, inverse polynomial in n, for which computing an e-Nash equilibrium in 2-player games with n actions per player is PPAD-complete. Lipton and Markakis -LSB- 15 -RSB- study the algebraic properties of Nash equilibria, and point out that standard quantifier elimination algorithms can be used to solve them. Note that these algorithms are not polynomial-time in general. The games we study in this paper have polynomial-time computable Nash equilibria in which all mixed strategies are rational numbers, but an optimal Nash equilibrium may necessarily include mixed strategies with high algebraic degree. Any Nash equilibrium is a CE but the converse does not hold in general. In contrast with Nash equilibria, correlated equilibria can be found for low-degree graphical games -LRB- as well as other classes of conciselyrepresented multiplayer games -RRB- in polynomial time -LSB- 17 -RSB-. But, for graphical games it is NP-hard to find a correlated equilibrium that maximizes total payoff -LSB- 18 -RSB-. However, the NP-hardness results apply to more general games than the one we consider here, in particular the graphs are not trees. From -LSB- 2 -RSB- it is also known that there exist 2-player, 2-action games for which the expected total payoff of the best correlated equilibrium is higher than the best Nash equilibrium, and we discuss this issue further in Section 7. 7. CONCLUSIONS We have studied the problem of equilibrium selection in graphical games on bounded-degree trees. We considered several criteria for selecting a Nash equilibrium, such as maximizing the social welfare, ensuring a lower bound on the expected payoff of each player, etc.. First, we focused on the algebraic complexity of a social welfare-maximizing Nash equilibrium, and proved strong negative results for that problem. Namely, we showed that even for graphical games on paths, any algebraic number \u03b1 E -LSB- 0, 1 -RSB- may be the only strategy available to some player in all social welfaremaximizing Nash equilibria. This is in sharp contrast with the fact that graphical games on trees always possess a Nash equilibrium in which all players ' strategies are rational numbers. We then provided approximation algorithms for selecting Nash equilibria with special properties. While the problem of finding approximate Nash equilibria for various classes of games has received a lot of attention in recent years, most of the existing work aims to find E-Nash equilibria that satisfy -LRB- or are E-close to satisfying -RRB- certain properties. Our approach is different in that we insist on outputting an exact Nash equilibrium, which is E-close to satisfying a given requirement. As argued in the introduction, there are several reasons to prefer a solution that constitutes an exact Nash equilibrium. While we prove our results for games on a path, they can be generalized to any tree for which the best response policies have compact representations as unions of rectangles. In the full version of the paper we describe our algorithms for the general case. Further work in this vein could include extensions to the kinds of guarantees sought for Nash equilibria, such as guaranteeing total payoffs for subsets of players, selecting equilibria in which some players are receiving significantly higher payoffs than their peers, etc.. At the moment however, it is perhaps more important to inves tigate whether Nash equilibria of graphical games can be computed in a decentralized manner, in contrast to the algorithms we have introduced here. It is natural to ask if our results or those of -LSB- 9 -RSB- can be generalized to games with three or more actions. However, it seems that this will make the analysis significantly more difficult. In particular, note that one can view the bounded payoff games as a very limited special case of games with three actions per player. Namely, given a two-action game with payoff bounds, consider a game in which each player Vi has a third action that guarantees him a payoff of Ti no matter what everyone else does. Then checking if there is a Nash equilibrium in which none of the players assigns a nonzero probability to his third action is equivalent to checking if there exists a Nash equilibrium that satisfies the payoff bounds in the original game, and Section 5.1 shows that finding an exact solution to this problem requires new ideas. Alternatively it may be interesting to look for similar results in the context of correlated equilibria -LRB- CE -RRB-, especially since the best CE may have higher value -LRB- total expected payoff -RRB- than the best NE. It is known from -LSB- 1 -RSB- that the mediation value of 2-player, 2-action games with non-negative payoffs is at most 43, and they exhibit a 3-player game for which it is infinite.", "keyphrases": ["graphic game", "nash equilibrium", "approxim scheme", "exponenti-time algorithm", "approxim", "variou sociallydesir properti", "overal payoff", "distribut profit", "social welfar", "integ-payoff graphic game g", "sever drawback", "strategi profil", "degre-bound graph"]}
{"file_name": "I-22", "text": "Realistic Cognitive Load Modeling for Enhancing Shared Mental Models in Human-Agent Collaboration ABSTRACT Human team members often develop shared expectations to predict each other 's needs and coordinate their behaviors. In this paper the concept `` Shared Belief Map '' is proposed as a basis for developing realistic shared expectations among a team of Human-Agent-Pairs -LRB- HAPs -RRB-. The establishment of shared belief maps relies on inter-agent information sharing, the effectiveness of which highly depends on agents ' processing loads and the instantaneous cognitive loads of their human partners. We investigate HMM-based cognitive load models to facilitate team members to `` share the right information with the right party at the right time ''. The shared belief map concept and the cognitive/processing load models have been implemented in a cognitive agent architecture -- SMMall. A series of experiments were conducted to evaluate the concept, the models, and their impacts on the evolving of shared mental models of HAP teams. 1. INTRODUCTION Human-centered multiagent teamwork has thus attracted increasing attentions in multi-agent systems field -LSB- 2, 10, 4 -RSB-. Humans and autonomous In short, humans and agents can team together to achieve better performance, given that they could establish certain mutual awareness to coordinate their mixed-initiative activities. However, the foundation of human-agent collaboration keeps being challenged because of nonrealistic modeling of mutual awareness of the state of affairs. In particular, few researchers look beyond to assess the principles of modeling shared mental constructs between a human and his/her assisting agent. Moreover, human-agent relationships can go beyond partners to teams. Therefore, there is a clear demand for investigations to broaden and deepen our understanding on the principles of shared mental modeling among members of a mixed human-agent team. There are lines of research on multi-agent teamwork, both theoretically and empirically. For instance, Joint Intention -LSB- 3 -RSB- and SharedPlans -LSB- 5 -RSB- are two theoretical frameworks for specifying agent collaborations. One of the drawbacks is that, although both have a deep philosophical and cognitive root, they do not accommodate the modeling of human team members. Cognitive studies suggested that teams which have shared mental models are expected to have common expectations of the task and team, which allow them to predict the behavior and resource needs of team members more accurately -LSB- 14, 6 -RSB-. Cannon-Bowers et al. -LSB- 14 -RSB- explicitly argue that team members should hold compatible models that lead to common `` expectations ''. We agree on this and believe that the establishment of shared expectations among human and agent team members is a critical step to advance human-centered teamwork research. It has to be noted that the concept of shared expectation can broadly include role assignment and its dynamics, teamwork schemas and progresses, communication patterns and intentions, etc.. While the long-term goal of our research is to understand how shared cognitive structures can enhance human-agent team performance, the specific objective of the work reported here is to develop a computational cognitive 6. CONCLUSION Recent research attention on human-centered teamwork highly demands the design of agent systems as cognitive aids that can model and exploit human partners ' cognitive capacities to offer help unintrusively. In this paper, we investigated several factors surrounding the challenging problem of evolving shared mental models of teams composed of human-agent-pairs. The major contribution of this research includes -LRB- 1 -RRB- HMM-based load models were proposed for an agent to estimate its human partner 's cognitive load and other HAP teammates ' processing loads ; -LRB- 2 -RRB- The shared belief map concept was introduced and implemented. It allows group members to effectively represent and reason about shared mental models ; -LRB- 3 -RRB- Experiments were conducted to evaluate the HMM-based cognitive/processing load models and the impacts of multi-party communication on the evolving of team SMMs. The usefulness of shared belief maps was also demonstrated during the experiments.", "keyphrases": ["share belief map", "multiag teamwork", "heurist", "reason", "problem-solv", "collabor", "teamwork", "expect", "teamwork schema", "human-agent team perform", "cognit load theori", "human perform", "resourc alloc", "task perform", "info-share", "multi-parti commun"]}
{"file_name": "J-23", "text": "Frugality Ratios And Improved Truthful Mechanisms for Vertex Cover * In set-system auctions, there are several overlapping teams of agents, and a task that can be completed by any of these teams. The auctioneer 's goal is to hire a team and pay as little as possible. Examples of this setting include shortest-path auctions and vertex-cover auctions. Recently, Karlin, Kempe and Tamir introduced a new definition offrugality ratio for this problem. Informally, the `` frugality ratio '' is the ratio of the total payment of a mechanism to a desired payment bound. The ratio captures the extent to which the mechanism overpays, relative to perceived fair cost in a truthful auction. In this paper, we propose a new truthful polynomial-time auction for the vertex cover problem and bound its frugality ratio. We show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst-case bound ; this is the first auction for this problem to have these properties. Moreover, we show how to transform any truthful auction into a frugal one while preserving the approximation ratio. Also, we consider two natural modifications of the definition of Karlin et al., and we analyse the properties of the resulting payment bounds, such as monotonicity, computational hardness, and robustness with respect to the draw-resolution rule. We study the relationships between the different payment bounds, both for general set systems and for specific set-system auctions, such as path auctions and vertex-cover auctions. We use these new definitions in the proof of our main result for vertex-cover auctions via a bootstrapping technique, which may be of independent interest. 1. INTRODUCTION In a set system auction there is a single buyer and many vendors that can provide various services. It is assumed that the buyer 's requirements can be satisfied by various subsets of the vendors ; these subsets are called the feasible sets. We assume that each vendor has a cost of providing his services, but submits a possibly larger bid to the auctioneer. Based on these bids, the auctioneer selects a feasible subset of vendors, and makes payments to the vendors in this subset. Each selected vendor enjoys a profit of payment minus cost. Vendors want to maximise profit, while the buyer wants to minimise the amount he pays. A natural goal in this setting is to design a truthful auction, in which vendors have an incentive to bid their true cost. This can be achieved by paying each selected vendor a premium above her bid in such a way that the vendor has no incentive to overbid. An interesting question in mechanism design is how much the auctioneer will have to overpay in order to ensure truthful bids. In the context of path auctions this topic was first addressed by Archer and Tardos -LSB- 1 -RSB-. They define the frugality ratio of a mechanism as the ratio between its total payment and the cost of the cheapest path disjoint from the path selected by the mechanism. They show that, for a large class of truthful mechanisms for this problem, the frugality ratio is as large as the number of edges in the shortest path. Talwar -LSB- 21 -RSB- extends this definition of frugality ratio to general set systems, and studies the frugality ratio of the classical VCG mechanism -LSB- 22, 4, 14 -RSB- for many specific set systems, such as minimum spanning trees and set covers. While the definition of frugality ratio proposed by -LSB- 1 -RSB- is wellmotivated and has been instrumental in studying truthful mechanisms for set systems, it is not completely satisfactory. Consider, for example, the graph of Figure 1 with the costs CAB = CBC = Figure 1 : The diamond graph This graph is 2-connected and the VCG payment to the winning path ABCD is bounded. However, the graph contains no A -- D path that is disjoint from ABCD, and hence the frugality ratio of VCG on this graph remains undefined. At the same time, there is no monopoly, that is, there is no vendor that appears in all feasible sets. To deal with this problem, Karlin et al. -LSB- 16 -RSB- suggest a better benchmark, which is defined for any monopoly-free set system. Based on this new definition, the authors construct new mechanisms for the shortest path problem and show that the overpayment of these mechanisms is within a constant factor of optimal. 1.1 Our results Vertex cover auctions We propose a truthful polynomial-time auction for vertex cover that outputs a solution whose cost is within a factor of 2 of optimal, and whose frugality ratio is at most 2\u0394, where \u0394 is the maximum degree of the graph -LRB- Theorem 4 -RRB-. We complement this result by proving -LRB- Theorem 5 -RRB- that for any \u0394 and n, there are graphs of maximum degree \u0394 and size \u0398 -LRB- n -RRB- for which any truthful mechanism has frugality ratio at least \u0394 / 2. This means that the solution quality of our auction is with a factor of 2 of optimal and the frugality ratio is within a factor of 4 of the best possible bound for worst-case inputs. To the best of our knowledge, this is the first auction for this problem that enjoys these properties. Moreover, we show how to transform any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio. Frugality ratios Our vertex cover results naturally suggest two modifications of the definition of \u03bd in -LSB- 16 -RSB-. These modifications can be made independently of each other, resulting in four different payment bounds TUmax, TUmin, NTUmax, and NTUmin, where NTUmin is equal to the original payment bound \u03bd of in -LSB- 16 -RSB-. While our main result about vertex cover auctions -LRB- Theorem 4 -RRB- is with respect to NTUmin = \u03bd, we make use of the new definitions by first comparing the payment of our mechanism to a weaker bound NTUmax, and then bootstrapping from this result to obtain the desired bound. Inspired by this application, we embark on a further study of these payment bounds. Our results here are as follows : 1. We observe -LRB- Proposition 1 -RRB- that the four payment bounds always obey a particular order that is independent of the choice of the set system and the cost vector, namely, TUmin < NTUmin < NTUmax < TUmax. We provide examples -LRB- Proposition 5 and Corollaries 1 and 2 -RRB- showing that for the vertex cover problem any two consecutive bounds can differ by a factor of n \u2212 2, where n is the number of agents. We then show -LRB- Theorem 2 -RRB- that this separation is almost best possible for general set systems by proving that for any set system TUmax/TUmin < n. In contrast, we demonstrate -LRB- Theorem 3 -RRB- that for path auctions TUmax/TUmin < 2. We provide examples -LRB- Propositions 2, 3 and 4 -RRB- showing that this bound is tight. We see this as an argument for the study of vertexcover auctions, as they appear to be more representative of the general team - selection problem than the widely studied path auctions. 2. This observation suggests that the four payment bounds should be studied in a unified framework ; moreover, it leads us to believe that the bootstrapping technique of Theorem 4 may have other applications. 3. We evaluate the payment bounds introduced here with respect to a checklist of desirable features. This can be seen as an argument in favour of using weaker but efficiently computable bounds NTUmax and TUmax. Related work Vertex-cover auctions have been studied in the past by Talwar -LSB- 21 -RSB- and Calinescu -LSB- 5 -RSB-. Both of these papers are based on the definition of frugality ratio used in -LSB- 1 -RSB- ; as mentioned before, this means that their results only apply to bipartite graphs. Talwar -LSB- 21 -RSB- shows that the frugality ratio of VCG is at most \u0394. However, since finding the cheapest vertex cover is an NP-hard problem, the VCG mechanism is computationally infeasible. The first -LRB- and, to the best of our knowledge, only -RRB- paper to investigate polynomial-time truthful mechanisms for vertex cover is -LSB- 5 -RSB-. This paper studies an auction that is based on the greedy allocation algorithm, which has an approximation ratio of log n. While the main focus of -LSB- 5 -RSB- is the more general set cover problem, the results of -LSB- 5 -RSB- imply a frugality ratio of 2\u03942 for vertex cover. 2. PRELIMINARIES In most of this paper, we discuss auctions for set systems. In set system auctions, each element e of the ground set is owned by an independent agent and has an associated non-negative cost ce. The goal of the centre is to select -LRB- purchase -RRB- a feasible set. Each element e in the selected set incurs a cost of ce. The elements that are not selected incur no costs. The auction proceeds as follows : all elements of the ground set make their bids, the centre selects a feasible set based on the bids and makes payments to the agents. Formally, an auction is defined by an allocation rule A : R '' _, F and a payment rule P : R '' _, R ''. The allocation rule takes as input a vector of bids and decides which of the sets in F should be selected. The payment rule also takes as input a vector of bids and decides how much to pay to each agent. The standard requirements are individual rationality, i.e., the payment to each agent should be at least as high as his incurred cost -LRB- 0 for agents not in the selected set and ce for agents in the selected set -RRB- and incentive compatibility, or truthfulness, i.e., each agent 's dominant strategy is to bid his true cost. An allocation rule is monotone if an agent can not increase his chance of getting selected by raising his bid. Given a monotone allocation rule A and a bid vector b, the threshold bid te of an agent e E A -LRB- b -RRB- is the highest bid of this agent that still wins the auction, given that the bids of other participants remain the same. It is well known -LRB- see, e.g. -LSB- 19, 13 -RSB- -RRB- that any auction that has a monotone allocation rule and pays each agent his threshold bid is truthful ; conversely, any truthful auction has a monotone allocation rule. The VCG mechanism is a truthful mechanism that maximises the `` social welfare '' and pays 0 to the losing agents. For set system auctions, this simply means picking a cheapest feasible set, paying each agent in the selected set his threshold bid, and paying 0 to all other agents. Note, however, that the VCG mechanism may be difficult to implement, since finding a cheapest feasible set may be intractable. If U is a set of agents, c -LRB- U -RRB- denotes Ew \u2208 U cw. Similarly, b -LRB- U -RRB- denotes Ew \u2208 U bw.", "keyphrases": ["frugal ratio", "bootstrap techniqu", "vertex-cover auction", "transfer util", "consecut payment bound", "monoton alloc rule", "co-oper", "polynomi-time", "nonmonoton"]}
{"file_name": "I-11", "text": "Real-Time Agent Characterization and Prediction ABSTRACT Reasoning about agents that we observe in the world is challenging. Our available information is often limited to observations of the agent 's external behavior in the past and present. To understand these actions, we need to deduce the agent 's internal state, which includes not only rational elements -LRB- such as intentions and plans -RRB-, but also emotive ones -LRB- such as fear -RRB-. In addition, we often want to predict the agent 's future actions, which are constrained not only by these inward characteristics, but also by the dynamics of the agent 's interaction with its environment. BEE -LRB- Behavior Evolution and Extrapolation -RRB- uses a faster-than-real-time agentbased model of the environment to characterize agents ' internal state by evolution against observed behavior, and then predict their future behavior, taking into account the dynamics of their interaction with the environment. 1. INTRODUCTION Reasoning about agents that we observe in the world must integrate two disparate levels. Our observations are often limited to the agent 's external behavior, which can frequently be summarized numerically as a trajectory in space-time -LRB- perhaps punctuated by actions from a fairly limited vocabulary -RRB-. However, this behavior is driven by the agent 's internal state, which -LRB- in the case of a human -RRB- may involve high-level psychological and cognitive concepts such as intentions and emotions. A central challenge in many application domains is reasoning from external observations of agent behavior to an estimate of their internal state. Such reasoning is motivated by a desire to predict the agent 's behavior. This problem has traditionally been addressed under the rubric of `` plan recognition '' or `` plan inference. '' Many realistic problems deviate from these conditions. \u2022 Increasing the number of agents leads to a combinatorial explosion that can swamp conventional analysis. \u2022 Environmental dynamics can frustrate agent intentions. \u2022 The agents often are trying to hide their intentions -LRB- and even their presence -RRB-, rather than intentionally sharing information. \u2022 An agent 's emotional state may be at least as important as its rational state in determining its behavior. BEE -LRB- Behavioral Evolution and Extrapolation -RRB- is a novel approach to recognizing the rational and emotional state of multiple interacting agents based solely on their behavior, without recourse to intentional communications from them. It is inspired by techniques used to predict the behavior of nonlinear dynamical systems, in which a representation of the system is continually fit to its recent past behavior. For nonlinear dynamical systems, the representation is a closed-form mathematical equation. In BEE, it is a set of parameters governing the behavior of software agents representing the individuals being analyzed. The current version of BEE characterizes and predicts the behavior of agents representing soldiers engaged in urban combat -LSB- 8 -RSB-. Section 2 reviews relevant previous work. Section 3 describes the architecture of BEE. Section 4 reports results from experiments with the system. Section 5 concludes. 2. PREVIOUS WORK BEE bears comparison with previous research in AI -LRB- plan recognition -RRB-, Hidden Markov Models, and nonlinear dynamics systems -LRB- trajectory prediction -RRB-. 2.1 Plan Recognition in AI Agent theory commonly describes an agent 's cognitive state in terms of its beliefs, desires, and intentions -LRB- the so-called `` BDI '' model -LSB- 5, 20 -RSB- -RRB-. An agent 's beliefs are propositions about the state of the world that it considers true, based on its perceptions. Its desires are propositions about the world that it would like to be true. Desires are not necessarily consistent with one another : an agent might desire both to be rich and not to work at the same time. An agent 's intentions, or goals, are a subset of its desires that it has selected, based on its beliefs, to guide its future actions. Unlike desires, goals must be consistent with one another -LRB- or at least believed to be consistent by the agent -RRB-. An agent 's goals guide its actions. Thus one ought to be able to learn something about an agent 's goals by observing its past actions, and knowledge of the agent 's goals in turn enables conclusions about what the agent may do in the future. This process of reasoning from an agent 's actions to its goals is known as `` plan recognition '' or `` plan inference. '' Plan recognition is seldom pursued for its own sake. It usually supports a higher-level function. For example, in humancomputer interfaces, recognizing a user 's plan can enable the system to provide more appropriate information and options for user action. In a tutoring system, inferring the student 's plan is a first step to identifying buggy plans and providing appropriate remediation. In many cases, the higher-level function is predicting likely future actions by the entity whose plan is being inferred. We focus on plan recognition in support of prediction. An agent 's plan is a necessary input to a prediction of its future behavior, but hardly a sufficient one. At least two other influences, one internal and one external, need to be taken into account. The external influence is the dynamics of the environment, which may include other agents. The dynamics of the real world impose significant constraints. \u2022 The environment may interfere with the desires of the agent -LSB- 4, 10 -RSB-. \u2022 Most interactions among agents, and between agents and the world, are nonlinear. A rational analysis of an agent 's goals may enable us to predict what it will attempt, but any nontrivial plan with several steps will depend sensitively at each step to the reaction of the environment, and our prediction must take this reaction into account as well. Actual simulation of futures is one way -LRB- the only one we know now -RRB- to deal with the impact of environmental dynamics on an agent 's actions. Human agents are also subject to an internal influence. The agent 's emotional state can modulate its decision process and its focus of attention -LRB- and thus its perception of the environment -RRB-. In extreme cases, emotion can lead an agent to choose actions that from the standpoint of a logical analysis may appear irrational. Current work on plan recognition for prediction focuses on the rational plan, and does not take into account either external environmental influences or internal emotional biases. BEE integrates all three elements into its predictions. 2.2 Hidden Markov Models BEE is superficially similar to Hidden Markov Models -LRB- HMM 's -LSB- 19 -RSB- -RRB-. BEE offers two important benefits over HMM 's. First, a single agent 's hidden variables do not satisfy the Markov property. That is, their values at t + 1 depend not only on their values at t, but also on the hidden variables of other agents. One could avoid this limitation by constructing a single HMM over the joint state space of all of the agents, but this approach is combinatorially prohibitive. BEE combines the efficiency of independently modeling individual agents with the reality of taking into account interactions among them. Second, Markov models assume that transition probabilities are stationary. BEE 's evolutionary process continually updates the agents ' personalities based on actual observations, and thus automatically accounts for changes in the agents ' personalities. 2.3 Real-Time Nonlinear Systems Fitting Many systems of interest can be described by a vector of real numbers that changes as a function of time. The dimensions of the vector define the system 's state space. Long-range prediction of such a system is impossible. However, it is often useful to anticipate the system 's behavior a short distance into the future. This process is repeated constantly, providing the user with a limited look-ahead. This approach is robust and widely applied, but requires systems that can efficiently be described with mathematical equations. BEE extends this approach to agent behaviors, which it fits to observed behavior using a genetic algorithm. 5. CONCLUSIONS In many domains, it is important to reason from an entity 's observed behavior to an estimate of its internal state, and then to extrapolate that estimate to predict the entity 's future behavior. BEE performs this task using a faster-than-real-time simulation of swarming agents, coordinated through digital pheromones. This simulation integrates knowledge of threat regions, a cognitive analysis of the agent 's beliefs, desires, and intentions, a model of the agent 's emotional disposition and state, and the dynamics of interactions with the environment. By evolving agents in this rich environment, we can fit their internal state to their observed behavior. In realistic wargames, the system successfully detects deliberately played emotions and makes reasonable predictions about the entities ' future behaviors. BEE can only model internal state variables that impact the agent 's external behavior. It can not fit variables that the agent does not manifest externally, since the basis for the evolutionary cycle is a comparison of the outward behavior of the simulated agent with that of the real entity. This limitation is serious if our purpose is to understand the entity 's internal state for its own sake. If our purpose of fitting agents is to predict their subsequent behavior, the limitation is much less serious. State variables that do not impact behavior, while invisible to a behavior-based analysis, are irrelevant to a behavioral prediction. \u2022 Our initial limited repertoire of emotions is a small subset of those that have been distinguished by psychologists, and that might be useful for understanding and projecting behavior. We expect to extend the set of emotions and supporting dispositions that BEE can detect. \u2022 The mapping between an agent 's psychological -LRB- cognitive and emotional -RRB- state and its outward behavior is not one-to-one. Several different internal states might be consistent with a given observed behavior under one set of environmental conditions, but might yield distinct behaviors under other conditions. If the environment in the recent past is one that confounds such distinct internal states, we will be unable to distinguish them. As long as the environment stays in this state, our predictions will be accurate, whichever of the internal states we assign to the agent. If the environment then shifts to one under which the different internal states lead to different behaviors, using the previously chosen internal state will yield inaccurate predictions. One way to address these concerns is to probe the real world, perturbing it in ways that would stimulate distinct behaviors from entities whose psychological state is otherwise indistinguishable. Such probing is an important intelligence technique. BEE 's faster-than-real-time simulation may enable us to identify appropriate probing actions, greatly increasing the effectiveness of intelligence efforts.", "keyphrases": ["agent reason", "extern behavior", "intern state", "agent behavior predict", "behavior evolut and extrapol", "nonlinear dynam system", "agent's goal", "emot", "pheromon flavor", "disposit", "futur behavior"]}
{"file_name": "J-9", "text": "Computation in a Distributed Information Market \u2217 ABSTRACT According to economic theory -- supported by empirical and laboratory evidence -- the equilibrium price of a financial security reflects all of the information regarding the security 's value. We investigate the computational process on the path toward equilibrium, where information distributed among traders is revealed step-by-step over time and incorporated into the market price. We develop a simplified model of an information market, along with trading strategies, in order to formalize the computational properties of the process. We show that securities whose payoffs can not be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory. On the other hand, securities whose payoffs are threshold functions are guaranteed to converge, for all prior probability distributions. Moreover, these threshold securities converge in at most n rounds, where n is the number of bits of distributed information. We also prove a lower bound, showing a type of threshold security that requires at least n/2 rounds to converge in the worst case. \u2217 This work was supported by the DoD University Research Initiative -LRB- URI -RRB- administered by the Office of Naval Research under Grant N00014-01-1-0795. \u2020 Supported in part by ONR grant N00014-01-0795 and NSF grants CCR-0105337, CCR-TC-0208972, ANI-0207399, and ITR-0219018. \u2021 This work conducted while at NEC Laboratories America, Princeton, NJ. 1. INTRODUCTION The strong form of the efficient markets hypothesis states that market prices nearly instantly incorporate all information available to all traders. As a result, market prices encode the best forecasts of future outcomes given all information, even if that information is distributed across many sources. The process of information incorporation is, at its essence, a distributed computation. Each trader begins with his or her own information. As trades are made, summary information is revealed through market prices. Traders learn or infer what information others are likely to have by observing prices, then update their own beliefs based on their observations. Over time, if the process works as advertised, all information is revealed, and all traders converge to the same information state. At this point, the market is in what is called a rational expectations equilibrium -LSB- 11, 16, 19 -RSB-. All information available to all traders is now reflected in the going prices, and no further trades are desirable until some new information becomes available. While most markets are not designed with information aggregation as a primary motivation -- for example, derivatives In this paper, we investigate the nature of the computational process whereby distributed information is revealed and combined over time into the prices in information markets. To do so, in Section 3, we propose a model of an information market that is tractable for theoretical analysis and, we believe, captures much of the important essence of real information markets. We prove that only Boolean securities whose payoffs can be expressed as threshold functions of the distributed input bits of information are guaranteed to converge as predicted by rational expectations theory. Boolean securities with more complex payoffs may not converge under some prior distributions. We also provide upper and lower bounds on the convergence time for these threshold securities. We show that, for all prior distributions, the price of a threshold security converges to its rational expectations equilibrium price in at most n rounds, where n is the number of bits of distributed information. We show that this worst-case bound is tight within a factor of two by illustrating a situation in which a threshold security requires n/2 rounds to converge.", "keyphrases": ["econom theori", "empir and laboratori evid", "equilibrium price", "financi secur", "secur's valu", "comput process", "path toward equilibrium", "trader", "market price", "simplifi model", "trade strategi", "comput properti of the process", "secur", "payoff", "threshold function", "probabl distribut", "round", "number of bit", "distribut inform", "lower bound", "worst case", "inform market"]}
{"file_name": "H-19", "text": "Analyzing Feature Trajectories for Event Detection ABSTRACT We consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported, periodic and aperiodic words. A set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner. The document frequency of each word across time is treated like a time series, where each element is the document frequency - inverse document frequency -LRB- DFIDF -RRB- score at one time point. In this paper, we 1 -RRB- first applied spectral analysis to categorize features for different event characteristics : important and less-reported, periodic and aperiodic ; 2 -RRB- modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities, and subsequently detected each feature 's burst by the truncated Gaussian approach ; 3 -RRB- proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events. All of the above methods can be applied to time series data in general. We extensively evaluated our methods on the 1-year Reuters News Corpus -LSB- 3 -RSB- and showed that they were able to uncover meaningful aperiodic and periodic events. 1. INTRODUCTION There are more than 4,000 online news sources in the world. Manually monitoring all of them for important events has become difficult or practically impossible. In fact, the topic detection and tracking -LRB- TDT -RRB- community has for many years been trying to come up with a practical solution to help people monitor news effectively. solutions proposed for event detection -LSB- 20, 5, 17, 4, 21, 7, 14, 10 -RSB- are either too simplistic -LRB- based on cosine similarity -LSB- 5 -RSB- -RRB- or impractical due to the need to tune a large number of parameters -LSB- 9 -RSB-. Thus in this paper, we look at news stories and feature trends from the perspective of analyzing a time-series word signal. Previous work like -LSB- 9 -RSB- has attempted to reconstruct an event with its representative features. However, in many predictive event detection tasks -LRB- i.e., retrospective event detection -RRB-, there is a vast set of potential features only for a fixed set of observations -LRB- i.e., the obvious bursts -RRB-. Of these features, often only a small number are expected to be useful. In particular, we study the novel problem of analyzing feature trajectories for event detection, borrowing a well-known technique from signal processing : identifying distributional correlations among all features by spectral analysis. To evaluate our method, we subsequently propose an unsupervised event detection algorithm for news streams. Figure 1 : Feature correlation -LRB- DFIDF : time -RRB- between a -RRB- Easter and April b -RRB- Unaudited and Ended. As an illustrative example, consider the correlation between the words Easter and April from the Reuters Corpus '. From the plot of their normalized DFIDF in Figure 1 -LRB- a -RRB-, we observe the heavy overlap between the two words circa 04/1997, which means they probably both belong to the same event during that time -LRB- Easter feast -RRB-. In this example, the hidden event Easter feast is a typical important aperiodic event over 1-year data. Another example is given by Figure 1 -LRB- b -RRB-, where both the words Unaudited and Ended ` Reuters Corpus is the default dataset for all examples. exhibit similar behaviour over periods of 3 months. These two words actually originated from the same periodic event, net income-loss reports, which are released quarterly by publicly listed companies. Other observations drawn from Figure 1 are : 1 -RRB- the bursty period of April is much longer than Easter, which suggests that April may exist in other events during the same period ; 2 -RRB- Unaudited has a higher average DFIDF value than Ended, which indicates Unaudited to be more representative for the underlying event. These two examples are but the tip of the iceberg among all word trends and correlations hidden in a news stream like Reuters. If a large number of them can be uncovered, it could significantly aid TDT tasks. In particular, it indicates the significance of mining correlating features for detecting corresponding events. To summarize, we postulate that : 1 -RRB- An event is described by its representative features. Based on these observations, we can either mine representative features given an event or detect an event from a list of highly correlated features. In this paper, we focus on the latter, i.e., how correlated features can be uncovered to form an event in an unsupervised manner. 1.1 Contributions This paper has three main contributions : 9 To the best of our knowledge, our approach is the first to categorize word features for heterogenous events. 9 We propose a simple and effective mixture densitybased approach to model and detect feature bursts. 9 We come up with an unsupervised event detection algorithm to detect both aperiodic and periodic events. Our algorithm has been evaluated on a real news stream to show its effectiveness. 2. RELATED WORK Moreover, most TDT research so far has been concerned with clustering/classifying documents into topic types, identifying novel sentences -LSB- 6 -RSB- for new events, etc., without much regard to analyzing the word trajectory with respect to time. Swan and Allan -LSB- 18 -RSB- first attempted using co-occuring terms to construct an event. However, they only considered named entities and noun phrase pairs, without considering their periodicities. On the contrary, our paper considers all of the above. Recently, there has been significant interest in modeling an event in text streams as a `` burst of activities '' by incorporating temporal information. Nevertheless, none of the existing work specifically identified features for events, except for Fung et al. -LSB- 9 -RSB-, who clustered busty features to identify various bursty events. Our work differs from -LSB- 9 -RSB- in several ways : 1 -RRB- we analyze every single feature, not only bursty features ; 2 -RRB- we classify features along two categorical dimensions -LRB- periodicity and power -RRB-, yielding altogether five primary feature types ; 3 -RRB- we do not restrict each feature to exclusively belong to only one event. Spectral analysis techniques have previously been used by Vlachos et al. -LSB- 19 -RSB- to identify periodicities and bursts from query logs. Their focus was on detecting multiple periodicities from the power spectrum graph, which were then used to index words for `` query-by-burst '' search. In this paper, we use spectral analysis to classify word features along two dimensions, namely periodicity and power spectrum, with the ultimate goal of identifying both periodic and aperiodic bursty events. 8. CONCLUSIONS This paper took a whole new perspective of analyzing feature trajectories as time domain signals. By considering the word document frequencies in both time and frequency domains, we were able to derive many new characteristics about news streams that were previously unknown, e.g., the different distributions of stopwords during weekdays and weekends. For the first time in the area of TDT, we applied a systematic approach to automatically detect important and less-reported, periodic and aperiodic events. The key idea of our work lies in the observations that -LRB- a -RRB- periodic events have -LRB- a -RRB- periodic representative features and -LRB- un -RRB- important events have -LRB- in -RRB- active representative features, differentiated by their power spectrums and time periods. To address the real event detection problem, a simple and effective mixture density-based approach was used to identify feature bursts and their associated bursty periods. We also designed an unsupervised greedy algorithm to detect both aperiodic and periodic events, which was successful in detecting real events as shown in the evaluation on a real news stream. Although we have not made any benchmark comparison against another approach, simply because there is no previous work in the addressed problem. Nevertheless, we believe our simple and effective method will be useful for all TDT practitioners, and will be especially useful for the initial exploratory analysis of news streams.", "keyphrases": ["event detect", "word trajectori", "aperiod event", "period event", "word signal", "spectral analysi", "topic detect", "topic track", "text stream", "new stream", "time seri"]}
{"file_name": "H-3", "text": "Using Query Contexts in Information Retrieval ABSTRACT User query is an element that specifies an information need, but it is not the only one. Studies in literature have found many contextual factors that strongly influence the interpretation of a query. Recent studies have tried to consider the user 's interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the user. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query. The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. In this paper, both types of context are integrated in an IR model based on language modeling. Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness. 1. INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need. Many relevant terms can be absent from queries and terms included may be ambiguous. These issues have been addressed in a large number of previous studies. In these studies, however, it has been generally assumed that query is the only element available about the user 's information need. In reality, query is always formulated in a search context. These factors include, among many others, the user 's domain of interest, knowledge, preferences, etc.. All these elements specify the 8. CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need. Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile. Similarly to some previous studies, we propose to model topic domains instead of the user. Previous investigations on context focused on factors around the query. We showed in this paper that factors within the query are also important -- they help select the appropriate term relations to apply in query expansion. We have integrated the above contextual factors, together with feedback model, in a single language model. Our experimental results strongly confirm the benefit of using contexts in IR. This work also shows that the language modeling framework is appropriate for integrating many contextual factors. This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains. It would also be interesting to test the method on Web search using user search history.", "keyphrases": ["user profil", "queri-specif context", "user-centric on", "domain of interest", "context factor", "word sens disambigu", "inform need", "search context", "domain knowledg", "util of gener knowledg", "problem of knowledg ambigu", "context-independ", "context inform", "domain model", "radic solut", "googl person search"]}
{"file_name": "C-28", "text": "PackageBLAST : An Adaptive Multi-Policy Grid Service for Biological Sequence Comparison * ABSTRACT In this paper, we propose an adaptive task allocation framework to perform BLAST searches in a grid environment against sequence database segments. The framework, called PackageBLAST, provides an infrastructure to choose or incorporate task allocation strategies. Furthermore, we propose a mechanism to compute grid nodes execution weight, adapting the chosen allocation policy to the current computational power of the nodes. Our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios. 1. INTRODUCTION SW -LSB- 14 -RSB- is an exact algorithm that finds the best local alignment between two sequences of size n in quadratic time and space. For this reason, heuristics like BLAST -LSB- 3 -RSB- were proposed to reduce execution time. CLS. Supported by ACM. Resource scheduling is one of the most important components of a grid system. The choice of the best resources for a particular application is called task allocation, which is an NP-Complete problem. Grid applications usually do not have high communication rates and many of them follow the master/slave model -LSB- 13 -RSB-. In order to schedule master/slave applications many task allocation policies were proposed such as Self Scheduling -LSB- 15 -RSB- and FAC2 -LSB- 8 -RSB-. The choice of the best allocation policy depends on the application access pattern and on the environment in which it runs -LSB- 13 -RSB-. In this paper, we propose PackageBLAST, an adaptive multi-policy grid service to run BLAST searches in grids composed by segmented genetic databases. PackageBLAST executes on Globus 3 -LSB- 4 -RSB- and, by now, provides five allocation policies. Also, we propose an adaptive mechanism to assign weights to the grid nodes, taking into account their current workload. As far as we know, this is the first grid service that runs BLAST with multiple task policies with a segmented database in a heterogeneous non-dedicated platform. This paper is organized as follows. Section 2 presents the sequence comparison problem and the BLAST algorithm. Section 3 describes allocation policies for grids. Section 4 discusses related work. Section 5 presents the design of PackageBLAST. Experimental results are discussed in section 6. Section 7 concludes the paper. 4. RELATED WORK First, the genetic database is segmented. Then, the queries are evenly distributed among the nodes. If the node does not have a database fragment, a local copy is made. A method is proposed that associates data fragments to nodes, trying to minimize the number of copies. BLAST + + -LSB- 10 -RSB- groups multiple sequences to reduce the number of database accesses. A master/slave approach is used that allocates the queries to the slaves according to the fixed policy -LRB- section 3.3 -RRB-. Each worker executes BLAST + + independently and, finally, the results are collected and combined by the master. GridBlast -LSB- 9 -RSB- is a master/slave grid application that uses Globus 2. It distributes sequences among the grid nodes using two allocation policies : FCFS and minmax. However, to use minmax, the total execution time of each BLAST task must be known. Having decided which sequences will be compared by each node, GridBlast sends the sequences, the executable files and the whole database to the chosen node. When the search finishes, the results are compacted and sent to the master. Grid Blast Toolkit -LRB- GBTK -RRB- -LSB- 12 -RSB- is a web portal to execute BLAST searches in Globus 3. All genetic databases are statically placed on the grid nodes -LRB- without replication -RRB-. GBTK is a master/slave application that receives the sequences and the name of the genetic database. It then verifies if the node that contains the database is available. If the node is not available, the less loaded node is chosen and the database is copied to it. The database is replicated in the nodes, but only part of it is processed in each node Figure 2 : PackageBLAST segmentation and distribution mechanism. 7. CONCLUSION In this article, we proposed and evaluated PackageBLAST, an adaptive multi-policy grid service to execute master/slave BLAST searches. PackageBLAST contains a framework where the user can choose or incorporate allocation policies. We also defined a strategy, PSS, that adapts the chosen policy to a heterogeneous non-dedicated grid environment. The results collected by running PackageBLAST with 5 allocation policies in a grid testbed were very good. In order to compare a 10KBP real DNA sequence against the nr genetic database, we were able to reduce execution time from 30.88 min to 2.11 min. Also, we showed that, in our testbed, there is no allocation policy that always achieves the best performance and that makes evident the importance of providing multiple policies. Moreover, we showed that the introduction of PSS led to very good performance gains for some policies. As future work, we intend to run PackageBLAST in a geographically dispersed grid, to evaluate the impact of high network latencies in the allocation policies and in PSS. Also, we intend to provide support for genomic database synchronization and dynamic join/leave operations for slaves.", "keyphrases": ["biolog sequenc comparison", "adapt multi-polici grid servic", "task alloc", "blast search", "packageblast", "bioinformat", "grid comput", "comput biologi", "genom project", "segment genet databas", "heterogen non-dedic platform", "grid environ", "pss", "packag weight adapt self-schedul"]}
{"file_name": "C-14", "text": "Sensor Deployment Strategy for Target Detection ABSTRACT In order to monitor a region for traffic traversal, sensors can be deployed to perform collaborative target detection. Such a sensor network achieves a certain level of detection performance with an associated cost of deployment. This paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps. It illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step. 1. INTRODUCTION Such a network can be used to monitor the environment, detect, classify and locate specific events, and track targets over a specific region. The deployment of sensor networks varies with the application considered. It can be predetermined when the environment is sufficiently known and under control, in which case the sensors can be strategically hand placed. This paper investigates deployment strategies for sensor networks performing target detection over a region of interest. Since the local observations made by the sensors depend on their position, the performance of the detection algorithm is a function of the deployment. One possible measure of the goodness of deployment for target detection is called path exposure. It is a measure of the likelihood of detecting a target traversing the region using a given path. The higher the path exposure, the better the deployment. The set of paths to be considered may be constrained by the environment. For example, if the target is expected to be following a road, only the paths consisting of the roads need to be considered. In this study, the deployment is assumed to be random which corresponds to many practical applications where the region to be monitored is not accessible for precise placement of sensors. The focus of this paper is to determine the number of sensors to be deployed to carry out target detection in a region of interest. The tradeoffs lie between the network performance, the cost of the sensors deployed, and the cost of deploying the sensors. This paper is organized as follows. In section 2, a definition for path exposure is proposed and a method to evaluate the exposure of a given path is developed. In section 3, the problem of random deployment is formulated and several solutions are presented. The paper concludes with section 7. 7. CONCLUSION This paper addresses the problem of sensor deployment in a region to be monitored for target intrusion. A mechanism for sensor collaboration to perform target detection is proposed and analyzed to evaluate the exposure of paths through the region. The minimum exposure is used as a measure of the goodness of deployment, the goal being to maximize the exposure of the least exposed path in the region. In the case where sensors are randomly placed in a region to be monitored, a mechanism for sequential deployment in steps is developed. The strategy consists of deploying a limited number of sensors at a time until the desired minimum exposure is achieved. The cost function used in this study depends on the number of sensors deployed in each step and the cost of each deployment. Through simulation, the distribution of minimum exposure obtained by random deployment was evaluated for varying number of sensors deployed. These results were used to evaluate the cost of deployment for varying number of sensors deployed in each step. We found that the optimal number of sensors deployed in each step varies with the relative cost assigned to deployment and sensors. The results of this study can be extended to larger regions with different target parameters. The solution proposed in this paper can also be improved by considering deploying variable number of sensors at each step and this multiple variables problem requires further investigation.", "keyphrases": ["target detect", "sensor network", "path exposur", "number of sensor", "sequenti deploy", "minimum exposur", "random sensor placement", "sensor field", "target decai"]}
{"file_name": "C-6", "text": "Design and Implementation of a Distributed Content Management System ABSTRACT The convergence of advances in storage, encoding, and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices. Keeping track of -LRB- or managing -RRB- such content remains challenging due to the sheer volume of data. Storing `` live '' continuous media -LRB- such as TV or radio content -RRB- adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with. Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network, making the task of content management all but impossible to deal with without a content management system. In this paper we present the design and implementation of the Spectrum content management system, which deals with rich media content effectively in this environment. Spectrum has a modular architecture that allows its application to both stand-alone and various networked scenarios. A unique aspect of Spectrum is that it requires one -LRB- or more -RRB- retention policies to apply to every piece of content that is stored in the system. This means that there are no eviction policies. Content that no longer has a retention policy applied to it is simply removed from the system. Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication. This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media, to content. We not only describe the details of the Spectrum architecture but also give typical use cases. 1. INTRODUCTION Manipulating and managing content is and has always been one of the primary functions of a computer. Initial computing applications include text formatters and program compilers. Content was initially managed by explicit user interaction through the use of files and filesystems. As technology has advanced, both the types of content and the way people wish to use it have greatly changed. New content types such as continuous multimedia streams have become commonplace due to the convergence of advances in storage, encoding, and networking technologies. Another example is the combination of encoding and broadband networking technology. This combination has allowed users to access and share multimedia content in both local and remote area networks with the network itself acting as a huge data repository. The proliferation of high quality content enabled by these advances in storage, encoding, and networking technology creates the need for new ways to manipulate and manage the data. The focus of our work is on the storage of media rich content and in particular the storage of continuous media content in either pre-packaged or `` live '' forms. \u2022 While true for all types of content the storage of continuous media content is especially problematic. First continuous media content is still very demanding in terms of storage resources which means that a policy-less approach to storing it will not work for all but the smallest systems. Second, the storing of `` live '' content such as TV or radio is inherently problematic as these signals are continuous streams with no endpoints. This means that before one can even think about managing such content there is a need to abstract it into something that could be manipulated and managed. . When dealing with stored continuous media there is a need to manage such content at both a fine-grained as well as an aggregate level. For example, an individual PVR user wanting to keep only the highlights of a particular sporting event should not be required to have to store the content pertaining to the complete event. . As indicated above, trying to keep track of content on a standalone system without a content management system is very difficult. However, when the actual storage devices are distributed across a network the task of keeping track of content is almost impossible. This scenario is increasingly common in network based content distribution systems and is likely to also become important in home-networking scenarios. It would seem clear then that a content management system that can efficiently handle media rich content while also exploiting the networked capability of storage devices is needed. This system should allow efficient storage of and access to content across heterogeneous network storage devices according to user preferences. The content management system should translate user preferences into appropriate low-level storage policies and should allow those preferences to be expressed at a fine level of granularity -LRB- while not requiring it in general -RRB-. The content management system should allow the user to manipulate and reason about -LRB- i.e. change the storage policy associated with -RRB- the storage of -LRB- parts of -RRB- continuous media content. Addressing this distributed content management problem is difficult due to the number of requirements placed on the system. For example :. The content management system must operate on a large number of heterogeneous systems. In some cases the system may be managing content stored on a local filesystem, while in others the content may be stored on a separate network storage appliance. The content manager may be responsible for implementing the policies it uses to reference content or that role may be delegated to a separate computer. A application program interface -LRB- API -RRB- and associated network protocols are needed in order for the content management system to provide a uniform interface. . The content management system should be flexible and be able to handle differing requirements for content management policies. These policies reflect what content should be obtained, when it should be fetched, how long it should be retained, and under what circumstances it should be discarded. This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. . The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. . The content management system must handle the interaction between implicit and explicit population of content at the network edge. . The content system must be able to efficiently manage large sets of content, including continuous streams. It needs to be able to package this content in such a way that it is convenient for users to access. To address these issues we have designed and implemented the Spectrum content management system architecture. It allows multiple applications to reference content using differing policies. Note that the Spectrum architecture assumes the existence of a content distribution network -LRB- CDN -RRB- that can facilitate the efficient distribution of content -LRB- for example, the PRISM CDN architecture -LSB- 2 -RSB- -RRB-. Section 2 describes the architecture of our content management system. In Section 3 we describe both our implementation of the Spectrum architecture and examples of its use. 4. RELATED WORK Several authors have addressed the problem of the management of content in distributed networks. Much of the work focuses on the policy management aspect. For example in -LSB- 5 -RSB-, the problem of serving multimedia content via distributed servers is considered. Content is distributed among server resources in proportion to user demand using a Demand Dissemination Protocol. The performance of the scheme is benchmarked via simulation. In -LSB- 1 -RSB- content is distributed among sub-caches. The Cache Knowledge base allows sophisticated policies to be employed. Simulation is used to compare the proposed scheme with well-known replacement algorithms. Our work differs in that we are considering more than the policy management aspects of the problem. After carefully considering the required functionality to implement content management in the networked environment, we have partitioned the system into three simple functions, namely Content manager, Policy manager and Storage manager. This has allowed us to easily implement and experiment with a prototype system. Other related work involves so called TV recommendation systems which are used in PVRs to automatically select content for users, e.g. -LSB- 6 -RSB-. Finally, in the commercial CDN environment vendors -LRB- e.g. Cisco and Netapp -RRB- have developed and implemented content management products and tools. 5. CONCLUSION AND FUTURE WORK In this paper we presented the design and implementation of the Spectrum content management architecture. Spectrum allows storage policies to be applied to large volumes of content to facilitate efficient storage. Specifically, the system allows different policies to be applied to the same content without replication. Spectrum can also apply policies that are `` time-aware '' which effectively deals with the storage of continuous media content. Finally, the modular design of the Spectrum architecture allows both stand-alone and distributed realizations so that the system can be deployed in a variety of applications. There are a number of open issues that will require future work. Some of these issues include : \u2022 We envision Spectrum being able to manage content on systems ranging from large CDNs down to smaller appliances such as TiVO -LSB- 8 -RSB-. In order for these smaller systems to support Spectrum they will require networking and an external API. When that API becomes available, we will have to work out how it can be fit into the Spectrum architecture. \u2022 Spectrum names content by URL, but we have intentionally not defined the format of Spectrum URLs, how they map back to the content 's actual name, or how the names and URLs should be presented to the user. \u2022 In this paper we 've focused on content management for continuous media objects. \u2022 Any project that helps allow multimedia content to be easily shared over the Internet will have legal hurdles to overcome before it can achieve widespread acceptance. Adapting Spectrum to meet legal requirements will likely require more technical work.", "keyphrases": ["spectrum content manag system", "continu media storag", "home-network scenario", "applic program interfac", "content distribut network", "uniform resourc locat", "polici manag", "network enabl dvr", "high-perform databas system", "carrier-grade spectrum manag"]}
{"file_name": "H-11", "text": "Laplacian Optimal Design for Imag e Retrieval ABSTRACT Relevance feedback is a powerful technique to enhance ContentBased Image Retrieval -LRB- CBIR -RRB- performance. It solicits the user 's relevance judgments on the retrieved images returned by the CBIR systems. The user 's labeling is then used to learn a classifier to distinguish between relevant and irrelevant images. However, the top returned images may not be the most informative ones. The challenge is thus to determine which unlabeled images would be the most informative -LRB- i.e., improve the classifier the most -RRB- if they were labeled and used as training samples. In this paper, we propose a novel active learning algorithm, called Laplacian Optimal Design -LRB- LOD -RRB-, for relevance feedback image retrieval. Our algorithm is based on a regression model which minimizes the least square error on the measured -LRB- or, labeled -RRB- images and simultaneously preserves the local geometrical structure of the image space. Specifically, we assume that if two images are sufficiently close to each other, then their measurements -LRB- or, labels -RRB- are close as well. By constructing a nearest neighbor graph, the geometrical structure of the image space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images, which gives us the most amount of information. Experimental results on Corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval. 1. INTRODUCTION In many machine learning and information retrieval tasks, there is no shortage of unlabeled data but labels are expensive. The challenge is thus to determine which unlabeled samples would be the most informative -LRB- i.e., improve the classifier the most -RRB- if they were labeled and used as training samples. This problem is typically called active learning -LSB- 4 -RSB-. Many real world applications can be casted into active learning framework. Particularly, we consider the problem of relevance feedback driven Content-Based Image Retrieval -LRB- CBIR -RRB- -LSB- 13 -RSB-. Content-Based Image Retrieval has attracted substantial interests in the last decade -LSB- 13 -RSB-. It is motivated by the fast growth of digital image databases which, in turn, require efficient search schemes. Rather than describe an image using text, in these systems an image query is described using one or more example images. The low level visual features -LRB- color, texture, shape, etc. -RRB- are automatically extracted to represent the images. To narrow down the semantic gap, relevance feedback is introduced into CBIR -LSB- 12 -RSB-. In many of the current relevance feedback driven CBIR systems, the user is required to provide his/her relevance judgments on the top images returned by the system. The labeled images are then used to train a classifier to separate images that match the query concept from those that do not. However, in general the top returned images may not be the most informative ones. In the worst case, all the top images labeled by the user may be positive and thus the standard classification techniques can not be applied due to the lack of negative examples. Unlike the standard classification problems where the labeled samples are pregiven, in relevance feedback image retrieval the system can actively select the images to label. Thus active learning can be naturally introduced into image retrieval. Despite many existing active learning techniques, Support Vector Machine -LRB- SVM -RRB- active learning -LSB- 14 -RSB- and regression based active learning -LSB- 1 -RSB- have received the most interests. The major disadvantage of SVM active learning is that the estimated boundary may not be accurate enough. Moreover, it may not be applied at the beginning of the retrieval when there is no labeled images. Some other SVM based active learning algorithms can be found in -LSB- 7 -RSB-, -LSB- 9 -RSB-. In statistics, the problem of selecting samples to label is typically referred to as experimental design. The sample x is referred to as experiment, and its label y is referred to as measurement. The study of optimal experimental design -LRB- OED -RRB- -LSB- 1 -RSB- is concerned with the design of experiments that are expected to minimize variances of a parameterized model. The intent of optimal experimental design is usually to maximize confidence in a given model, minimize parameter variances for system identification, or minimize the model 's output variance. Classical experimental design approaches include A-Optimal Design, D-Optimal Design, and E-Optimal Design. All of these approaches are based on a least squares regression model. Comparing to SVM based active learning algorithms, experimental design approaches are much more efficient in computation. However, this kind of approaches takes only measured -LRB- or, labeled -RRB- data into account in their objective function, while the unmeasured -LRB- or, unlabeled -RRB- data is ignored. Benefit from recent progresses on optimal experimental design and semi-supervised learning, in this paper we propose a novel active learning algorithm for image retrieval, called Laplacian Optimal Design -LRB- LOD -RRB-. Unlike traditional experimental design methods whose loss functions are only defined on the measured points, the loss function of our proposed LOD algorithm is defined on both measured and unmeasured points. Specifically, we introduce a locality preserving regularizer into the standard least-square-error based loss function. The new loss function aims to find a classifier which is locally as smooth as possible. In other words, if two points are sufficiently close to each other in the input space, then they are expected to share the same label. Once the loss function is defined, we can select the most informative data points which are presented to the user for labeling. It would be important to note that the most informative images may not be the top returned images. The rest of the paper is organized as follows. In Section 2, we provide a brief description of the related work. Our proposed Laplacian Optimal Design algorithm is introduced in Section 3. In Section 4, we compare our algorithm with the state-or-the-art algorithms and present the experimental results on image retrieval. Finally, we provide some concluding remarks and suggestions for future work in Section 5. 2. RELATED WORK Since our proposed algorithm is based on regression framework. The most related work is optimal experimental design -LSB- 1 -RSB-, including A-Optimal Design, D-Optimal Design, and EOptimal Design. In this Section, we give a brief description of these approaches. 2.1 The Active Learning Problem The generic problem of active learning is the following. In other words, the points zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- can improve the classifier the most if they are labeled and used as training points. 2.2 Optimal Experimental Design We consider a linear regression model Different observations have errors that are independent, but with equal variances \u03c32. Thus, the maximum likelihood estimate for the weight vector, \u02c6w, is that which minimizes the sum squared error The three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are : \u2022 D-optimal design : determinant of Hsse. \u2022 A-optimal design : trace of Hsse. \u2022 E-optimal design : maximum eigenvalue of Hsse. Since the computation of the determinant and eigenvalues of a matrix is much more expensive than the computation of matrix trace, A-optimal design is more efficient than the other two. Some recent work on experimental design can be found in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONS AND FUTURE WORK This paper describes a novel active learning algorithm, called Laplacian Optimal Design, to enable more effective relevance feedback image retrieval. Our algorithm is based on an objective function which simultaneously minimizes the empirical error and preserves the local geometrical structure of the data space. Using techniques from experimental design, our algorithm finds the most informative images to label. These labeled images and the unlabeled images in the database are used to learn a classifier. The experimental results on Corel database show that both active learning and semi-supervised learning can significantly improve the retrieval performance. In this paper, we consider the image retrieval problem on a small, static, and closed-domain image data. For Web image search, it is possible to collect a large amount of user click information. This information can be naturally used to construct the affinity graph in our algorithm.", "keyphrases": ["relev feedback", "imag represent", "contentbas imag retriev", "activ learn", "least squar regress model", "optim experiment design", "top return imag", "precis rate", "intrins geometr structur", "patten recognit", "label"]}
{"file_name": "J-27", "text": "Learning From Revealed Preference ABSTRACT A sequence of prices and demands are rationalizable if there exists a concave, continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price. Afriat -LSB- 1 -RSB- presented necessary and sufficient conditions for a finite sequence to be rationalizable. Varian -LSB- 20 -RSB- and later Blundell et al. -LSB- 3, 4 -RSB- continued this line of work studying nonparametric methods to forecasts demand. Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast. The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts. Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable, but that there exist interesting and potentially useful classes that are learnable from finite samples. We also present a learning algorithm that is an adaptation of a new proof of Afriat 's theorem due to Teo and Vohra -LSB- 17 -RSB-. 1. INTRODUCTION The preference relation is therefore the key factor in understanding consumer behavior. One of the common assumptions in this theory is that the preference relation is represented by a utility function and that agents strive to maximize their utility given a budget constraint. This pattern of behavior is the essence of supply and demand, general equilibria and other aspects of consumer theory. Furthermore, as we elaborate in section 2, basic observations on market demand behavior suggest that utility functions are monotone and concave. This brings us to the question, first raised by Samuelson -LSB- 18 -RSB-, to what degree is this theory refutable? Given observations of price and demand, under what circumstances can we conclude that the data is consistent with the behavior of a utility maximizing agent equipped with a monotone concave utility function and subject to a budget constraint? Samuelson gave a necessary but insufficient condition on the underlying preference known as the weak axiom of revealed preference. Uzawa -LSB- 16 -RSB- and Mas-Colell -LSB- 10, 11 -RSB- introduced a notion of income-Lipschitz and showed that demand functions with this property are rationalizable. These properties do not require any parametric assumptions and are technically refutable, but they do assume knowledge of the entire demand function and rely heavily on the differential properties of demand functions. Hence, an infinite amount of information is needed to refute the theory. It is often the case that apart form the demand observations there is additional information on the system and it is sensible to make parametric assumptions, namely, to stipulate some functional form of utility. Consistency with utility maximization would then depend on fixing the parameters of the utility function to be consistent with the observations and with a set of equations called the Slutski equations. If such parameters exist, we conclude that the stipulated utility form is consistent with the observations. This approach is useful when there is reason to make these stipulations, it gives an explicit utility function which can be used to make precise forecasts on demand for unob served prices. The downside of this approach is that real life data is often inconsistent with convenient functional forms. Moreover, if the observations are inconsistent it is unclear whether this is a refutation of the stipulated functional form or of utility maximization. He askes when can it be determined that a finite set of observations is consistent with utility maximization without making parametric assumptions? He showes that rationalizability of a finite set of observations is equivalent to the strong axiom of revealed preference. Richter -LSB- 15 -RSB- showes that strong axiom of revealed preference is equivalent to rationalizability by a strictly concave monotone utility function. Afriat -LSB- 1 -RSB- gives another set of rationalizability conditions the observations must satisfy. Varian -LSB- 20 -RSB- introduces the generalized axiom of revealed preference -LRB- GARP -RRB-, an equivalent form of Afriat 's consistency condition that is easier to verify computationally. Afriat -LSB- 1 -RSB- proved his theorem by an explicit construction of a utility function witnessing consistency. Varian -LSB- 20 -RSB- took this one step further progressing from consistency to forecasting. Varian 's forecasting algorithm basically rules out bundles that are revealed inferior to observed bundles and finds a bundle from the remaining set that together with the observations is consistent with GARP. Furthermore, he introduces Samuelson 's '' money metric '' as a canonical utility function and gives upper and lower envelope utility functions for the money metric. Knoblauch -LSB- 9 -RSB- shows these envelopes can be computed efficiently. A different approach is presented by Blundell et al. -LSB- 3, 4 -RSB-. These papers introduce a model where an agent observes prices and Engel curves for these prices. This gives an improvement on Varian 's original bounds, though the basic idea is still to rule out demands that are revealed inferior. This model is in a sense a hybrid between Mas-Colell and Afriat 's aproaches. The former requires full information for all prices, the latter for a finite number of prices. On the other hand the approach taken by Blundell et al. requires full information only on a finite number of price trajectories. Different segments of the population face the same prices with different budgets, and as much as aggregate data can testify on individual preferences, show how demand varies with the budget. Applying non parametric statistical methods, they reconstruct a trajectory from the observed demands of different segments and use it to obtain tighter bounds. Both these methods would most likely give a good forecast for a fixed demand function after sufficiently many observations assuming they were spread out in a reasonable manner. However, these methods do not consider the complexity of the demand functions and do not use any probabilistic model of the observations. Therefore, they are unable to provide any estimate of the number of observations that would be sufficient for a good forecast or the degree of confidence in such a forecast. In this paper we examine the feasibility of demand forecasting with a high degree of confidence using Afriat 's conditions. We formulate the question in terms of whether the class of demand functions derived from monotone concave utilities is efficiently PAC-learnable. Our first result is negative. We show, by computing the fat shattering dimension, that without any prior assumptions, the set of all demand functions induced by monotone concave utility functions is too rich to be efficiently PAC-learnable. However, under some prior assumptions on the set of demand functions we show that the fat shattering dimension is finite and therefore the corresponding sets are PAC-learnable. In section 2 we briefly discuss the basic assumptions of demand theory and their implications. In section 3 we present a new proof to Afriat 's theorem incorporating an algorithm for efficiently generating a forecasting function due to Teo and Vohra -LSB- 17 -RSB-. We show that this algorithm is computationally efficient and can be used as a learning algorithm. In section 4 we give a brief introduction to PAC learning including several modifications to learning real vector valued functions. We also sketch results on upper bounds. In section 5 we study the learnability of demand functions and directly compute the fat shattering dimension of the class of all demand functions and a class of income-Lipschitzian demand functions with a bounded global income-Lipschitz constant.", "keyphrases": ["learn from reveal prefer", "complex problem", "forecast", "probabl approxim correct", "monoton concav util function", "demand function", "rationaliz", "finit set of observ", "incom-lipschitz", "fat shatter dimens"]}
{"file_name": "C-18", "text": "An Initial Analysis and Presentation of Malware Exhibiting Swarm-Like Behavior ABSTRACT The Slammer, which is currently the fastest computer worm in recorded history, was observed to infect 90 percent of all vulnerable Internets hosts within 10 minutes. Although the main action that the Slammer worm takes is a relatively unsophisticated replication of itself, it still spreads so quickly that human response was ineffective. Most proposed countermeasures strategies are based primarily on rate detection and limiting algorithms. However, such strategies are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer. In our work, we put forth the hypothesis that next generation worms will be radically different, and potentially such techniques will prove ineffective. Specifically, we propose to study a new generation of worms called '' Swarm Worms '', whose behavior is predicated on the concept of '' emergent intelligence ''. Emergent Intelligence is the behavior of systems, very much like biological systems such as ants or bees, where simple local interactions of autonomous members, with simple primitive actions, gives rise to complex and intelligent global behavior. In this manuscript we will introduce the basic principles behind the idea of '' Swarm Worms '', as well as the basic structure required in order to be considered a '' swarm worm ''. In addition, we will present preliminary results on the propagation speeds of one such swarm worm, called the ZachiK worm. We will show that ZachiK is capable of propagating at a rate 2 orders of magnitude faster than similar worms without swarm capabilities. 1. INTRODUCTION AND PREVIOUS WORK In the early morning hours -LRB- 05:30 GMT -RRB- of January 25, 2003 the fastest computer worm in recorded history began spreading throughout the Internet. Since Slammer, researchers have explored the behaviors of fast spreading worms, and have designed countermeasures strategies based primarily on rate detection and limiting algorithms. For example, Zou, et al., -LSB- 2 -RSB-, proposed a scheme where a Kalman filter is used to detect the early propagation of a worm. That is, systems are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer. In the work described here, we put forth the hypothesis that next generation worms will be different, and therefore such techniques may have some significant limitations. Specifically, we propose to study a new generation of worms called '' Swarm Worms '', whose behavior is predicated on the concept of '' emergent intelligence ''. The concept of emergent intelligence was first studied in association with biological systems. In such studies, early researchers discovered a variety of interesting insect or animal behaviors in the wild. A flock of birds sweeps across the sky. In general, this kind of aggregate motion has been called '' swarm behavior. '' Biologists, and computer scientists in the field of artificial intelligence have studied such biological swarms, and attempted to create models that explain how the elements of a swarm interact, achieve goals, and evolve. The basic concepts that have been developed over the last decade to explain '' swarms, and '' swarm behavior '' include four basic components. These are : 1. Simplicity of logic & actions : A swarm is composed of N agents whose intelligence is limited. Agents in the swarm use simple local rules to govern their actions. Some models called this primitive actions or behaviors ; 2. Local Communication Mechanisms : Agents interact with other members in the swarm via simple '' local '' communication mechanisms. For example, a bird in a flock senses the position of adjacent bird and applies a simple rule of avoidance and follow. 3. 4. '' Emergent Intelligence '' : Aggregate behavior of autonomous agents results in complex '' intelligent '' behaviors ; including self-organization ''. In order to understand fully the behavior of such swarms it is necessary to construct a model that explains the behavior of what we will call generic worms. This model, which extends the work by Weaver -LSB- 5 -RSB- is presented here in section 2. In addition, we intend to extend said model in such a way that it clearly explains the behaviors of this new class of potentially dangerous worms called Swarm Worms. Swarm Worms behave very much like biological swarms and exhibit a high degree of learning, communication, and distributed intelligence. Such Swarm Worms are potentially more harmful than their similar generic counterparts. Specifically, the first instance, to our knowledge, of such a learning worm was created, called ZachiK. ZachiK is a simple password cracking swarm worm that incorporates different learning and information sharing strategies. Such a swarm worm was deployed in both a local area network of thirty - -LRB- 30 -RRB- hosts, as well as simulated in a 10,000 node topology. Preliminary results showed that such worms are capable of compromising hosts at rates up to two orders of magnitude faster than their generic counterpart. The rest of this manuscript is structure as follows. In section 2 an abstract model of both generic worms as well as swarm worms is presented. This model is used in section 2.6 to described the first instance of a swarm worm, ZachiK. In section 4, preliminary results via both empirical measurements as well as simulation is presented. Finally, in section 5 our conclusions and insights into future work are presented. 5. SUMMARY AND FUTURE WORK In this manuscript, we have presented an abstract model, similar in some aspects to that of Weaver -LSB- 5 -RSB-, that helps explain the generic nature of worms. The model presented in section 2 was extended to incorporate a new class of potentially dangerous worms called Swarm Worms. Swarm Worms behave very much like biological swarms and exhibit a high degree of learning, communication, and distributed intelligence. Such Swarm Worms are potentially more harmful than their generic counterparts. In addition, the first instance, to our knowledge, of such a learning worm was created, called ZachiK. ZachiK is a simple password cracking swarm worm that incorporates different learning and information sharing strategies. Such a swarm worm was deployed in both a local area network of thirty - -LRB- 30 -RRB- hosts, as well as simulated in a 10,000 node topology. Preliminary results showed that such worms is capable of compromising hosts a rates up to 2 orders of magnitude faster than its generic counterpart while retaining stealth capabilities. This work opens up a new area of interesting problems. Some of the most interesting and pressing problems to be consider are as follows : \u2022 Is it possible to apply some of learning concepts developed over the last ten years in the areas of swarm intelligence, agent systems, and distributed control to the design of sophisticated swarm worms in such a way that true emergent behavior takes place? \u2022 Are the current techniques being developed in the design of Intrusion Detection & CounterMeasure Systems and Survivable systems effective against this new class of worms? ; and \u2022 What techniques, if any, can be developed to create defenses against swarm worms?", "keyphrases": ["malwar", "swarm worm", "emerg intellig", "slammer worm", "local commun mechan", "zachik", "prng method", "pre-gener target list", "distribut intellig", "intrus detect", "countermeasur system"]}
{"file_name": "J-26", "text": "Combinatorial Agency ABSTRACT Much recent research concerns systems, such as the Internet, whose components are owned and operated by different parties, each with his own `` selfish '' goal. The field of Algorithmic Mechanism Design handles the issue of private information held by the different parties in such computational settings. This paper deals with a complementary problem in such settings : handling the `` hidden actions '' that are performed by the different parties. Our model is a combinatorial variant of the classical principalagent problem from economic theory. In our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf, but their actions are hidden from him. Our focus is on cases where complex combinations of the efforts of the agents influence the outcome. The principal motivates the agents by offering to them a set of contracts, which together put the agents in an equilibrium point of the induced game. We present formal models for this setting, suggest and embark on an analysis of some basic issues, but leave many questions open. 1. INTRODUCTION 1.1 Background One of the most striking characteristics of modern computer networks -- in particular the Internet -- is that different parts of it are owned and operated by different individuals, firms, and organizations. The analysis and design of protocols for this environment thus naturally needs to take into account the different `` selfish '' economic interests of the different participants. In particular, the field of algorithmic mechanism design -LSB- 6 -RSB- uses appropriate incentives to `` extract '' the private information from the participants. This paper deals with the complementary lack of knowledge, that of hidden actions. In many cases the actual behaviors -- actions -- of the different participants are `` hidden '' from others and only influence the final outcome indirectly. How can we ensure that the right combination of allocations is actually made by the different servers? A related class of examples concerns security issues : each `` link '' in a complex system may exert different levels of effort for protecting some desired security property of the system. How can we ensure that the desired level of 5. ALGORITHMIC ASPECTS Our analysis throughout the paper sheds some light on the algorithmic aspects of computing the best contract. In this section we state these implications -LRB- for the proofs see -LSB- 2 -RSB- -RRB-. We first consider the general model where the technology function is given by an arbitrary monotone function t -LRB- with rational values -RRB-, and we then consider the case of structured technologies given by a network representation of the underlying Boolean function. 5.1 Binary-Outcome Binary-Action Technologies Here we assume that we are given a technology and value v as the input, and our output should be the optimal contract, i.e. the set S * of agents to be contracted and the contract pi for each i E S *. In the general case, the success function t is of size exponential in n, the number of agents, and we will need to deal with that. In the special case of anonymous technologies, the description of t is only the n +1 numbers t0,..., tn, and in this case our analysis in section 3 completely suffices for computing the optimal contract. \u2022 The orbit of the technology in both the agency and the non-strategic cases. \u2022 An optimal contract for any given value v, for both the agency and the non-strategic cases. \u2022 The price of unaccountability POU -LRB- t, ~ c -RRB-. PROOF. We prove the claims for the non-anonymous case, the proof for the anonymous case is similar. We first show how to construct the orbit of the technology -LRB- the same procedure apply in both cases -RRB-. To construct the orbit we find all transition points and the sets that are in the orbit. The empty contract is always optimal for v = 0. Assume that we have calculated the optimal contracts and the transition points up to some transition point v for which S is an optimal contract with the highest success probability. We show how to calculate the next transition point and the next optimal contract. By Lemma 3 the next contract on the orbit -LRB- for higher values -RRB- has a higher success probability -LRB- there are no two sets with the same success probability on the orbit -RRB-. We calculate the next optimal contract by the following procedure. We go over all sets T such that t -LRB- T -RRB- > t -LRB- S -RRB-, and calculate the value for which the principal is indifferent between contracting with T and contracting with S. The minimal indifference value is the next transition point and the contract that has the minimal indifference value is the next optimal contract. Linearity of the utility in the value and monotonicity of the success probability of the optimal contracts ensure that the above works. Clearly the above calculation is polynomial in the input size. Once we have the orbit, it is clear that an optimal contract for any given value v can be calculated. We find the largest transition point that is not larger than the value v, and the optimal contract at v is the set with the higher success probability at this transition point. Finally, as we can calculate the orbit of the technology in both the agency and the non-strategic cases in polynomial time, we can find the price of unaccountability in polynomial time. By Lemma 1 the price of unaccountability POU -LRB- t -RRB- is obtained at some transition point, so we only need to go over all transition points, and find the one with the maximal social welfare ratio. A more interesting question is whether if given the function t as a black box, we can compute the optimal contract in time that is polynomial in n. We can show that, in general this is not the case : THEOREM 5. Given as input a black box for a success function t -LRB- when the costs are identical -RRB-, and a value v, the number of queries that is needed, in the worst case, to find the optimal contract is exponential in n. PROOF. Consider the following family of technologies. For some small e > 0 and k = -LSB- n/2 -RSB- we define the success probability for a given set T as follows. If the algorithm queries about at most -LRB- n -RRB- -- 2 sets fin/2 -RSB- of size k, then it can not always determine the optimal contract -LRB- as any of the sets that it has not queried about might be the optimal one -RRB-. We conclude that -LRB- n -RRB- -- 1 queries fin/2 -RSB- are needed to determine the optimal contract, and this is exponential in n. 5.2 Structured Technologies In this section we will consider the natural representation of read-once networks for the underlying Boolean function. Thus the problem we address will be : The Optimal Contract Problem for Read Once Networks : Input : A read-once network G = -LRB- V, E -RRB-, with two specific vertices s, t ; rational values - ye, \u03b4e for each player e \u2208 E -LRB- and ce = 1 -RRB-, and a rational value v. Output : A set S of agents who should be contracted in an optimal contract. Let t -LRB- E -RRB- denote the probability of success when each edge succeeds with probability \u03b4e. We first notice that even computing the value t -LRB- E -RRB- is a hard problem : it is called the network reliability problem and is known to be #P \u2212 hard -LSB- 8 -RSB-. Just a little effort will reveal that our problem is not easier : THEOREM 6. The Optimal Contract Problem for Read Once Networks is #P - hard -LRB- under Turing reductions -RRB-. PROOF. We will show that an algorithm for this problem can be used to solve the network reliability problem. Given an instance of a network reliability problem < G, -LCB- -LRB- e -RCB- eEE > -LRB- where -LRB- e denotes e 's probability of success -RRB-, we define an instance of the optimal contract problem as follows : first define a new graph G ' which is obtained by '' And '' ing G with a new player x, with - yx very close to 21 and \u03b4x = 1 \u2212 - yx. Once we find such a value, we choose - yx s.t. c 1 -- 2\u03b3x is larger than that value -RRB-. Let us denote \u03b2x = 1 \u2212 2-yx. The critical value of v where player x enters the optimal contract of G ', can be found using binary search over the algorithm that supposedly finds the optimal contract for any network and any value. Note that at this critical value v, the principal is indifferent between the set E and E \u222a -LCB- x -RCB-. thus, if we can always find the optimal contract we are also able to compute the value of t -LRB- E -RRB-. In conclusion, computing the optimal contract in general is hard. These results suggest two natural research directions. The first avenue is to study families of technologies whose optimal contracts can be computed in polynomial time. The second avenue is to explore approximation algorithms for the optimal contract problem. A possible candidate for the first direction is the family of series-parallel networks, for which the network reliability problem -LRB- computing the value of t -RRB- is polynomial.", "keyphrases": ["optim set of contract", "classic princip-agent", "qualiti of servic", "combinatori agenc", "nash equilibrium", "contract action", "k-orbit", "anonym technolog", "seri-parallel network", "price of unaccount"]}
{"file_name": "J-11", "text": "Trading Networks with Price-Setting Agents ABSTRACT In a wide range of markets, individual buyers and sellers often trade through intermediaries, who determine prices via strategic considerations. Typically, not all buyers and sellers have access to the same intermediaries, and they trade at correspondingly different prices that reflect their relative amounts of power in the market. We model this phenomenon using a game in which buyers, sellers, and traders engage in trade on a graph that represents the access each buyer and seller has to the traders. In this model, traders set prices strategically, and then buyers and sellers react to the prices they are offered. We show that the resulting game always has a subgame perfect Nash equilibrium, and that all equilibria lead to an efficient -LRB- i.e. socially optimal -RRB- allocation of goods. We extend these results to a more general type of matching market, such as one finds in the matching of job applicants and employers. Finally, we consider how the profits obtained by the traders depend on the underlying graph -- roughly, a trader can command a positive profit if and only if it has an `` essential '' connection in the network structure, thus providing a graph-theoretic basis for quantifying the amount of competition among traders. Our work differs from recent studies of how price is affected by network structure through our modeling of price-setting as a strategic activity carried out by a subset of agents in the system, rather than studying prices set via competitive equilibrium or by a truthful mechanism. 1. INTRODUCTION In a range of settings where markets mediate the interactions of buyers and sellers, one observes several recurring properties : Individual buyers and sellers often trade through intermediaries, not all buyers and sellers have access to the same intermediaries, and not all buyers and sellers trade at the same price. One example of this setting is the trade of agricultural goods in developing countries. Given inadequate transportation networks, and poor farmers ' limited access to capital, many farmers have no alternative to trading with middlemen in inefficient local markets. A developing country may have many such partially overlapping markets existing alongside modern efficient markets -LSB- 2 -RSB-. Financial markets provide a different example of a setting with these general characteristics. In these markets much of the trade between buyers and sellers is intermediated by a variety of agents ranging from brokers to market makers to electronic trading systems. For many assets there is no one market ; trade in a single asset may occur simultaneously on the floor of an exchange, on crossing networks, on electronic exchanges, and in markets in other countries. Some buyers and sellers have access to many or all of these trading venues ; others have access to only one or a few of them. The price at which the asset trades may differ across these trading venues. In fact, there is no `` price '' as different traders pay or receive different prices. In many settings there is also a gap between the price a buyer pays for an asset, the ask price, and the price a seller receives for the asset, the bid price. Spreads, defined as the difference between bid and ask prices, differ significantly across these markets, even though the same asset is being traded in the two markets. In this paper, we develop a framework in which such phenomena emerge from a game-theoretic model of trade, with buyers, sellers, and traders interacting on a network. The edges of the network connect traders to buyers and sellers, and thus represent the access that different market participants have to one another. The traders serve as intermediaries in a two-stage trading game : they strategically choose bid and ask prices to offer to the sellers and buyers they are connected to ; the sellers and buyers then react to the prices they face. Thus, the network encodes the relative power in the structural positions of the market participants, including the implicit levels of competition among traders. We show that this game always has a subgame perfect Nash equilibrium, and that all equilibria lead to an efficient -LRB- i.e. socially optimal -RRB- allocation of goods. We also analyze how trader profits depend on the network structure, essentially characterizing in graph-theoretic terms how a trader 's payoff is determined by the amount of competition it experiences with other traders. By developing a network model that explicitly includes traders as price-setting agents, in a system together with buyers and sellers, we are able to capture price formation in a network setting as a strategic process carried out by intermediaries, rather than as the result of a centrally controlled or exogenous mechanism. The Basic Model : Indistinguishable Goods. Our goal in formulating the model is to express the process of price-setting in markets such as those discussed above, where the participants do not all have uniform access to one another. We are given a set B of buyers, a set S of sellers, and a set T of traders. There is an undirected graph G that indicates who is able to trade with whom. This reflects the constraints that all buyer-seller transactions go through traders as intermediaries. In the most basic version of the model, we consider identical goods, one copy of which is initially held by each seller. Buyers and sellers each have a value for one copy of the good, and we assume that these values are common knowledge. We will subsequently generalize this to a setting in which goods are distinguishable, buyers can value different goods differently, and potentially sellers can value transactions with different buyers differently as well. Having different buyer valuations captures settings like house purchases ; adding different seller valuations as well captures matching markets -- for example, sellers as job applicants and buyers as employers, with both caring about who ends up with which `` good '' -LRB- and with traders acting as services that broker the job search -RRB-. Thus, to start with the basic model, there is a single type of good ; the good comes in individisible units ; and each seller initially holds one unit of the good. All three types of agents value money at the same rate ; and each i E B U S additionally values one copy of the good at \u03b8i units of money. No agent wants more than one copy of the good, so additional copies are valued at 0. Each agent has an initial endowment of money that is larger than any individual valuation \u03b8i ; the effect of this is to guarantee that any buyer who ends up without a copy of the good has been priced out of the market due to its valuation and network position, not a lack of funds. We picture each good that is sold flowing along a sequence of two edges : from a seller to a trader, and then from the trader to a buyer. The particular way in which goods flow is determined by the following game. First, each trader offers a bid price to each seller it is connected to, and an ask price to each buyer it is connected to. Sellers and buyers then choose from among the offers presented to them by traders. If multiple traders propose the same price to a seller or buyer, then there is no strict best response for the seller or buyer. Finally, each trader buys a copy of the good from each seller that accepts its offer, and it sells a copy of the good to each buyer that accepts its offer. If a particular trader t finds that more buyers than sellers accept its offers, then it has committed to provide more copies of the good than it has received, and we will say that this results in a large penalty to the trader for defaulting ; the effect of this is that in equilibrium, no trader will choose bid and ask prices that result in a default. More precisely, a strategy for each trader t is a specification of a bid price 3ti for each seller i to which t is connected, and an ask price \u03b1tj for each buyer j to which t is connected. -LRB- We can also handle a model in which a trader may choose not to make an offer to certain of its adjacent sellers or buyers. -RRB- Each seller or buyer then chooses at most one incident edge, indicating the trader with whom they will transact, at the indicated price. -LRB- The choice of a single edge reflects the facts that -LRB- a -RRB- sellers each initially have only one copy of the good, and -LRB- b -RRB- buyers each only want one copy of the good. -RRB- The payoffs are as follows : For each seller i, the payoff from selecting trader t is 3ti, while the payoff from selecting no trader is \u03b8i. -LRB- In the former case, the seller receives 3ti units of money, while in the latter it keeps its copy of the good, which it values at \u03b8i. -RRB- For each buyer j, the payoff from selecting trader t is \u03b8j -- \u03b1tj, whle the payoff from selecting no trader is 0. -LRB- In the former case, the buyer receives the good but gives up \u03b1tj units of money. -RRB- For each trader t, with accepted offers from sellers i1,..., is and buyers j1,..., jb, the payoff is Pr \u03b1tjr -- Pr 3tir, minus a penalty \u03c0 if b > s. The penalty is chosen to be large enough that a trader will never incur it in equilibrium, and hence we will generally not be concerned with the penalty. This defines the basic elements of the game. The equilibrium concept we use is subgame perfect Nash equilibrium. Some Examples. To help with thinking about the model, we now describe three illustrative examples, depicted in Figure 1. All sellers in the examples will have valuations for the good equal to 0 ; the valuation of each buyer is drawn inside its circle ; and the bid or ask price on each edge is drawn on top of the edge. In Figure 1 -LRB- a -RRB-, we show how a standard second-price auction arises naturally from our model. Suppose the buyer valuations from top to bottom are w > x > y > z. The bid and ask prices shown are consistent with an equilibrium in which i1 and j1 accept the offers of trader t1, and no other buyer accepts the offer of its adjacent trader : thus, trader t1 receives the good with a bid price of x, and makes w -- x by selling the good to buyer j1 for w. In this way, we can consider this particular instance as an auction for a single good in which the traders act as `` proxies '' for their adjacent buyers. The buyer with the highest valuation for the good ends up with it, and the surplus is divided between the seller and the associated trader. Note that one can construct a k-unit auction with f > k buyers just as easily, by building a complete bipartite graph on k sellers and f traders, and then attaching each trader to a single distinct buyer. In Figure 1 -LRB- b -RRB-, we show how nodes with different positions in the network topology can achieve different payoffs, even when all Figure 1 : -LRB- a -RRB- An auction, mediated by traders, in which the buyer with the highest valuation for the good ends up with it. -LRB- b -RRB- A network in which the middle seller and buyer benefit from perfect competition between the traders, while the other sellers and buyers have no power due to their position in the network. -LRB- c -RRB- A form of implicit perfect competition : all bid/ask spreads will be zero in equilibrium, even though no trader directly `` competes '' with any other trader for the same buyer-seller pair. buyer valuations are the same numerically. Specifically, seller i2 and buyer j2 occupy powerful positions, because the two traders are competing for their business ; on the other hand, the other sellers and buyers are in weak positions, because they each have only one option. And indeed, in every equilibrium, there is a real number x E -LSB- 0, 1 -RSB- such that both traders offer bid and ask prices of x to i2 and j2 respectively, while they offer bids of 0 and asks of 1 to the other sellers and buyers. Thus, this example illustrates a few crucial ingredients that we will identify at a more general level shortly. Specifically, i2 and j2 experience the benefits of perfect competition, in that the two traders drive the bid-ask spreads to 0 in competing for their business. On the other hand, the other sellers and buyers experience the downsides of monopoly -- they receive 0 payoff since they have only a single option for trade, and the corresponding trader makes all the profit. Note further how this natural behavior emerges from the fact that traders are able to offer different prices to different agents -- capturing the fact that there is no one fixed `` price '' in the kinds of markets that motivate the model, but rather different prices reflecting the relative power of the different agents involved. The previous example shows perhaps the most natural way in which a trader 's profit on a particular transaction can drop to 0 : when there is another trader who can replicate its function precisely. -LRB- In that example, two traders each had the ability to move a copy of the good from i2 to j2. -RRB- But as our subsequent results will show, traders make zero profit more generally due to global, graph-theoretic reasons. The example in Figure 1 -LRB- c -RRB- gives an initial indication of this : one can show that for every equilibrium, there is a y E -LSB- 0, 1 -RSB- such that every bid and every ask price is equal to y. In other words, all traders make zero profit, whether or not a copy of the good passes through them -- and yet, no two traders have any seller-buyer paths in common. The price spreads have been driven to zero by a global constraint imposed by the long cycle through all the agents ; this is an example of implicit perfect competition determined by the network topology. Extending the Model to Distinguishable Goods. We extend the basic model to a setting with distinguishable goods, as follows. A strategy for a trader now consists of offering a bid to each seller that specifies both a price and a buyer, and offering an ask to each buyer that specifies both a price and a seller. -LRB- We can also handle a model in which a trader offers bids -LRB- respectively, asks -RRB- in the form of vectors, essentially specifying a `` menu '' with a price attached to each buyer -LRB- resp. seller -RRB-. -RRB- Each buyer and seller selects an offer from an adjacent trader, and the payoffs to all agents are determined as before. Here the sellers are job applicants, buyers are employers, and traders are the agents that mediate the job market. Of course, if one specifies pairwise valuations on buyers but just single valuations for sellers, we model a setting where buyers can distinguish among the goods, but sellers do n't care whom they sell to -- this -LRB- roughly -RRB- captures settings like housing markets. Our Results. To make these precise, we introduce the following notation. -LRB- Sellers appearing in no triple keep their copy of the good. -RRB- We say that the value of the allocation is equal to Pe \u2208 M \u03b8jeie -- \u03b8ieje. Let \u03b8 \u2217 denote the maximum value of any allocation M that is feasible given the network. We show that every instance of our game has an equilibrium, and that in every such equilibrium, the allocation has value \u03b8 \u2217 -- in other words, it achieves the best value possible. Thus, equilibria in this model are always efficient, in that the market enables the `` right '' set of people to get the good, subject to the network constraints. We establish the existence and efficiency of equilibria by constructing a linear program to capture the flow of goods through the network ; the dual of this linear program contains enough information to extract equilibrium prices. By the definition of the game, the value of the equilibrium allocation is divided up as payoffs to the agents, and it is interesting to ask how this value is distributed -- in particular how much profit a trader is able to make based on its position in the network. We find that, although all equilibria have the same value, a given trader 's payoff can vary across different equilibria. We also obtain results for the sum of all trader profits. Related Work. The standard baseline approach for analyzing the interaction of buyers and sellers is the Walrasian model in which anonymous buyers and sellers trade a good at a single market clearing price. This reduced form of trade, built on the idealization of a market price, is a powerful model which has led to many insights. But it is not a good model to use to examine where prices come from or exactly how buyers and sellers and trade with each other. The difficulty is that in the Walrasian model there is no agent who sets the price, and agents do n't actually trade with each other. In fact there is no market, in the everyday sense of that word, in the Walrasian model. That is, there is no physical or virtual place where buyers and sellers interact to trade and set prices. Thus in this simple model, all buyers and sellers are uniform and trade at the same price, and there is also no role for intermediaries. There are several literatures in economics and finance which examine how prices are set rather than just determining equilibrium prices. The literature on imperfect competition is perhaps the oldest of these. Here a monopolist, or a group of oliogopolists, choose prices in order to maximize their profits -LRB- see -LSB- 14 -RSB- for the standard textbook treatment of these markets -RRB-. A monopolist uses its knowledge of market demand to choose a price, or a collection of prices if it discriminates. Oliogopolists play a game in which their payoffs depend on market demand and the actions of their competitors. In this literature there are agents who set prices, but the fiction of a single market is maintained. In the equilibrium search literature, firms set prices and consumers search over them -LRB- see -LSB- 3 -RSB- -RRB-. Consumers do end up paying different prices, but all consumers have access to all firms and there are no intermediaries. In the general equilibrium literature there have been various attempts to introduce price determination. A standard proof technique for the existence of competitive equilibrium involves a price adjustment mechanism in which prices respond to excess demand. More sophisticated processes have been introduced to study the stability of equilibrium prices or the information necessary to compute them. But again there are no price-setting agents here. In the finance literature the work on market microstructure does have price-setting agents -LRB- specialists -RRB-, parts of it do determine separate bid and ask prices, and different agents receive different prices for the same asset -LRB- see -LSB- 12 -RSB- for a treatment of microstructure theory -RRB-. Work in information economics has identified similar phenomena -LRB- see e.g. -LSB- 7 -RSB- -RRB-. But there is little research in these literatures examining the effect of restrictions on who can trade with whom. There have been several approaches to studying how network structure determines prices. These have posited price determination through definitions based on competitive equilibrium or the core, or through the use of truthful mechanisms. In briefly reviewing this work, we will note the contrast with our approach, in that we model prices as arising from the strategic behavior of agents in the system. In recent work, Kakade et al. -LSB- 8 -RSB- have studied the distribution of prices at competitive equilibrium in a bipartite graph on buyers and sellers, generated using a probabilistic model capable of producing heavy-tailed degree distributions -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- build on this to consider the strategic aspects of network formation when prices arise from competitive equilibrium. Leonard studies VCG prices in this setting ; Babaioff et al. and Chu and Shen additionally provide a a budget-balanced mechanism. In contrast, our model has known valuations and prices arising from the strategic behavior of traders. Demange, Gale, and Sotomayor -LSB- 5 -RSB-, and Kranton and Minehart -LSB- 9 -RSB-, analyze the prices at which trade occurs in a network, working within the framework of mechanism design. Kranton and Minehart use a bipartite graph with direct links between buyers and sellers, and then use an ascending auction mechanism, rather than strategic intermediaries, to determine the prices. Their auction has desirable equilibrium properties but as Kranton and Minehart note it is an abstraction of how goods are allocated and prices are determined that is similar in spirit to the Walrasian auctioneer abstraction.", "keyphrases": ["algorithm game theori", "market", "trade network", "interact of buyer and seller", "initi endow of monei", "bid price", "perfect competit", "benefit", "maximum and minimum amount", "econom and financ", "strateg behavior of trader", "complementari slack", "monopoli"]}
{"file_name": "I-15", "text": "Information Searching and Sharing in Large-Scale Dynamic Networks ABSTRACT Finding the right agents in a large and dynamic network to provide the needed resources in a timely fashion, is a long standing problem. This paper presents a method for information searching and sharing that combines routing indices with tokenbased methods. The proposed method enables agents to search effectively by acquiring their neighbors ' interests, advertising their information provision abilities and maintaining indices for routing queries, in an integrated way. Specifically, the paper demonstrates through performance experiments how static and dynamic networks of agents can be ` tuned ' to answer queries effectively as they gather evidence for the interests and information provision abilities of others, without altering the topology or imposing an overlay structure to the network of acquaintances. 1. INTRODUCTION networks of associated agents. On the other hand, there is a lot of research on semantic peer to peer search networks and social networks -LSB- 1,5,6,8,9,10,16,18,19 -RSB- many of which deal with tuning a network of peers for effective information searching and sharing. They do it mostly by imposing logical and semantic overlay structures. However, as far as we know there is no work that demonstrates the effectiveness of a gradual tuning process in large-scale dynamic networks that studies the impact of the information gathered by agents as more and more queries are issued and served in concurrent sessions in the network. The main issue in this paper concerns ` tuning ' a network of agents, each with a specific expertise, for efficient and effective information searching and sharing, without altering the topology or imposing an overlay structure via clustering, introduction of shortcut indices, or re-wiring. ` Tuning ' is the task of sharing and gathering the necessary knowledge for agents to propagate requests to the right acquaintances, minimizing the searching effort, increasing the efficiency and the benefit of the system. Specifically, this paper proposes a method for information searching and sharing in dynamic and large scale networks, which combines routing indices with token-based methods for information sharing in large-scale multi-agent systems. This paper is structured as follows : Section 2 presents related work and motivates the proposed method. Section 3 states the problem and section 4 presents in detail the individual techniques and the overall proposed method. Section 5 presents the experimental setup and results, and section 6 concludes the paper, sketching future work. 2. RELATED WORK Information provision and sharing can be considered to be a decentralized partially-observable Markov decision process -LSB- 3,4,11,14 -RSB-. In the general case, decentralized control of largescale dynamic systems of cooperative agents is a hard problem. Optimal solutions can only be approximated by means of heuristics, by relaxations of the original problem or by centralized solutions. However, in a large-scale dynamic system with decentralized control it is very hard for agents to possess accurate partial views of the environment, and it is even more hard for agents to possess a global view of the environment. Furthermore, agents ' observations can not be assumed independent, as one agent 's actions can affect the observations of others : For instance, when one agent joins/leaves the system, then this may affect other agents ' assessment of neighbours ' information provision abilities. Considering independent activities and observations, authors in -LSB- 4 -RSB- propose a decision-theoretic solution treating standard action and information exchange as explicit choices that the decision maker must make. They approximate the solution using a myopic algorithm. Their work differs in the one reported here in the following aspects : First, it aims at optimizing communication, while the goal here is to tune the network for effective information sharing, reducing communication and increasing system 's benefit. Third, they consider that transitions and observations made by agents are independent, which, as already discussed, is not true in the general case. Last, in contrast to their approach where agents broadcast messages, here agents decide not only when to communicate, but to whom to send a message too. Token based approaches are promising for scaling coordination and therefore information provision and sharing to large-scale systems effectively. In -LSB- 11 -RSB- authors provide a mathematical framework for routing tokens, providing also an approximation to solving the original problem in case of independent agents ' activities. The proposed method requires a high volume of computations that authors aim to reduce by restricting its application to static logical teams of associated agents. In accordance to this approach, in -LSB- 12,13,14 -RSB-, information sharing is considered only for static networks and self-tuning of networks is not demonstrated. As it will be shown in section 5, our experiments show that although these approaches can handle information sharing in dynamic networks, they require a larger amount of messages in comparison to the approach proposed here and can not tune the network for efficient information sharing. Proactive communication has been proposed in -LSB- 17 -RSB- as a result of a dynamic decision theoretic determination of communication strategies. This approach is based on the specification of agents as `` providers '' and `` needers '' : This is done by a plan-based precomputation of information needs and provision abilities of agents. However, this approach can not scale to large and dynamic networks, as it would be highly inefficient for each agent to compute and determine its potential needs and information provision abilities given its potential interaction with 100s of other agents. Viewing information retrieval in peer-to-peer systems from a multi-agent system perspective, the approach proposed in -LSB- 18 -RSB- is based on a language model of agents ' documents collection. Exploiting the models of other agents in the network, agents construct their view of the network which is being used for forming routing decisions. Initially, agents build their views using the models of their neighbours. Then, the system reorganizes by forming clusters of agents with similar content. Clusters are being exploited during information retrieval using a kNN approach and a gradient search scheme. Although this work aims at tuning a network for efficient information provision -LRB- through reorganization -RRB-, it does not demonstrate the effectiveness of the approach with respect to this issue. Moreover, although during reorganization and retrieval they measure the similarity of content between agents, a more fine grained approach is needed that would allow agents to measure similarities of information items or sub-collections of information items. Based on their work on peer-to-peer systems, H.Zhand and V.Lesser in -LSB- 19 -RSB- study concurrent search sessions. Considering research in semantic peer-to-peer systems1, most of the approaches exploit what can be loosely stated a `` routing index ''. A major question concerning information searching is `` what information has to be shared between peers, when, and what adjustments have to be made so as queries to be routed to trustworthy information sources in the most effective and efficient way ''. REMINDIN ' -LSB- 10 -RSB- peers gather information concerning the queries that have been answered successfully by other peers, so as to subsequently select peers to forward requests to : This is a lazy learning approach that does not involve advertisement of peer information provision abilities. This results in a tuning process where the overall recall increases over time, while the number of messages per query remains about the same. Here, agents actively advertise their information provision abilities based on the assessed interests of their peers : This results in a much lower number of messages per query than those reported in REMINDIN '. In -LSB- 5,6 -RSB- peers, using a common ontology, advertise their expertise, which is being exploited for the formation of a semantic overlay network : Queries are propagated in this network depending on their similarity with peers ' expertise. According to our approach, agents advertise selectively their information provision abilities about specific topics to their neighbours with similar information interests -LRB- and only to these -RRB-. However, this is done as time passes and while agents ' receive requests from their peers. They generate a substantial overhead in highly dynamic settings, where nodes join/leave the system. 248 The Sixth Intl.. Joint Conf. agents advertise their information provision abilities given the interests of their neighbours. Given the success of this method, we shall study how the addition of logical paths and gradual evolution of the network topology can further increase the effectiveness of the proposed method. 6. CONCLUSIONS This paper presents a method for semantic query processing in large networks of agents that combines routing indices with information sharing methods. The presented method enables agents to keep records of acquaintances ' interests, to advertise their information provision abilities to those that have a high interest on them, and to maintain indices for routing queries to those agents that have the requested information provision abilities. Specifically, the paper demonstrates through extensive performance experiments : -LRB- a -RRB- How networks of agents can be ` tuned ' so as to provide requested information effectively, increasing the benefit and the efficiency of the system. -LRB- b -RRB- How different types of local knowledge -LRB- number, local information repositories, percentage, interests and information provision abilities of acquaintances -RRB- can guide agents to effectively answer queries, balancing between efficiency and efficacy. -LRB- c -RRB- That the proposed `` tuning '' task manages to increase the efficiency of information searching and sharing in highly dynamic and large networks. -LRB- d -RRB- That the information gathered and maintained by agents supports efficient and effective information searching and sharing : Initial information about acquaintances information provision abilities is not necessary and a small percentage of acquaintances suffices. Further work concerns experimenting with real data and ontologies, differences in ontologies between agents, shifts in expertise and the parallel construction of overlay structure.", "keyphrases": ["inform search and share", "social network", "cooper agent", "peer to peer search network", "peer-to-peer system", "dynam and larg scale network", "decentr partial-observ markov decis process", "decentr control", "myopic algorithm", "knn approach", "gradient search scheme"]}
{"file_name": "J-1", "text": "Generalized Trade Reduction Mechanisms ABSTRACT When designing a mechanism there are several desirable properties to maintain such as incentive compatibility -LRB- IC -RRB-, individual rationality -LRB- IR -RRB-, and budget balance -LRB- BB -RRB-. It is well known -LSB- 15 -RSB- that it is impossible for a mechanism to maximize social welfare whilst also being IR, IC, and BB. There have been several attempts to circumvent -LSB- 15 -RSB- by trading welfare for BB, e.g., in domains such as double-sided auctions -LSB- 13 -RSB-, distributed markets -LSB- 3 -RSB- and supply chain problems -LSB- 2, 4 -RSB-. In this paper we provide a procedure called a Generalized Trade Reduction -LRB- GTR -RRB- for single-value players, which given an IR and IC mechanism, outputs a mechanism which is IR, IC and BB with a loss of welfare. We bound the welfare achieved by our procedure for a wide range of domains. In particular, our results improve on existing solutions for problems such as double sided markets with homogenous goods, distributed markets and several kinds of supply chains. Furthermore, our solution provides budget balanced mechanisms for several open problems such as combinatorial double-sided auctions and distributed markets with strategic transportation edges. 1. INTRODUCTION When designing a mechanism there are several key properties that are desirable to maintain. In many of the mechanisms the goal function that a mechanism designer attempts to maximize is the social welfare ' - the total benefit to society. However, it is well known from -LSB- 15 -RSB- that any mechanism that maximizes social welfare while maintaining individual rationality and incentive compatibility runs a deficit perforce, i.e., is not budget balanced. To maintain the BB property in an IR and IC mechanism it is necessary to compromise on the optimality of the social welfare. 1.1 Related Work and Specific Solutions There have been several attempts to design budget balanced mechanisms for particular domains2. In the distributed markets problem -LRB- and closely related problems -RRB- goods are transported between geographic locations while incurring some constant cost for transportation. -LSB- 16, 9, 3 -RSB- present mechanisms that approximate the social welfare while achieving an IR, IC and BB mechanism. For supply chain problems -LSB- 2, 4 -RSB- bounds the loss of social welfare that is necessary to inflict on the mechanism in order to achieve the desired combination of IR, IC, and BB. Despite the works discussed above, the question of how to design a general mechanism that achieves IR, IC, and BB independently of the problem domain remains open. Furthermore, there are several domains where the question of how to design an IR, IC and BB mechanism which approx imates the social welfare remains an open problem. For example, in the important domain of combinatorial doublesided auctions there is no known result that bounds the loss of social welfare needed to achieve budget balance. Another interesting example is the open question left by -LSB- 3 -RSB- : How can one bound the loss in social welfare that is needed to achieve budget balance in an IR and IC distributed market where the transportation edges are strategic. Naturally an answer to the BB distributed market with strategic edges has vast practical implications, for example to transportation networks. 1.2 Our Contribution In this paper we unify all the problems discussed above -LRB- both the solved as well as the open ones -RRB- into one solution concept procedure. The solution procedure called the Generalized Trade Reduction -LRB- GTR -RRB-. GTR accepts an IR and IC mechanism for single-valued players and outputs an IR, IC and BB mechanism. The output mechanism may suffer some welfare loss as a tradeoff of achieving BB. There are problem instances in which no welfare loss is necessary but by -LSB- 15 -RSB- there are problem instances in which there is welfare loss. Nevertheless for a wide class of problems we are able to bound the loss in welfare. A particularly interesting case is one in which the input mechanism is an efficient allocation. In addition to unifying many of the BB problems under a single solution concept, the GTR procedure improves on existing results and solves several open problems in the literature. The existing solutions our GTR procedure improves are homogeneous double-sided auctions, distributed markets -LSB- 3 -RSB-, and supply chain -LSB- 2, 4 -RSB-. For the homogeneous doublesided auctions the GTR solution procedure improves on the well known solution -LSB- 13 -RSB- by allowing for some cases of no trade reduction at all. For the distributed markets -LSB- 3 -RSB- and the supply chain -LSB- 2, 4 -RSB- the GTR solution procedure improves on the welfare losses ' bound, i.e., allows one to achieve an IR, IC and BB mechanism with smaller loss on the social welfare. Recently we also learned that the GTR procedure allows one to turn the model newly presented -LSB- 6 -RSB- into a BB mechanism. In addition to the main contribution described above, this paper also defines an important classification of problem domains. We define class based domain and procurement class based domains. The above definitions build on the different competition `` powers '' of players in a mechanisms called internal and external competition.", "keyphrases": ["trade reduct", "budget balanc", "intern competit", "extern competit", "effici", "power of player", "gener trade reduct", "gtr", "optim", "inequ in welfar", "multi-mind player", "budget-balanc mechan", "homogen good", "spatial distribut market"]}
{"file_name": "H-24", "text": "Investigating the Querying and Browsing Behavior of Advanced Search Engine Users ABSTRACT One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing, and use this knowledge to benefit everyone. In this paper we study the interaction logs of advanced search engine users -LRB- and those not so advanced -RRB- to better understand how these user groups search. The results show that there are marked differences in the queries, result clicks, post-query browsing, and search success of users we classify as advanced -LRB- based on their use of query operators -RRB-, relative to those classified as non-advanced. Our findings have implications for how advanced users should be supported during their searches, and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies. 1. INTRODUCTION The formulation of query statements that capture both the salient aspects of information needs and are meaningful to Information Retrieval -LRB- IR -RRB- systems poses a challenge for many searchers -LSB- 3 -RSB-. These techniques can be useful in improving result precision yet, other than via log analyses -LRB- e.g., -LSB- 15 -RSB- -LSB- 27 -RSB- -RRB-, they have generally been overlooked by the research community in attempts to improve the quality of search results. IR research has generally focused on alternative ways for users to specify their needs rather than increasing the uptake of advanced syntax. Research on practical techniques to supplement existing search technology and support users has been intensifying in recent years -LRB- e.g. -LSB- 18 -RSB- -LSB- 34 -RSB- -RRB-. However, it is challenging to implement such techniques at large scale with tolerable latencies. Typical queries submitted to Web search engines take the form of a series of tokens separated by spaces. There is generally an implied Boolean AND operator between tokens that restricts search results to documents containing all query terms. De Lima and Pedersen -LSB- 7 -RSB- investigated the effect of parsing, phrase recognition, and expansion on Web search queries. They showed that the automatic recognition of phrases in queries can improve result precision in Web search. However, the value of advanced syntax for typical searchers has generally been limited, since most users do not know about advanced syntax or do not understand how to use it -LSB- 15 -RSB-. In this paper we explore the use of query operators in more detail and propose alternative applications that do not require all users to use advanced syntax explicitly. We hypothesize that searchers who use advanced query syntax demonstrate a degree of search expertise that the majority of the user population does not ; an assertion supported by previous research -LSB- 13 -RSB-. Studying the behavior of these advanced search engine users may yield important insights about searching and result browsing from which others may benefit. Using logs gathered from a large number of consenting users, we investigate differences between the search behavior of those that use advanced syntax and those that do not, and differences in the information those users target. We are interested in answering three research questions : -LRB- i -RRB- Is there a relationship between the use of advanced syntax and other characteristics of a search? -LRB- ii -RRB- Is there a relationship between the use of advanced syntax and post-query navigation behaviors? -LRB- iii -RRB- Is there a relationship between the use of advanced syntax and measures of search success? Through an experimental study and analysis, we offer potential answers for each of these questions. A relationship between the use of advanced syntax and any of these features could support the design of systems tailored to advanced search engine users, or use advanced users ' interactions to help non-advanced users be more successful in their searches. We describe related work in Section 2, the data we used in this log-based study in Section 3, the search characteristics on which we focus our analysis in Section 4, and the findings of this analysis in Section 5. 2. RELATED WORK Factors such as lack of domain knowledge, poor understanding of the document collection being searched, and a poorly developed information need can all influence the quality of the queries that users submit to IR systems -LRB- -LSB- 24 -RSB-, -LSB- 28 -RSB- -RRB-. There has been a variety of research into different ways of helping users specify their information needs more effectively. Belkin et al. -LSB- 4 -RSB- experimented with providing additional space for users to type a more verbose description of their information needs. A similar approach was attempted by Kelly et al. -LSB- 18 -RSB-, who used clarification forms to elicit additional information about the search context from users. These approaches have been shown to be effective in best-match retrieval systems where longer queries generally lead to more relevant search results -LSB- 4 -RSB-. However, in Web search, where many of the systems are based on an extended Boolean retrieval model, longer queries may actually hurt retrieval performance, leading to a small number of potentially irrelevant results being retrieved. It is not simply sufficient to request more information from users ; this information must be of better quality. Relevance Feedback -LRB- RF -RRB- -LSB- 22 -RSB- and interactive query expansion -LSB- 9 -RSB- are popular techniques that have been used to improve the quality of information that users provide to IR systems regarding their information needs. In the case of RF, the user presents the system with examples of relevant information that are then used to formulate an improved query or retrieve a new set of documents. It has proven difficult to get users to use RF in the Web domain due to difficulty in conveying the meaning and the benefit of RF to typical users -LSB- 17 -RSB-. Query suggestions offered based on query logs have the potential to improve retrieval performance with limited user burden. This approach is limited to re-executing popular queries, and searchers often ignore the suggestions presented to them -LSB- 1 -RSB-. In addition, both of these techniques do not help users learn to produce more effective queries. Most commercial search engines provide advanced query syntax that allows users to specify their information needs in more detail. Boolean operators -LRB- AND, OR, and NOT -RRB- can join terms and phrases, and modifiers such as `` site : '' and `` link : '' can be used to restrict the search space. Queries created with these techniques can be powerful. Log-based analysis of users ' interactions with the Excite and AltaVista search engines has shown that only 10-20 % of queries contained any advanced syntax -LSB- 14 -RSB- -LSB- 25 -RSB-. This analysis can be a useful way of capturing characteristics of users interacting with IR systems. Research in user modeling -LSB- 6 -RSB- and personalization -LSB- 30 -RSB- has shown that gathering more information about users can improve the effectiveness of searches, but require more information about users than is typically available from interaction logs alone. Unless coupled with a qualitative technique, such as a post-session questionnaire -LSB- 23 -RSB-, it can be difficult to associate interactions with user characteristics. In our study we conjecture that given the difficulty in locating advanced search features within the typical search interface, and the potential problems in understanding the syntax, those users that do use advanced syntax regularly represent a distinct class of searchers who will exhibit other common search behaviors. Other studies of advanced searchers ' search behaviors have attempted to better understand the strategic knowledge they have acquired. Nonetheless, they can give valuable insight about the behaviors of users with domain, system, or search expertise that exceeds that of the average user. Querying behavior in particular has been studied extensively to better understand users -LSB- 31 -RSB- and support other users -LSB- 16 -RSB-. In this paper we study other search characteristics of users of advanced syntax in an attempt to determine whether there is anything different about how these search engine users search, and whether their searches can be used to benefit those who do not make use of the advanced features of search engines. To do this we use interaction logs gathered from large set of consenting users over a prolonged period. In the next section we describe the data we use to study the behavior of the users who use advanced syntax, relative to those that do not use this syntax. SIGIR 2007 Proceedings Session 11 : Interaction behavior such as querying, result clickthrough, post-query navigation, and search success. Crude classification of users based on just one feature that is easily extractable from the query stream yields remarkable results about the interaction behavior of users that do not use the syntax and those that do. As we have suggested, search systems may leverage the interactions of these users for improved document ranking, page recommendation, or even user training.", "keyphrases": ["search engin", "queri", "relev inform", "search strategi", "toler latenc", "advanc syntax", "navig behavior", "search behavior", "search success", "relev feedback", "relev"]}
{"file_name": "J-8", "text": "Strong Equilibrium in Cost Sharing Connection Games * ABSTRACT In this work we study cost sharing connection games, where each player has a source and sink he would like to connect, and the cost of the edges is either shared equally -LRB- fair connection games -RRB- or in an arbitrary way -LRB- general connection games -RRB-. We study the graph topologies that guarantee the existence of a strong equilibrium -LRB- where no coalition can improve the cost of each of its members -RRB- regardless of the specific costs on the edges. Our main existence results are the following : -LRB- 1 -RRB- For a single source and sink we show that there is always a strong equilibrium -LRB- both for fair and general connection games -RRB-. -LRB- 2 -RRB- For a single source multiple sinks we show that for a series parallel graph a strong equilibrium always exists -LRB- both for fair and general connection games -RRB-. -LRB- 3 -RRB- For multi source and sink we show that an extension parallel graph always admits a strong equilibrium in fair connection games. As for the quality of the strong equilibrium we show that in any fair connection games the cost of a strong equilibrium is \u0398 -LRB- log n -RRB- from the optimal solution, where n is the number of players. -LRB- This should be contrasted with the \u03a9 -LRB- n -RRB- price of anarchy for the same setting. -RRB- For single source general connection games and single source single sink fair connection games, we show that a strong equilibrium is always an optimal solution. * Research supported in part by a grant of the Israel Science Foundation, Binational Science Foundation -LRB- BSF -RRB-, GermanIsraeli Foundation -LRB- GIF -RRB-, Lady Davis Fellowship, an IBM faculty award, and the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflects the authors ' views. 1. INTRODUCTION Computational game theory has introduced the issue of incentives to many of the classical combinatorial optimization problems. Consider classical routing and transportation problems such as multicast or multi-commodity problems, which are many times viewed as follows. We are given a graph with edge costs and connectivity demands between nodes, and our goal is to find a minimal cost solution. The game theory point of view would assume that each individual demand is controlled by a player that optimizes its own utility, and the resulting outcome could be far from the optimal solution. When considering individual incentives one needs to discuss the appropriate solution concept. Much of the research in computational game theory has focused on the classical Nash equilibrium as the primary solution concept. Indeed Nash equilibrium has many benefits, and most importantly it always exists -LRB- in mixed strategies -RRB-. However, the solution concept of Nash equilibrium is resilient only to unilateral deviations, while in reality, players may be able to coordinate their actions. A strong equilibrium -LSB- 4 -RSB- is a state from which no coalition -LRB- of any size -RRB- can deviate and improve the utility of every member of the coalition -LRB- while possibly lowering the utility of players outside the coalition -RRB-. This resilience to deviations by coalitions of the players is highly attractive, and one can hope that once a strong equilibrium is reached it is highly likely to sustain. From a computational game theory point of view, an additional benefit of a strong equilibrium is that it has a potential to reduce the distance between the optimal solution and the solution obtained as an outcome of selfish behavior. The strong price of anarchy -LRB- SPoA -RRB-, introduced in -LSB- 1 -RSB-, is the ratio between the cost of the worst strong equilibrium and the cost of an optimal solution. Obviously, SPoA is meaningful only in those cases where a strong equilibrium exists. A major downside of strong equilibrium is that most games do not admit any strong equilibrium. Even simple classical games like the prisoner 's dilemma do not posses any strong equilibrium -LRB- which is also an example of a congestion game that does not posses a strong equilibriums -RRB-. This unfortunate fact has reduced the concentration in strong equilibrium, despite its highly attractive properties. In this work we concentrate on cost sharing connection games, introduced by -LSB- 3, 2 -RSB-. In such a game, there is an underlying directed graph with edge costs, and individual users have connectivity demands -LRB- between a source and a sink -RRB-. We consider two models. The fair cost connection model -LSB- 2 -RSB- allows each player to select a path from the source to the sink2. In this game the cost of an edge is shared equally between all the players that selected the edge, and the cost of the player is the sum of its costs on the edges it selected. The general connection game -LSB- 3 -RSB- allows each player to offer prices for edges. In this game an edge is bought if the sum of the offers at least covers its cost, and the cost of the player is the sum of its offers on the bought edges -LRB- in both games we assume that the player has to guarantee the connectivity between its source and sink -RRB-. In this work we focus on two important issues. The first one is identifying under what conditions the existence of a strong equilibrium is guaranteed, and the second one is the quality of the strong equilibria. For the existence part, we identify families of graph topologies that possess some strong equilibrium for any assignment of edge costs. One can view this separation between the graph topology and the edge costs, as a separation between the underlying infrastructure and the costs the players observe to purchase edges. While one expects the infrastructure to be stable over long periods of time, the costs the players observe can be easily modified over short time periods. Our results are as follows. For the single commodity case -LRB- all the players have the same source and sink -RRB-, there is a strong equilibrium in any graph -LRB- both for fair and general connection games -RRB-. Moreover, the strong equilibrium is also swhile any congestion game is known to admit at least one Nash equilibrium in pure strategies -LSB- 16 -RSB-. 2The fair cost sharing scheme is also attractive from a mechanism design point of view, as it is a strategyproof costsharing mechanism -LSB- 14 -RSB-. the optimal solution -LRB- namely, the players share a shortest path from the common source to the common sink -RRB-. For the case of a single source and multiple sinks -LRB- for example, in a multicast tree -RRB-, we show that in a fair connection game there is a strong equilibrium if the underlying graph is a series parallel graph, and we show an example of a nonseries parallel graph that does not have a strong equilibrium. For the case of multi-commodity -LRB- multi sources and sinks -RRB-, we show that in a fair connection game if the graph is an extension parallel graph then there is always a strong equilibrium, and we show an example of a series parallel graph that does not have a strong equilibrium. As far as we know, we are the first to provide a topological characterization for equilibrium existence in multi-commodity and single-source network games. For any fair connection game we show that if there exists a strong equilibrium it is at most a factor of \u0398 -LRB- log n -RRB- from the optimal solution, where n is the number of players. This should be contrasted with the \u0398 -LRB- n -RRB- bound that exists for the price of anarchy -LSB- 2 -RSB-. For single source general connection games, we show that any series parallel graph possesses a strong equilibrium, and we show an example of a graph that does not have a strong equilibrium. In this case we also show that any strong equilibrium is optimal. Related work Topological characterizations for single-commodity network games have been recently provided for various equilibrium properties, including equilibrium existence -LSB- 12, 7, 8 -RSB-, equilibrium uniqueness -LSB- 10 -RSB- and equilibrium efficiency -LSB- 17, 11 -RSB-. The existence of pure Nash equilibrium in single-commodity network congestion games with player-specific costs or weights was studied in -LSB- 12 -RSB-. The existence of strong equilibrium was studied in both utility-decreasing -LRB- e.g., routing -RRB- and utility-increasing -LRB- e.g., fair cost-sharing -RRB- congestion games. -LSB- 7, 8 -RSB- have provided a full topological characterization for a SE existence in single-commodity utility-decreasing congestion games, and showed that a SE always exists if and only if the underlying graph is extension-parallel. -LSB- 19 -RSB- have shown that in single-commodity utility-increasing congestion games, the topological characterization is essentially equivalent to parallel links. In addition, they have shown that these results hold for correlated strong equilibria as well -LRB- in contrast to the decreasing setting, where correlated strong equilibria might not exist at all -RRB-. While the fair cost sharing games we study are utility increasing network congestion games, we derive a different characterization than -LSB- 19 -RSB- due to the different assumptions regarding the players ' actions.3 4. GENERAL CONNECTION GAMES In this section, we derive our results for general connection games. 4.1 Existence of Strong Equilibrium We begin with a characterization of the existence of a strong equilibrium in symmetric general connection games. Similar to Theorem 3.1 -LRB- using a similar proof -RRB- we establish, THEOREM 4.1. In every symmetric fair connection game there exists a strong equilibrium. While every single source general connection game possesses a pure Nash equilibrium -LSB- 3 -RSB-, it does not necessarily admit some strong equilibrium.11 the fair-connection game inspired this example. THEOREM 4.2. There exists a single source general connection game that does not admit any strong equilibrium. PROOF. Consider single source general connection game with 3 players on the graph depicted in Figure 4. We showed that none of the NE are SE, and thus the game does not possess any SE. Next we show that for the class of series parallel graphs, there is always a strong equilibrium in the case of a single source. PROOF. Let \u039b be a single source general connection game on a SPG G = -LRB- V, E -RRB- with source s and sink t. We first consider the following partial order between the players. For players i and j, we have that i \u2192 j if there is a directed path from ti to tj. The algorithm COMPUTE-SE, considers the players in an increasing order, starting with player 1. Each player i will fully buy a subset of the edges, and any player j > i will consider the cost of those -LRB- bought -RRB- edges as zero. When COMPUTE-SE considers player j, the cost of the edges that players 1 to j \u2212 1 have bought is set to zero, and player j fully buys a shortest path Qj from s to tj. Namely, for every edges e G Qj \\ Ui < jQi we have pj -LRB- e -RRB- = ce and otherwise pj -LRB- e -RRB- = 0. We next show that the algorithm COMPUTESE computes a SE. Assume by way of contradiction that the profile p is not a SE. Then, there exists a coalition that can improve the costs of all its players by a deviation. Let \u0393 be such a coalition of minimal size and let player i = max -LCB- j G \u0393 -RCB-. For a player j G \u0393 let \u00af Qj and \u00af pj be the path and payment of player j after the deviation, respectively. Let Q ' be a path from the sink of player i, i.e. ti, to the sink of G, i.e. t. Then Q = \u00af Qi U Q ' is a path from the source s to the sink t. For any player j < i, let yj be the intersecting vertex of Q and tj -LRB- by Lemma 2.1 one is guarantee to exist -RRB-. Let y be the furthest vertex on the path Q such that y = yj for some j < i. The path from the source s to node y was fully paid for by players j < i in p -LRB- before the deviation -RRB-. There are two cases we consider. case a : After the deviation player i does not pay for edges in U j \u2208 \u0393 \\ -LCB- i -RCB- \u00af Qj. Before the deviation of the coalition \u0393, a path from s to y was fully paid for by the players j < i. Next we show that no player k > i pays for any edge on any path from s to ti. Consider a player k > i and let Q0k = Qk U Q00k, where Q00k is a path connecting tk to t. Let yk be the intersecting vertex of Q0k and ti. Since there exists a path from s to yk that was fully paid for by players j < k before the deviation, in particularly the path Qis, yk, player k will not pay for any edge on any path connecting s and yk. Therefore player i fully pays for all edges on the path \u00af Qiy, ti, i.e., \u00af pi -LRB- e -RRB- = ce for all edges e E \u00af Qiy, ti. Now consider the algorithm COMPUTESE at the step when player i selects a shortest path from the source s to its sink ti and determines his payment pi. At this point, player i could buy the path \u00af Qiy, ti, since a path from s to y was already paid for by players j < i. Hence, ci -LRB- \u00af p -RRB- > ci -LRB- p -RRB-. This contradicts the fact that player i improved its cost and therefore not all the players in \u0393 reduce their cost. This implies that p is a strong equilibrium. 4.2 Strong Price of Anarchy While for every single source general connection game, it holds that PoS = 1 -LSB- 3 -RSB-, the price of anarchy can be as large as n, even for two parallel edges. Here, we show that any strong equilibrium in single source general connection games yields the optimal cost. PROOF. Let p = -LRB- p1,..., pn -RRB- be a strong equilibrium, and let T \u2217 be the minimum cost Steiner tree on all players, rooted at the -LRB- single -RRB- source s. Let Te \u2217 be the subtree of T \u2217 disconnected from s when edge e is removed. Let \u0393 -LRB- Te -RRB- be the set of players which have sinks in Te. For a set of edges E, let c -LRB- E -RRB- = Ee \u2208 E ce. Assume by way of contradiction that c -LRB- p -RRB- > c -LRB- T \u2217 -RRB-. We will show that there exists a sub-tree T0 of T \u2217, that connects a subset of players \u0393 C _ N, and a new set of payments \u00af p, such that for each i E \u0393, ci -LRB- \u00af p -RRB- < ci -LRB- p -RRB-. This will contradict the assumption that p is a strong equilibrium. First we show how to find a sub-tree T0 of T \u2217, such that for any edge e, the payments of players with sinks in Te \u2217 is more than the cost of Te \u2217 U -LCB- e -RCB-. To build T0, define an edge e to be bad if the cost of Te \u2217 U -LCB- e -RCB- is at least the payments of the players with sinks in Te \u2217, i.e., c -LRB- Te \u2217 U -LCB- e -RCB- -RRB- > P -LRB- Te \u2217 -RRB-. Let B be the set of bad edges. Therefore, in T0 for every edge e, we have that c -LRB- Te0 U -LCB- e -RCB- -RRB- < P -LRB- T0e -RRB-. What remain is to find payments p \u00af for the players in \u0393 -LRB- T0 -RRB- such that they will buy the tree T0 and every player in \u0393 -LRB- T0 -RRB- will lower its cost, i.e. ci -LRB- p -RRB- > ci -LRB- \u00af p -RRB- for i E \u0393 -LRB- T0 -RRB-. -LRB- Recall that the payments have the restriction that player i can only pay for edges on the path from s to ti. -RRB- We will now define the coalition payments \u00af p. Let ci -LRB- \u00af p, T0 e \u2208 Te \u00af pi -LRB- e -RRB- be the payments of player i for the subtree T0e. Consider the following bottom up process that defines \u00af p. We assign the payments of edge e in T0, after we assign payments to all the edges in T0e. Therefore, we can update the payments p \u00af of players i E \u0393 -LRB- T0e -RRB-, by setting where we used the fact that E e -RRB-.", "keyphrases": ["cost share connect game", "number of player", "singl sourc and sink", "singl sourc multipl sink", "multi sourc and sink", "cost of the edg", "fair connect game", "gener connect game", "graph topolog", "strong equilibrium", "coalit", "specif cost", "extens parallel graph", "optim solut"]}
{"file_name": "C-3", "text": "Self-Adaptive Applications on the Grid Abstract Grids are inherently heterogeneous and dynamic. One important problem in grid computing is resource selection, that is, finding an appropriate resource set for the application. Another problem is adaptation to the changing characteristics of the grid environment. Existing solutions to these two problems require that a performance model for an application is known. However, constructing such models is a complex task. In this paper, we investigate an approach that does not require performance models. We start an application on any set of resources. During the application run, we periodically collect the statistics about the application run and deduce application requirements from these statistics. Then, we adjust the resource set to better fit the application needs. This approach allows us to avoid performance bottlenecks, such as overloaded WAN links or very slow processors, and therefore can yield significant performance improvements. We evaluate our approach in a number of scenarios typical for the Grid. 1. Introduction In recent years, grid computing has become a real alternative to traditional parallel computing. A grid provides much computational power, and thus offers the possibility to solve very large problems, especially if applications can run on multiple sites at the same time -LRB- 7 ; 15 ; 20 -RRB-. However, the complexity of Grid environments also is many times larger than that of traditional parallel machines like clusters and supercomputers. One important problem is resource selection - selecting a set of compute nodes such that the application achieves good performance. In a grid environment this problem is even more difficult, because of the heterogeneity of resources : the compute nodes have various Another important problem is that the performance and availability of grid resources varies over time : the network links or compute nodes may become overloaded, or the compute nodes may become unavailable because of crashes or because they have been claimed by a higher priority application. Also, new, better resources may become available. To maintain a reasonable performance level, the application therefore needs to adapt to the changing conditions. The adaptation problem can be reduced to the resource selection problem : the resource selection phase can be repeated during application execution, either at regular intervals, or when a performance problem is detected, or when new resources become available. This approach has been adopted by a number of systems -LRB- 5 ; 14 ; 18 -RRB-. For resource selection, the application runtime is estimated for some resource sets and the set that yields the shortest runtime is selected for execution. Predicting the application runtime on a given set of resources, however, requires knowledge about the application. Typically, an analytical performance model is used, but constructing such a model is inherently difficult and requires an expertise which application programmers may not have. In this paper, we introduce and evaluate an alternative approach to application adaptation and resource selection which does not need a performance model. We start an application on any set of resources. During the application run, we periodically collect information about the communication times and idle times of the processors. We use these statistics to automatically estimate the resource requirements of the application. Next, we adjust the resource set the application is running on by adding or removing compute nodes or even entire clusters. Processors are added or deleted to stay between the thresholds, thus adapting automatically to the changing environment. A major advantage of our approach is that it improves application performance in many different situations that are typical for grid computing. It handles all of the following cases : Our work assumes the application is malleable and can run -LRB- efficiently -RRB- on multiple sites of a grid -LRB- i.e., using co-allocation -LRB- 15 -RRB- -RRB-. area latencies. We have applied our ideas to divide-and-conquer applications, which satisfy these requirements. Divide-and-conquer has been shown to be an attractive paradigm for programming grid applications -LRB- 4 ; 20 -RRB-. We believe that our approach can be extended to other classes of applications with the given assumptions. We implemented our strategy in Satin, which is a Java-centric framework for writing grid-enabled divide-and-conquer applications -LRB- 20 -RRB-. The rest of this paper is structured as follows. In Section 2, we explain what assumptions we are making about the applications and grid resources. In Section 3, we present our resource selection and adaptation strategy. In Section 4, we describe its implementation in the Satin framework. In Section 5, we evaluate our approach in a number of grid scenarios. In Section 6, we compare our approach with the related work. Finally, in Section 7, we conclude and describe future work. 2. Background and assumptions In this section, we describe our assumptions about the applications and their resources. We assume the following resource model. The applications are running on multiple sites at the same time, where sites are clusters or supercomputers. Processors belonging to one site are connected by a fast LAN with a low latency and high bandwidth. The different sites are connected by a WAN. Communication between sites suffers from high latencies. We studied the adaptation problem in the context of divide-andconquer applications. However, we believe that our methodology can be used for other types of applications as well. In this section we summarize the assumptions about applications that are important to our approach. The first assumption we make is that the application is malleable, i.e., it is able to handle processors joining and leaving the on-going computation. In -LRB- 23 -RRB-, we showed how divide-andconquer applications can be made fault tolerant and malleable. Processors can be added or removed at any point in the computation with little overhead. The second assumption is that the application can efficiently run on processors with different speeds. This can be achieved by using a dynamic load balancing strategy, such as work stealing used by divide-and-conquer applications -LRB- 19 -RRB-. Also, master-worker applications typically use dynamic load-balancing strategies -LRB- e.g., MW -- a framework for writing gridenabled master-worker applications -LRB- 12 -RRB- -RRB-. We find it a reasonable assumption for a grid application, since applications for which the slowest processor becomes a bottleneck will not be able to efficiently utilize grid resources. Finally, the application should be insensitive to wide-area latencies, so it can run efficiently on a widearea grid -LRB- 16 ; 17 -RRB-. 6. Related work A number of Grid projects address the question of resource selection and adaptation. In GrADS -LRB- 18 -RRB- and ASSIST -LRB- 1 -RRB-, resource selection and adaptation requires a performance model that allows predicting application runtimes. In the resource selection phase, a number of possible resource sets is examined and the set of resources with the shortest predicted runtime is selected. If performance degradation is detected during the computation, the resource selection phase is repeated. GrADS uses the ratio of the predicted execution times -LRB- of certain application phases -RRB- to the real execution times as an indicator of application performance. ASSIST uses the number of iterations per time unit -LRB- for iterative applications -RRB- or the number of tasks per time unit -LRB- for regular master-worker applications -RRB- as a performance indicator. The main difference between these approaches and our approach is the use of performance models. The main advantage is that once the performance model is known, the system is able to take more accurate migration decisions than with our approach. However, even if the performance no adaptation with adaptation Figure 7. Barnes-Hut iteration durations with/without adaptation, crashing CPUs model is known, the problem of finding an optimal resource set -LRB- i.e. the resource set with the minimal execution time -RRB- is NP-complete. As the number of available grid resources increases, the accuracy of this approach diminishes, as the subset of possible resource sets that can be examined in a reasonable time becomes smaller. Another disadvantage of these systems is that the performance degradation detection is suitable only for iterative or regular applications. Cactus -LRB- 2 -RRB- and GridWay -LRB- 14 -RRB- do not use performance models. However, these frameworks are only suitable for sequential -LRB- GridWay -RRB- or single-site applications -LRB- Cactus -RRB-. In that case, the resource selection problem boils down to selecting the fastest machine or cluster. Processor clock speed, average load and a number of processors in a cluster -LRB- Cactus -RRB- are used to rank resources and the resource with the highest rank is selected. The application is migrated if performance degradation is detected or better resources are discovered. Both Cactus and GridWay use the number of iterations per time unit as the performance indicator. The main limitation of this methodology is that it is suitable only for sequential or single-site applications. Moreover, resource selection based on clock speed is not always accurate. Finally, performance degradation detection is suitable only for iterative applications and can not be used for irregular computations such as search and optimization problems. The resource selection problem was also studied by the AppLeS project -LRB- 5 -RRB-. In the context of this project, a number of applications were studied and performance models for these applications were created. Based on such a model a scheduling agent is built that uses the performance model to select the best resource set and the best application schedule on this set. AppLeS scheduling agents are written on a case-by-case basis and can not be reused for another application. Two reusable templates were also developed for specific classes of applications, namely master-worker -LRB- AMWAT template -RRB- and parameter sweep -LRB- APST template -RRB- applications. 2 out of 9 clusters crash started adding nodes 96 nodes reached In -LRB- 13 -RRB-, the problem of scheduling master-worker applications is studied. Therefore, the problem is reduced to finding the right number of workers. The approach here is similar to ours in that no performance model is used. Instead, the system tries to deduce the application requirements at runtime and adjusts the number of workers to approach the ideal number. 7. Conclusions and future work In this paper, we investigated the problem of resource selection and adaptation in grid environments. Existing approaches to these problems typically assume the existence of a performance model that allows predicting application runtimes on various sets of resources. However, creating performance models is inherently difficult and requires knowledge about the application. We propose an approach that does not require in-depth knowledge about the application. We start the application on an arbitrary set of resources and monitor its performance. The performance monitoring allows us to learn certain application requirements such as the number of processors needed by the application or the application 's bandwidth requirements. We use this knowledge to gradually refine the resource set by removing inadequate nodes or adding new nodes if necessary. This approach does not result in the optimal resource set, but in a reasonable resource set, i.e. a set free from various performance bottlenecks such as slow network connections or overloaded processors. Our approach also allows the application to adapt to the changing grid conditions. If the weighted average efficiency drops below a certain level, the adaptation coordinator starts removing `` worst '' nodes. If the weighted average efficiency raises above a certain level, new nodes are added. The application adapts fully automatically to changing conditions. Future work will involve extending our adaptation strategy to support opportunistic migration. This, however, requires grid schedulers with more sophisticated functionality than currently exists. Further research is also needed to decrease the benchmarking overhead. Another line of research that we wish to investigate is using feedback control to refine the adaptation strategy during the application run. Finally, the centralized implementation of the adaptation coordinator might become a bottleneck for applications which are running on very large numbers of nodes -LRB- hundreds or thousands -RRB-.", "keyphrases": ["grid comput", "resourc select", "grid environ", "parallel comput", "homogen parallel environ", "heterogen of resourc", "high-bandwidth local-area network", "lower-bandwidth wide-area network", "network link", "commun time", "idl time of the processor", "degre of parallel", "overload resourc", "divid-and-conquer"]}
{"file_name": "I-19", "text": "Bidding Optimally in Concurrent Second-Price Auctions of Perfectly Substitutable Goods ABSTRACT We derive optimal bidding strategies for a global bidding agent that participates in multiple, simultaneous second-price auctions with perfect substitutes. We first consider a model where all other bidders are local and participate in a single auction. For this case, we prove that, assuming free disposal, the global bidder should always place non-zero bids in all available auctions, irrespective of the local bidders ' valuation distribution. Furthermore, for non-decreasing valuation distributions, we prove that the problem of finding the optimal bids reduces to two dimensions. These results hold both in the case where the number of local bidders is known and when this number is determined by a Poisson distribution. This analysis extends to online markets where, typically, auctions occur both concurrently and sequentially. In addition, by combining analytical and simulation results, we demonstrate that similar results hold in the case of several global bidders, provided that the market consists of both global and local bidders. Finally, we address the efficiency of the overall market, and show that information about the number of local bidders is an important determinant for the way in which a global bidder affects efficiency. 1. INTRODUCTION The recent surge of interest in online auctions has resulted in an increasing number of auctions offering very similar or In eBay alone, for example, there are often hundreds or sometimes even thousands of concurrent auctions running worldwide selling such substitutable items1. Against this background, it is essential to develop bidding strategies that autonomous agents can use to operate effectively across a wide number of auctions. As we will show, however, this analysis is also relevant to a wider context where auctions are conducted sequentially, as well as concurrently. In contrast, here we consider bidding strategies for markets with multiple concurrent auctions and perfect substitutes. In particular, our focus is on Vickrey or second-price sealed bid auctions. However, our results generalise to settings with English auctions since these are strategically equivalent to second-price auctions. Within this setting, we are able to characterise, for the first time, a bidder 's utilitymaximising strategy for bidding simultaneously in any number of such auctions and for any type of bidder valuation distribution. In more detail, we first consider a market where a single bidder, called the global bidder, can bid in any number of auctions, whereas the other bidders, called the local bidders, are assumed to bid only in a single auction. For this case, we find the following results : \u2022 Whereas in the case of a single second-price auction a bidder 's best strategy is to bid its true value, the best strategy for a global bidder is to bid below it. \u2022 We are able to prove that, even if a global bidder requires only one item, the expected utility is maximised by participating in all the auctions that are selling the desired item. \u2022 Finding the optimal bid for each auction can be an arduous task when considering all possible combinations. However, for most common bidder valuation distribu 2. RELATED WORK Research in the area of simultaneous auctions can be segmented along two broad lines. Such analyses are typically used when the auction format employed in the concurrent auctions is the same -LRB- e.g. there are M Vickrey auctions or M first-price auctions -RRB-. This paper adopts the former approach in studying a market of M simultaneous Vickrey auctions since this approach yields provably optimal bidding strategies. Their work analyses a market consisting of couples having equal valuations that want to bid for a dresser. Thus, the couple 's bid space can at most contain two bids since the husband and wife can be at most at two geographically distributed auctions simultaneously. They derive a mixed strategy Nash equilibrium for the special case where the number of buyers is large. Our analysis differs from theirs in that we study concurrent auctions in which bidders have different valuations and the global bidder can bid in all the auctions concurrently -LRB- which is entirely possible given autonomous agents -RRB-. Following this, -LSB- 7 -RSB- then studied the case of simultaneous auctions with complementary goods. They analyse the case of both local and global bidders and characterise the bidding of the buyers and resultant market efficiency. The setting provided in -LSB- 7 -RSB- is further extended to the case of common values in -LSB- 9 -RSB-. However, neither of these works extend easily to the case of substitutable goods which we consider. The space of symmetric mixed equilibrium strategies is derived for this special case, but again our result is more general. Finally, -LSB- 11 -RSB- considers the case of concurrent English auctions, in which he develops bidding algorithms for buyers with different risk attitudes. However, he forces the bids to be the same across auctions, which we show in this paper not always to be optimal. 7. CONCLUSIONS In this paper, we derive utility-maximising strategies for bidding in multiple, simultaneous second-price auctions. We first analyse the case where a single global bidder bids in all auctions, whereas all other bidders are local and bid in a single auction. For this setting, we find the counter-intuitive result that it is optimal to place non-zero bids in all auctions that sell the desired item, even when a bidder only requires a single item and derives no additional benefit from having more. Thus, a potential buyer can achieve considerable benefit by participating in multiple auctions and employing an optimal bidding strategy. For a number of common valuation distributions, we show analytically that the problem of finding optimal bids reduces to two dimensions. This considerably simplifies the original optimisation problem and can thus be used in practice to compute the optimal bids for any number of auctions. Furthermore, we investigate a setting with multiple global bidders by combining analytical solutions with a simulation approach. We find that a global bidder 's strategy does not stabilise when only global bidders are present in the market, but only converges when there are local bidders as well. We argue, however, that real-world markets are likely to contain both local and global bidders. The converged results are then very similar to the setting with a single global bidder, and we find that a bidder benefits by bidding optimally in multiple auctions. For the more complex setting with multiple global bidders, the simulation can thus be used to find these bids for specific cases. Finally, we compare the efficiency of a market with multiple concurrent auctions with and without a global bidder. We show that, if the bidder can accurately predict the number of local bidders in each auction, the efficiency slightly increases. In contrast, if there is much uncertainty, the efficiency significantly diminishes as the number of auctions increases due to the increased probability that a global bidder wins more than two items. These results show that the way in which the efficiency, and thus social welfare, is affected by a global bidder depends on the information that is available to that global bidder. In future work, we intend to extend the results to imperfect substitutes -LRB- i.e., when a global bidder gains from winning additional items -RRB-, and to settings where the auctions are no longer identical. The latter arises, for example, when the number of -LRB- average -RRB- local bidders differs per auction or the auctions have different settings for parameters such as the reserve price.", "keyphrases": ["optim bid strategi", "global bid agent", "simultan second-price auction", "non-decreas valuat distribut", "onlin market", "multiag system", "market effici", "perfect substitut", "vickrei auction", "social and behavior scienc", "utilitymaximis strategi"]}
{"file_name": "J-2", "text": "Worst-Case Optimal Redistribution of VCG Payments in Heterogeneous-Item Auctions with Unit Demand ABSTRACT Many important problems in multiagent systems involve the allocation of multiple resources among the agents. For resource allocation problems, the well-known VCG mechanism satisfies a list of desired properties, including efficiency, strategy-proofness, individual rationality, and the non-deficit property. However, VCG is generally not budget-balanced. Under VCG, agents pay the VCG payments, which reduces social welfare. To offset the loss of social welfare due to the VCG payments, VCG redistribution mechanisms were introduced. These mechanisms aim to redistribute as much VCG payments back to the agents as possible, while maintaining the aforementioned desired properties of the VCG mechanism. We continue the search for worst-case optimal VCG redistribution mechanisms -- mechanisms that maximize the fraction of total VCG payment redistributed in the worst case. Previously, a worst-case optimal VCG redistribution mechanism -LRB- denoted by WCO -RRB- was characterized for multi-unit auctions with nonincreasing marginal values -LSB- 7 -RSB-. Later, WCO was generalized to settings involving heterogeneous items -LSB- 4 -RSB-, resulting in the HETERO mechanism. -LSB- 4 -RSB- conjectured that HETERO is feasible and worst-case optimal for heterogeneous-item auctions with unit demand. In this paper, we propose a more natural way to generalize the WCO mechanism. We prove that our generalized mechanism, though represented differently, actually coincides with HETERO. Based on this new representation of HETERO, we prove that HETERO is indeed feasible and worst-case optimal in heterogeneous-item auctions with unit demand. Finally, we conjecture that HETERO remains feasible and worst-case optimal in the even more general setting of combinatorial auctions with gross substitutes. 1. INTRODUCTION 1.1 VCG Redistribution Mechanisms Many important problems in multiagent systems involve the allocation of multiple resources among the agents. For resource allocation problems, the well-known VCG mechanism satisfies the following list of desired properties : \u2022 Efficiency : the allocation maximizes the agents ' total valuation -LRB- without considering payments -RRB-. \u2022 Strategy-proofness : for any agent, reporting truthfully is a dominant strategy, regardless of the other agents ' types. \u2022 -LRB- Ex post -RRB- individual rationality : Every agent 's final utility -LRB- after deducting her payment -RRB- is always nonnegative. \u2022 Non-deficit : the total paymentfrom the agents is nonnegative. However, VCG is generally not budget-balanced. Under VCG, agents pay the VCG payments, which reduces social welfare. To offset the loss of social welfare due to the VCG payments, VCG redistribution mechanisms were introduced. These mechanisms still allocate the resources using VCG. On top of VCG, these mechanisms try to redistribute as much VCG payments back to the agents as possible. We require that an agent 's redistribution be independent of her own type. This is sufficient for maintaining strategyproofness and efficiency -LRB- an agent has no control over her own redistribution -RRB-. For smoothly connected domains -LRB- including multiunit auctions with nonincreasing marginal values and heterogeneousitem auctions with unit demand -RRB-, the above requirement is also necessary for maintaining strategy-proofness and efficiency -LSB- 8 -RSB-. A VCG redistribution mechanism is feasible if it maintains all the desired properties of the VCG mechanism. That is, we also require that the redistribution process maintains individual rationality and the non-deficit property. Let n be the number of agents. Since all VCG redistribution mechanisms start by allocating according to the VCG mechanism, a VCG redistribution mechanism is characterized by its redistribution scheme r ~ = -LRB- r1, r2,..., rn -RRB-. Under VCG redistribution mechanism ~ r, agent i 's redistribution equals ri -LRB- 01,..., 0i \u2212 1, 0i +1,..., 0n -RRB-, where 0j is agent j 's type. -LRB- We do not have to differentiate between an agent 's true type and her reported type, since all VCG redistribution mechanisms are strategy-proof. -RRB- An anonymous VCG redistribution mechanism is characterized by a single function r. Under -LRB- anonymous -RRB- VCG redistribution mechanism r, agent i 's redistribution equals r -LRB- 0 \u2212 i -RRB-, where 0 \u2212 i is the multiset of the types of the agents other than i. We use \u03b8 ~ to denote the type profile. Let V CG -LRB- ~ \u03b8 -RRB- be the total We organize existing results by their settings. VCG payment for this type profile. A VCG redistribution mechanism r satisfies the non-deficit property if the total redistribution never exceeds the total VCG payment. A VCG redistribution mechanism r is -LRB- ex post -RRB- individually rational if every agent 's final utility is always nonnegative. After redistribution, agent i 's utility is exactly her redistribution r -LRB- \u03b8 \u2212 i -RRB-. We want to find VCG redistribution mechanisms that maximize the fraction of total VCG payment redistributed in the worst-case. This mechanism design problem is equivalent to the following functional optimization model : In this paper, we will analytically characterize one worst-case optimal VCG redistribution mechanism for heterogeneous-item auctions with unit demand.1 We conclude this subsection with an example VCG redistribution mechanism in the simplest setting of single-item auctions. In a single-item auction, an agent 's type is a nonnegative real number representing her utility for winning the item. In single-item auctions, the Bailey-Cavallo VCG redistribution mechanism -LSB- 2, 3 -RSB- works as follows : \u2022 Allocate the item according to VCG : Agent 1 wins the item and pays \u03b82. The other agents win nothing and do not pay. \u2022 Every agent receives a redistribution that equals n1 times the second highest other type : Agent 1 and 2 each receives n1 \u03b83. The other agents each receives n1\u03b82. The above mechanism obviously maintains strategy-proofness and efficiency -LRB- an agent 's redistribution does not depend on her own type -RRB-. It also maintains individual rationality because all redistributions are nonnegative. The total redistribution equals 2n \u03b83 + the above mechanism maintains the non-deficit property. Finally, the total redistribution 2 n item auctions, this example mechanism 's worst-case redistribution fraction is n \u2212 2 n 1.2 Previous Research on Worst-Case Optimal VCG Redistribution Mechanisms In this subsection, we review existing results on worst-case optimal VCG redistribution mechanisms. Worst-Case Optimal Redistribution in Multi-Unit Auctions with Unit Demand -LSB- 7, 12 -RSB- : In multi-unit auctions with unit demand, the items for sale are identical. Each agent wants at most one copy of the item. -LRB- Single-item auctions are special cases of multi-unit auctions with unit demand. -RRB- Let m be the number of items. Throughout this paper, we only consider cases where m \u2264 n \u2212 2.2 Here, an agent 's type is a nonnegative real number representing her valuation for winning one copy of the item. -LSB- 7 -RSB- also characterized a VCG redistribution mechanism for multiunit auctions with unit demand, called the WCO mechanism.3 WCO 's worst-case redistribution fraction is exactly \u03b1 \u2217. That is, it is worst-case optimal. WCO was obtained by optimizing within the family of linear VCG redistribution mechanisms. A linear VCG redistribution mechanism r takes the following form : Here, the ci are constants. -LRB- We only consider the ci that correspond to feasible VCG redistribution mechanisms. -RRB- -LSB- \u03b8 \u2212 i -RSB- j is the j-th highest type among \u03b8 \u2212 i. Linear mechanism r is characterized by the values of the ci. The optimal values the ci are as follows : The characterization of WCO then follows : Worst-Case Optimal Redistribution in Multi-Unit Auctions with Nonincreasing Marginal Values -LSB- 7 -RSB- : Multi-unit auctions with non2 -LSB- 7 -RSB- showed that for multi-unit auctions with unit demand, when m = n \u2212 1, the worst-case redistribution fraction -LRB- of any feasible VCG redistribution mechanism -RRB- is at most 0. Since the setting studied in this paper is more general -LRB- heterogeneous-item auctions with unit demand -RRB-, we also have that the worst-case redistribution fraction is at most 0 when m = n \u2212 1. Since heterogeneous-item auctions with x units are special cases of heterogeneous-item auctions with x + 1 units, we have that for our setting the worst-case redistribution fraction is at most 0 when m \u2265 n \u2212 1. That is, not redistributing anything is worst-case optimal when m \u2265 n \u2212 1. Also, for -LSB- 12 -RSB- 's objective, the optimal mechanism coincides with WCO only when the individual rationality constraint is enforced. increasing marginal values are more general than multi-unit auctions with unit demand. In this more general setting, the items are still identical, but an agent may demand more than one copy of the item. An agent 's valuation for winning the first copy of the item is called her initial/first marginal value. Similarly, an agent 's additional valuation for winning the i-th copy of the item is called her i-th marginal value. An agent 's type contains m nonnegative real numbers -LRB- i-th marginal value for i = 1,..., m -RRB-. In this setting, it is further assumed that the marginal values are nonincreasing. As discussed earlier, in this more general setting, any VCG redistribution mechanism 's worst-case redistribution fraction is still bounded above by \u03b1 *. -LSB- 7 -RSB- generalized WCO to this setting, and proved that its worst-case redistribution fraction remains the same. Therefore, WCO -LRB- after generalization -RRB- is also worst-case optimal for multi-unit auctions with nonincreasing marginal values. The original definition of WCO does not directly generalize to multi-unit auctions with nonincreasing marginal values. When it comes to multi-unit auctions with nonincreasing marginal values, an agent 's type is no longer a single value, which means that there is no such thing as `` the j-th highest type among \u03b8_i ''. We abuse notation by not differentiating the agents and their types. For example, \u03b8_i is equivalent to the set of agents other than i. Let S be a set of agents. i \u2212 1 -RRB-. Here, U -LRB- S, j -RRB- is the new set of agents, after removing the agent with the j-th highest initial marginal value in S from S. The general form of WCO is as follows : Worst-Case Optimal Redistribution in Heterogeneous-Item Auctions with Unit Demand -LSB- 4 -RSB- : In heterogeneous-item auctions with unit demand, the items for sale are different. Each agent demands at most one item. Here, an agent 's type consists of m nonnegative real numbers -LRB- her valuation for winning item i for i = 1,..., m -RRB-. Heterogeneous-item auctions with unit demand is the main focus of this paper. Since heterogeneous-item auctions with unit demand is more general than multi-unit auctions with unit demand, \u03b1 * is still an upper bound on the worst-case redistribution fraction. -LSB- 4 -RSB- proposed the HETERO mechanism, by generalizing WCO. The authors conjectured that HETERO is feasible and has a worst-case redistribution fraction that equals \u03b1 *. That is, the authors conjectured that HETERO is worst-case optimal in this setting. The main contribution of this paper is a proof of this conjecture. Redistribution in Combinatorial Auctions with Gross Substitutes -LSB- 6 -RSB- : The gross substitutes condition was first proposed in -LSB- 9 -RSB-. Like unit demand, the gross substitutes condition is a condition on an agent 's type -LRB- does not depend on the mechanism under discussion -RRB-. In words, an agent 's type satisfies the gross substitutes condition if her demand for an item does not decrease when the prices of the other items increase. Both multi-unit auctions with nonincreasing marginal values and heterogeneous-item auctions with unit demand are special cases of combinatorial auctions with gross substitutes -LSB- 5, 9 -RSB-. The authors did not find a worst-case optimal mechanism for this setting. At the end of this paper, we conjecture that HETERO is optimal for combinatorial auctions with gross substitutes. Finally, Naroditskiy et al. -LSB- 13 -RSB- proposed a numerical technique for designing worst-case optimal redistribution mechanisms. The proposed technique only works for single-parameter domains. It does not apply to our setting -LRB- multi-parameter domain -RRB-. 1.3 Our contribution We generalize WCO to heterogeneous-item auctions with unit demand. We prove that the generalized mechanism, though represented differently, coincides with the HETERO mechanism proposed in -LSB- 4 -RSB-. That is, what we proposed is not a new mechanism, but a new representation of an existing mechanism. Based on our new representation of HETERO, we prove that HETERO is indeed feasible and worst-case optimal when applied to heterogeneousitem auctions with unit demand, thus confirming the conjecture raised in -LSB- 4 -RSB-. We conclude with a new conjecture that HETERO remains feasible and worst-case optimal in the even more general setting of combinatorial auctions with gross substitutes. 4. CONCLUSION We conclude our paper with the following conjecture : CONJECTURE 1. Gross substitutes implies redistribution monotonicity. That is, HETERO remainsfeasible and worst-case optimal in combinatorial auctions with gross substitutes. The idea is that both multi-unit auctions with nonincreasing marginal values and heterogeneous-item auctions with unit demand satisfy redistribution monotonicity. A natural conjecture is that the `` most restrictive joint '' of these two settings also satisfies redistribution monotonicity. There are many well-studied auction settings that contain both multi-unit auctions with nonincreasing marginal values and heterogeneous-item auctions with unit demand -LRB- a list of which can be found in -LSB- 10 -RSB- -RRB-. Among these well-studied settings, combinatorial auctions with gross substitutes is the most restrictive.", "keyphrases": ["mechan design", "vickrei-clark-grove", "redistribut payment", "effici mechan", "strategi-proof", "individu ration mechan", "mechan", "linear vcg redistribut mechan", "transform to linear program", "analyt character", "worst-case optim mechan"]}
{"file_name": "H-4", "text": "Towards Task-based Personal Information Management Evaluations ABSTRACT Personal Information Management -LRB- PIM -RRB- is a rapidly growing area of research concerned with how people store, manage and re-find information. A feature of PIM research is that many systems have been designed to assist users manage and re-find information, but very few have been evaluated. This has been noted by several scholars and explained by the difficulties involved in performing PIM evaluations. The difficulties include that people re-find information from within unique personal collections ; researchers know little about the tasks that cause people to re-find information ; and numerous privacy issues concerning personal information. In this paper we aim to facilitate PIM evaluations by addressing each of these difficulties. In the first part, we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation. 1. INTRODUCTION Personal Information Management -LRB- PIM -RRB- is a rapidly growing area of research concerned with how people store, manage and re-find information. PIM systems - the methods and procedures by which people handle, categorize, and retrieve information on a day-to-day basis -LSB- 18 -RSB- - are becoming increasingly popular. However the evaluation of these PIM systems is problematic. One of the main difficulties is caused by the personal nature of PIM. People collect information as a natural consequence of completing other tasks. This means that the collections people generate are unique to them alone and the information within a collection is intrinsically linked with the owner 's personal experiences. As personal collections are unique, we can not create evaluation tasks that are applicable to all participants in an evaluation. Secondly, personal collections may contain information that the participants are uncomfortable sharing within an evaluation. The precise nature of this information - what information individuals would prefer to keep private - varies across individuals making it difficult to base search tasks on the contents of individual collections. Therefore, experimenters face a number of challenges in order to conduct realistic but controlled PIM evaluations. Recently, however, researchers have started to focus on ways to address the problem of PIM evaluation. Capra -LSB- 6 -RSB- also identifies the need for controlled PIM lab evaluations to complement other evaluation techniques, placing specific emphasis on the need to understand PIM behaviour at the task level. In this paper, we attempt to address the difficulties involved to faciliate controlled laboratory PIM evaluations. In the first part of this paper we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. We also look at the features of the tasks that make re-finding difficult. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using different methods of task creation. Thus, this paper offers two contributions to the field : an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations. 2. RELATED WORK A variety of approaches are available to study PIM. Naturalistic approaches study participants performing naturally, completing their own tasks as they occur, within familiar environments. These approaches allow researchers to overcome many of the difficulties caused by the personal nature of PIM. As the tasks performed are `` real '' and not simulated, the participants can utilise their own experiences, previous knowledge and information collections to complete the tasks. Both ethnographic and fieldwork methods require the presence of an experimenter to assess how PIM is performed, which raises a number of issues. Firstly, evaluation in this way is expensive ; taking long time periods to study small numbers of participants and these small samples may not be representative of the behaviour of larger populations. Secondly, because participants can not be continually observed, experimenters must choose when to observe and this may affect the findings. An alternative strategy to conducting naturalistic evaluations is to utilise log file analysis. This approach makes use of logging software that captures a broad sampling of user activities in the context of natural use of a system. This reveals a need to complement naturalistic studies with controlled experiments where the experimenter can relate the behaviour of study participants to goals associated with known search tasks. One difficulty in performing this kind of evaluation is sourcing collections to evaluate. Kelly -LSB- 16 -RSB- proposes the introduction of a shared test collection that would provide sharable, reusable data sets, tasks and metrics for those interested in conducting PIM research. However, a shared collection would be unsuitable for user studies because it would not be possible to incorporate the personal aspects of PIM while using a common, unfamiliar collection. One alternative approach is to ask users to provide their own information collections to simulate familiar environments within the lab. This approach has been applied to study the re-finding of personal photographs -LSB- 11 -RSB-, email messages -LSB- 20 -RSB-, and web-bookmarks -LSB- 21 -RSB-. The usefulness of this approach depends on how easy it is to transfer the collection or gain remote access. Another solution is to use the entire web as a collection when studying web page re-finding -LSB- 4 -RSB-. This may be appropriate for studying web page re-finding because previous studies have shown that people often use web search engines for this purpose -LSB- 5 -RSB-. A second difficulty in performing PIM laboratory studies is creating tasks for participants to perform that can be solved by searching a shared or personal collection. Tasks relate to the activity that results in a need for information -LSB- 14 -RSB- and are acknowledged to be important in determining user behaviour -LSB- 26 -RSB-. A large body of work has been carried out to understand the nature of tasks and how the type of task influences user information seeking behaviour. For example, tasks have been categorised in terms of increasing complexity -LSB- 3 -RSB- and task complexity has been suggested to affect how searchers perceive their information needs -LSB- 25 -RSB- and how they try to find information -LSB- 3 -RSB-. Other previous work has provided methodologies that allow the simulation of tasks when studying information seeking behaviour -LSB- 2 -RSB-. However, little is known about the kinds of tasks that cause people to search their personal stores or re-find information that they have seen before. Consequently, it is difficult to devise simulated work task situations for PIM. The exception is the study of personal photograph management, where Rodden 's work on categorising personal photograph search tasks has facilitated the creation of simulated work task situations -LSB- 22 -RSB-. There have been other suggestions as to how to classify PIM tasks. While these are interesting properties that may affect how a task will be performed, they do not give experimenters enough scope to devise tasks. Personal collections are one reason why task creation is so difficult. Rodden 's photo task taxonomy provides a solution here because it allows tasks, tailored to private collections to be categorised. Systems can then be compared across task types for different users -LSB- 11 -RSB-. Unfortunately, no equivalent taxonomy exists for other types of information object. Further, other types of object are more sensitive to privacy than photographs ; it is unlikely that participants would be as content to allow researchers to browse their email collections to create tasks as they were with photographs in -LSB- 11 -RSB-. This presents a serious problem - how can researchers devise tasks that correspond to private collections without an understanding of the kinds of tasks people perform or jeopardising the privacy of study participants? A few methods have been proposed. For example, -LSB- 20 -RSB- studied email search by asking participants to re-find emails that had been sent to every member in a department ; allowing the same tasks to be used for all of the study participants. This approach ensured that privacy issues were avoided and participants could use things that they remember to complete tasks. Nevertheless, the systems were only tested using one type of task - participants were asked to find single emails, each of which shared common properties. In section 4 we show that people perform a wider range of email re-finding tasks than this. In -LSB- 4 -RSB-, generic search tasks were artificially created by running evaluations over two sessions. In the first session, participants were asked to complete work tasks that involved finding some unknown information. In the second session, participants completed the same tasks again, which naturally involved some re-finding behaviour. The limitations of this technique are that it does not allow participants to exploit any personal connections with the information because the information they are looking for may not correspond to any other aspect of their lives. Our review of evaluation approaches motivates a requirement for controlled laboratory experiments that allow tightly defined aspects of systems or interfaces to be tested. Unfortunately, it has also been shown that there are difficulties involved in performing this type of evaluation - it is difficult to source collections and to devise tasks that correspond to private collections, while at the same time protect the privacy of the study participants. In the following section we present a diary study of refinding tasks for email and web pages. The outcome is a classification of tasks similar to that devised by Rodden for personal photographs -LSB- 22 -RSB-. In section 5 we build on this work by examining methods for creating tasks that do not compromise the privacy of participants and discuss how our work can facilitate task-based PIM user evaluations. We show that by collecting tasks using electronic diaries, not only can we learn about the tasks that cause people to re-find personal information, but we can learn about the contents of private collections without compromising the privacy of the participants. This knowledge can then be used to construct tasks for use in PIM evaluations. 6. CONCLUSIONS This paper has focused on overcoming the difficulties involved in performing PIM evaluations. The personal nature of PIM means that it is difficult to construct balanced experiments because participants each have their own unique collections that are self-generated by completing other tasks. We suggested that to incorporate the personal aspects of PIM in evaluations, the performance of systems or users should be examined when users complete tasks on their own collections. This approach itself has problems because task creation for personal collections is difficult : researchers do n't know much about the kinds of re-finding tasks people perform and they do n't know what information is within individual personal collections. In this paper we described ways of overcoming these challenges to facilitate task based PIM user evaluations. In the first part of the paper we performed a diary study that examined the tasks that caused people to re-find email messages and web pages. The collected data included a wide range of both work and non-work related tasks, and based on the data we created a taxonomy of web and email re-finding tasks. We discovered that people perform three main types of re-finding task : tasks that require specific information from within a single resource, tasks that require a single complete resource, and tasks that require information to be recovered from multiple resources. In the second part of the paper, we discussed the significance of the taxonomy with respect to PIM evaluation. We demonstrated that balanced experiments could be conducted comparing system or user performance on the task categories within the taxonomy. We also suggested two methods of creating tasks that can be completed on personal collections. These methods do not compromise the privacy of study participants. We examined the techniques suggested, firstly by simulating an experimental situation - participants were asked to re-perform their own tasks as they recorded them, and secondly, in the context of a full evaluation. Performing evaluations in this way will allow systems that have been proposed to improve users ' ability to manage and re-find their information to be tested, so that we can learn about the needs and desires of users. Thus, this paper has offered two contributions to the field : an increased understanding of PIM behaviour at the task level and an evaluation method that will facilitate further investigations.", "keyphrases": ["person inform manag", "measur", "experiment", "human factor", "re-find inform", "privaci issu", "taxonomi", "individu collect", "email messag", "naturalist approach", "laboratori-base studi"]}
{"file_name": "I-12", "text": "Sharing Experiences to Learn User Characteristics in Dynamic Environments with Sparse Data ABSTRACT This paper investigates the problem of estimating the value of probabilistic parameters needed for decision making in environments in which an agent, operating within a multi-agent system, has no a priori information about the structure of the distribution of parameter values. The agent must be able to produce estimations even when it may have made only a small number of direct observations, and thus it must be able to operate with sparse data. The paper describes a mechanism that enables the agent to significantly improve its estimation by augmenting its direct observations with those obtained by other agents with which it is coordinating. To avoid undesirable bias in relatively heterogeneous environments while effectively using relevant data to improve its estimations, the mechanism weighs the contributions of other agents ' observations based on a real-time estimation of the level of similarity between each of these agents and itself. The `` coordination autonomy '' module of a coordination-manager system provided an empirical setting for evaluation. Simulation-based evaluations demonstrated that the proposed mechanism outperforms estimations based exclusively on an agent 's own observations as well as estimations based on an unweighted aggregate of all other agents ' observations. 1. INTRODUCTION For many real-world scenarios, autonomous agents need to operate in dynamic, uncertain environments in which they have only incomplete information about the results of their actions and characteristics of other agents or people with whom they need to cooperate or collaborate. In such environments, agents can benefit from sharing information they gather, pooling their individual experiences to improve their estimations of unknown parameters required for reasoning about actions under uncertainty. This paper addresses the problem of learning the distribution of the values of a probabilistic parameter that represents a characteristic of a person who is interacting with a computer agent. The characteristic to be learned is -LRB- or is clearly related to -RRB- an important factor in the agent 's decision making.1 The basic setting we consider is one in which an agent accumulates observations about a specific user characteristic and uses them to produce a timely estimate of some measure that depends on that characteristic 's distribution. Typically, agents must make decisions in real time, concurrent with task execution, and in the midst of great uncertainty. In the remainder of this paper, we use the term `` fast-paced '' to refer to such environments. In fast-paced environments, information gathering may be limited, and it is not possible to learn offline or to wait until large amounts of data are collected before making decisions. Thus, the goal of the estimation methods presented in this paper is to minimize the average error over time, rather than to determine an accurate value at the end of a long period of interaction. That is, the agent is expected to work with the user for a limited time, and it attempts to minimize the overall error in its estimations. In such environments, an agent 's individually acquired data -LRB- its own observations -RRB- are too sparse for it to obtain good estimations in the requisite time frame. Given the no-structure-constraint of the environment, approaches that depend on structured distributions may result in a significantly high estimation bias. We consider this problem in the context of a multi-agent distributed system in which computer agents support people who are carrying out complex tasks in a dynamic environment. The fact that agents are part of a multi-agent setting, in which other agents may also be gathering data to estimate a similar characteristic of their users, offers the possibility for an agent to augment its own observations with those of other agents, thus improving the accuracy of its learning process. Furthermore, in the environments we consider, agents are usually accumulating data at a relatively similar rate. Nonetheless, the extent to which the observations of other agents will be useful to a given agent depends on the extent to which their users ' characteristics ' distributions are correlated with that of this agent 's user. There is no guarantee that the distribution for two different agents is highly, positively correlated, let alone that they are the same. Therefore, to use a data-sharing approach, a learning mechanism must be capable of effectively identifying the level of correlation between the data collected by different agents and to weigh shared data depending on the level of correlation. The design of a coordination autonomy -LRB- CA -RRB- module within a coordination-manager system -LRB- as part of the DARPA Coordinators project -LSB- 18 -RSB- -RRB-, in which agents support a distributed scheduling task, provided the initial motivation and a conceptual setting for this work. However, the mechanisms themselves are general and can be applied not only to other fast-paced domains, but also in other multi-agent settings in which agents are collecting data that overlaps to some extent, at approximately similar rates, and in which the environment imposes the no-structure, limited - and early-use constraints defined above -LRB- e.g., exploration of remote planets -RRB-. In particular, our techniques would be useful in any setting in which a group of agents undertakes a task in a new environment, with each agent obtaining observations at a similar rate of individual parameters they need for their decision-making. In this paper, we present a mechanism that was used for learning key user characteristics in fast-paced environments. The mechanism provides relatively accurate estimations within short time frames by augmenting an individual agent 's direct observations with observations obtained by other agents with which it is coordinating. In particular, we focus on the related problems of estimating the cost of interrupting a person and estimating the probability that that person will have the information required by the system. The mechanism was successfully tested using a system that simulates a Coordinators environment. The next section of the paper describes the problem of estimating user-related parameters in fastpaced domains. Section 3 provides an overview of the methods we developed. The implementation, empirical setting, and results are given in Sections 4 and 5. A comparison with related methods is given in Section 6 and conclusions in section 7. 6. RELATED WORK In addition to the interruption management literature reviewed in Section 2, several other areas of prior work are relevant to the selective-sharing mechanism described in this paper. Collaborative filtering, which makes predictions -LRB- filtering -RRB- about the interests of a user -LSB- 7 -RSB-, operates similarly to selective-sharing. However, collaborative filtering systems exhibit poor performance when there is not sufficient information about the users and when there is not sufficient information about a new user whose taste the system attempts to predict -LSB- 7 -RSB-. Selective-sharing relies on the ability to find similarity between specific parts of the probability distribution function associated with a characteristic of different users. This capability is closely related to clustering and classification, an area widely studied in machine learning. Given space considerations, our review of this area is restricted to some representative approaches for clustering. Of particular importance is that the CA needs to find similarity between functions, defined over a continuous interval, with no distinct pre-defined attributes. An additional difficulty is defining the distance measure. Many clustering techniques have been used in data mining -LSB- 2 -RSB-, with particular focus on incremental updates of the clustering, due to the very large size of the databases -LSB- 3 -RSB-. However the applicability of these to fast-paced domains is quite limited because they rely on a large set of existing data. The most relevant method for our purposes is the Kullback-Leibler relative entropy index that is used in probability theory and information theory -LSB- 12 -RSB-. However, the method will perform poorly in scenarios in which the functions alternate between different levels while keeping the `` general '' structure and moments. 208 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- them, while our Wilcoxon-based approach will give them the highest rank in terms of similarity. While the Wilcoxon test is a widely used statistical procedure -LSB- 22, 14 -RSB-, it is usually used for comparing two sets of single-variate data. To our knowledge, no attempt has been made yet to extend its properties as an infrastructure for determining with whom and to what extent information should be shared, as presented in this paper. In these applications, it is used primarily as an identification tool and ranking criterion.", "keyphrases": ["probabilist paramet", "agent", "inform share", "decis make", "fast-pace environ", "multi-agent distribut system", "learn mechan", "select-share", "paramet estim"]}
{"file_name": "I-1", "text": "Aborting Tasks in BDI Agents ABSTRACT Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent 's own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system -LRB- such as JACK, Jadex, Jason, or SPARK -RRB-. A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence. 1. INTRODUCTION Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan can not be guaranteed -LSB- 13 -RSB-. Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence -LSB- 21 -RSB-. In agent architectures inspired by the Belief-Desire-Intention -LRB- BDI -RRB- model -LSB- 16 -RSB-, these properties are often characterized by the interactions between beliefs, goals, and plans -LSB- 2 -RSB-.1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success -LSB- 12, 14 -RSB-. Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent 's execution cycle. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. Aborting a task or plan is distinct from its failure. In contrast, aborting says nothing about the ability to perform ; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up. There has been a considerable amount of work on plan failures -LRB- such as detecting and resolving resource conflicts -LSB- 20, 10 -RSB- -RRB- and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct 1One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of `` the task having been performed '', and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use `` task '' to also refer to goals. fails -LRB- common approaches include either letting the other branch run to completion unhindered or dropping it completely -RRB-. In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN -LSB- 23, 17 -RSB-. Our focus is on a single agent, complementary to related work that considers exception handling for single - and multiagent systems -LRB- e.g., -LSB- 22, 5, 6 -RSB- -RRB-. This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work. 5. RELATED WORK Plan failure is handled in the extended version of AgentSpeak found in the Jason system -LSB- 6 -RSB-. Failure `` clean-up '' plans are triggered from goal deletion events --! g. In a goal deletion plan, the programmer can specify any `` undo '' actions and whether to attempt the goal again. If no goal deletion plan is provided, Jason 's default behaviour is to not reattempt the goal. Failure handling is applied only to plans triggered by addition of an achievement or test goal ; in particular, goal deletion events are not posted for failure of a goal deletion plan. The implementation of H \u00a8 ubner et al. -LSB- 6 -RSB- requires Jason 's internal actions. A requirement for implementing our approach is a reflective capability in the BDI agent implementation. All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states. Such meta level facilities are also required by the approach of Unruh et al. -LSB- 21 -RSB-, who define goal-based semantic compensation for an agent. Failure-handling goals are invoked according to failurehandling strategy rules, by a dedicated agent Failure Handling Component -LRB- FHC -RRB- that tracks task execution. These goals are specified by the agent programmer and attached to tasks, much like our FAb -LRB- P, PF, PA -RRB- construct associates failure and abort methods with a plan P. Note, however, that in contrast to both -LSB- 6 -RSB- and our semantics, -LSB- 21 -RSB- attach the failure-handling knowledge at the goal, not plan, level. Their failure-handling goals may consist of stabilization goals that perform localized, immediate `` clean-up '' to restore the agent 's state to a known, stable state, and compensation goals that perform `` undo '' actions. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure -LRB- i.e., if the FHC directs the agent to retry the failed goal and the retry is successful -RRB-. This contrasts with simplistic plan-level failure handling in which the what and how are intermingled in domain task knowledge. While our approach is defined at the plan level, our extended BDI semantics provides for the separation of execution and failure handling. Further, the FHC explicitly maintains data structures to track agent execution. We leverage the existing execution structures and self-reflective ability of a BDI agent to accomplish both aborting and failure handling without additional overhead. FHC 's failure-handling strategy rules -LRB- e.g., whether to retry a failed goal -RRB- are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent 's nature -LRB- e.g., blindly committed -RRB-. The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism -LRB- although not tied to any one agent system -RRB-. 14 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- a state-based protocol. This approach, together with state checkpointing, is used for multi-agent systems in -LSB- 22 -RSB-. The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery. Other work on multi-agent exception handling includes AOEX 's distributed exception handling agents -LSB- 5 -RSB-, and the similar sentinels of -LSB- 8 -RSB-. In both cases, failure-handling logic and knowledge are decoupled from the agents ; by contrast, while separating exception handling from domain-specific knowledge, Unruh et al. 's FHC and our approach both retain failure-handling logic within an agent. 6. CONCLUSION AND FUTURE WORK The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them -LRB- perhaps at the request of another agent to do so -RRB-, or by unbidden factors that lead to failure. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN. We are planning to implement an instance of our approach in the SPARK agent system -LSB- 9 -RSB- ; in particular, the work of this paper will be the basis for SPARK 's abort handling mechanism. An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action. We have assumed the default behaviour of a BDI-style agent, according to its nature : for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain -LRB- in which case to fail the task -RRB-. Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish -LSB- 19, 12 -RSB-. Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process. This would in particular influence the commitment the agent has towards a particular task : the higher the cost, the greater the commitment. A further item of interest is extending our approach to failure and abort to maintenance goals -LSB- 1 -RSB-. For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves.", "keyphrases": ["intellig agent", "failur", "deal", "cleanup method", "abort-method", "oper semant", "task", "goal", "goal construct"]}
{"file_name": "I-9", "text": "Temporal Linear Logic as a Basis for Flexible Agent Interactions ABSTRACT Interactions between agents in an open system such as the Internet require a significant degree of flexibility. A crucial aspect of the development of such methods is the notion of commitments, which provides a mechanism for coordinating interactive behaviors among agents. In this paper, we investigate an approach to model commitments with tight integration with protocol actions. This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments. We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic, which incorporates both temporal and resource-sensitive reasoning. We also discuss the application of this framework to scenarios such as online commerce. 1. INTRODUCTION AND MOTIVATION The agent paradigm has become well suited as a design metaphor to deal with complex systems comprising many components each having their own thread of control and purposes and involved in dynamic and complex interactions. In multi-agent environments, agents often need to interact with each other to fulfill their goals. Protocols are used to regulate interactions. In traditional approaches to protocol specification, like those using Finite State Machines or Petri Nets, protocols are often predetermined legal sequences of interactive behaviors. Therefore, agents are required to adapt their interactive behaviors to succeed and interactions among agents should not be constructed rigidly. To achieve flexibility, as characterized by Yolum and Singh in -LSB- 11 -RSB-, interaction protocols should ensure that agents have autonomy over their interactive behaviors, and be free from any unnecessary constraints. Also, agents should be allowed to adjust their interactive actions to take advantages of opportunities or handle exceptions that arise during interaction. For example, consider the scenario below for online sales. Cus has a goal of obtaining from Mer a cricket bat at some time. There are two options for Cus to pay. If Cus uses credit payment, Mer needs a bank Ebank to check Cus 's credit. If Cus 's credit is approved, Ebank will arrange the credit payment. Otherwise, Cus may then take the option to pay via PayPal. The interaction ends when goods are delivered and payment is arranged. A flexible approach to this example should include several features. Secondly, there should be no unnecessary constraint on the order in which actions are performed, such as which of making payments and sending the cricket bat should come first. Thirdly, choosing a sequence of interactive actions should be based on reasoning about the intrinsic meanings of protocol actions, which are based on the notion of commitment, i.e. which refers to a strong promise to other agent -LRB- s -RRB- to undertake some courses of action. Current approaches -LSB- 11, 12, 10, 1 -RSB- to achieve flexibilities using the notion of commitment make use of an abstract layer of commitments. However, in these approaches, a mapping from protocol actions onto operations on commitments as well as handling and enforcement mechanisms of commitments must be externally provided. Execution of protocol actions also requires concurrent execution of operations on related commitments. As a result, the overhead of processing the commitment layer makes specification and execution of protocols more complicated and error prone. There is also a lack of a logic to naturally express aspects of resources, internal and external choices as well as time of protocols. Rather than creating another layer of commitment outside protocol actions, we try to achieve a modeling of commitments that is integrated with protocol actions. Both commitments and protocol actions can then be reasoned about in one consistent system. In order to achieve that, we specify protocols in a declarative manner, i.e. what is to be achieved rather then how agents should interact. A key to this is using logic. Temporal logic, in particular, is suitable for describing and reasoning about temporal constraints while linear logic -LSB- 3 -RSB- is quite suitable for modeling resources. We suggest using a combination of linear logic and temporal logic to construct a commitment based interaction framework which allows both temporal and resource-related reasoning for interaction protocols. This provides a natural manipulation and reasoning mechanism as well as internal enforcement mechanisms for commitments based on proof search. Section 2 discusses the background material of linear logic, temporal linear logic and commitments. Section 3 introduces our modeling framework and specification of protocols. Section 4 discusses how our framework can be used for an example of online sale interactions between a merchant, a bank and a customer. We then discuss the advantages and limitations of using our framework to model interaction protocols and achieve flexibility in Section 5. Section 6 presents our conclusions and items of further work. 2. BACKGROUND In order to increase the agents ' autonomy over their interactive behaviors, protocols should be specified in terms of what is to be achieved rather than how the agents should act. In other words, protocols should be specified in a declarative manner. Using logic is central to this specification process. 2.1 Linear Logic Logic has been used as formalism to model and reason about agent systems. Linear logic -LSB- 3 -RSB- is well-known for modeling resources as well as updating processes. It has been considered in agent systems to support agent negotiation and planning by means of proof search -LSB- 5, 8 -RSB-. In real life, resources are consumed and new resources are created. In such logic as classical or temporal logic, however, a direct mapping of resources onto formulas is troublesome. If we model resources like A as `` one dollar '' and B as `` a chocolate bar '', then A = * B in classical logic is read as `` from one dollar we can get a chocolate bar ''. In order to resolve such resource - formula mapping issues, Girard proposed the constraints on which formulas will be used exactly once and can no longer be freely added or removed in derivations and hence treating linear logic formulas as resources. In linear logic, a linear implication A -- B, however, allows A to be removed after deriving B, which means the dollar is gone after using one dollar to buy a chocolate bar. Classical conjunction -LRB- and -RRB- and disjunction -LRB- or -RRB- are recast over different uses of contexts - multiplicative as combining and additive as sharing to come up with four connectives. The ability to specify choices via the additive connectives is a particularly useful feature of linear logic. A & -LRB- additive conjunction -RRB- B, stands for one own choice, either of A or B but not both. In agent systems, this duality between inner and outer choices is manifested by one agent having the power to choose between alternatives and the other having to react to whatever choice is made. Moreover, during interaction, the ability to match consumption and supply of resources among agents can simplify the specification of resource allocations. Linear logic is a natural mechanism to provide this ability -LSB- 5 -RSB-. In addition, it is emphasized in -LSB- 8 -RSB- that linear logic is used to model agent states as sets of consumable resources and particularly, linear implication is used to model transitions among states and capabilities of agents. 2.2 Temporal Linear Logic While linear logic provides advantages to modeling and reasoning about resources, it does not deal naturally with time constraints. Temporal logic, on the other hand, is a formal system which addresses the description and reasoning about the changes of truth values of logic expressions over time -LSB- 2 -RSB-. Temporal logic can be used for specification and verification of concurrent and reactive programs -LSB- 2 -RSB-. Temporal Linear Logic -LRB- TLL -RRB- -LSB- 6 -RSB- is the result of introducing temporal logic into linear logic and hence is resourceconscious as well as deals with time. The temporal operators used are Q -LRB- next -RRB-, \u2751 -LRB- anytime -RRB-, and O -LRB- sometime -RRB- -LSB- 6 -RSB-. Formulas with no temporal operators can be considered as being available only at present. Adding Q to a formula A, i.e. QA, means that A can be used only at the next time and exactly once. Similarly, \u2751 A means that A can be used at any time and exactly once. OA means that A can be used once at some time. Though both \u2751 and O refer to a point in time, the choice of which time is different. Regarding \u2751, the choice is an internal choice, as appropriate to one 's own capability. With O, the choice is externally decided by others. 2.3 Commitment The concept of social commitment has been recognized as fundamental to agent interaction. Indeed, social commitment provides intrinsic meanings of protocol actions and states -LSB- 11 -RSB-. In particular, persistence in commitments introduces into agents ' consideration a certain level of predictability of other agents ' actions, which is important when agents deal with issues of inter-dependencies, global constraints or The Sixth Intl.. Joint Conf. resources sharing -LSB- 7 -RSB-. Commitment based approaches associate protocols actions with operations on commitments and protocol states with the set of effective commitments -LSB- 11 -RSB-. Completing the protocol is done via means-end reasoning on commitment operations to bring the current state to final states where all commitments are resolved. From then, the corresponding legal sequences of interactive actions are determined. Hence, the approaches systematically enhance a variety of legal computations -LSB- 11 -RSB-. Commitments can be reduced to a more fundamental form known as pre-commitments. A pre-commitment here refers to a potential commitment that specifies what the owner agent is willing to commit -LSB- 4 -RSB-, like performing some actions or achieving a particular state. Agents can negotiate about pre-commitments by sending proposals of them to others. Once a precommitment is agreed, it then becomes a commitment and the process moves from negotiation phase to commitment phase, in which the agents act to fulfill their commitments.", "keyphrases": ["multi-agent environ", "interact behavior", "tempor constraint", "interact protocol", "linear logic", "multipl conjunct", "classic conjunct", "level of predict", "pre-commit", "linear implic", "emerg protocol", "condit commit", "request messag", "causal relationship"]}
{"file_name": "J-3", "text": "Budget Optimization in Search-Based Advertising Auctions ABSTRACT Internet search companies sell advertisement slots based on users ' search queries via an auction. While there has been previous work on the auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return -LRB- the number of user clicks on their ads -RRB- for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least a 1 \u2212 1/e fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem. 1. INTRODUCTION Online search is now ubiquitous and Internet search companies such as Google, Yahoo! and MSN let companies and * Work done while visiting Google, Inc., New York, NY. individuals advertise based on search queries posed by users. Conventional media outlets, such as TV stations or newspapers, price their ad slots individually, and the advertisers buy the ones they can afford. In contrast, Internet search companies find it difficult to set a price explicitly for the advertisements they place in response to user queries. Thus, they rely on the market to determine suitable prices by using auctions amongst the advertisers. It is a challenging problem to set up the auction in order to effect a stable market in which all the parties -LRB- the advertisers, users as well as the Internet search company -RRB- are adequately satisfied. The perspective in this paper is not of the Internet search company that displays the advertisements, but rather of the advertisers. The challenge from an advertiser 's point of view is to understand and interact with the auction mechanism. The advertiser determines a set of keywords of their interest and then must create ads, set the bids for each keyword, and provide a total -LRB- often daily -RRB- budget. When a user poses a search query, the Internet search company determines the advertisers whose keywords match the query and who still have budget left over, runs an auction amongst them, and presents the set of ads corresponding to the advertisers who `` win '' the auction. The advertiser whose ad appears pays the Internet search company if the user clicks on the ad. The focus in this paper is on how the advertisers bid. For the particular choice of keywords of their interest1, an advertiser wants to optimize the overall effect of the advertising campaign. The Internet search companies are supportive to1The choice of keywords is related to the domain-knowledge of the advertiser, user behavior and strategic considerations. Internet search companies provide the advertisers with summaries of the query traffic which is useful for them to optimize their keyword choices interactively. We do not directly address the choice of keywords in this paper, which is addressed elsewhere -LSB- 13 -RSB-. wards advertisers and provide statistics about the history of click volumes and prediction about the future performance of various keywords. 9 There are complex interactions between keywords because a user query may match two or more keywords, since the advertiser is trying to cover all the possible keywords in some domain. In effect the advertiser ends up competing with herself. As a result, the advertisers face a challenging optimization problem. The focus of this paper is to solve this optimization problem. 1.1 The Budget Optimization Problem We present a short discussion and formulation of the optimization problem faced by advertisers ; a more detailed description is in Section 2. A given advertiser sees the state of the auctions for searchbased advertising as follows. There is a set K of keywords of interest ; in practice, even small advertisers typically have a large set K. There is a set Q of queries posed by the users. For each query q G Q, there are functions giving the clicksq -LRB- b -RRB- and costq -LRB- b -RRB- that result from bidding a particular amount b in the auction for that query, which we model more formally in the next section. There is a bipartite graph G on the two vertex sets representing K and Q. For any query q G Q, the neighbors of q in K are the keywords that are said to `` match '' the query q. 2 The budget optimization problem is as follows. Given graph G together with the functions clicksq -LRB-. -RRB- and costq -LRB-. -RRB- on the queries, as well as a budget U, determine the bids bk for each keyword k G K such that Pq clicksq -LRB- bq -RRB- is maximized subject to Pq costq -LRB- bq -RRB- < U, where the `` effective bid '' bq on a query is some function of the keyword bids in the neighborhood of q. While we can cast this problem as a traditional optimization problem, there are different challenges in practice depending on the advertiser 's access to the query and graph information, and indeed the reliability of this information -LRB- e.g., it could be based on unstable historical data -RRB-. Thus it is important to find solutions to this problem that not only get many clicks, but are also simple, robust and less reliant on the information. In this paper we define the notion of a `` uniform '' strategy which is essentially a strategy that bids uniformly on all keywords. Since this type of strategy obviates the need to know anything about the particulars of the graph, and effectively aggregates the click and cost functions on the queries, it is quite robust, and thus desirable in practice. What is surprising is that uniform strategy actually performs well, which we will prove. 1.2 Our Main Results and Technical Overview We present positive and negative results for the budget optimization problem. In particular, we show : 9 Nearly all formulations of the problem are NP-Hard. In cases slightly more general than the formulation above, where the clicks have weights, the problem is inapproximable better than a factor of 1 -- 1e, unless P = NP. 9 We give a -LRB- 1 -- 1/e -RRB- - approximation algorithm for the budget optimization problem. The strategy found by the algorithm is a two-bid uniform strategy, which means that it randomizes between bidding some value b1 on all keywords, and bidding some other value b2 on all keywords until the budget is exhausted3. We show that this approximation ratio is tight for uniform strategies. We also give a -LRB- 1/2 -RRB- - approximation algorithm that offers a single-bid uniform strategy, only using one value b1. -LRB- This is tight for single-bid uniform strategies. -RRB- These strategies can be computed in time nearly linear in JQJ + JKJ, the input size. Uniform strategies may appear to be naive in first consideration because the keywords vary significantly in their click and cost functions, and there may be complex interaction between them when multiple keywords are relevant to a query. After all, the optimum can configure arbitrary bids on each of the keywords. Even for the simple case when the graph is a matching, the optimal algorithm involves placing different bids on different keywords via a knapsack-like packing -LRB- Section 2 -RRB-. So, it might be surprising that a simple two-bid uniform strategy is 63 % or more effective compared to the optimum. Our proof of the 1 -- 1/e approximation ratio relies on an adversarial analysis. We define a factor-revealing LP -LRB- Section 4 -RRB- where primal solutions correspond to possible instances, and dual solutions correspond to distributions over bidding strategies. We have conducted simulations using real auction data from Google. The results of these simulations, which are highlighted at the end of Section 4, suggest that uniform bidding strategies could be useful in practice. 8. CONCLUDING REMARKS Another interesting generalization is to consider weights on the clicks, which is a way to model conversions. -LRB- A conversion corresponds to an action on the part of the user who clicked through to the advertiser site ; e.g., a sale or an account sign-up. -RRB- Finally, we have looked at this system as a black box returning clicks as a function of bid, whereas in reality it is a complex repeated game involving multiple advertisers. In -LSB- 3 -RSB-, it was shown that when a set of advertisers use a strategy similar to the one we suggest here, under a slightly modified first-price auction, the prices approach a well-understood market equilibrium.", "keyphrases": ["budget optim", "search-base advertis auction", "internet", "advertis", "game theori", "intrigu heurist", "keyword", "uniform bid strategi", "vickrei clark grove", "lp", "gener second price"]}
{"file_name": "I-18", "text": "Collaboration Among a Satellite Swarm ABSTRACT The paper deals with on-board planning for a satellite swarm via communication and negotiation. We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements. We will present the formalization of the problem, a communication protocol, a solving method based on reactive decision rules, and first results. 1. INTRODUCTION Multi-agent architectures have been developed for satellite swarms -LSB- 36, 38, 42 -RSB- but strong assumptions on deliberation and communication capabilities are made in order to build a collective plan. In a multi-agent context, agents that build a collective plan must be able to change their goals, reallocate resources and react to environment changes and to the others ' choices. However, this step needs high communication and computation capabilities. In order to relax communication constraints, coordination based on norms and conventions -LSB- 16 -RSB- or strategies -LSB- 17 -RSB- are considered. Norms constraint agents in their decisions in such a way that the possibilities of conflicts are reduced. Strategies are private decision rules that allow an agent to draw benefit from the knowledgeable world without communication. However, communication is still needed in order to share information and build collective conjectures and plans. Communication can be achieved through a stigmergic approach -LRB- via the environment -RRB- or through message exchange and a protocol. A protocol defines interactions between agents and can not be uncoupled from its goal, e.g. exchanging information, finding a trade-off, allocating tasks and so on. Protocols can be viewed as an abstraction of an interaction -LSB- 9 -RSB-. However, an agent can not always communicate with another agent or the communication possibilites are restricted to short time intervals. At the individual level, agents are deliberative in order to create a local plan but at the collective level, they use normative decision rules in order to coordinate with one another. We will present the features of our problem, a communication protocol, a method for request allocation and finally, collaboration strategies. 7. EXPERIMENTS Satellite swarm simulations have been implemented in JAVA with the JADE platform -LSB- 3 -RSB-. The on-board planner is implemented with linear programming using ILOG CPLEX -LSB- 1 -RSB-. The simulation scenario implements 3 satellites on 6hour orbits. Two scenarios have been considered : the first one with a set of 40 requests with low mutual exclusion and conflict rate and the second one with a set of 74 requests with high mutual exclusion and conflict rate. In the case of low mutual exclusion and conflict rate -LRB- Table 1 -RRB-, centralized and isolated simulations lead to the same number of observations, with the same average priorities. Isolation leading to a lower cost is due to the high number of redundancies : many agents carry out the same request at different costs. The informed simulation reduces the number of redundancies but sligthly increases the average cost for the same reason. We can notice that the use of 5For instance, the rank-1-expert agent withdraws due to the altruist strategy and the cost increases by a in the worst case, then rank-2-expert agent withdraws due to the altruist strategy and the cost increases by e in the worst case. So the cost has increased by 2e in the worst case. 292 The Sixth Intl.. Joint Conf. Table 1 : Scenario 1 - the 40-request simulation results Table 2 : Scenario 2 - the 74-request simulation results collaboration strategies allows the number of redundancies to be much more reduced but the number of observations decreases owing to the constraint created by commitments. Furthermore, the average cost is increased too. Nevertheless each avoided redundancy corresponds to saved resources to realize on-board generated requests during the simulation. In the case of high mutual exclusion and conflict rate -LRB- Table 2 -RRB-, noteworthy differences exist between the centralized and isolated simulations. We can notice that all informed simulations -LRB- with or without strategies -RRB- allow to perform more observations than isolated agents do with less redundancies. Likewise, we can notice that all politics reduce the average cost contrary to the first scenario. The drastic politics is interesting because not only does it allow to perform more observations than isolated agents do but it allows to highly reduce the average cost with the lowest number of redundancies. As far as the number of exchanged messages is concerned, there are 12 meetings between 2 agents during the simulations. In the worst case, at each meeting each agent sends N pieces of information on the requests plus 3N pieces of information on the agents ' intentions plus 1 message for the end of communication, where N is the total number of requests. Consequently, 3864 messages are exchanged in the worst case for the 40-request simulations and 7128 messages for the 74-request simulations. These numbers are much higher than the number of messages that are actually exchanged. We can notice that the informed simulations, that communicate only requests, allow a higher reduction. In the general case, using communication and strategies allows to reduce redundancies and saves resources but increases the average cost : if a request is realized, agents that know it do not plan it even if its cost can be reduce afterwards. It is not the case with isolated agents. Using strategies on little constrained problems such as scenario 1 constrains the agents too much and causes an additional cost increase. Strategies are more useful on highly constrained problems such as scenario 2. Although agents constrain themselves on the number of observations, the average cost is widely reduce. 8. CONCLUSION AND FUTURE WORK An observation satellite swarm is a cooperative multiagent system with strong constraints in terms of communication and computation capabilities. In order to increase the global mission outcome, we propose an hybrid approach : deliberative for individual planning and reactive for collaboration. Agents reason both on requests to carry out and on the other agents ' intentions -LRB- candidacies -RRB-. An epidemic communication protocol uses all communication opportunities to update this information. Reactive decision rules -LRB- strategies -RRB- are proposed to solve conflicts that may arise between agents. Through the tuning of the strategies -LRB- \u03b1, e and \u03bb -RRB- and their plastic interlacing within the protocol, it is possible to coordinate agents without additional communication : the number of exchanged messages remains nearly the same between informed simulations and simulations implementing strategies. Some simulations have been made to experimentally validate these protocols and the first results are promising but raise many questions. What is the trade-off between the constraint rate of the problem and the need of strategies? To what extent are the number of redundancies and the average cost affected by the tuning of the strategies? Future works will focus on new strategies to solve new conflicts, specially those arising when relaxing the independence assumption between the requests. A second point is to take into account the complexity of the initial planning problem. Indeed, the chosen planning approach results in a combinatory explosion with big sets of requests : an anytime or a fully reactive approach has to be considered for more complex problems.", "keyphrases": ["on-board plan", "satellit swarm", "commun and negoti", "reactiv decis rule", "inform system applic", "multiag system", "task and resourc alloc", "objectag architectur", "teamag", "dip", "prospect ant"]}
{"file_name": "H-16", "text": "The Impact of Caching on Search Engines ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. 1. INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial. Caching can be applied at different levels with increasing response latencies or processing requirements. The decision of what to cache is either off-line -LRB- static -RRB- or online -LRB- dynamic -RRB-. A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss. Such online decisions are based on a cache policy, and several different policies have been studied in the past. For a search engine, there are two possible ways to use a cache memory : Caching answers : As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries. Caching terms : As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing. Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers. Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges : when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient. Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time. Figure 1 : One caching level in a distributed search architecture. In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms. More concretely, our main conclusions are that : \u2022 Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists ; \u2022 Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the KNAPSACK problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90 % ; \u2022 Changes of the query distribution over time have little impact on static caching. Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2. RELATED WORK There is a large body of work devoted to query optimization. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists -LSB- 1, 4, 15 -RSB-. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. Markatos -LSB- 10 -RSB- shows the existence of temporal locality in queries, and compares the performance of different caching policies. Fagni et al. follow Markatos ' work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio -LSB- 7 -RSB-. Different from our work, they consider caching and prefetching of pages of results. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system -LSB- 13 -RSB-. Their goal for such systems has been to improve response time for hierarchical engines. In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput. Long and Suel propose a caching system structured according to three different levels -LSB- 9 -RSB-. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy. Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching -LSB- 17 -RSB-, Web caching -LSB- 5 -RSB-, and even caching of posting lists -LSB- 9 -RSB-, but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists. 8. CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching. Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50 % using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12 %. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10 % higher compared these strategies. We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. Figure 14 : Impact of distribution changes on the static caching of posting lists.", "keyphrases": ["effici cach system", "web search engin", "static cach", "dynam cach", "cach queri result", "cach post list", "static cach", "answer and post list", "queri log", "effect of static cach", "distribut of the queri", "data-access hierarchi", "disk layer", "remot server layer"]}
{"file_name": "J-32", "text": "Nash Equilibria in Graphical Games on Trees Revisited * Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents. When the number of players is large, and the underlying graph has low degree, they provide a concise way to represent the players ' payoffs. It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD, indicating that it is unlikely that there is any polynomial-time algorithm for this problem. In this paper, we study the complexity of graphical games with two actions per player on bounded-degree trees. This setting was first considered by Kearns, Littman and Singh, who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games. The running time of their algorithm is exponential, though approximate equilibria can be computed efficiently. Later, Littman, Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time. We show that this modified algorithm is incorrect -- the output is not always a Nash equilibrium. We then propose a new algorithm that is based on the ideas of Kearns et al. and computes all Nash equilibria in quadratic time if the input graph is a path, and in polynomial time if it is an arbitrary graph of maximum degree 2. Moreover, our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees, but the running time can be exponential, even when the tree has bounded degree. We show that this is inevitable -- any algorithm of this type will take exponential time, even on bounded-degree trees with pathwidth 2. It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1, but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete -LRB- so is unlikely to be tractable -RRB-. * This research is supported by the EPSRC research grants `` Algorithmics of Network-sharing Games '' and `` Discontinuous Behaviour in the Complexity of randomized Algorithms ''. 1. INTRODUCTION Graphical games were introduced in the papers of Kearns et al. -LSB- 8 -RSB- and Littman et al. -LSB- 9 -RSB- as a succinct representation of games with a large number of players. The classical normal form -LRB- or matrix form -RRB- representation has a size that is exponential in the number of players, making it unsuitable for large-scale distributed games. A graphical game associates each player with a vertex of an underlying graph G, and the payoff to that player is a function of the actions chosen by himself and his neighbours in G ; if G has low degree, this is a concise way to represent a game with many players. The papers -LSB- 8, 9 -RSB- give a dynamic-programming algorithm for finding Nash equilibria in graphical games where there are two actions per player and G is a tree. The first of these papers describes a generic algorithm for this problem that can be specialized in two ways : as an algorithm that computes approximations to all Nash equilibria in time polynomial in the input size and the approximation quality, or as an exponential-time algorithm that allows the exact computation of all Nash equilibria in G. In -LSB- 9 -RSB-, the authors propose a modification to the latter algorithm that aims to find a single Nash equilibrium in polynomial time. This does not quite work, as we show in Section 3, though it introduces a useful idea. 1.1 Background The generic algorithm of -LSB- 8 -RSB- consists of two phases which we will refer to as the upstream pass and the downstream pass ; 1 the former starts at the leaves of the tree and ends at the root, while the latter starts at the root and ends at the leaves. there is a Nash equilibrium in the graphical game downstream of V -LRB- inclusive -RRB- given that W plays w -LRB- for a more technical definition, the reader is referred to Section 2 -RRB-. The generic algorithm does not address the problem of representing the best response policy ; in fact, the most important difference between the two instantiations of the generic algorithm described in -LSB- 8 -RSB- is in their approach to this issue. The computation is performed inductively : the best response policy for V is computed based on the best response policies of V 's children U1,..., Uk. By the end of the upstream pass, all children of the root have computed their best response policies. In the beginning of the downstream pass, the root selects its strategy and informs its children about its choice. It also selects a strategy for each child. A necessary and sufficient condition for the algorithm to proceed is that the strategy of the root is a best response to the strategies of its children and, for each child, the chosen strategy is one of the pre-computed potential best responses to the chosen strategy of the root. The equilibrium then propagates downstream, with each vertex selecting its children 's actions. The action of the child is chosen to be any strategy from the pre-computed potential best responses to the chosen strategy of the parent. To bound the running time of this algorithm, the paper -LSB- 8 -RSB- shows that any best response policy can be represented as a union of an exponential number of rectangles ; the polynomial time approximation algorithm is obtained by combining this representation with a polynomial-sized grid. 1.2 Our Results One of the main contributions of our paper is to show that the algorithm proposed by -LSB- 9 -RSB- is incorrect. In Section 3 we describe a simple example for which the algorithm of -LSB- 9 -RSB- outputs a vector of strategies that does not constitute a Nash equilibrium of the underlying game. In Sections 4, 5 and 6 we show how to fix the algorithm of -LSB- 9 -RSB- so that it always produces correct output. Section 4 considers the case in which the underlying graph is a path of length n. For this case, we show that the number of rectangles in each of the best response policies is O -LRB- n2 -RRB-. This gives us an O -LRB- n3 -RRB- algorithm for finding a Nash equilibrium, and for computing a representation of all Nash equilibria. -LRB- This algorithm is a special case of the generic algorithm of -LSB- 8 -RSB- -- we show that it runs in polynomial time when the underlying graph is a path. -RRB- We can improve the running time of the generic algorithm using the ideas of -LSB- 9 -RSB-. In particular, we give an O -LRB- n2 -RRB- algorithm for finding a Nash equilibrium of a graphical game on a path of length n. Instead of storing best response policies, this algorithm stores appropriately-defined subsets, which, following -LSB- 9 -RSB-, we call breakpoint policies -LRB- modifying the definition as necessary -RRB-. We obtain the following theorem THEOREM 1. There is an O -LRB- n2 -RRB- algorithm that finds a Nash equilibrium of a graphical game with two actions per player on an n-vertex path. There is an O -LRB- n3 -RRB- algorithm that computes a representation of all Nash equilibria of such a game. In Section 5 we extend the results of Section 4 to general degree2 graphs, obtaining the following theorem. THEOREM 2. There is a polynomial-time algorithm thatfinds a Nash equilibrium of a graphical game with two actions per player on a graph with maximum degree 2. In Section 6 we extend our algorithm so that it can be used to find a Nash equilibrium of a graphical game on an arbitrary tree. Even when the tree has bounded degree, the running time can be exponential. We show that this is inevitable by constructing a family of graphical games on bounded-degree trees for which best response policies of some of the vertices have exponential size, and any twopass algorithm -LRB- i.e., an algorithm that is similar in spirit to that of -LSB- 8 -RSB- -RRB- has to store almost all points of the best response policies. In particular, we show the following. THEOREM 3. There is an infinite family ofgraphical games on bounded-degree trees with pathwidth 2 such that any two-pass algorithm for finding Nash equilibria on these trees requires exponential time and space. It is interesting to note that the trees used in the proof of Theorem 3 have pathwidth 2, that is, they are very close to being paths. It is an open question whether our algorithm runs in polynomial time for graphs of pathwidth 1. This question can be viewed as a generalization of a very natural computational geometry problem -- we describe it in more detail in Section 8. In Section 7, we give a complexity-theoretic intractability result for the problem of finding a Nash equilibrium of a graphical game on a graph with small pathwidth. We prove the following theorem. THEOREM 4. Consider the problem offinding a Nash equilibrium for a graphical game in which the underlying graph has maximum degree 3 and pathwidth k. There is a constant k such that this problem is PPAD-complete. Theorem 4 limits the extent to which we can exploit `` path-like '' properties of the underlying graph, in order to find Nash equilibria. To prove Theorem 4, we use recent PPAD-completeness results for games, in particular the papers -LSB- 7, 4 -RSB- which show that the problem of finding Nash equilibria in graphical games of degree d -LRB- for d > 3 -RRB- is computationally equivalent to the problem of solving r-player normal-form games -LRB- for r > 4 -RRB-, both of which are PPAD-complete. 8. OPEN PROBLEMS The most important problem left open by this paper is whether it is possible to find a Nash equilibrium of a graphical game on a bounded-degree tree in polynomial time. Our construction shows that any two-pass algorithm that explicitly stores breakpoint policies needs exponential time and space. However, it does not preclude the existence of an algorithm that is based on a similar idea, but, instead of computing the entire breakpoint policy for each vertex, uses a small number of additional passes through the graph to decide which -LRB- polynomial-sized -RRB- parts of each breakpoint policy should be computed. In particular, such an algorithm may be based on the approximation algorithm of -LSB- 8 -RSB-, where the value of e is chosen adaptively. Another intriguing question is related to the fact that the graph for which we constructed an exponential-sized breakpoint policy has pathwidth 2, while our positive results are for a path, i.e., a graph of pathwidth 1. It is not clear if for any bounded-degree graph of pathwidth 1 the running time of -LRB- the breakpoint policybased version of -RRB- our algorithm will be polynomial. In particular, it is instructive to consider a `` caterpillar '' graph, i.e., the graph that can be obtained from Tn by deleting the vertices S1,..., Sn. This implies that the problem of bounding the size of the best response policy -LRB- or, alternatively, the breakpoint policy -RRB-, can be viewed as a generalization of the following computational geometry problem, which we believe may be of independent interest : PROBLEM 1. Ifyes, can it be the case that in this set, there is no path with a polynomial number of turns that connects the endpoints of the original segment? This implies that even for a caterpillar, the best response policy can be exponentially large. However, in our example -LRB- which is omitted from this version of the paper due to space constraints -RRB-, there exists a polynomial-size path through the best response policy, i.e., it does not prove that the breakpoint policy is necessarily exponential in size. If one can prove that this is always the case, it may be possible to adapt this proof to show that there can be an exponential gap between the sizes of best response policies and breakpoint policies.", "keyphrases": ["graphic game", "larg-scale distribut network", "nash equilibrium", "degre", "dynam program-base algorithm", "ppad-complet", "bound-degre tree", "gener algorithm", "respons polici", "downstream pass", "breakpoint polici"]}
{"file_name": "I-16", "text": "An Advanced Bidding Agent for Advertisement Selection on Public Displays ABSTRACT In this paper we present an advanced bidding agent that participates in first-price sealed bid auctions to allocate advertising space on BluScreen -- an experimental public advertisement system that detects users through the presence of their Bluetooth enabled devices. Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts, and the auctions that it participates within. It then uses these models to maximise the exposure that its adverts receive. We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy, random allocation, and a centralised optimal allocation with perfect foresight. Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation, and in a mixed population of agents it is able to expose its adverts to 25 % more users than the simple bidding strategy. Moreover, its performance is within 7.5 % of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate. 1. INTRODUCTION Electronic displays are increasingly being used within public environments, such as airports, city centres and retail stores, in order to advertise commercial products, or to entertain and inform passersby. of interactive public displays have been proposed. As such, these systems assume prior knowledge about the target audience, and require either that a single user has exclusive access to the display, or that users carry specific tracking devices so that their presence can be identified -LSB- 6, 11 -RSB-. However, these approaches fail to work in public spaces, where no prior knowledge regarding the users who may view the display exists, and where such displays need to react to the presence of several users simultaneously. By contrast, Payne et al. have developed an intelligent public display system, named BluScreen, that detects and tracks users through the Bluetooth enabled devices that they carry with them everyday -LSB- 8 -RSB-. Within this system, a decentralised multi-agent auction mechanism is used to efficiently allocate advertising time on each public display. Each advert is represented by an individual advertising agent that maintains a history of users who have already been exposed to the advert. This agent then seeks to acquire advertising cycles -LRB- during which it can display its advert on the public displays -RRB- by submitting bids to a marketplace agent who implements a sealed bid auction. The value of these bids is based upon the number of users who are currently present in front of the screen, the history of these users, and an externally derived estimate of the value of exposing an advert to a user. In this paper, we present an advanced bidding agent that significantly extends the sophistication of this approach. In particular, we consider the more general setting in which it is impossible to determine an a priori valuation for exposing an advert to a user. In addition, it is also likely to be the case within new commercial installations where limited market experience makes estimating a valuation impossible. The advertising agent is then simply tasked with using this budget to maximum effect -LRB- i.e. to achieve the maximum possible advert exposure within this time period -RRB-. Now, in order to achieve this goal, the advertising agent must be capable of modelling the behaviour of the users in order to predict the number who will be present in any future advertising cycle. In addition, it must also understand the auction environment in which it competes, in order that it may make best use of its limited budget. Thus, in developing an advanced bidding agent that achieves this, we advance the state of the art in four key ways : 1. We enable the advertising agents to model the arrival and departure of users as independent Poisson processes, and to make maximum likelihood estimates of the rates of these processes based on their observations. We show how these agents can then calculate the expected number of users who will be present during any future advertising cycle. 2. Using a decision theoretic approach we enable the advertising agents to model the probability of winning any given auction when a specific amount is bid. The cumulative form of the gamma distribution is used to represent this probability, and its parameters are fitted using observations of both the closing price of previous auctions, and the bids that that advertising agent itself submits. 3. We show that our explicit assumption that the advertising agent derives no additional benefit by showing an advert to a single user more than once, causes the expected utility of each future advertising cycle to be dependent on the expected outcome of all the auctions that precede it. We thus present a stochastic optimisation algorithm based upon simulated annealing that enables the advertising agent to calculate the optimal sequence of bids that maximises its expected utility. 4. The remainder of this paper is organised as follows : Section 2 discusses related work where agents and auction-based marketplaces are used to allocated advertising space. Section 3 describes the prototype BluScreen system that motivates our work. In section 4 we present a detailed description of the auction allocation mechanism, and in section 5 we describe our advanced bidding strategy for the advertising agents. In section 6 we present an empirical validation of our approach, and finally, we conclude in section 7. 2. RELATED WORK The commercial attractiveness of targeted advertising has been amply demonstrated on the internet, where recommendation systems and contextual banner adverts are the norm -LSB- 1 -RSB-. Attempts to apply these approaches within the real world have been much more limited. Gerding et al. present a simulated system -LRB- CASy -RRB- whereby a Vickrey auction mechanism is used to sell advertising space within a modelled electronic shopping mall -LSB- 2 -RSB-. The auction is used to rank a set of possible advertisements provided by different retail outlets, and the top ranking advertisements are selected for presentation on public displays. Feedback is provided through subsequent sales information, allowing the model to build up a profile of a user 's preferences. However, unlike the BluScreen Figure 1 : A deployed BluScreen prototype. system that we consider here, it is not suitable for advertising to many individuals simultaneously, as it requires explicit interaction with a single user to acquire the user 's preferences. User identification is based on infrared badges and embedded sensors within an office environment. When several users pass by the display, a centralised system compares the user 's profiles to identify common areas of interest, and content that matches this common interest is shown. Thus, whilst CASy is a simulated system that allows advertisers to compete for the attention of single user, GroupCast is a prototype system that detects the presence of groups of users and selects content to match their profiles. Despite their similarities, neither system addresses the settings that interests us here : how to allocate advertising space between competing advertisers who face an audience of multiple individuals about whom there is no a priori profile information. Thus, in the next section we describe the prototype BluScreen system that motivates our work. 7. CONCLUSIONS In this paper, we presented an advanced bidding strategy for use by advertising agents within the BluScreen advertising system. This bidding strategy enabled advertising agents to model and predict the arrival and departure of users, and also to model their success within a first-price sealed bid auction by observing both the bids that they themselves submitted and the winning bid. The ex The Sixth Intl.. Joint Conf. Figure 8 : Comparison of an evenly mixed population of advertising agents using simple and advanced bidding strategies over a range of parameter settings. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. Figure 9 : Comparison of an unevenly mixed population of advertising agents using simple and advanced bidding strategies. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. pected utility, measured as the number of users who the advertising agent exposes its advert to, was shown to depend on these factors, and resulted in a complex expression where the expected utility of each auction depended on the success or otherwise of earlier auctions. We presented an algorithm based upon simulated annealing to solve for the optimal bidding strategy, and in simulation, this bidding strategy was shown to significantly outperform a simple bidding strategy that had none of these features. Its performance closely approached that of a central optimal allocation, with perfect knowledge of the arrival and departure of users, despite the uncertain environment in which the strategy must operate. This work will continue to be done in conjunction with the deployment of more BluScreen prototypes in order to gain further real world experience.", "keyphrases": ["advanc bid agent", "bluscreen", "experiment public advertis system", "bluetooth", "probabilist model", "centralis optim alloc", "distribut artifici intellig", "decentralis multi-agent auction mechan", "independ poisson process", "decis theoret approach", "stochast optimis algorithm"]}
{"file_name": "J-18", "text": "Mediators in Position Auctions ABSTRACT A mediator is a reliable entity, which can play on behalf of agents in a given game. A mediator however can not enforce the use of its services, and each agent is free to participate in the game directly. In this paper we introduce a study of mediators for games with incomplete information, and apply it to the context of position auctions, a central topic in electronic commerce. VCG position auctions, which are currently not used in practice, possess some nice theoretical properties, such as the optimization of social surplus and having dominant strategies. These properties may not be satisfied by current position auctions and their variants. We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions. We require that accepting the mediator services, and reporting honestly to the mediator, will form an ex post equilibrium, which satisfies the following rationality condition : an agent 's payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator 's services, or by the agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price -LRB- Google-like -RRB- position auctions, as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k = 1, the self-price position auction, we show that the existence of such mediator depends on the tie breaking rule used in the auction. 1. INTRODUCTION Consider an interaction in a multi-agent system, in which every player holds some private information, which is called the player 's type. For example, in an auction interaction the type of a player is its valuation, or, in more complex auctions, its valuation function. This interaction is modeled as a game with incomplete information. This game is called a Bayesian game, when a commonly known probability measure on the profiles of types is added to the system. Otherwise it is called a pre-Bayesian game. In this paper we deal only with pre-Bayesian games. Consider the following simple example of a pre-Bayesian game, which possesses an ex post equilibrium. The game is denoted by H.", "keyphrases": ["auction", "mediat", "ex post equilibrium", "agent", "posit auction", "electron commerc", "richer class of posit auction", "next-price posit auction", "multi-agent system", "t-strategi", "vcg outcom function", "self-price posit auction"]}
{"file_name": "H-29", "text": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension : the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given query 's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness -LRB- worst-case performance -RRB- by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method. 1. INTRODUCTION Uncertainty is an inherent feature of information retrieval. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback -LRB- PRF -RRB- tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio 's formula -LSB- 16 -RSB-, or more recent language modeling approaches such as Relevance Models -LSB- 10 -RSB-. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model '. The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model 's weight combines two complementary factors : the model 's probability of generating the query, and the variance of the model, with high-variance models getting lower weight. ` For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings -LRB- see -LSB- 10 -RSB-, p. 62 -RRB-. Figure 1 : Estimating the uncertainty of the feedback model for a single query. 4. RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision. The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. On the document side, recent work by Zhou & Croft -LSB- 21 -RSB- explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. -LSB- 17 -RSB- proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. Greiff, Morgan and Ponte -LSB- 8 -RSB- explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance -- high noise -- increases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models. Estimates of output variance have recently been used for improved text classification. Lee et al. -LSB- 11 -RSB- used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Ando and Zhang proposed a method that they call structural feedback -LSB- 3 -RSB- and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector \u02c6wi of the documents is calculated. Principal component analysis -LRB- PCA -RRB- is then applied to the \u02c6wi to obtain the matrix 4 -RRB- of H left singular vectors \u03c6h that are used to obtain the new, expanded query The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai -LSB- 18 -RSB- describe a method for more robust feedback that allows each document to have a different feedback \u03b1. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter \u03b7 based on a function of some quality measure of feedback documents. 5. CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.", "keyphrases": ["feedback method", "posterior distribut", "enhanc feedback model", "inform retriev", "queri expans", "probabl distribut", "pseudo-relev feedback", "vector space-base algorithm", "risk", "feedback model", "estim uncertainti", "languag model", "feedback distribut"]}
{"file_name": "H-20", "text": "New Event Detection Based on Indexing-tree and Named Entity ABSTRACT New Event Detection -LRB- NED -RRB- aims at detecting from one or multiple streams of news stories that which one is reported on a new event -LRB- i.e. not reported previously -RRB-. With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium -LRB- LDC -RRB- datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems. 1. INTRODUCTION New Event Detection -LRB- NED -RRB- is one of the five tasks in TDT. A Topic is defined as `` a seminal event or activity, along with directly related events and activities '' -LSB- 2 -RSB-. An Event is defined as `` something -LRB- non-trivial -RRB- happening in a certain place at a certain time '' -LSB- 3 -RSB-. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering. In most of state-of-the-art -LRB- currently -RRB- NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. The core problem of NED is to identify whether two stories are on the same topic. Obviously, these systems can not take advantage of topic information. Other systems organize previous stories into clusters -LRB- each cluster corresponds to a topic -RRB-, and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic. On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities -LSB- 10, 11, 12, 13 -RSB-. However, none of the systems have considered that terms of different types -LRB- e.g. Noun, Verb or Person name -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. For example, the names of election candidates -LRB- Person name -RRB- are very important for stories of election class ; the locations -LRB- Location name -RRB- where accidents happened are important for stories of accidents class. -LRB- 2 -RRB- How to make good use of cluster -LRB- topic -RRB- information to improve accuracy? -LRB- 3 -RRB- How to obtain better news story representation by better understanding of named entities. Driven by these problems, we have proposed three approaches in this paper. -LRB- 1 -RRB- To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity. Comparisons between current story and previous clusters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. -LRB- 2 -RRB- We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster -LRB- topic -RRB- information is used properly, so the problem of theme decentralization is avoided. -LRB- 3 -RRB- Based on observations on the statistics obtained from training data, we found that terms of different types -LRB- e.g. Noun and Verb -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2. Section 3 presents the basic model for NED that most current systems use. Section 4 describes our new detection procedure based on news indexing-tree. In section 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evaluation metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2. RELATED WORK Papka et al. proposed Single-Pass clustering on NED -LSB- 6 -RSB-. When a new story was encountered, it was processed immediately to extract term features and a query representation of the story 's content is built up. Then it was compared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic -LSB- 7 -RSB-. In this manner comparisons happen between stories and clusters. Recent years, most work focus on proposing better methods on comparison of stories and document representation. Good improvements on TDT bench-marks were shown. Stokes et al. -LSB- 9 -RSB- utilized a combination of evidence from two distinct representations of a document 's content. One of the representations was the usual free text vector, the other made use of lexical chains -LRB- created using WordNet -RRB- to build another term vector. Then the two representations are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used. Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities -LSB- 10 -RSB-. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity -LSB- 11 -RSB- -LSB- 12 -RSB-. UMass -LSB- 13 -RSB- research group split document representation into two parts : named entities and non-named entities. And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both -LSB- 10 -RSB- and -LSB- 13 -RSB- used text categorization technique to classify news stories in advance. In -LSB- 13 -RSB- news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In -LSB- 10 -RSB- frequent terms for each class are removed from document representation. In their work, effectiveness of different kinds of names -LRB- or terms with different POS -RRB- for NED in different news classes are not investigated. 8. CONCLUSION We have proposed a news indexing-tree based detection procedure in our model. It reduces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensions to the basic TF-IDF model. The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set. And the second extension to basic TF-IDF model is better use of term types -LRB- named entities types and part-of-speed -RRB- according to news categories. Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy. For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.", "keyphrases": ["new event detect", "stream of new stori", "volum of new", "new index-tree", "term reweight approach", "ned accuraci", "term weight", "statist", "train data", "name entiti reweight mode", "class of stori", "linguist data consortium", "baselin system", "exist system"]}
{"file_name": "H-7", "text": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user 's interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens. 1. INTRODUCTION For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a user 's history. One major personalization topic studied in the information retrieval community is content-based personal recommendation systems '. These systems learn user-specific pro'Content - based recommendation is also called adaptive fil files from user feedback so that they can recommend information tailored to each individual user 's interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems. A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the `` cold start '' problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile. There has been much research on improving classification accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal -LSB- 26 -RSB-. Another approach is using domain knowledge. The third approach is borrowing training data from other resources -LSB- 5 -RSB- -LSB- 7 -RSB-. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data. One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user -LSB- 27 -RSB- -LSB- 25 -RSB-. In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data. A mature recommendation system usually works for millions of users. It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario -LRB- Section 4 -RRB-, we find that the EM tering, or item-based collaborative filtering. In this paper, the words `` filtering '' and `` recommendation '' are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O -LRB- MK -RRB-, where M is the number of users and K is the number of dimensions. This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the `` Modified EM algorithm. '' This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results. The organization of the remaining parts of this paper is as follows : Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6. 2. RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970 's. The approaches that have been used to solve this problem can be roughly classified into two major categories : content based filtering versus collaborative filtering. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user. Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past. Memorybased heuristics and model based approaches have been used in collaborative filtering task -LSB- 15 -RSB- -LSB- 8 -RSB- -LSB- 2 -RSB- -LSB- 14 -RSB- -LSB- 12 -RSB- -LSB- 11 -RSB-. This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks -LSB- 27 -RSB- -LSB- 25 -RSB-. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback. Similar to some other researchers -LSB- 18 -RSB- -LSB- 1 -RSB- -LSB- 21 -RSB-, we found that a recommendation system will be more effective when both techniques are combined. 7. CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors. This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a scenario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users. The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model parameters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems.", "keyphrases": ["model", "content-base", "recommend system", "linear regress", "collabor filter", "paramet", "learn techniqu", "ir", "em algorithm", "classif", "rate"]}
{"file_name": "I-32", "text": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions ABSTRACT Multiagent environments are often not cooperative nor collaborative ; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties -LRB- e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model -RRB-. We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms ' appropriateness. 1. INTRODUCTION Early research in multiagent systems -LRB- MAS -RRB- considered cooperative groups of agents ; because individual agents had MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries. Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states -LRB- e.g., -LSB- 8, 4, 5 -RSB- -RRB-. However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments. In addition, traditional search methods -LRB- like Min-Max -RRB- do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning -LSB- 9, 3, 11 -RSB-. In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans -LSB- 4 -RSB- model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms ; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings. We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players ' behavior using a large set of completed match log 4. RELATED WORK However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents ' behavior in it. The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. Additional Adversarial planning work was done by Willmott et al. -LSB- 13 -RSB-, which provided an adversarial planning approach to the game of GO. The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agent 's advantage. However, the basic limitations of those search methods still apply ; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5. CONCLUSIONS We presented an Adversarial Environment model for a 2These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines. The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment. The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research.", "keyphrases": ["multiag environ", "adversari interact", "adversari environ", "behavior axiom", "bilater and multilater instanti", "evalu function", "benefici action", "connect-four game", "empir studi", "axiomat model", "zero-sum encount", "treatment group", "eval valu", "interact"]}
{"file_name": "I-29", "text": "Distributed Management of Flexible Times Schedules ABSTRACT We consider the problem of managing schedules in an uncertain, distributed environment. We assume a team of collaborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unexpected events will force changes to some prescribed activities and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms : -LRB- 1 -RRB- a `` flexible times '' representation of the agent 's schedule -LRB- using a Simple Temporal Network -RRB- and -LRB- 2 -RRB- an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent 's schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportunities for solution improvement through joint change. Using a simulator to model the environment, we compare the performance of our multi-agent system with that of an expected optimal -LRB- but non-scalable -RRB- centralized MDP solver. 1. INTRODUCTION The practical constraints of many application environments require distributed management of executing plans and schedules. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency -LRB- fall-back -RRB- options. Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. In a flexible-times representation of an agent 's schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. However their use in distributed problem solving settings has been quite sparse -LRB- -LSB- 7 -RSB- is one exception -RRB-, and prior approaches to multi-agent scheduling -LRB- e.g., -LSB- 6, 13, 5 -RSB- -RRB- have generally operated with fixed-times representations of agent schedules. We define an agent architecture centered around incremental management of a flexible times schedule. Local change is ac Figure 1 : A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process -LRB- similar in some respects to -LSB- 5 -RSB- -RRB-, aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans. 8. STATUS AND DIRECTIONS Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods. This scale places much higher computational demands on all of the agent 's components. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems. To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms. To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling -LSB- 7 -RSB- to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules.", "keyphrases": ["manag schedul", "distribut environ", "agent architectur", "schedul", "inter-depend activ", "geograph separ", "flexibl time", "central plan", "manag", "schedul-execut", "slack", "shortest path algorithm", "activ alloc", "conflict-driven approach", "optimist synchron", "inter-agent coordin", "perform"]}
{"file_name": "C-30", "text": "Bullet : High Bandwidth Data Dissemination Using an Overlay Mesh ABSTRACT In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include : i -RRB- an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii -RRB- a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii -RRB- a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node 's parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate. 1. INTRODUCTION In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable -LRB- and hence extensible -RRB- end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree -LSB- 2, 6 -RSB-. However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node 's single parent in the tree. Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically transmit disjoint data sets to various points in the network. Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the `` last object '', which may only be available at a few nodes. To illustrate Bullet 's behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available -LRB- TCP-friendly -RRB- bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data. Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly -LSB- 15 -RSB-. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to `` peer '' with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants ; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure `` high up '' in a multicast overlay tree. In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques -LSB- 1, 18, 21, 24, 34 -RSB-. Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Finally, we use TFRC -LSB- 15 -RSB- to transfer data both down the overlay tree and among peers. One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source -LRB- its parent -RRB-. Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research -LSB- 20, 35 -RSB-, accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system. Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. We have completed a prototype of Bullet running on top of a number of overlay trees. Our evaluation of a 1000-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree -LRB- using an offline algorithm and global network topology information -RRB-, all while remaining TCP friendly. For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 2 presents Bullet 's system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet 's performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions. 5. RELATED WORK Snoeren et al. -LSB- 36 -RSB- use an overlay mesh to achieve reliable and timely delivery of mission-critical data. In this system, every node chooses n `` parents '' from which to receive duplicate packet streams. Since its foremost emphasis is reliability, the system does not attempt to improve the bandwidth delivered to the overlay participants by sending disjoint data at each level. Further, during recovery from parent failure, it limits an overlay router 's choice of parents to nodes with a level number that is less than its own level number. Kazaa nodes are organized into a scalable, hierarchical structure. Individual users search for desired content in the structure and proceed to simultaneously download potentially disjoint pieces from nodes that already have it. Since Kazaa does not address the multicast communication model, a large fraction of users downloading the same file would consume more bandwidth than nodes organized into the Bullet overlay structure. BitTorrent -LSB- 3 -RSB- is another example of a file distribution system currently deployed on the Internet. The tracker poses a scalability limit, as it continuously updates the systemwide distribution of the file. Similar to Bullet, BitTorrent incorporates the notion of `` choking '' at each node with the goal of identifying receivers that benefit the most by downloading from that particular source. FastReplica -LSB- 11 -RSB- addresses the problem of reliable and efficient file distribution in content distribution networks -LRB- CDNs -RRB-. In the basic algorithm, nodes are organized into groups of fixed size -LRB- n -RRB-, with full group membership information at each node. To distribute the file, a node splits it into n equal-sized portions, sends the portions to other group members, and instructs them to download the missing pieces in parallel from other group members. Since only a fixed portion of the file is transmitted along each of the overlay links, the impact of congestion is smaller than in the case of tree distribution. However, since it treats all paths equally, FastReplica does not take full advantage of highbandwidth overlay links in the system. There are numerous protocols that aim to add reliability to IP multicast. In Scalable Reliable Multicast -LRB- SRM -RRB- -LSB- 16 -RSB-, nodes multicast retransmission requests for missed packets. Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. In pbcast -LSB- 2 -RSB-, a node has global group membership, and periodically chooses a random subset of peers to send a digest of its received packets. A node that receives the digest responds to the sender with the missing packets in a last-in, first-out fashion. Since lbpcast does not require an underlying tree for data distribution and relies on the push-gossiping model, its network overhead can be quite high. Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not `` blindly '' request retransmissions from their peers. Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Further, a Bullet node splits the retransmission load between all of its peers. We note that pbcast nodes contain a mechanism to rate-limit retransmitted packets and to send different packets in response to the same digest. However, this does not guarantee that packets received in parallel from multiple peers will not be duplicates. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree. Narada -LSB- 19 -RSB- builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. It then runs a standard routing protocol on top of the overlay mesh to construct forwarding trees using each node as a possible source. Narada nodes maintain global knowledge about all group participants, limiting system scalability to several tens of nodes. Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Overcast -LSB- 21 -RSB- is an example of a bandwidth-efficient overlay tree construction algorithm. In this system, all nodes join at the root and migrate down to the point in the tree where they are still able to maintain some minimum level of bandwidth. Bullet is expected to be more resilient to node departures than any tree, including Overcast. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers. Overcast convergence time is limited by probes to immediate siblings and ancestors. Bullet is able to provide approximately a target bandwidth without having a fully converged tree. In parallel to our own work, SplitStream -LSB- 9 -RSB- also has the goal of achieving high bandwidth data dissemination. It operates by splitting the multicast stream into k stripes, transmitting each stripe along a separate multicast tree built using Scribe -LSB- 34 -RSB-. Perhaps more importantly, SplitStream assumes that there is enough available bandwidth to carry each stripe on every link of the tree, including the links between the data source and the roots of individual stripe trees independently chosen by Scribe. To some extent, Bullet and SplitStream are complementary. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet -LSB- 29 -RSB- considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. In the case of on-demand streaming, CoopNet -LSB- 30 -RSB- addresses the flash-crowd problem at the central server by redirecting incoming clients to a fixed number of nodes that have previously retrieved portions of the same content. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file. 6. CONCLUSIONS Typically, high bandwidth overlay data streaming takes place over a distribution tree. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures. Specifically, this paper makes the following contributions : 9 We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques. 9 We provide a technique for recovering missing data from peers in a scalable and efficient manner. RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants. 9 We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes. 9 A large-scale evaluation of 1000 overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree.", "keyphrases": ["overlai mesh", "data dissemin", "overlai network", "ip multicast", "multipoint commun", "high-bandwidth data distribut", "larg-file transfer", "real-time multimedia stream", "bullet", "bandwidth probe", "peer-to-peer", "ransub", "content deliveri", "tfrc"]}
{"file_name": "I-30", "text": "Distributed Task Allocation in Social Networks ABSTRACT This paper proposes a new variant of the task allocation problem, where the agents are connected in a social network and tasks arrive at the agents distributed over the network. We show that the complexity of this problem remains NPhard. Moreover, it is not approximable within some factor. We develop an algorithm based on the contract-net protocol. Our algorithm is completely distributed, and it assumes that agents have only local knowledge about tasks and resources. We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time. Three different types of networks, namely small-world, random and scale-free networks, are used to represent various social relationships among agents in realistic applications. The results demonstrate that our algorithm works well and that it scales well to large-scale applications. 1. INTRODUCTION Recent years have seen a lot of work on task and resource allocation methods, which can potentially be applied to many real-world applications. However, some interesting applications where relations between agents play a role require a slightly more general model. range of task allocation methods. The question is how VOs are to be dynamically composed and re-composed from individual agents, when different tasks and subtasks need to be performed. This would be done by allocating them to different agents who may each be capable of performing different subsets of those tasks. In this paper, we study the problem of task allocation from the perspective of such a complex interrelated structure. Specifically, therefore, we consider agents to be connected to each other in a social network. Other than modeling the interrelated structure between business partners, the social network introduced in this paper can also be used to represent other types of connections or constraints among autonomous entities that arise from other application domains. The next section gives a formal description of the task allocation problem on social networks. In Section 3, we prove that the complexity of this problem remains NP-hard. We then proceed to develop a distributed algorithm in Section 4, and perform a series of experiments with this algorithm, as described in Section 5. Section 6 discusses related work, and Section 7 concludes. 6. RELATED WORK Task allocation in multiagent systems has been investigated by many researchers in recent years with different assumptions and emphases. However, most of the research to date on task allocation does not consider social connections among agents, and studies the problem in a centralized The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 505 Figure 6 : The quality of the GDAP algorithm for a uniform and a skewed task benefit distribution related to the resource ratio -LRB- the first graph -RRB-, and the network degree -LRB- the second graph -RRB-. setting. For example, Kraus et al. -LSB- 12 -RSB- develop an auction protocol that enables agents to form coalitions with time constraints. It assumes each agent knows the capabilities of all others. The proposed protocol is centralized, where one manager is responsible for allocating the tasks to all coalitions. Manisterski at al. -LSB- 14 -RSB- discuss the possibilities of achieving efficient allocations in both cooperative and noncooperative settings. They propose a centralized algorithm to find the optimal solution. In contrast to this work, we introduce also an efficient completely distributed protocol that takes the social network into account. Task allocation has also been studied in distributed settings by for example Shehory and Kraus -LSB- 18 -RSB- and by Lerman and Shehory -LSB- 13 -RSB-. They propose distributed algorithms with low communication complexity for forming coalitions in large-scale multiagent systems. However, they do not assume the existence of any agent network. The work of Sander et al. -LSB- 16 -RSB- introduces computational geometry-based algorithms for distributed task allocation in geographical domains. Agents are then allowed to move and actively search for tasks, and the capability of agents to perform tasks is homogeneous. In order to apply their approach, agents need to have some knowledge about the geographical positions of tasks and some other agents. Other work -LSB- 17 -RSB- proposes a location mechanism for open multiagent systems to allocate tasks to unknown agents. In this approach each agent caches a list of agents they know. The analysis of the communication complexity of this method is based on lattice-like graphs, while we investigate how to efficiently solve task allocation in a social network, whose topology can be arbitrary. Networks have been employed in the context of task allocation in some other works as well, for example to limit the Figure 8 : The quality of the GDAP algorithm compared to the upper bound. interactions between agents and mediators -LSB- 1 -RSB-. Mediators in this context are agents who receive the task and have connections to other agents. They break up the task into subtasks, and negotiate with other agents to obtain commitments to execute these subtasks. Their focus is on modeling the decision process of just a single mediator. Another approach is to partition the network into cliques of nodes, representing coalitions which the agents involved may use as a coordination mechanism -LSB- 20 -RSB-. The focus of that work is distributed coalition formation among agents, but in our approach, we do not need agents to form groups before allocating tasks. Easwaran and Pitt -LSB- 6 -RSB- study ` complex tasks ' that require ` services ' for their accomplishment. The problem concerns the allocation of subtasks to service providers in a supply chain. Another study of task allocation in supply chains is -LSB- 21 -RSB-, where it is argued that the defining characteristic of Supply Chain Formation is hierarchical subtask decomposition -LRB- HSD -RRB-. HSD is implemented using task dependency networks -LRB- TDN -RRB-, with agents and goods as nodes, and I/O relations between them as edges. Here, the network is given, and the problem is to select a subgraph, for which the authors propose a market-based algorithm, in particular, a series of auctions. Compared to these works, our approach is more general in the sense that we are able to model different types of connections or constraints among agents for different problem domains in addition to supply chain formation. Finally, social networks have been used in the context of team formation. Previous work has shown how to learn which relations are more beneficial in the long run -LSB- 8 -RSB-, and adapt the social network accordingly. We believe these results can be transferred to the domain of task allocation as well, leaving this as a topic for further study. Figure 7 : The run time of the GDAP algorithm. 506 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 7. CONCLUSIONS In this paper we studied the task allocation problem in a social network -LRB- STAP -RRB-, which can be seen as a new, more general, variant of the TAP. We believe it has a great amount of potential for realistic problems. We provided complexity results on computing the efficient solution for the STAP, as well as a bound on possible approximation algorithms. Next, we presented a distributed protocol, related to the contractnet protocol. We also introduced an exponential algorithm to compute the optimal solution, as well as a fast upperbound algorithm. The results presented in this paper show that the distributed algorithm performs well in small-world, scale-free, and random networks, and for many different settings. Also other experiments were done -LRB- e.g. on grid networks -RRB- and these results held up over a wider range of scenarios. Furthermore, we showed that it scales well to large networks, both in terms of quality and of required computation time. The results also suggest that small-world networks are slightly better suited for local task allocation, because there are no nodes with very few neighbors. There are many interesting extensions to our current work. In this paper, we focus on the computational aspect in the design of the distributed algorithm. In our future work, we would also like to address some of the related issues in game theory, such as strategic agents, and show desirable properties of a distributed protocol in such a context. In the current algorithm we assume that agents can only contact their neighbors to request resources, which may explain why our algorithm performs not as good in the scalefree networks as in the small-world networks. Our future work may allow agents to reallocate -LRB- sub -RRB- tasks. We are interested in seeing how such interactions will affect the performance of task allocation in different social networks. A third interesting topic for further work is the addition of reputation information among the agents. This may help to model changing business relations and incentivize agents to follow the protocol. Finally, it would be interesting to study real-life instances of the social task allocation problem, and see how they relate to the randomly generated networks of different types studied in this paper. Acknowledgments.", "keyphrases": ["social network", "social relationship", "task alloc", "util", "alloc", "algorithm", "commun messag", "behavior", "multiag system", "strateg agent", "interact"]}
{"file_name": "J-21", "text": "A Strategic Model for Information Markets ABSTRACT Information markets, which are designed specifically to aggregate traders ' information, are becoming increasingly popular as a means for predicting future events. Recent research in information markets has resulted in two new designs, market scoring rules and dynamic parimutuel markets. We develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection game, that serves as a useful model for information markets. We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets, and also captures the essence of the strategies in market scoring rules. The projection game is tractable to analyze, and has an attractive geometric visualization that makes the strategic moves and interactions more transparent. We use it to prove several strategic properties about the dynamic parimutuel market. We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule, and it is strategically similar to other scoring rules. Finally, we illustrate two applications of the model to analysis of complex strategic scenarios : we analyze the precision of a market in which traders have inertia, and a market in which a trader can profit by manipulating another trader 's beliefs. 1. INTRODUCTION Markets have long been used as a medium for trade. As a side effect of trade, the participants in a market reveal something about their preferences and beliefs. For example, in a financial market, agents would buy shares which they think are undervalued, and sell shares which they think are overvalued. It has long been observed that, because the market price is influenced by all the trades taking place, it aggregates the private information of all the traders. Thus, in a situation in which future events are uncertain, and each trader might have a little information, the aggregated information contained in the market prices can be used to predict future events. This has motivated the creation of information markets, which are mechanisms for aggregating the traders ' information about an uncertain event. Information markets can be modeled as a game in which the participants bet on a number of possible outcomes, such as the results of a presidential election, by buying shares of the outcomes and receiving payoffs when the outcome is realized. As in financial markets, the participants aim to maximize their profit by buying low and selling high. The benefit of well-designed information markets goes beyond information aggregation ; they can also be used as a hedging instrument, to allow traders to insure against risk. Recently, researchers have turned to the problem of designing market structures specifically to achieve better information aggregation properties than traditional markets. Two designs for information markets have been proposed : the Dynamic Parimutuel Market -LRB- DPM -RRB- by Pennock -LSB- 10 -RSB- and the Market Scoring Rules -LRB- MSR -RRB- by Hanson -LSB- 6 -RSB-. Both the DPM and the MSR were designed with the goal of giving informed traders an incentive to trade, and to reveal their information as soon as possible, while also controlling the subsidy that the market designer needs to pump into the market. One version of the DPM was implemented in the Yahoo! Buzz market -LSB- 8 -RSB- to experimentally test the market 's prediction properties. The innovation in the MSR is to use these scoring rules as instruments that can be traded, thus providing traders who have new information an incentive to trade. The MSR was to be used in a policy analysis market in the Middle East -LSB- 15 -RSB-, which was subsequently withdrawn. Information markets rely on informed traders trading for their own profit, so it is critical to understand the strategic properties of these markets. This is not an easy task, because markets are complex, and traders can influence each other 's beliefs through their trades, and hence, can potentially achieve long term gains by manipulating the market. For the DPM, we are not aware of any prior strategic analysis of this nature ; in fact, a strategic hole was discovered while testing the DPM in the Yahoo! Buzz market -LSB- 8 -RSB-. 1.1 Our Results In this paper, we seek to develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection 1 game, that serves as a useful model for information markets. The projection game is conceptually simpler than the MSR and DPM, and thus it is easier to analyze. In addition it has an attractive geometric visualization, which makes the strategic moves and interactions more transparent. We present an analysis of the optimal strategies and profits in this game. We then undertake an analysis of traders ' costs and profits in the dynamic parimutuel market. Remarkably, we find that the cost of a sequence of trades in the DPM is identical to the cost of the corresponding moves in the projection game. Further, if we assume that the traders beliefs at the end of trading match the true probability of the event being predicted, the traders ' payoffs and profits in the DPM are identical to their payoffs and profits in a corresponding projection game. We use the equivalence between the DPM and the projection game to prove that the DPM is arbitrage-free, deduce profitable strategies in the DPM, and demonstrate that constraints on the agents ' trades are necessary to prevent a strategic breakdown. We also prove an equivalence between the projection game and the MSR : We show that play in the MSR is strategically equivalent to play in a restricted projection game, at least for myopic strategies and small trades. This allows us to use the projection game as a conceptual model for market scoring rules. Further, because the restricted projection game corresponds to a DPM with a natural trading constraint, this sheds light on an intriguing connection between the MSR and the DPM. Lastly, we illustrate how the projection game model can be used to analyze the potential for manipulation of information markets for long-term gain.2 We present an example scenario in which such manipulation can occur, and suggest additional rules that might mitigate the possibility of manipulation. We also illustrate another application to analyzing how a market maker can improve the prediction accuracy of a market in which traders will not trade unless their expected profit is above a threshold. 1.2 Related Work Numerous studies have demonstrated empirically that market prices are good predictors of future events, and seem to aggregate the collected wisdom of all the traders -LSB- 2, 3, 12, 1, 5, 16 -RSB-. A number of recent studies have addressed the design of the market structure and trading rules for information markets, as well as the incentive to participate and other strategic issues. However, strategic issues in information markets have also been studied by Mangold et al. -LSB- 8 -RSB- and by Hanson, Oprea and Porter -LSB- 7 -RSB-. An upcoming survey paper -LSB- 11 -RSB- discusses costfunction formulations of automated market makers. Organization of the paper The rest of this paper is organized as follows : In Section 2, we describe the projection game, and analyze the players ' costs, profits, and optimal strategies in this game. In Section 3, we study the dynamic parimutuel market, and show that trade in a DPM is equivalent to a projection game. We establish a connection between the projection game and the MSR in Section 4. In Section 5, we illustrate how the projection game can be used to analyze non-myopic, and potentially manipulative, actions. We present our conclusions, and suggestions for future work, in Section 6. 6. CONCLUSIONS AND FUTURE WORK We have presented a simple geometric game, the projection game, that can serve as a model for strategic behavior in information markets, as well as a tool to guide the design of new information markets. We have used this model to analyze the cost, profit, and strategies of a trader in a dynamic parimutuel market, and shown that both the dynamic parimutuel market and the spherical market scoring rule are strategically equivalent to the restricted projection game under slight distortion of the prior probabilities. The general analysis was based on the assumption that traders do not actively try to mislead other traders for future profit. In section 5, however, we analyze a small example market without this assumption. We demonstrate that the projection game can be used to analyze traders ' strategies in this scenario, and potentially to help design markets with better strategic properties. Our results raise several very interesting open questions. Firstly, the payoffs of the projection game can not be directly implemented in situations in which the true probability is not ultimately revealed. Finally, the existence of long-range manipulative strategies in information markets is of great interest. The example we studied in section 5 merely scratches the surface of this area. A general study of this class of manipulations, together with a characterization of markets in which it can or can not arise, would be very useful for the design of information markets.", "keyphrases": ["inform market", "dynam parimutuel market", "project game model", "predict market", "market score rule", "spheric score rule", "long-rang manipul strategi", "social and behavior scienc-econom", "liquid time"]}
{"file_name": "H-18", "text": "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents ABSTRACT Topic detection and tracking -LSB- 26 -RSB- and topic segmentation -LSB- 15 -RSB- play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information -LRB- MI -RRB- and weighted mutual information -LRB- WMI -RRB- that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI -LRB- or WMI -RRB-. Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation. 1. INTRODUCTION Many researchers have worked on topic detection and tracking -LRB- TDT -RRB- -LSB- 26 -RSB- and topic segmentation during the past decade. Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure. Topic segmentation tasks usually fall into two categories -LSB- 15 -RSB- : text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics. Traditional approaches perform topic segmentation on documents one at a time -LSB- 15, 25, 6 -RSB-. Most of them perform badly in subtle tasks like coherent document segmentation -LSB- 15 -RSB-. Often, end-users seek documents that have the similar content. At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest. Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized. Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment -LSB- 15, 25, 6 -RSB-. However, they usually suffer the issue of identifying stop words. For example, additional document-dependent stop words are removed together with the generic stop words in -LSB- 15 -RSB-. There are two reasons that we do not remove stop words directly. First, identifying stop words is another issue -LSB- 12 -RSB- that requires estimation in each domain. Removing common stop words may result in the loss of useful information in a specific domain. We employ a soft classification using term weights. Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster. Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem. Usually, human readers can identify topic transition based on cue words, and can ignore stop words. Inspired by this, we give each term -LRB- or term cluster -RRB- a weight based on entropy among different documents and different segments of documents. Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words. These words are common in a document. Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed -LSB- 15 -RSB-. Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment. The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria. Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents. Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly. Obviously, our approach can handle single documents as a special case when multiple documents are unavailable. It can detect shared topics among documents to judge if they are multiple documents on the same topic. We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further. We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice. Some of our prior work is in -LSB- 24 -RSB-. The rest of this paper is organized as follows : In Section 2, we review related work. Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI. In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by us Figure 1 : Illustration of multi-document segmentation and alignment. ing dynamic programming. In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm. Conclusions and some future directions of the research work are discussed in Section 6. 2. PREVIOUS WORK Supervised learning usually has good performance, since it learns functions from labelled training sets. However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired. Some approaches also focus on cue words as hints of topic transitions -LSB- 11 -RSB-. While some existing methods only consider information in single documents -LSB- 6, 15 -RSB-, others utilize multiple documents -LSB- 16, 14 -RSB-. There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents. Previous research studied methods to find shared topics -LSB- 16 -RSB- and topic segmentation and summarization between just a pair of documents -LSB- 14 -RSB-. Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods. Criteria of these approaches can be utilized in the issue of topic segmentation. Some of those methods have been extended into the area of topic segmentation, such as PLSA -LSB- 5 -RSB- and maximum entropy -LSB- 7 -RSB-, but to our best knowledge, using MI for topic segmentation has not been studied. 6. CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases. We used dynamic programming to optimize our algorithm. Our approach outperforms all the previous methods on singledocument cases. Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously. Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance. We only tested our method on limited data sets. More data sets especially complicated ones should be tested. More previous methods should be compared with. Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries. Supervised learning also can be considered.", "keyphrases": ["topic detect", "track", "topic segment", "local and sequenti inform of document", "singl document", "multipl document", "wmu", "share topic", "optim boundari", "singl-document segment", "multi-document segment", "cue term", "stop word", "term weight", "perform of topic segment"]}
{"file_name": "I-6", "text": "Dynamic Semantics for Agent Communication Languages ABSTRACT This paper proposes dynamic semantics for agent communication languages -LRB- ACLs -RRB- as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems. Based on the idea of providing alternative semantic `` variants '' for speech acts and transition rules between them that are contingent on previous agent behaviour, our framework provides an improved notion of grounding semantics in ongoing interaction, a simple mechanism for distinguishing between compliant and expected behaviour, and a way to specify sanction and reward mechanisms as part of the ACL itself. We extend a common framework for commitment-based ACL semantics to obtain these properties, discuss desiderata for the design of concrete dynamic semantics together with examples, and analyse their properties. 1. INTRODUCTION The field of agent communication language -LRB- ACL -RRB- research has long been plagued by problems of verifiability and grounding -LSB- 10, 13, 17 -RSB-. Unable to safeguard themselves against abuse by malicious, deceptive or malfunctioning agents, mentalistic semantics are inherently unreliable and inappropriate for use in open MAS in which agents with potentially conflicting objectives might deliberately exploit their adversaries ' conceptions of message semantics to provoke a certain behaviour. Commitment-based semantics -LSB- 6, 8, 14 -RSB-, on the other hand, define the meaning of messages exchanged among agents in terms of publicly observable commitments, i.e. pledges to bring about a state of affairs or to perform certain actions. Such semantics solve the verifiability problem as they allow for tracing the status of existing commitments at any point in time given observed messages and actions so that any observer can, for example, establish whether an agent has performed a promised action. Further, this implies that the semantics specification does not provide an interface to agents ' deliberation and planning mechanisms and hence it is unclear how rational agents would be able to decide whether to subscribe to a suggested ACL semantics when it is deployed. Finally, none of the existing approaches allows the ACL to specify how to respond to a violation of its semantics by individual agents. Secondly, existing approaches fail to exploit the possibilities of sanctioning and rewarding certain behaviours in a communication-inherent way by modifying the future meaning of messages uttered or received by compliant/deviant agents. In this paper, we propose dynamic semantics -LRB- DSs -RRB- for ACLs as a solution to these problems. Our notion of DS is based on the very simple idea of defining different alternatives for the meaning of individual speech acts -LRB- so-called semantic variants -RRB- in an ACL semantics specification, and transition rules between semantic states -LRB- i.e. collections of variants for different speech acts -RRB- that describe the current meaning of the ACL. These elements taken together result in a FSM-like view of ACL specifications where each individual state provides a complete ACL semantics and state transitions are triggered by observed agent behaviour in order to -LRB- 1 -RRB- reflect future expectations based on previous interaction experience and -LRB- 2 -RRB- sanction or reward certain kinds of behaviour. In defining a DS framework for commitment-based ACLs, this paper makes three contributions : 1. An extension of commitment-based ACL semantics to provide an improved notion of grounding commitments in agent interaction and to allow ACL specifications to be directly used for planning-based rational decision making. 2. A simple way of distinguishing between compliant and expected behaviour with respect to an ACL specification that enables reasoning about the potential behaviour of agents purely from an ACL semantics perspective. 3. A mechanism for specifying how meaning evolves with agent behaviour and how this can be used to describe communication-inherent sanctioning and rewarding mechanisms essential to the design of open MASs.. Furthermore, we discuss desiderata for DS design that can be derived from our framework, present examples and analyse their properties. The remainder of this paper is structured as follows : Section 2 introduces a formal framework for dynamic ACL semantics. In section 3 we present an analysis and discussion of this framework and discuss desiderata for the design of ACLs with dynamic semantics. Section 4 reviews related approaches, and section 5 concludes. 4. RELATED WORK Expectation-based reasoning about interaction was first proposed in -LSB- 2 -RSB-, considering the evolution of expectations described as probabilistic expectations of communication and action sequences. The same authors suggested a more general framework for expectation-based communication semantics -LSB- 9 -RSB-, and argue for a `` consequentialist '' view of semantics that is based on defining the meaning of utterances in terms of their expected consequences and updating these expectations with new observations -LSB- 11 -RSB-. However, their approach does not use an explicit notion of commitments which in our framework mediates between communication and behaviour-based grounding, and provides a clear distinction between a normative notion of compliance and a more empirical notion of expectation. Grounding for -LRB- mentalistic -RRB- ACL semantics has been investigated in -LSB- 7 -RSB- where grounded information is viewed as `` information that is publicly expressed and accepted as being true by all the agents participating in a conversation ''. Like -LSB- 1 -RSB- -LRB- which bases the notion of `` publicly expressed '' on roles rather than internal states of agents -RRB- these authors ' main concern is to provide a verifiable basis for determining the semantics of expressed mental states and commitments. 11In a non-trivial sense, i.e. when some initial transitions are possible in principle 106 The Sixth Intl.. Joint Conf. Our framework is also related to deontic methods for the specification of obligations, norms and sanctions. In this area, -LSB- 16 -RSB- is the only framework that we are aware of which considers dynamic obligations, norms and sanctions. However, as we have described above we solely utilise semantic evolution as a sanctioning and rewarding mechanism, i.e. unlike this work we do not assume that agents can be directly punished or rewarded. 5. CONCLUSION This paper introduces dynamic semantics for ACLs as a method for dealing with some fundamental problems of agent communication in open systems, the simple underlying idea being that different courses of agent behaviour can give rise to different interpretations of meaning of the messages exchanged among agents. Based on a common framework of commitment-based semantics, we presented a notion of grounding for commitments based on notions of compliant and expected behaviour. We then defined dynamic semantics as state transition systems over different semantic states that can be viewed as different `` versions '' of ACL semantics in the traditional sense, and can be easily associated with a planning-based view of reasoning about communication. Thereby, our focus was on simplicity and on providing mechanisms for tracking semantic evolution in a `` down-toearth '', algorithmic fashion to ensure applicability to many different agent designs. We discussed the properties of our framework showing how it can be used as a powerful communication-inherent mechanism for rewarding and sanctioning agent behaviour in open systems without compromising agent autonomy, discussed its integration with agents ' planning processes, complexity issues, and presented a list of desiderata for the design of ACLs with such semantics.", "keyphrases": ["agent commun languag", "dynam semant", "social reason", "commit-base semant", "state transit system", "reput-base adapt", "mutual of expect", "recoveri mechan", "non-redund"]}
{"file_name": "J-13", "text": "On The Complexity of Combinatorial Auctions : Structured Item Graphs and Hypertree Decompositions ABSTRACT The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth -LRB- called structured item graphs -RRB-. Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists -LRB- and if so, computing one -RRB- was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with -LRB- dual -RRB- hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph. 1. INTRODUCTION Combinatorial auctions. Combinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items. This is desirable when a bidder 's valuation of a bundle of items is not equal to the sum of her valuations of the individual items. An outcome for -LRB- Z, B -RRB- is a subset b of B such that item -LRB- Bi -RRB- n item -LRB- Bj -RRB- = 0, for each pair Bi and Bj of bids in b with i = ~ j. The winner determination problem. A crucial problem for combinatorial auctions is to determine the outcome b \u2217 that maximizes the sum of the accepted bid prices -LRB- i.e., Bi \u2208 b \u2217 pay -LRB- Bi -RRB- -RRB- over all the possible outcomes. This problem, called winner determination problem -LRB- e.g., -LSB- 11 -RSB- -RRB-, is known to be intractable, actually NP-hard -LSB- 17 -RSB-, and even not approximable in polynomial time unless NP = ZPP -LSB- 19 -RSB-. Hence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions -LRB- e.g., -LSB- 20, 5, 2, 8, 23 -RSB- -RRB- and to identify classes of instances where solving the winner determination problem is feasible in polynomial time -LRB- e.g., -LSB- 15, 22, 12, 21 -RSB- -RRB-. In fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions. Item graphs. Currently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any Figure 1 : Example MaxWSP problem : -LRB- a -RRB- Hypergraph H -LRB- To, go -RRB-, and a packing h for it ; -LRB- b -RRB- Primal graph for H -LRB- To, go -RRB- ; and, -LRB- c, d -RRB- Two item graphs for H -LRB- To, go -RRB-. bid, the items occurring in it induce a connected subgraph. Indeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure -LSB- 3 -RSB- -- formally bounded treewidth -LSB- 16 -RSB-. To have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H -LRB- T, g -RRB- such that its set of nodes N -LRB- H -LRB- T, g -RRB- -RRB- coincides with set of items I, and where its edges E -LRB- H -LRB- T, g -RRB- -RRB- are precisely the bids of the buyers -LCB- item -LRB- Bi -RRB- | Bi \u2208 B -RCB-. A special item graph for ~ I, B ~ is the primal graph of H -LRB- T, g -RRB-, denoted by G -LRB- H -LRB- T, g -RRB- -RRB-, which contains an edge between any pair of nodes in some hyperedge of H -LRB- T, g -RRB-. Then, any item graph for H -LRB- T, g -RRB- can be viewed as a simplification of G -LRB- H -LRB- T, g -RRB- -RRB- obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge. EXAMPLE 1. The hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB- is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = -LCB- I1,..., I5 -RCB-, and item -LRB- Bi -RRB- = hi, for each 1 \u2264 i \u2264 3. The primal graph for H -LRB- To, go -RRB- is reported in Figure 1. -LRB- b -RRB-, while two example item graphs are reported in Figure 1. -LRB- c -RRB- and -LRB- d -RRB-, where edges required for maintaining the connectivity for h1 are depicted in bold. < Open Problem : Computing structured item graphs efficiently. The above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined. However, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain -LRB- constant -RRB- treewidth exists, and if so, how to compute such a structured item graph efficiently. Weighted Set Packing. Let us note that the hypergraph representation H -LRB- T, g -RRB- of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear -LRB- e.g., -LSB- 17 -RSB- -RRB-. Formally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h ' \u2208 h with h = ~ h ', it holds that h \u2229 h ' = \u2205. Then, the set of the solutions for the weighted set packing problem for H -LRB- T, g -RRB- w.r.t. w -LRB- T, g -RRB- coincides with the set of the solutions for the winner determination problem on ~ I, B ~. EXAMPLE 2. Consider again the hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB-. An example packing for H -LRB- To, go -RRB- is h = -LCB- h1 -RCB-, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1. Indeed, the packing Contributions The primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable. Towards this aim, we first study structured item graphs and solve the open problem in -LSB- 3 -RSB-. The result is very bad news : \u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3. More formally, letting C -LRB- ig, k -RRB- denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph -LRB- associated with a combinatorial auction problem -RRB- belongs to C -LRB- ig, 3 -RRB- is NP-complete. In the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem. E -LRB- H -RRB- -RCB- is in E. We show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width -LSB- 7 -RSB- bounded by k -LRB- short : class C -LRB- hw, k -RRB- of hypergraphs -RRB-. Note that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \u00af instead of the auction hypergraph H. In fact, we can show that MaxWSP remains NP-hard even when H is acyclic -LRB- i.e., when it has hypertree width 1 -RRB-, even when each node is contained in 3 hyperedges at most. \u25ba For some relevant special classes of hypergraphs in C -LRB- hw, k -RRB-, we design a higly-parallelizeable algorithm for MaxWSP. Recall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P -LRB- see, e.g., -LSB- 9 -RSB- -RRB-. \u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs. To the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs. In fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach. Intuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition. \u25ba The proof of the above results give us some interesting insight into the notion of structured item graph. Indeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions. The rest of the paper is organized as follows. Section 2 discusses the intractability of structured item graphs. Section 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable. The comparison between the classes C -LRB- ig, k -RRB- and C -LRB- hw, k -RRB- is discussed in Section 4. Finally, in Section 5 we draw our conclusions by also outlining directions for further research. 5. CONCLUSIONS We have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario. The result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3. Motivated by this result, we investigated the use of hypertree decomposition -LRB- on the dual hypergraph associated with the scenario -RRB- and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width. For some special, yet relevant cases, a highly parallelizable algorithm is also discussed. Interestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width -LRB- hence, the reason of their intractability is not their generality -RRB-. In particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions -LRB- on the dual hypergraph -RRB-, called query decompositions -LRB- see, e.g., -LSB- 7 -RSB- -RRB-. In the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.", "keyphrases": ["hypergraph", "combinatori auction", "hypertre decomposit", "well-known mechan for resourc and task alloc", "hypertre-base decomposit method", "hypergraph hg", "complex of structur item graph", "simplif of the primal graph", "structur item graph", "fix treewidth", "accept bid price", "polynomi time"]}
{"file_name": "C-33", "text": "Rewards-Based Negotiation for Providing Context Information ABSTRACT How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly. 1. INTRODUCTION Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates -LSB- 1 -RSB-. Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers -LRB- sensors or other data sources of context information -RRB- with different quality levels. For example, Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized `` arbitrator '' provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers ' interests. Moreover, when such `` arbitrator '' is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide `` good '' context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages : 1. We do not need an `` arbitrator '' provided by the middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware. 2. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized `` arbitrator ''. 3. It can guarantee providers ' interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. 4. This approach can choose an appropriate provider au tomatically. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process -LRB- i.e. rewards -RRB-. It will cost more time to reach agreement. It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. Section 2 presents some assumptions. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers ' strategies. Section 4 evaluates our approach. In section 5 we introduce some related work and conclude in section 6. 5. RELATED WORK In -LSB- 4 -RSB-, Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. Their adaptive middleware uses utility functions to choose the best context provider -LRB- given the QoC requirements of applications and the QoC of alternative means of context acquisition -RRB-. In our negotiation model, the calculation of utility function Uc was inspired by this approach. Henricksen and Indulska propose an approach to modelling and using imperfect information in -LSB- 3 -RSB-. They characterize various types and sources of imperfect context information and present a set of novel context modelling constructs. They also outline a software infrastructure that supports the management and use of imperfect context information. -LSB- 10 -RSB- presents a framework for realizing dynamic context consistency management. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. Most approaches to provide appropriate context utilize a centralized `` arbitrator ''. In our approach, we let distributed context providers themselves decide who can provide appropriate context information. Our approach can reduce the burden of the middleware, because we do not need the middleware to provide a context selection mechanism. Also, it can guarantee context providers ' interests. 6. CONCLUSION AND FUTURE WORK How to provide the appropriate context information is a challenging problem in pervasive computing. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers ' interests by a relatively fair proceeds allocation. In this paper, we only consider how to choose an appropriate context provider from two providers. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem. We assume that the context provider will fulfill its promise of reward in the next negotiation process. In fact, the context provider might deceive its opponent and provide illusive promise. We should solve this problem in the future.", "keyphrases": ["context-awar", "context provid", "negoti", "context-awar comput", "concret negoti model", "distribut applic", "pervas comput", "reput", "qualiti of context", "persuas argument"]}
{"file_name": "H-12", "text": "Fast Generation of Result Snippets in Web Search ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58 % over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache. 1. INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type. In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents. Snippets are short fragments of text extracted from the document content -LRB- or its metadata -RRB-. A query-biased snippet is one selectively extracted on the basis of its relation to the searcher 's query. The addition of informative snippets to search results may substantially increase their value to searchers. Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored. In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searcher 's real information need, such as the contact details of a person or an organization. Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load -LRB- remembering that each search typically generates ten snippets -RRB-. The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required. The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load. Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems. Note that the utility of snippets is by no means restricted to whole-of-Web search applications. Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov -LRB- c. 25 million pages -RRB- and govsearch.australia.gov.au -LRB- c. 5 million pages -RRB- and within large enterprises such as IBM -LSB- 2 -RSB- -LRB- c. 50 million pages -RRB-. Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present. We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets. We report the proportion of time spent for disk seeks, disk reads and cpu processing ; demonstrating that the time for locating each document -LRB- seek time -RRB- dominates, as expected. As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required. However, this is only true if there is no caching of documents in RAM. Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation. In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided : the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists -LSB- 24 -RSB-. As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document. This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2. RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query. Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document. Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms. To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page -LSB- 1 -RSB- only under the heading of future work. Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary -LSB- 20, 21 -RSB-, rather than efficient generation of summaries. Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature. White et al -LSB- 21 -RSB- report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. N/M documents. The total amount of RAM required by a single machine, therefore, would be N/M -LRB- 8.192 + 10.24 + 8 -RRB- bytes. Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine. These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache. In this work we have deliberately avoided committing to one particular scoring method for sentences in documents. Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets -LSB- 20 -RSB-. The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here can not be used. As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper. We will explore alternate compression schemes in future work.", "keyphrases": ["search engin", "snippet gener", "document cach", "link graph measur", "perform", "web summari", "special-purpos filesystem", "ram", "document compact", "text fragment", "precomput final result page", "vbyte code scheme", "semi-static compress"]}
{"file_name": "H-10", "text": "Regularized Clustering for Documents * ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization -LRB- CLGR -RRB-. We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods. 1. INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies. In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful. Generally, document clustering methods can be mainly categorized into two classes : hierarchical methods and partitioning methods. The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches. For example, hierarchical agglomerative clustering -LRB- HAC -RRB- -LSB- 13 -RSB- is a typical bottom-up hierarchical clustering method. It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster. On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions. For instance, K-means -LSB- 13 -RSB- is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers. In this paper, we will focus on the partitioning methods. In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods -LSB- 19 -RSB- -LSB- 28 -RSB-. Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community. The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points. Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph. After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal. In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph. In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. So we call our method Clustering with Local and Global Regularization -LRB- CLGR -RRB-. The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning -LSB- 31 -RSB-, and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods. The rest of this paper is organized as follows : in section 2 we will introduce our CLGR algorithm in detail. The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 4. CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization. Our method preserves the merit of local learning algorithms and spectral clustering. Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm.", "keyphrases": ["document cluster", "regular", "global regular", "cluster hierarchi", "spectrum", "specifi search", "hierarch method", "partit method", "label predict", "function estim", "manifold"]}
{"file_name": "C-32", "text": "BuddyCache : High-Performance Object Storage for Collaborative Strong-Consistency Applications in a WAN * ABSTRACT Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area networks because of high network latency. BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers. We have implemented a BuddyCache prototype and evaluated its performance. Analytical results, confirmed by measurements of the BuddyCache prototype using the multiuser 007 benchmark indicate that for typical Internet latencies, e.g. ranging from 40 to 80 milliseconds round trip time to the storage server, peers using BuddyCache can reduce by up to 50 % the latency of access to shared objects compared to accessing the remote servers directly. 1. INTRODUCTION Nevertheless, distributed applications may perform poorly in wide-area network environments. Network bandwidth problems will improve in the foreseeable future, but improvement in network latency is fundamentally limited. BuddyCache is a new object caching technique that addresses the network latency problem for collaborative applications in wide-area network environment. Collaborative applications provide a shared work environment for groups of networked users collaborating on a common task, for example a team of engineers jointly overseeing a construction project. Strong-consistency collaborative applications, for example CAD systems, use client/server transactional object storage systems to ensure consistent access to shared persistent data. Up to now however, users have rarely considered running consistent network storage systems over wide-area networks as performance would be unacceptable -LSB- 24 -RSB-. For transactional storage systems, the high cost of wide-area network interactions to maintain data consistency is the main cost limiting the performance and therefore, in wide-area network environments, collaborative applications have been adapted to use weaker consistency storage systems -LSB- 22 -RSB-. Adapting an application to use weak consistency storage system requires significant effort since the application needs to be rewritten to deal with a different storage system semantics. If shared persistent objects could be accessed with low-latency, a new field of distributed strong-consistency applications could be opened. Cooperative web caching -LSB- 10, 11, 15 -RSB- is a well-known approach to reducing client interaction with a server by allowing one client to obtain missing objects from a another client instead of the server. However, cooperative web caching techniques do not provide two important properties needed by collaborative applications, strong consistency and efficient access to fine-grained objects. Cooperative object caching systems -LSB- 2 -RSB- provide these properties. However, they rely on interaction with the server to provide fine-grain cache coherence that avoids the problem of false sharing when accesses to unrelated objects appear to conflict because they occur on the same physical page. Interaction with the server increases latency. The contribution of this work is extending cooperative caching techniques to provide strong consistency and efficient access to fine-grain objects in wide-area environments. The engineers use a collaborative CAD application to revise and update complex project design documents. The shared documents are stored in transactional repository servers at the company home site. The engineers use workstations running repository clients. The workstations are interconnected by a fast local Ethernet but the network connection to the home repository servers is slow. To improve access latency, clients fetch objects from repository servers and cache and access them locally. A coherence protocol ensures that client caches remain consistent when objects are modified. The performance problem facing the collaborative application is coordinating with the servers consistent access to shared objects. With BuddyCache, a group of close-by collaborating clients, connected to storage repository via a high-latency link, can avoid interactions with the server if needed objects, updates or coherency information are available in some client in the group. BuddyCache presents two main technical challenges. One challenge is how to provide efficient access to shared finegrained objects in the collaborative group without imposing performance overhead on the entire caching system. The other challenge is to support fine-grain cache coherence in the presence of slow and failed nodes. BuddyCache uses a '' redirection '' approach similar to one used in cooperative web caching systems -LSB- 11 -RSB-. A redirector server, interposed between the clients and the remote servers, runs on the same network as the collaborating group and, when possible, replaces the function of the remote servers. If the client request can not be served locally, the redirector forwards it to a remote server. When one of the clients in the group fetches a shared object from the repository, the object is likely to be needed by other clients. BuddyCache redirects subsequent requests for this object to the caching client. Similarly, when a client creates or modifies a shared object, the new data is likely to be of potential interest to all group members. BuddyCache uses redirection to support peer update, a lightweight '' application-level multicast '' technique that provides group members with consistent access to the new data committed within the collaborating group without imposing extra overhead outside the group. Nevertheless, in a transactional system, redirection interferes with shared object availability. Solo commit, is a validation technique used by BuddyCache to avoid the undesirable client dependencies that reduce object availability when some client nodes in the group are slow, or clients fail independently. A salient feature of solo commit is supporting fine-grained validation using inexpensive coarse-grained coherence information. We designed and implemented a BuddyCache prototype and studied its performance benefits and costs using analytical modeling and system measurements. We compared the storage system performance with and without BuddyCache and considered how the cost-benefit balance is affected by network latency. These strong performance gains could make transactional object storage systems more attractive for collaborative applications in wide-area environments. 2. RELATED WORK Cooperative caching techniques -LSB- 20, 16, 13, 2, 28 -RSB- provide access to client caches to avoid high disk access latency in an environment where servers and clients run on a fast local area network. These techniques use the server to provide redirection and do not consider issues of high network latency. Cooperative Web caching techniques, -LRB- e.g. -LSB- 11, 15 -RSB- -RRB- investigate issues of maintaining a directory of objects cached in nearby proxy caches in wide-area environment, using distributed directory protocols for tracking cache changes. This work does not consider issues of consistent concurrent updates to shared fine-grained objects. This multicast transport level solution is geared to the single writer semantics of web objects. In contrast, BuddyCache uses '' application level '' multicast and a sender-reliable coherence protocol to provide similar access latency improvements for transactional objects. Application level multicast solution in a middle-ware system was described by Pendarakis, Shi and Verma in -LSB- 27 -RSB-. The schema supports small multi-sender groups appropriate for collaborative applications and considers coherence issues in the presence of failures but does not support strong consistency or fine-grained sharing. The protocol uses leases to provide fault-tolerant call-backs and takes advantage of nearby caches to reduce the cost of lease extensions. The study uses simulation to investigate latency and fault tolerance issues in hierarchical avoidance-based coherence scheme. In contrast, our work uses implementation and analysis to evaluate the costs and benefits of redirection and fine grained updates in an optimistic system. Anderson, Eastham and Vahdat in WebFS -LSB- 29 -RSB- present a global file system coherence protocol that allows clients to choose on per file basis between receiving updates or invalidations. Updates and invalidations are multicast on separate channels and clients subscribe to one of the channels. The protocol exploits application specific methods e.g. last-writer-wins policy for broadcast applications, to deal with concurrent updates but is limited to file systems. BuddyCache provides similar bandwidth improvements when objects are available in the group cache. 7. CONCLUSION Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area network because of high network latency. This paper described BuddyCache, a new transactional cooperative caching -LSB- 20, 16, 13, 2, 28 -RSB- technique that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The technique improves performance yet provides strong correctness and availability properties in the presence of node failures and slow clients. Redirection, however, can interfere with object availability. Solo commit, is a new validation technique that allows a client in a group to commit independently of slow or failed peers. It provides fine-grained validation using inexpensive coarse-grain version information. We have designed and implemented BuddyCache prototype in Thor distributed transactional object storage system -LSB- 23 -RSB- and evaluated the benefits and costs of the system over a range of network latencies. fine-grain strong-consistency access in high-latency environments, 2. an implementation of the system prototype that yields strong performance gains over the base system, 3. analytical and measurement based performance evaluation of the costs and benefits of the new techniques capturing the dominant performance cost, high network latency.", "keyphrases": ["object storag system", "collabor strong-consist applic", "wide-area network", "cooper web cach", "fine-grain share", "transact", "fault-toler properti", "buddycach", "domin perform cost", "optimist system", "peer fetch", "multi-user oo7 benchmark"]}
{"file_name": "J-17", "text": "Truthful Mechanism Design for Multi-Dimensional Scheduling via Cycle Monotonicity ABSTRACT We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design, where the machines are the strategic players. This is a multidimensional scheduling domain, and the only known positive results for makespan minimization in such a domain are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We study a well-motivated special case of this problem, where the processing time of a job on each machine may either be `` low '' or `` high '', and the low and high values are public and job-dependent. This preserves the multidimensionality of the domain, and generalizes the restricted-machines -LRB- i.e., -LCB- pj, \u221e -RCB- -RRB- setting in scheduling. We give a general technique to convert any c-approximation algorithm to a 3capproximation truthful-in-expectation mechanism. This is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion. When the low and high values are the same for all jobs, we devise a deterministic 2-approximation truthful mechanism. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Our constructions are novel in two respects. First, we do not utilize or rely on explicit price definitions to prove truthfulness ; instead we design algorithms that satisfy cycle monotonicity. Cycle monotonicity -LSB- 23 -RSB- is a necessary and sufficient condition for truthfulness, is a generalization of value monotonicity for multidimensional domains. However, whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains, ours is the first work that leverages cycle monotonicity in the multidimensional setting. Second, our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem, and then converting it into a truthfulin-expectation mechanism. This builds upon a technique of -LSB- 16 -RSB-, and shows the usefulness of fractional mechanisms in truthful mechanism design. 1. INTRODUCTION Mechanism design studies algorithmic constructions under the presence of strategic players who hold the inputs to the algorithm. Algorithmic mechanism design has focused mainly on settings were the social planner or designer wishes to maximize the social welfare -LRB- or equivalently, minimize social cost -RRB-, or on auction settings where revenuemaximization is the main goal. In this paper, we consider such an alternative goal in the context of machine scheduling, namely, makespan minimization. There are n jobs or tasks that need to be assigned to m machines, where each job has to be assigned to exactly one machine. Hence, we approach the problem via mechanism design : the social designer, who holds the set of jobs to be assigned, needs to specify, in addition to a schedule, suitable payments to the players in order to incentivize them to reveal their true processing times. Such a mechanism is called a truthful mechanism. Instead, it corresponds to maximizing the minimum welfare and the notion of max-min fairness, and appears to be a much harder problem from the viewpoint of mechanism design. In particular, the celebrated VCG -LSB- 26, 9, 10 -RSB- family of mechanisms does not apply here, and we need to devise new techniques. The possibility of constructing a truthful mechanism for makespan minimization is strongly related to assumptions on the players ' processing times, in particular, the `` dimensionality '' of the domain. Nisan and Ronen considered the setting of unrelated machines where the pij values may be arbitrary. This is a multidimensional domain, since a player 's private value is its entire vector of processing times -LRB- pij -RRB- j. Very few positive results are known for multidimensional domains in general, and the only positive results known for multidimensional scheduling are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We emphasize that regardless of computational considerations, even the existence of a truthful mechanism with a significantly better -LRB- than m -RRB- approximation ratio is not known for any such scheduling domain. On the negative side, -LSB- 22 -RSB- showed that no truthful deterministic mechanism can achieve approximation ratio better than 2, and strengthened this lower bound to m for two specific classes of deterministic mechanisms. Recently, -LSB- 20 -RSB- extended this lower bound to randomized mechanisms, and -LSB- 8 -RSB- improved the deterministic lower bound. In stark contrast with the above state of affairs, much stronger -LRB- and many more -RRB- positive results are known for a special case of the unrelated machines problem, namely, the setting of related machines. Here, we have pij = pj/si for every i, j, where pj is public knowledge, and the speed si is the only private parameter of machine i. This assumption makes the domain of players ' types single-dimensional. Truthfulness in such domains is equivalent to a convenient value-monotonicity condition -LSB- 21, 3 -RSB-, which appears to make it significantly easier to design truthful mechanisms in such domains. Archer and Tardos -LSB- 3 -RSB- first considered the related machines setting and gave a randomized 3-approximation truthful-in-expectation mechanism. The gap between the single-dimensional and multidimensional domains is perhaps best exemplified by the fact that -LSB- 3 -RSB- showed that there exists a truthful mechanism that always outputs an optimal schedule. -LRB- Recall that in the multidimensional unrelated machines setting, it is impossible to obtain a truthful mechanism with approximation ratio better than 2. -RRB- Various follow-up results -LSB- 2, 4, 1, 13 -RSB- have strengthened the notion of truthfulness and/or improved the approximation ratio. Such difficulties in moving from the single-dimensional to the multidimensional setting also arise in other mechanism design settings -LRB- e.g., combinatorial auctions -RRB-. Thus, in addition to the specific importance of scheduling in strategic environments, ideas from multidimensional scheduling may also have a bearing in the more general context of truthful mechanism design for multidimensional domains. In this paper, we consider the makespan-minimization problem for a special case of unrelated machines, where the processing time of a job is either `` low '' or `` high '' on each machine. We call this model the `` jobdependent two-values '' case. This model generalizes the classic `` restricted machines '' setting, where pij \u2208 -LCB- Lj, \u221e -RCB- which has been well-studied algorithmically. A special case of our model is when Lj = L and Hj = H for all jobs j, which we denote simply as the `` two-values '' scheduling model. Both of our domains are multidimensional, since the machines are unrelated : one job may be low on one machine and high on the other, while another job may follow the opposite pattern. Thus, the private information of each machine is a vector specifying which jobs are low and high on it. Thus, they retain the core property underlying the hardness of truthful mechanism design for unrelated machines, and by studying these special settings we hope to gain some insights that will be useful for tackling the general problem. Our Results and Techniques We present various positive results for our multidimensional scheduling domains. Our first result is a general method to convert any capproximation algorithm for the job-dependent two values setting into a 3c-approximation truthful-in-expectation mechanism. This is one of the very few known results that use an approximation algorithm in a black-box fashion to obtain a truthful mechanism for a multidimensional problem. Our result implies that there exists a 3-approximation truthfulin-expectation mechanism for the Lj-Hj setting. Our second result applies to the twovalues setting -LRB- Lj = L, Hj = H -RRB-, for which we improve both the approximation ratio and strengthen the notion of truthfulness. We obtain a deterministic 2-approximation truthful mechanism -LRB- along with prices -RRB- for this problem. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Complementing this, we observe that even this seemingly simple setting does not admit truthful mechanisms that return an optimal schedule -LRB- unlike in the case of related machines -RRB-. By exploiting the multidimensionality of the domain, we prove that no truthful deterministic mechanism can obtain an approximation ratio better than 1.14 to the makespan -LRB- irrespective of computational considerations -RRB-. The main technique, and one of the novelties, underlying our constructions and proofs, is that we do not rely on explicit price specifications in order to prove the truthfulness of our mechanisms. Instead we exploit certain algorithmic monotonicity conditions that characterize truthfulness to first design an implementable algorithm, i.e., an algorithm for which prices ensuring truthfulness exist, and then find these prices -LRB- by further delving into the proof of implementability -RRB-. This kind of analysis has been the method of choice in the design of truthful mechanisms for singledimensional domains, where value-monotonicity yields a convenient characterization enabling one to concentrate on the algorithmic side of the problem -LRB- see, e.g., -LSB- 3, 7, 4, 1, 13 -RSB- -RRB-. Our work is the first to leverage monotonicity conditions for truthful mechanism design in arbitrary domains. The monotonicity condition we use, which is sometimes called cycle monotonicity, was first proposed by Rochet -LSB- 23 -RSB- -LRB- see also -LSB- 11 -RSB- -RRB-. It is a generalization of value-monotonicity and completely characterizes truthfulness in every domain. Our methods and analyses demonstrate the potential benefits of this characterization, and show that cycle monotonicity can be effectively utilized to devise truthful mechanisms for multidimensional domains. Consider, for example, our first result showing that any c-approximation algorithm can be `` exported '' to a 3c-approximation truthful-in-expectation mechanism. At the level of generality of an arbitrary approximation algorithm, it seems unlikely that one would be able to come up with prices to prove truthfulness of the constructed mechanism. But, cycle monotonicity does allow us to prove such a statement. In fact, some such condition based only on the underlying algorithm -LRB- and not on the prices -RRB- seems necessary to prove such a general statement. The method for converting approximation algorithms into truthful mechanisms involves another novel idea. Our randomized mechanism is obtained by first constructing a truthful mechanism that returns a fractional schedule. Moving to a fractional domain allows us to `` plug-in '' truthfulness into the approximation algorithm in a rather simple fashion, while losing a factor of 2 in the approximation ratio. We then use a suitable randomized rounding procedure to convert the fractional assignment into a random integral assignment. This preserves truthfulness, but we lose another additive factor equal to the approximation ratio. Our construction uses and extends some observations of Lavi and Swamy -LSB- 16 -RSB-, and further demonstrates the benefits of fractional mechanisms in truthful mechanism design. Related Work Nisan and Ronen -LSB- 22 -RSB- first considered the makespan-minimization problem for unrelated machines. They gave an m-approximation positive result and proved various lower bounds. This been improved in -LSB- 2, 4, 1, 13 -RSB- to : a 2-approximation randomized mechanism -LSB- 2 -RSB- ; an FPTAS for any fixed number of machines given by Andelman, Azar and Sorani -LSB- 1 -RSB-, and a 3-approximation deterministic mechanism by Kov \u00b4 acs -LSB- 13 -RSB-. The algorithmic problem -LRB- i.e., without requiring truthfulness -RRB- of makespan-minimization on unrelated machines is well understood and various 2-approximation algorithms are known. Lenstra, Shmoys and Tardos -LSB- 18 -RSB- gave the first such algorithm. Shmoys and Tardos -LSB- 25 -RSB- later gave a 2approximation algorithm for the generalized assignment problem, a generalization where there is a cost cij for assigning a job j to a machine i, and the goal is to minimize the cost subject to a bound on the makespan. Recently, Kumar, Marathe, Parthasarathy, and Srinivasan -LSB- 14 -RSB- gave a randomized rounding algorithm that yields the same bounds. We use their procedure in our randomized mechanism. The characterization of truthfulness for arbitrary domains in terms of cycle monotonicity seems to have been first observed by Rochet -LSB- 23 -RSB- -LRB- see also Gui et al. -LSB- 11 -RSB- -RRB-. This generalizes the value-monotonicity condition for single-dimensional domains which was given by Myerson -LSB- 21 -RSB- and rediscovered by -LSB- 3 -RSB-. As mentioned earlier, this condition has been exploited numerous times to obtain truthful mechanisms for single-dimensional domains -LSB- 3, 7, 4, 1, 13 -RSB-. For convex domains -LRB- i.e., each players ' set of private values is convex -RRB-, it is known that cycle monotonicity is implied by a simpler condition, called weak monotonicity -LSB- 15, 6, 24 -RSB-. But even this simpler condition has not found much application in truthful mechanism design for multidimensional problems. Objectives other than social-welfare maximization and revenue maximization have received very little attention in mechanism design. In the context of combinatorial auctions, the problems of maximizing the minimum value received by a player, and computing an envy-minimizing allocation have been studied briefly. Lavi, Mu'alem, and Nisan -LSB- 15 -RSB- showed that the former objective can not be implemented truthfully ; Bezakova and Dani -LSB- 5 -RSB- gave a 0.5-approximation mechanism for two players with additive valuations. These lower bounds were strengthened in -LSB- 20 -RSB-. 2. PRELIMINARIES 2.1 The scheduling domain In our scheduling problem, we are given n jobs and m machines, and each job must be assigned to exactly one machine. In the unrelated-machines setting, each machine i is characterized by a vector of processing times -LRB- pij -RRB- j, where pij E R \u2265 0 U -LCB- oo -RCB- denotes i 's processing time for job j with the value oo specifying that i can not process j. We consider two special cases of this problem : 1. The job-dependent two-values case, where pij E -LCB- Lj, Hj -RCB- for every i, j, with Lj < Hj, and the values Lj, Hj are known. This generalizes the classic scheduling model of restricted machines, where Hj = oo. 2. We say that a job j is low on machine i if pij = Lj, and high if pij = Hj. We will use the terms schedule and assignment interchangeably. We will also consider randomized algorithms and algorithms that return a fractional assignment. We denote the load of machine i -LRB- under a given assignj xijpij, and the makespan of a schedule is defined as the maximum load on any machine, i.e., maxi li. The goal in the makespan-minimization problem is to assign the jobs to the machines so as to minimize the makespan of the schedule. 2.2 Mechanism design We consider the makespan-minimization problem in the above scheduling domains in the context of mechanism design. Mechanism design studies strategic settings where the social designer needs to ensure the cooperation of the different entities involved in the algorithmic procedure. Following the work of Nisan and Ronen -LSB- 22 -RSB-, we consider the machines to be the strategic players or agents. The social designer holds the set of jobs that need to be assigned, but does not know the -LRB- true -RRB- processing times of these jobs on the different machines. Each machine is a selfish entity, that privately knows its own processing time for each job. We consider direct-revelation mechanisms : each machine reports its -LRB- possibly false -RRB- vector of processing times, the mechanism then computes a schedule and hands out payments to the players -LRB- i.e., machines -RRB- to compensate them for the cost they incur in processing their assigned jobs. A -LRB- direct-revelation -RRB- mechanism thus consists of a tuple -LRB- x, P -RRB- : x specifies the schedule, and P = -LCB- Pi -RCB- specifies the payments handed out to the machines, where both x and the Pis are functions of the reported processing times p = -LRB- pij -RRB- i, j. The mechanism must therefore incentivize the machines/players to truthfully reveal their processing times via the payments. This is made precise using the notion of dominant-strategy truthfulness. To put it in words, in a truthful mechanism, no machine can improve its utility by declaring a false processing time, no matter what the other machines declare. We will also consider fractional mechanisms that return a fractional assignment, and randomized mechanisms that are allowed to toss coins and where the assignment and the payments may be random variables. The notion of truthfulness for a fractional mechanism is the same as in Definition 2.1, where x1, x2 are now fractional assignments. For a randomized mechanism, we will consider the notion of truthfulness in expectation -LSB- 3 -RSB-, which means that a machine -LRB- player -RRB- maximizes her expected utility by declaring her true processing-time vector. For our two scheduling domains, the informational assumption is that the values Lj, Hj are publicly known. The private information of a machine is which jobs have value Lj -LRB- or L -RRB- and which ones have value Hj -LRB- or H -RRB- on it. We emphasize that both of our domains are multidimensional, since each machine i needs to specify a vector saying which jobs are low and high on it.", "keyphrases": ["mechan design", "approxim algorithm", "schedul", "multi-dimension schedul", "cycl monoton", "makespan minim", "algorithm", "random mechan", "us of fraction mechan", "truth mechan design", "fraction domain"]}
{"file_name": "I-26", "text": "Sequential Decision Making in Parallel Two-Sided Economic Search ABSTRACT This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships. In each search stage, each of the agents is randomly matched with several other agents in parallel, and makes a decision whether to accept a potential partnership with one of them. The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized -LRB- instantaneous -RRB- decision protocol and can sequentially accept and reject partnerships within the same search stage. We analyze the dynamics which drive the agents ' strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model. By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments. 1. INTRODUCTION A two-sided economic search is a distributed mechanism for forming agents ' pairwise partnerships -LSB- 5 -RSB-.1 On every stage of the process, each of the agents is randomly matched with another agent 1Notice that the concept of '' search '' here is very different from the classical definition of '' search '' in AI. While AI search is an active process in which an agent finds a sequence of actions that will bring it from the initial state to a goal state, economic search refers to the identification of the best agent to commit to a partnership with. and the two interact bilaterally in order to learn the benefit encapsulated in a partnership between them. The interaction does not involve bargaining thus each agent merely needs to choose between accepting or rejecting the partnership with the other agent. A typical market where this kind of two-sided search takes place is the marriage market -LSB- 22 -RSB-. Recent literature suggests various software agent-based applications where a two-sided distributed -LRB- i.e., with no centralized matching mechanisms -RRB- search takes place. An important class of such applications includes secondary markets for exchanging unexploited resources. For example, through a twosided search, agents, representing different service providers, can exchange unused bandwidth -LSB- 21 -RSB- and communication satellites can transfer communication with a greater geographical coverage. Twosided agents-based search can also be found in applications of buyers and sellers in eMarkets and peer-to-peer applications. The twosided nature of the search suggests that a partnership between a pair of agents is formed only if it is mutually accepted. By forming a partnership the agents gain an immediate utility and terminate their search. When resuming the search, on the other hand, a more suitable partner might be found however some resources will need to be consumed for maintaining the search process. In this paper we focus on a specific class of two-sided search matching problems, in which the performance of the partnership applies to both parties, i.e., both gain an equal utility -LSB- 13 -RSB-. The equal utility scenario is usually applicable in domains where the partners gain from the synergy between them. In all these applications, any two agents can form a partnership and the performance of any given partnership depends on the skills or the characteristics of its members. Furthermore, the equal utility scenario can also hold whenever there is an option for side-payments and the partnership 's overall utility is equally split among the two agents forming it -LSB- 22 -RSB-. While the two-sided search literature offers comprehensive equilibrium analysis for various models, it assumes that the agents ' search is conducted in a purely sequential manner : each agent locates and interacts with one other agent in its environment at a time -LSB- 5, 22 -RSB-. Nevertheless, when the search is assigned to autonomous software agents a better search strategy can be used. Here an agent can take advantage of its unique inherent filtering and information processing capabilities and its ability to efficiently -LRB- in comparison to people -RRB- maintain concurrent interactions with several other agents at each stage of its search. Such use of parallel interactions in search is favorable whenever the average cost2 per interaction with another agent, when interacting in parallel with a batch of other agents, is smaller than the cost of maintaining one interaction at a time -LRB- i.e., advantage to size -RRB-. For example, the analysis of the costs associated with evaluating potential partnerships between service providers reveals both fixed and variable components when using the parallel search, thus the average cost per interaction decreases as the number of parallel interactions increases -LSB- 21 -RSB-. Despite the advantages identified for parallel interactions in adjacent domains -LRB- e.g., in one-sided economic search -LSB- 7, 16 -RSB- -RRB-, a first attempt for modeling a repeated pairwise matching process in which agents are capable of maintaining interaction with several other agents at a time was introduced only recently -LSB- 21 -RSB-. However, the agents in that seminal model are required to synchronize their decision making process. Thus each agent, upon reviewing the opportunities available in a specific search stage, has to notify all other agents of its decision whether to commit to a partnership -LRB- at most with one of them -RRB- or reject the partnership -LRB- with the rest of them -RRB-. This inherent restriction imposes a significant limitation on the agents ' strategic behavior. In our model, the agents are free to notify the other agents of their decisions in an asynchronous manner. The asynchronous approach allows the agents to re-evaluate their strategy, based on each new response they receive from the agents they interact with. The new model is a much more realistic pairwise model and, as we show in the analysis section, is always preferred by any single agents participating in the process. In the absence of other economic two-sided parallel search models, we use the model that relies on an instantaneous -LRB- synchronous -RRB- decision making process -LSB- 21 -RSB- -LRB- denoted I-DM throughout the rest of the paper -RRB- as a benchmark for evaluating the usefulness of our proposed sequential -LRB- asynchronous -RRB- decision making strategy -LRB- denoted S-DM -RRB-. The main contributions of this paper are threefold : First, we formally model and analyze a two-sided search process in which the agents have no temporal decision making constraints concerning the rejection of or commitment to potential partnerships they encounter in parallel -LRB- the S-DM model -RRB-. This model is a general search model which can be applied in various -LRB- not necessarily software agents-based -RRB- domains. Second, we prove that the agents ' SDM strategy weakly dominates the I-DM strategy, thus every agent has an incentive to deviate to the S-DM strategy when all other agents are using the I-DM strategy. Finally, by using an innovative recursive presentation of the acceptance probabilities of different potential partnerships, we identify unique characteristics of the equilibrium strategies in the new model. These are used for supplying an appropriate computational means that facilitates the calculation of the agents ' equilibrium strategy. This latter contribution is We manage to extract the agents ' new equilibrium strategies without increasing the computational complexity in comparison to the I-DM model. Throughout the paper we demonstrate the different properties of the new model and compare it with the I-DM model using an artificial synthetic environment. In the following section we formally present the S-DM model. An equilibrium analysis and computational means for finding the equilibrium strategy are provided in Section 3. In Section 4 we review related MAS and economic search theory literature. 4. RELATED WORK The two-sided economic search for partnerships in AI literature is a sub-domain of coalition formation8. As in the general 8The use of the term '' partnership '' in this context refers to the agreement between two individual agents to cooperate in a pre-defined manner. For example, in the buyer-seller application a partnership is defined as an agreed transaction between the two-parties -LSB- 9 -RSB-. coalition formation case, agents have the incentive to form partnerships when they are incapable of executing a task by their own or when the partnership can improve their individual utilities -LSB- 14 -RSB-. Various centralized matching mechanisms can be found in the literature -LSB- 6, 2, 8 -RSB-. However, in many MAS environments, in the absence of any reliable central matching mechanism, the matching process is completely distributed. While the search in agent-based environments is well recognized to be costly -LSB- 11, 21, 1 -RSB-, most of the proposed coalition formation mechanisms assume that an agent can scan as many partnership opportunities in its environment as needed or have access to central matchers or middle agents -LSB- 6 -RSB-. The incorporation of costly search in this context is quite rare -LSB- 21 -RSB- and to the best of our knowledge, a distributed two-sided search for partners model similar to the S-DM model has not been studied to date. Classical economic search theory -LRB- -LSB- 15, 17 -RSB-, and references therein -RRB- widely addresses the problem of a searcher operating in a costly environment, seeking to maximize his long term utility. In these models, classified as one-sided search, the focus is on establishing the optimal strategies for the searcher, assuming no mutual search activities -LRB- i.e., no influence on the environment -RRB-. Here the sequential search procedure is often applied, allowing the searcher to investigate a single -LSB- 15 -RSB- or multiple -LSB- 7, 19 -RSB- opportunities at a time. While the latter method is proven to be beneficial for the searcher, it was never used in the '' two-sided '' search models that followed -LRB- where dual search activities are modeled -RRB- -LSB- 22, 5, 18 -RSB-. Therefore, in these models, the equilibrium strategies are always developed based on the assumption that the agents interact with others sequentially -LRB- i.e., with one agent at a time -RRB-. A first attempt to integrate the parallel search into a two-sided search model is given in -LSB- 21 -RSB-, as detailed in the introduction section. The models presented in this area do not associate the coalition formation process with search costs, which is the essence of the analysis that economic search theory aims to supply. Furthermore, even in repeated pairwise bargaining -LSB- 10 -RSB- models the agents are always limited to initiating a single bargaining interaction at a time.", "keyphrases": ["pairwis partnership", "decis", "peer-to-peer applic", "inform process", "util", "search cost", "multi-equilibrium scenario", "equilibrium strategi", "parallel interact", "bound methodolog", "coalit format", "partnership format", "partnership", "costli environ", "search perform", "instantan decis make", "sequenti decis make", "two-side search"]}
{"file_name": "H-8", "text": "Robust Test Collections for Retrieval Evaluation ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable : the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability : with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. 1. INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments? Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem : for most retrieval tasks, it is impossible to judge the relevance of every document ; there are simply too many of them. The solution used by NIST at TREC -LRB- Text REtrieval Conference -RRB- is the pooling method -LSB- 19, 20 -RSB- : all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool -LSB- 21 -RSB-. This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of -LRB- paid -RRB- annotator time. As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems. Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be `` reusable ''. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. We need a more careful definition of reusability. Specifically, the question of reusability is not how accurately we can evaluate new systems. A `` malicious adversary '' can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree. Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done `` in-house '' for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time. 6. CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of `` reusability '' of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments : focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judg Table 5 : Accuracy, W, mean \u03c4, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100 % confidence in every evaluation -- and there is a full test collection for the task. It could be applied to evaluation on a dynamic test collection as defined by Soboroff -LSB- 18 -RSB-. The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration. We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust : with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.", "keyphrases": ["inform retriev", "evalu", "relev judgement", "reusabl", "lowerest-confid comparison", "mtc", "rtc", "expect", "varianc", "distribut of relev"]}
{"file_name": "H-21", "text": "Robust Classification of Rare Queries Using Web Knowledge ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique : given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. 1. INTRODUCTION One thing, however, has remained constant : people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not -LRB- yet! -RRB- omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in today 's search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. However, the `` tail '' queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries -LSB- 22 -RSB-. More recent studies -LSB- 18, 21 -RSB- also attempted to gather some additional knowledge from the Web. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising -LSB- 11 -RSB-. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure ; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge -LRB- e.g., using search summaries vs. entire crawled pages -RRB-. We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification -LSB- 20 -RSB-, but is supported by research in mainstream text classification -LSB- 5 -RSB-. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri -LSB- 22 -RSB-, or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results -LSB- 16, 15 -RSB-. More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. -LSB- 10 -RSB- found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. -LSB- 1 -RSB- used semi-supervised learning as well as unlabeled data -LSB- 2 -RSB-. Gravano et al. -LSB- 6 -RSB- classified queries with respect to geographic locality in order to determine whether their intent is local or global. The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories -LSB- 11, 18, 20, 9, 21 -RSB-. The KDD task specification provided a small taxonomy -LRB- 67 nodes -RRB- along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are -LRB- 1 -RRB- how to build a document classifier, -LRB- 2 -RRB- how to translate its classifications into the target taxonomy, and -LRB- 3 -RRB- how to determine the query class based on document classifications. The winning solution of the KDD Cup -LSB- 18 -RSB- proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue -LRB- 1 -RRB- above, their solution used the Open Directory Project -LRB- ODP -RRB- to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper -LSB- 19 -RSB-, Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set -LRB- ODP -RRB- is used. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies -LRB- we used a much larger taxonomy -RRB-, our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. When using search results, one can either use only summaries of the results provided by 3Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages -LRB- Table 1 -RRB-. These results are consistent with prior studies -LSB- 5 -RSB-, which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. -LSB- 19 -RSB-, who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the -LRB- fixed -RRB- taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. We plan to further investigate this direction in our future work. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.", "keyphrases": ["queri classif", "search engin", "search advertis", "machin learn", "relev feedback", "vote scheme", "crawl", "topic taxonomi", "affin score", "condit probabl", "adapt", "inform retriev"]}
{"file_name": "C-4", "text": "Intra-flow Loss Recovery and Control for ABSTRACT `` Best effort '' packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold : on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal out is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss lengths with which we can describe the loss distribution within a These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs -LRB- PCM -RRB- with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs -LRB- G. 729 -RRB- on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may `` break '' at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data to minimize the impact of packet loss. We designate these methods as loss recovery and control. At the end-to-end level, identification of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow to -LRB- statistically -RRB- trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both ets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term. 1. INTRODUCTION Considering that a real-time may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity : Temporal sensitivity : Loss of which is correlated in time may lead to disruptions in the service. For voice, as a single packet contains typically several -LRB- voice frames -RRB- this effect is thus more significant than e.g. for video. It translates basically to isolated packet losses versus losses that occur in bursts. Figure 1 : Schematic utility functions dependent on the loss of more and less -LRB- -1 -RRB- important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance -LRB- we same size, frequency and no interdependence between the frames -RRB-. Under the loss of 50 % of the packets, the perceptual quality varies hugely between the where the 50 % of the frames with high perceptual importance are received and the where the 50 % less important frames received. Network support for real-time multimedia flows can on one hand aim at offering a service, which, however, to be implemented within pa & et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. Let us now consider the case that 50 % of packets of flow identified more important -LRB- designated by or less important due to any of the above sensitivity constraints. Figure 1 a -RRB- shows a generic utility function describing the level Quality of Service dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the and -1 packets are equal. Due to the over-proportional sensitivity of the packets to loss as well as the dependence of the end loss recovery performance on the packets, the utility function is decreasing significantly in a non-linear way -LRB- approximated in the figure by piece-wise linear functions -RRB- with an increasing loss rate. Figure 1 b -RRB- presents the where all packets are protected at the expense of -1 The decay of the utility function -LRB- for loss rates < 50 % -RRB- is reduced, because the packets are protected and the endto-end loss recovery can thus operate properly a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application 's utility. Note that the higher the non-linearity of the utility contribution of the packets is -LRB- deviation from the dotted curve in Fig. 1 a -RRB-, the higher is the potential gain in utility when the protection for is enabled. Results for actual perceived quality utility for multimedia applications exhibit such non-linear behavior *. As mechanisms have to be implemented within the network -LRB- hopby-hop -RRB- and/or in the end systems -LRB- end-to-end -RRB-, we have another axis of classification. The adaptation of the sender 's to the current network congestion state an scheme -LRB- loss avoidance, is difficult to apply to voice. Considering that voice flows have very low the relative cost of transmitting the feedback information is -LRB- when compared e.g. to a video flow -RRB-. The major however, the lack of a codec is truly scalable in terms of its output and corresponding perceptual quality. when the availability of computing power is assumed, the lowest codec can be chosen permanently without actually decreasing the perceptual quality. For loss on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction -LRB- FEC -RRB- have been proposed While attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. Using redundancy has also implications to the delay adaptation -LRB- -LSB- lo -RSB- -RRB- employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to we have obtained results which confirm the shape of the `` overall utility '' curve shown in Fig. 1, clearly the utility functions of the `` sub ''. flows and their relationship are more complex and only approximately additive. Table 1 : State and transition probabilities computed for an end-to-end Internet trace using a general Markov model -LRB- third order -RRB- by Yajnik et. al.. which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but can not come close to eliminate them. Therefore in this work we assume that the lowest possible trate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual and measure their utility in the presence of packet loss using objective speech quality measurement. The paper is structured as follows : Section 2 introduces packet - and user-level metrics. We employ these metrics to describe the sensitivity of traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and by-hop level. Section 6 concludes the paper.", "keyphrases": ["end-to-end model", "sampl-base codec", "loss recoveri and control", "loss sensit", "network support for real-time multimedia", "qualiti of servic", "end-to-end loss recoveri", "voip traffic", "intra-flow loss control", "packet-level metric", "gener markov model", "sensit of voip traffic", "queue manag algorithm", "frame-base codec"]}
{"file_name": "C-22", "text": "Runtime Metrics Collection for Middleware Supported Adaptation of Mobile Applications ABSTRACT This paper proposes, implements, and evaluates in terms of worst case performance, an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host managers, runtimes and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead. 1. INTRODUCTION Effective adaptation requires detailed and up to date information about both the system and the software itself. Metrics related to system wide information -LRB- e.g. processor, memory and network load -RRB- are referred to as environmental metrics -LSB- 5 -RSB-, while metrics representing application behaviour are referred as software metrics -LSB- 8 -RSB-. Furthermore, the type of metrics required for performing adaptation is dependent upon the type of adaptation required. For example, service-based adaptation, in which service quality or service behaviour is modified in response to changes in the runtime environment, generally requires detailed environmental metrics but only simple software metrics -LSB- 4 -RSB-. On the other hand, adaptation via object mobility -LSB- 6 -RSB-, also requires detailed software metrics -LSB- 9 -RSB- since object placement is dependent on the execution characteristics of the mobile objects themselves. With the exception of MobJeX -LSB- 6 -RSB-, existing mobile object systems such as Voyager -LSB- 10 -RSB-, FarGo -LSB- 11, 12 -RSB-, and JavaParty -LSB- 13 -RSB- do not provide automated adaptation, and therefore lack the metrics collection process required to support this process. In the case of MobJeX, although an adaptation engine has been implemented -LSB- 5 -RSB-, preliminary testing was done using synthetic pre-scripted metrics since there is little prior work on the dynamic collection of software metrics in mobile object frameworks, and no existing means of automatically collecting them. Consequently, the main contribution of this paper is a solution for dynamic metrics collection to support adaptation via object mobility for mobile applications. This problem is non-trivial since typical mobile object frameworks consist of multiple application and middleware components, and thus metrics collection must be performed at different locations and the results efficiently propagated to the adaptation engine. The rest of this paper is organised as follows : Section 2 describes the general structure and implementation of mobile object frameworks in order to understand the challenges related to the collection, propagation and delivery of metrics as described in section 3. Section 4 describes some initial testing and results and section 5 closes with a summary, conclusions and discussion of future work. 2. BACKGROUND In general, an object-oriented application consists of objects collaborating to provide the functionality required by a given problem domain. Mobile object frameworks allow some of these objects to be tagged as mobile objects, providing middleware support for such objects to be moved at runtime to other hosts. At a minimum, a mobile object framework with at least one running mobile application consists of the following components : runtimes, mobile objects, and proxies -LSB- 14 -RSB-, although the terminology used by individual frameworks can differ -LSB- 6, 10-13 -RSB-. A runtime is a container process for the management of mobile objects. For example, in FarGo -LSB- 15 -RSB- this component is known as a core and in most systems separate runtimes are required to allow different applications to run independently, although this is not the case with MobJeX, which can run multiple applications in a single runtime using threads. The applications themselves comprise mobile objects, which interact with each other through proxies -LSB- 14 -RSB-. Upon migration, proxy objects move with the source object. The Java based system MobJeX, which is used as the implementation platform for the metrics collection solution described in this paper, adds a number of additional middleware components. Firstly, a host manager -LRB- known as a service in MobJeX -RRB- provides a central point of communication by running on a known port on a per host basis, thus facilitating the enumeration or lookup of components such as runtimes or mobile objects. Secondly, MobJeX has a per-application mobile object container called a transport manager -LRB- TM -RRB-. As such the host and transport managers are considered in the solution provided in the next section but could be omitted in the general case. Finally, depending on adaptation mode, MobJeX can have a centralised system controller incorporating a global adaptation engine for performing system wide optimisation. 5. SUMMARY AND CONCLUSIONS Given the challenges of developing mobile applications that run in dynamic/heterogeneous environments, and the subsequent interest in application adaptation, this paper has proposed and implemented an online metrics collection strategy to assist such adaptation using a mobile object framework and supporting middleware. Controlled lab studies were conducted to determine worst case performance, as well as show the reduction in collection overhead when applying simple collection criteria. In addition, further testing provided an initial indication of the characteristics of application objects -LRB- based on method execution time -RRB- that would be good candidates for adaptation using the worst case implementation of the proposed metrics collection strategy. A key feature of the solution was the specification of multiple configurable criteria to control the propagation of metrics through the system, thereby reducing collection overhead. Furthermore, such a temporal history could also facilitate intelligent decisions regarding the collection of metrics since for example a metric that is known to be largely constant need not be frequently measured. Future work will also involve the evaluation of a broad range of adaptation scenarios on the MobJeX framework to quantity the gains that can be made via adaptation through object mobility and thus demonstrate in practise, the efficacy of the solution described in this paper. Finally, the authors wish to explore applying the metrics collection concepts described in this paper to a more general and reusable context management system -LSB- 20 -RSB-.", "keyphrases": ["data", "object-orient applic", "mobil object framework", "mobjex", "java", "metricscontain", "metric collect", "proxi", "perform and scalabl", "measur", "propag and deliveri", "framework"]}
{"file_name": "H-26", "text": "A Support Vector Method for Optimizing Average Precision ABSTRACT Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision -LRB- MAP -RRB-, despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora -LRB- WT10g -RRB-, comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores. 1. INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision -LRB- MAP -RRB-. Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy -LSB- 17, 15 -RSB-, ROCArea -LSB- 1, 5, 10, 11, 13, 21 -RSB- or modifications of ROCArea -LSB- 4 -RSB-, and NDCG -LSB- 2, 3 -RSB-. Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance -LSB- 7 -RSB-. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft -LSB- 16 -RSB- and Caruana et al. -LSB- 6 -RSB-, our technique is computationally efficient while finding a globally optimal solution. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We have also developed a software package implementing our algorithm that is available for public user. 6. CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance. The learning framework used by our method is fairly general.", "keyphrases": ["machin learn", "rank retriev system", "learn techniqu", "mean averag precis", "optim solut", "relax of map", "inform retriev system", "probabl", "surrog measur", "loss function", "supervis learn"]}
{"file_name": "H-25", "text": "Term Feedback for Information Retrieval with Language Models ABSTRACT I n t hi s paper w e s t udy t er m - based f eedback f or i nf or mat i on r etrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top. 1. INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents -LSB- 25, 13 -RSB-. This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user 's assistance for query model construction, in the sense that the refined query model -LRB- based on terms -RRB- is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query `` Hubble telescope achievements '', when a relevant document talks more about the telescope 's repair than its discoveries, irrelevant terms such as `` spacewalk '' can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a -LRB- reasonable -RRB- number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms : he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. This is especially helpful for interactive adhoc search. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user 's information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6. Section 7 concludes this paper. 2. RELATED WORK Relevance feedback -LSB- 17, 19 -RSB- has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the user 's information need better than the original one, which is often just a short keyword query. A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback -LSB- 5, 16 -RSB- and usually still brings performance improvement. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance -LSB- 1, 23 -RSB-. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as -LSB- 12 -RSB-, there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. In many cases term relevance feedback has been found to effectively improve retrieval performance -LSB- 6, 22, 12, 4, 10 -RSB-. For example, the study in -LSB- 12 -RSB- shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit -LSB- 3, 14 -RSB-, even if the user likes interacting with expansion terms. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. Our work differs from the previous ones in two important aspects. The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 7. CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C 's performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top. We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment. Third, we have plans to incorporate term feedback into our UCAIR toolbar -LSB- 20 -RSB-, an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents.", "keyphrases": ["term-base feedback", "inform retriev", "languag model", "queri expans process", "queri model", "interact adhoc search", "retriev perform", "probabl", "kl-diverg", "present term"]}
{"file_name": "H-9", "text": "Learn from Web Search Logs to Organize Search Results ABSTRACT Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well : -LRB- 1 -RRB- the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user 's perspective ; and -LRB- 2 -RRB- the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by -LRB- 1 -RRB- learning `` interesting aspects '' of a topic from Web search logs and organizing search results accordingly ; and -LRB- 2 -RRB- generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels. 1. INTRODUCTION The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly. Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization. The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results ; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document. As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively -LSB- 9, 15, 26, 27, 28 -RSB-. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. However, this clustering strategy has two deficiencies which make it not always work well : First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the user 's perspective. But the clusters discovered by the current methods may partition the results into `` local codes '' and `` international codes. '' Such clusters would not be very useful for users ; even the best cluster would still have a low precision. Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem : -LRB- 1 -RRB- The clusters are not corresponding to a user 's interests, so their labels would not be very meaningful or useful. For example, the ambiguous query `` jaguar '' may mean an animal or a car. A cluster may be labeled as `` panthera onca. '' In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following : First, we will learn `` interesting aspects '' of similar topics from search logs and organize search results based on these `` interesting aspects ''. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as `` jaguar '' we can expect to see some clear clusters corresponding different senses of `` jaguar ''. Such aspects can be very useful for organizing future search results about `` car ''. Second, we will generate more meaningful cluster labels using past query words entered by users. Thus they can be better labels than those extracted from the ordinary contents of search results. To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm -LSB- 2 -RSB- to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster. We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches. The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7. 2. RELATED WORK Our work is closely related to the study of clustering search results. In -LSB- 9, 15 -RSB-, the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis -LSB- 20 -RSB- that relevant documents tend to form clusters. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm -LRB- STC -RRB- was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in -LSB- 28 -RSB-, supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In -LSB- 13 -RSB-, the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo -LSB- 22 -RSB-. However, in all these works, the clusters are generated solely based on the search results. Thus the obtained clusters do not necessarily reflect users ' preferences and the generated labels may not be informative from a user 's viewpoint. Methods of organizing search results based on text categorization are studied in -LSB- 6, 8 -RSB-. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query. Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions -LRB- FAQ -RRB- is studied in -LSB- 24, 4 -RSB-. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm -LSB- 2 -RSB-, which is a graph partition based approach, to learn interesting aspects from search logs given a new query. 7. CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users ' perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse. Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results. There are several interesting directions for further extending our work : First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user -LRB- e.g., the aspect chosen by a user to view -RRB-. It would thus be interesting to study how to further improve the organization of the results based on such feedback information. Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user.", "keyphrases": ["retriev model", "rank function", "ambigu", "cluster view", "meaning cluster label", "histori collect", "past queri", "clickthrough", "star cluster algorithm", "suffix tree cluster algorithm", "search result snippet", "monothet cluster algorithm", "pseudo-document", "pairwis similar graph", "similar threshold paramet", "centroid-base method", "cosin similar", "centroid prototyp", "reciproc rank", "log-base method", "mean averag precis"]}
{"file_name": "H-5", "text": "Utility-based Information Distillation Over Temporally Sequenced Documents ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs -LRB- ` tasks ' with multiple queries -RRB-. Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys -LRB- nuggets -RRB- were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components. 1. INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering -LRB- AF -RRB- is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples -LRB- if available -RRB-, an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications : adaptive filtering setup -- he or she reacts to the system only when the system makes a ` yes ' decision on a document, by confirming or rejecting that decision. A more ` active ' alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents -LRB- or passages -RRB- per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists. The latter form of user interaction has been highly effective in standard retrieval for ad hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3. System-selected documents are often highly redundant. A conventional AF system would select all these redundant news stories for user feedback, wasting the user 's time while offering little gain. Clearly, techniques for novelty detection can help in principle -LSB- 25, 2, 22 -RSB- for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to user 's -LRB- fine-grained -RRB- feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user. We call the new process utility-based information distillation. Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus -- the TDT4 collection of news stories and TV broadcasts -- with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1. To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG -LRB- Normalized Discounted Cumulated Gain -RRB- -LSB- 9 -RSB- for assessing the utility of ranked passages as a function of both relevance and novelty. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAF \u00b4 E -- CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives. 7. CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAF \u00b4 E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off.", "keyphrases": ["util-base inform distil", "tempor order document", "passag rank", "adapt filter", "ad-hoc retriev", "novelti detect", "new evalu methodolog", "answer kei", "nugget-match rule", "unifi framework", "ndcg metric"]}
{"file_name": "C-8", "text": "Operation Context and Context-based Operational Transformation ABSTRACT Operational Transformation -LRB- OT -RRB- is a technique for consistency maintenance and group undo, and is being applied to an increasing number of collaborative applications. The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems, as well as the quality of those solutions. The theory of causality has been the foundation of all prior OT systems, but it is inadequate to capture essential correctness requirements. Past research had invented various patches to work around this problem, resulting in increasingly intricate and complicated OT algorithms. After having designed, implemented, and experimented with a series of OT algorithms, we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems, reducing its complexity, and supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The COT algorithm is capable of supporting both do and undo of any operations at anytime, without requiring transformation functions to preserve Reversibility Property, Convergence Property 2, Inverse Properties 2 and 3. The COT algorithm is not only simpler and more efficient than prior OT control algorithms, but also simplifies the design of transformation functions. We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications. 1. INTRODUCTION Operational Transformation -LRB- OT -RRB- was originally invented for consistency maintenance in plain-text group editors -LSB- 4 -RSB-. To effectively and efficiently support existing and new applications, we must continue to improve the capability and quality of OT in solving both old and new problems. The soundness of the theoretical foundation for OT is crucial in this process. However, the theory of causality is inadequate to capture essential OT conditions for correct transformation. The limitation of the causality theory had caused correctness problems from the very beginning of OT. The dOPT algorithm was the first OT algorithm and was based solely on the concurrency relationships among operations -LSB- 4 -RSB- : a pair of operations are transformable as long as they are concurrent. However, later research discovered that the concurrency condition alone is not sufficient to ensure the correctness of transformation. Another condition is that the two concurrent operations must be defined on the same document state. This puzzle was solved in various ways, but the theory of causality as well as its limitation were inherited by all follow-up OT algorithms. The causality theory limitation became even more prominent when OT was applied to solve the undo problem in group editors. The concept of causality is unsuitable to capture the relationships between an inverse operation -LRB- as an interpretation of a meta-level undo command -RRB- and other normal editing operations. In fact, the causality relation is not defined for inverse operations -LRB- see Section 2 -RRB-. Various patches were invented to work around this problem, resulting in more intricate complicated OT algorithms -LSB- 18, 21 -RSB-. supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. First, we define causal-dependency / - independency and briefly describe their limitations in Section 2. Then, we present the key elements of the operation context theory, including the definition of operation context, context-dependency / - independency relations, context-based conditions, and context vectors in Section 3. In Section 4, we present the basic COT algorithm for supporting consistency maintenance -LRB- do -RRB- and group undo under the assumption that underlying transformation functions are able to preserve some important transformation properties. Then, these transformation properties and their pre-conditions are discussed in Section 5. The COT solutions to these transformation properties are presented in Section 6. Comparison of the COT work to prior OT work, OT correctness issues, and future work are discussed in Section 7. Finally, major contributions of this work are summarized in Section 8. 8. CONCLUSIONS We have contributed the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The theory of operation context is capable of capturing essential relationships and conditions for all types of operation in an OT system ; it provides a new foundation for better understanding and resolving OT problems. The COT algorithm provides uniformed solutions to both consistency maintenance and undo problems ; it is simpler and more efficient than prior OT control algorithms with similar capabilities ; and it significantly simplifies the design of transformation functions. The COT algorithm has been implemented in a generic collaboration engine and used for supporting a range of novel collaborative applications -LSB- 24 -RSB-. Real-world applications provide exciting opportunities and challenges to future OT research. The theory of operation context and the COT algorithm shall serve as new foundations for addressing the technical challenges in existing and emerging OT applications.", "keyphrases": ["oper transform", "cot", "context-base ot", "causal-depend", "concurr condit", "concurr relat", "invers oper", "document state", "origin oper", "transform oper", "invers cluster", "vector represent of oper context", "histori buffer", "exclus transform"]}
{"file_name": "J-4", "text": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions ABSTRACT Keyword auctions lie at the core of the business models of today 's leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google 's auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders ' values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword. 1. INTRODUCTION Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for * This work was done while the author was at Yahoo! Research. `` iPod '', the highest paying advertisers -LRB- for example, Apple or Best Buy -RRB- for that keyword may appear in a separate `` sponsored '' section of the page above or to the right of the algorithmic results. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions. Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture 's success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft 's MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. The search engine evaluates the advertisers ' bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine can not directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. Until recently, Yahoo! ranked bidders in decreasing order of advertisers ' stated values per click, while Google ranks in decreasing order of advertisers ' stated values per impression. We refer to these rules as `` rank-by-bid '' and `` rank-by-revenue '', respectively. ' We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank ` These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. ing rules where bidders are ranked in decreasing order of score eqb, where e denotes an advertiser 's click-through rate -LRB- normalized for position -RRB- and b his bid. Notice that q = 0 corresponds to Yahoo! 's rank-by-bid rule and q = 1 corresponds to Google 's rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz -LSB- 3 -RSB- and Varian -LSB- 11 -RSB-. We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! keyword auctions. We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders ' values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results. Related work. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement `` locally envy-free equilibrium '' and `` symmetric equilibrium '', respectively. Varian also provides some empirical analysis. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani -LSB- 1 -RSB-. That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar -LSB- 5 -RSB- study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting. They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always -LRB- weakly -RRB- dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations. Lahaie -LSB- 8 -RSB- gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture. 8. CONCLUSIONS In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo! and Google models as special cases. In practice, it should be very simple to move between rules within the family : this simply involves changing the exponent q applied to advertiser effects. We also showed that, in principle, the same effect could be obtained by using bidding credits. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices. On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders. In principle the minimum revenue in Nash equilibrium can be found by linear programming. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept. Larger classes of ranking rules could be relevant. For instance, it is possible to introduce discounts ds and rank according to wsbs \u2212 ds ; the equilibrium analysis generalizes to this case as well. With this larger class the virtual score can equal the score, e.g. in the case of a uniform marginal distribution over values. Figure 4 : Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1.", "keyphrases": ["revenu", "keyword auction", "revenu-optim rank", "rank rule", "search engin", "advertis", "sponsor search", "rank-by-bid", "rank-by-revenu", "profit", "advertis revenu", "price search keyword", "optim auction design problem"]}
{"file_name": "C-31", "text": "Apocrita : A Distributed Peer-to-Peer File Sharing System for Intranets ABSTRACT Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Another problem arises when a document is made available on a user 's machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets. 1. INTRODUCTION The Peer-to-Peer -LRB- P2P -RRB- computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8. 7. RELATED WORK Several decentralized P2P systems -LSB- 1, 2, 3 -RSB- exist today that Apocrita features some of their functionality. However, Apocrita also has unique novel searching and indexing features that make this system unique. For example, Majestic-12 -LSB- 4 -RSB- is a distributed search and indexing project designed for searching the Internet. Each user would install a client, which is responsible for indexing a portion of the web. A central area for querying the index is available on the Majestic-12 web page. The index itself is not distributed, only the act of indexing is distributed. The distributed indexing aspect of this project most closely relates Apocrita goals. YaCy -LSB- 6 -RSB- is a peer-to-peer web search application. YaCy is designed to maintain a distributed index of the Internet. It used a distributed hash table -LRB- DHT -RRB- to maintain the index. The local node is used to query but all results that are returned are accessible on the Internet. YaCy used many peers and DHT to maintain a distributed index. Apocrita will also use a distributed index in future implementations and may benefit from using an implementation of a DHT. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita. 8. CONCLUSIONS AND FUTURE WORK We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet. It can help organizations with no network file server or necessary network infrastructure to share documents. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems.", "keyphrases": ["peer-to-peer", "file share system", "intranet", "author", "document", "apocrita", "jxta", "distribut index", "peer-to-peer distribut model", "idl queri", "index file", "incom file", "p2p search"]}
{"file_name": "C-20", "text": "Live Data Center Migration across WANs : A Robust Cooperative Context Aware Approach ABSTRACT A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocate a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities. We make use of server virtualization technologies to enable the replication and migration of server functions. We propose new network functions to enable server migration and replication across wide area networks -LRB- e.g., the Internet -RRB-, and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives. 1. INTRODUCTION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. A relatively minor outage can disrupt and inconvenience a large number of users. Today these services are almost exclusively hosted in data centers. Recent advances in server virtualization technologies -LSB- 8, 14, 22 -RSB- allow for the live migration of services within a local area network -LRB- LAN -RRB- environment. In the LAN environment, these technologies have proven to be a very effective tool to enable data center management in a non-disruptive fashion. Not only can it support planned maintenance events -LSB- 8 -RSB-, but it can also be used in a more dynamic fashion to automatically balance load between the physical servers in a data center -LSB- 22 -RSB-. When using these technologies in a LAN environment, services execute in a virtual server, and the migration services provided by the underlying virtualization framework allows for a virtual server to be migrated from one physical server to another, without any significant downtime for the service or application. In particular, since the virtual server retains the same network address as before, any ongoing network level interactions are not disrupted. Similarly, in a LAN environment, storage requirements are normally met via either network attached storage -LRB- NAS -RRB- or via a storage area network -LRB- SAN -RRB- which is still reachable from the new physical server location to allow for continued storage access. Unfortunately in a wide area environment -LRB- WAN -RRB-, live server migration is not as easily achievable for two reasons : First, live migration requires the virtual server to maintain the same network address so that from a network connectivity viewpoint the migrated server is indistinguishable from the original. Second, while fairly sophisticated remote replication mechanisms have been developed in the context of disaster recovery -LSB- 20, 7, 11 -RSB-, these mechanisms are ill suited to live data center migration, because in general the available technologies are unaware of application/service level semantics. In this paper we outline a design for live service migration across WANs. Our design makes use of existing server virtualization technologies and propose network and storage mechanisms to facilitate migration across a WAN. The essence of our approach is cooperative, context aware migration, where a migration management system orchestrates the data center migration across all three subsystems involved, namely the server platforms, the wide area network and the disk storage system. While conceptually similar in nature to the LAN based work described above, using migration technologies across a wide area network presents unique challenges and has to our knowledge not been achieved. Our main contribution is the design of a framework that will allow the migration across a WAN of all subsystems involved with enabling data center services. We describe new mechanisms as well as extensions to existing technologies to enable this and outline the cooperative, context aware functionality needed across the different subsystems to enable this. 4. RELATED WORK Prior work on this topic falls into several categories : virtual machine migration, storage replication and network support. At the core of our technique is the ability of encapsulate applications within virtual machines that can be migrated without application downtimes -LSB- 15 -RSB-. As indicated earlier, these techniques assume that migration is being done on a LAN. VM migration has also been studied in the Shirako system -LSB- 10 -RSB- and for grid environments -LSB- 17, 19 -RSB-. Current virtual machine software support a suspend and resume feature that can be used to support WAN migration, but with downtimes -LSB- 18, 12 -RSB-. Recently live WAN migration using IP tunnels was demonstrated in -LSB- 21 -RSB-, where an IP tunnel is set up from the source to destination server to transparently forward packets to and from the application ; we advocate an alternate approach that assumes edge router support. An excellent description of these and others, as well as a detailed taxonomy of the different approaches for replication can be found in -LSB- 11 -RSB-. The Ursa Minor system argues that no single fault model is optimal for all applications and proposed supporting data-type specific selections of fault models and encoding schemes for replication -LSB- 1 -RSB-. In the context of network support, our work is related to the RouterFarm approach -LSB- 2 -RSB-, which makes use of orchestrated network changes to realize near hitless maintenance on provider edge routers. In addition to being in a different application area, our approach differs from the RouterFarm work in two regards. Second, due to the stringent timing requirements of live migration, we expect that our approach would require new router functionality -LRB- as opposed to being realizable via the existing configuration interfaces -RRB-. In a similar spirit to ROC, we advocate using mechanisms from live VM migration to storage replication to support planned and unplanned outages in data centers -LRB- rather than full replication to mask such failures -RRB-. 5. CONCLUSION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocated a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We sought to achieve high availability of data center services in the face of both planned and incidental outages of data center facilities. We advocated using server virtualization technologies to enable the replication and migration of server functions. We proposed new network functions to enable server migration and replication across wide area networks -LRB- such as the Internet or a geographically distributed virtual private network -RRB-, and finally showed the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "keyphrases": ["internet-base servic", "data center migrat", "wan", "lan", "virtual server", "storag replic", "synchron replic", "asynchron replic", "network support", "storag", "voic-over-ip", "voip", "databas"]}
{"file_name": "J-20", "text": "Clearing Algorithms for Barter Exchange Markets : Enabling Nationwide Kidney Exchanges ABSTRACT In barter-exchange markets, agents seek to swap their items with one another, in order to improve their own utilities. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. We focus mainly on the upcoming national kidney-exchange market, where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed. Long cycles are forbidden, since, for incentive reasons, all transplants in a cycle must be performed simultaneously. Also, in barter-exchanges generally, more agents are affected if one drops out of a longer cycle. We prove that the clearing problem with this cycle-length constraint is NP-hard. Solving it exactly is one of the main challenges in establishing a national kidney exchange. We present the first algorithm capable of clearing these markets on a nationwide scale. The key is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop techniques that dramatically improve both runtime and memory usage. We conclude that column generation scales drastically better than constraint generation. Our algorithm also supports several generalizations, as demanded by real-world kidney exchanges. Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges. The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted. 1. INTRODUCTION The role of kidneys is to filter waste from blood. Kidney failure results in accumulation of this waste, which leads to death in months. One treatment option is dialysis, in which the patient goes to a hospital to have his/her blood filtered by an external machine. Several visits are required per week, and each takes several hours. The quality of life on dialysis can be extremely low, and in fact many patients opt to withdraw from dialysis, leading to a natural death. Only 12 % of dialysis patients survive 10 years -LSB- 23 -RSB-. Instead, the preferred treatment is a kidney transplant. Kidney transplants are by far the most common transplant. Unfortunately, the demand for kidneys far outstrips supply. In the United States in 2005, 4,052 people died waiting for a life-saving kidney transplant. During this time, almost 30,000 people were added to the national waiting list, while only 9,913 people left the list after receiving a deceaseddonor kidney. For many patients with kidney disease, the best option is to find a living donor, that is, a healthy person willing to donate one of his/her two kidneys. In 2005, there were 6,563 live donations in the US. and his intended recipient are blood-type or tissue-type incompatible. In the past, the incompatible donor was sent home, leaving the patient to wait for a deceased-donor kidney. However, there are now a few regional kidney exchanges in the United States, in which patients can swap their incompatible donors with each other, in order to each obtain a compatible donor. These markets are examples of barter exchanges. In a barter-exchange market, agents -LRB- patients -RRB- seek to swap their items -LRB- incompatible donors -RRB- with each other. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. Barter exchanges are ubiquitous : examples include Peerflix -LRB- DVDs -RRB- -LSB- 11 -RSB-, Read It Swap It -LRB- books -RRB- -LSB- 12 -RSB-, and Intervac -LRB- holiday houses -RRB- -LSB- 9 -RSB-. For many years, there has even been a large shoe exchange in the United States -LSB- 10 -RSB-. People with different-sized feet use this to avoid having to buy two pairs of shoes. Leg amputees have a separate exchange to share the cost of buying a single pair of shoes. We can encode a barter exchange market as a directed graph G = -LRB- V, E -RRB- in the following way. Construct one vertex for each agent. Add a weighted edge e from one agent vi to another vj, if vi wants the item of vj. The weight we of e represents the utility to vi of obtaining vj 's item. A cycle c in this graph represents a possible swap, with each agent in the cycle obtaining the item of the next agent. The weight wc of a cycle c is the sum of its edge weights. An exchange is a collection of disjoint cycles. The weight of an exchange is the sum of its cycle weights. A social welfare maximizing exchange is one with maximum weight. Figure 1 illustrates an example market with 5 agents, -LCB- v1, v2,..., v5 -RCB-, in which all edges have weight 1. The market has 4 cycles, c1 = -LRB- v1, v2 -RRB-, c2 = -LRB- v2, v3 -RRB-, c3 = -LRB- v3, v4 -RRB- and c4 = -LRB- v1, v2, v3, v4, v5 -RRB-, and two -LRB- inclusion -RRB- maximal exchanges, namely M1 = -LCB- c4 -RCB- and M2 = -LCB- c1, c3 -RCB-. Exchange M1 has both maximum weight and maximum cardinality -LRB- i.e., it includes the most edges/vertices -RRB-. Figure 1 : Example barter exchange market. The clearing problem is to find a maximum-weight exchange consisting of cycles with length at most some small constant L. This cycle-length constraint arises naturally for several reasons. For example, in a kidney exchange, all operations in a cycle have to be performed simultaneously ; otherwise a donor might back out after his incompatible partner has received a kidney. Due to such resource constraints, the upcoming national kidney exchange market will likely allow only cycles of length 2 and 3. Another motivation for short cycles is that if the cycle fails to exchange, fewer agents are affected. For example, last-minute testing in a kidney exchange often reveals new incompatibilities that were not detected in the initial testing -LRB- based on which the compatibility graph was constructed -RRB-. In Section 3, we show that -LRB- the decision version of -RRB- the clearing problem is NP-complete for L > 3. One approach then might be to look for a good heuristic or approximation algorithm. However, for two reasons, we aim for an exact algorithm based on an integer-linear program -LRB- ILP -RRB- formulation, which we solve using specialized tree search. 9 First, any loss of optimality could lead to unnecessary patient deaths. 9 Second, an attractive feature of using an ILP formula tion is that it allows one to easily model a number of variations on the objective, and to add additional constraints to the problem. Or, if for various -LRB- e.g., ethical -RRB- reasons one requires a maximum cardinality exchange, one can at least in a second pass find the solution -LRB- out of all maximum cardinality solutions -RRB- that has the fewest 3-cycles. Other variations one can solve for include finding various forms of `` fault tolerant '' -LRB- non-disjoint -RRB- collections of cycles in the event that certain pairs that were thought to be compatible turn out to be incompatible after all. In this paper, we present the first algorithm capable of clearing these markets on a nationwide scale. Straight-forward ILP encodings are too large to even construct on current hardware -- not to talk about solving them. The key then is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop a host of -LRB- mainly problemspecific -RRB- techniques that dramatically improve both runtime and memory usage. 1.1 Prior Work Several recent papers have used simulations and marketclearing algorithms to explore the impact of a national kidney exchange -LSB- 13, 20, 6, 14, 15, 17 -RSB-. For example, using Edmond 's maximum-matching algorithm -LSB- 4 -RSB-, -LSB- 20 -RSB- shows that a national pairwise-exchange market -LRB- using length-2 cycles only -RRB- would result in more transplants, reduced waiting time, and savings of $ 750 million in heath care costs over 5 years. Those results are conservative in two ways. Firstly, the simulated market contained only 4,000 initial patients, with 250 patients added every 3 months. It has been reported to us that the market could be almost double this size. Secondly, the exchanges were restricted to length-2 cycles -LRB- because that is all that can be modeled as maximum matching, and solved using Edmonds 's algorithm -RRB-. Allowing length-3 cycles leads to additional significant gains. This has been demonstrated on kidney exchange markets with 100 patients by using CPLEX to solve an integer-program encoding of the clearing problem -LSB- 15 -RSB-. In this paper, we present an alternative algorithm for this integer program that can clear markets with over 10,000 patients -LRB- and that same number of willing donors -RRB-. Allowing cycles of length more than 3 often leads to no improvement in the size of the exchange -LSB- 15 -RSB-. -LRB- Furthermore, in a simplified theoretical model, any kidney exchange can be converted into one with cycles of length at most 4 -LSB- 15 -RSB-. -RRB- Whilst this does not hold for general barter exchanges, or even for all kidney exchange markets, in Section 5.2.3 we make use of the observation that short cycles suffice to dramatically increase the speed of our algorithm. At a high-level, the clearing problem for barter exchanges is similar to the clearing problem -LRB- aka winner determination problem -RRB- in combinatorial auctions. In both settings, the idea is to gather all the pertinent information about the agents into a central clearing point and to run a centralized clearing algorithm to determine the allocation. Both problems are NP-hard. Both are best solved using tree search techniques. Since 1999, significant work has been done in computer science and operations research on faster optimal tree search algorithms for clearing combinatorial auctions. However, the kidney exchange clearing problem -LRB- with a limit of 3 or more on cycle size -RRB- is different from the combinatorial auction clearing problem in significant ways. The most important difference is that the natural formulations of the combinatorial auction problem tend to easily fit in memory, so time is the bottleneck in practice. In contrast, the natural formulations of the kidney exchange problem -LRB- with L = 3 -RRB- take at least cubic space in the number of patients to even model, and therefore memory becomes a bottleneck much before time does when using standard tree search, such as branch-andcut in CPLEX, to tackle the problem. Therefore, the approaches that have been developed for combinatorial auctions can not handle the kidney exchange problem. 1.2 Paper Outline The rest of the paper is organized as follows. Section 2 discusses the process by which we generate realistic kidney exchange market data, in order to benchmark the clearing algorithms. Section 3 contains a proof that the market clearing decision problem is NP-complete. Sections 4 and 5 each contain an ILP formulation of the clearing problem. We also detail in those sections our techniques used to solve those programs on large instances. Section 6 presents experiments on the various techniques. Section 7 discusses recent fielding of our algorithm. Finally, we present our conclusions in Section 8, and suggest future research directions. 7. FIELDING THE TECHNOLOGY Our algorithm and implementation replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges, in December 2006. We conduct a match run every two weeks, and the first transplants based on our solutions have already been conducted. While there are -LRB- for political/inter-personal reasons -RRB- at least four kidney exchanges in the US currently, everyone understands that a unified unfragmented national exchange would save more lives. We are in discussions with additional kidney exchanges that are interested in adopting our technology. This way our technology -LRB- and the processes around it -RRB- will hopefully serve as a substrate that will eventually help in unifying the exchanges. At least computational scalability is no longer an obstacle. 8. CONCLUSION AND FUTURE RESEARCH In this work we have developed the most scalable exact algorithms for barter exchanges to date, with special focus on the upcoming national kidney-exchange market in which patients with kidney disease will be matched with compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. Our work presents the first algorithm capable of clearing these markets on a nationwide scale. It optimally solves the kidney exchange clearing problem with 10,000 donordonee pairs. The best prior technology -LRB- vanilla CPLEX -RRB- can not handle instances beyond about 900 donor-donee pairs because it runs out of memory. The key to our improvement is incremental problem formulation. We adapted two paradigms for the task : constraint generation and column generation. For each, we developed a host of techniques that substantially improve both runtime and memory usage. Some of the techniques use domain-specific observations while others are domain independent. We conclude that column generation scales dramatically better than constraint generation. Undoubtedly, further parameter tuning and perhaps additional speed improvement techniques could be used to make the algorithm even faster. Our algorithm also supports several generalizations, as desired by real-world kidney exchanges. Because we use an ILP methodology, we can also support a variety of side constraints, which often play an important role in markets in practice -LSB- 19 -RSB-. We can also support forcing part of the allocation, for example, `` This acutely sick teenager has to get a kidney if possible. '' Our work has treated the kidney exchange as a batch problem with full information -LRB- at least in the short run, kidney exchanges will most likely continue to run in batch mode every so often -RRB-. Two important directions for future work are to explicitly address both online and limited-information aspects of the problem. The online aspect is that donees and donors will be arriving into the system over time, and it may be best to not execute the myopically optimal exchange now, but rather save part of the current market for later matches.", "keyphrases": ["barter-exchang market", "match", "column gener", "kidnei", "transplant", "market characterist", "instanc gener", "solut approach", "edg formul", "cycl formul"]}
{"file_name": "H-2", "text": "Personalized Query Expansion for the Web ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user 's Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach. 1. INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are * Part of this work was performed while the author was visiting Yahoo! Research, Barcelona, Spain. inherently ambiguous. The query `` canon book '' for example covers several different areas of interest : religion, photography, literature, and music. Clearly, one would prefer search output to be aligned with user 's topic -LRB- s -RRB- of interest, rather than displaying a selection of popular URLs from each category. Studies have shown that more than 80 % of the users would prefer to receive such personalized search results -LSB- 33 -RSB- instead of the currently generic ones. Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. It has been shown to perform very well over large data sets, especially with short input queries -LRB- see for example -LSB- 19, 3 -RSB- -RRB-. This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the user 's Personal Information Repository -LRB- PIR -RRB-, i.e., the personal collection of text documents, emails, cached Web pages, etc.. Several advantages arise when moving Web search personalization down to the Desktop level -LRB- note that by `` Desktop '' we refer to PIR, and we use the two terms interchangeably -RRB-. First is of course the quality of personalization : The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user. Our algorithms expand Web queries with keywords extracted from user 's PIR, thus implicitly personalizing the search output. After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1. We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best. In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri. The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG -LSB- 15 -RSB- improvements of up to 51.28 %. In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. This yields an additional improvement of 8.47 % over the previously identified best algorithm. We conclude and discuss further work in Section 5. 2. PREVIOUS WORK This paper brings together two IR areas : Search Personalization and Automatic Query Expansion. There exists a vast amount of algorithms for both domains. In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components : -LRB- 1 -RRB- User profiles, and -LRB- 2 -RRB- The actual search algorithm. This section splits the relevant background according to the focus of each article into either one of these elements. Approaches focused on the User Profile. Sugiyama et al. -LSB- 32 -RSB- analyzed surfing behavior and generated user profiles as features -LRB- terms -RRB- of the visited pages. Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile. Qiu and Cho -LSB- 26 -RSB- used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank -LSB- 13 -RSB-. User profiling based on browsing history has the advantage of being rather easy to obtain and process. This is probably why it is also employed by several industrial search engines -LRB- e.g., Yahoo! MyWeb2 -RRB-. However, it is definitely not sufficient for gathering a thorough insight into user 's interests. Moreover, none of these investigated the adaptive application of personalization. Approaches focused on the Personalization Algorithm. Haveliwala -LSB- 13 -RSB- computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval. It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in put keywords before identifying the matching documents returned as output. In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms : -LRB- 1 -RRB- Relevance feedback, -LRB- 2 -RRB- Collection based co-occurrence statistics, and -LRB- 3 -RRB- Thesaurus information. Some other approaches are also addressed in the end of the section. Relevance Feedback Techniques. The main idea of Relevance Feedback -LRB- RF -RRB- is that useful information can be extracted from the relevant documents returned for the initial query. First approaches were manual -LSB- 28 -RSB- in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents. Efthimiadis -LSB- 11 -RSB- presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.. We used some of these as inspiration for our Desktop specific techniques. Chang and Hsu -LSB- 5 -RSB- asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary. RF has also been shown to be effectively automatized by considering the top ranked documents as relevant -LSB- 37 -RSB- -LRB- this is known as Pseudo RF -RRB-. Lam and Jones -LSB- 21 -RSB- used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query. Finally, Yu et al. -LSB- 38 -RSB- selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein. Co-occurrence Based Techniques. Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query -LSB- 17 -RSB-. We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository. Thesaurus Based Techniques. A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords. Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality -LSB- 36 -RSB-. We also use WordNet based expansion terms. However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords. Other Techniques. There are many other attempts to extract expansion terms. Though orthogonal to our approach, two works are very relevant for the Web environment : Cui et al. -LSB- 8 -RSB- generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs. Kraft and Zien -LSB- 19 -RSB- showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 5. CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the user 's Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to user 's interests, personalizing the search output. In this context, the paper includes the following contributions : \u2022 We proposed five techniques for determining expansion terms from personal documents. Each of them produces additional query keywords by analyzing user 's Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri. Figure 1 : Relative NDCG gain -LRB- in % -RRB- for each algorithm overall, as well as separated per query category. \u2022 We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios. We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28 %. \u2022 We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. \u2022 Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47 % over the previously identified best approach. We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms. We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data. Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms.", "keyphrases": ["short keyword queri", "web retriev", "web queri", "person inform repositori", "search output", "addit queri keyword", "granular level", "term and compound level analysi", "global co-occurr statist", "extern thesauru", "extens empir analysi", "ambigu queri", "qualiti", "output rank", "person search framework", "expans process", "variou featur of each queri", "adapt algorithm", "signific improv", "static expans approach"]}
{"file_name": "J-30", "text": "Implementation with a Bounded Action Space ABSTRACT While traditional mechanism design typically assumes isomorphism between the agents ' type - and action spaces, in many situations the agents face strict restrictions on their action space due to, e.g., technical, behavioral or regulatory reasons. We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces. Our contribution is threefold. First, we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies, and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost. Second, we identify necessary conditions for the optimality of action-bounded mechanisms, and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives. Finally, we prove that for any multilinear social-choice rule, the optimal mechanism with k actions incurs an expected loss of O -LRB- k21 -RRB- compared to the optimal mechanisms with unrestricted action spaces. Our results apply to various economic and computational settings, and we demonstrate their applicability to signaling games, public-good models and routing in networks. 1. INTRODUCTION Mechanism design is a sub-field of game theory that studies how to design rules of games resulting in desirable outcomes, when the players are rational. In a standard setting, players hold some private information -- their `` types '' -- and choose `` actions '' from their action spaces to maximize their utilities. The social planner wishes to implement a social-choice function, which maps each possible state of the world -LRB- i.e., a profile of the players ' types -RRB- to a single alternative. For example, a government that wishes to undertake a public-good project -LRB- e.g., building a bridge -RRB- only if the total benefit for the players exceeds its cost. Much of the literature on mechanism design restricts attention to direct revelation mechanisms, in which a player 's action space is identical to his type space. This focus is owing to the revelation principle that asserts that if some mechanism achieves a certain result in an equilibrium, the same result can be achieved in a truthful one -- an equilibrium where each agent simply reports his private type -LSB- 15 -RSB-. Nonetheless, in many environments, direct-revelation mechanisms are not viable since the actions available for the players have a limited expressive power. Consider, for example, the well-studied `` screening '' model, where an insurance firm wishes to sell different types of policies to different drivers based on their caution levels, which is their private information. There are various reasons for such strict restrictions on the action spaces. The buyers in such environemnts face only two actions -- to buy or not to buy -- although they may have an infinite number of possible values for the item. In many similar settings, players might be also reluctant to reveal their accurate types, but willing to disclose partial information about them. For example, agents will typically be unwilling to reveal their types, even if it is beneficial for them in the short run, since it might harm them in future transactions. Agents may also not trust the mechanism to keep their valuations private -LSB- 16 -RSB-, or not even know their exact type while computing it may be expensive -LSB- 12 -RSB-. Consider for example a public-good model : a social planner needs to decide whether to build a bridge. The two players in the game have some privately known benefits \u03b81, \u03b82 \u2208 -LSB- 0, 1 -RSB- from using this bridge. The social planner aims to build the bridge only if the sum of these benfits exceeds the construction cost of the bridge. The social planner can not access the private data of the players, and can only learn about it from the players ' actions. When direct revelation is allowed, the social planner can run the well-known VCG mechanism, where the players have incentives to report their true data ; hence, the planner can elicit the exact private information of the players and build the bridge only when it should be built. Assume now that the players can not send their entire secret data, but can only choose an action out of two possible actions -LRB- e.g., `` 0 '' or `` 1 '' -RRB-. Now, the social planner will clearly no longer be able to always build the bridge according to her objective function, due to the limited expressivness of the players ' messages. In this work we try to analyze what can be achieved in the presence of such restrictions. Restrictions on the action space, for specific models, were studied in several earlier papers. They studied single-item auctions where bidders are allowed to send messages with severely bounded size. They characterized the optimal mechanisms under this restriction, and showed that nearly optimal results can be achieved even with very strict limitations on the action space. Our work generalizes the main results of Blumrosen et al. to a general mechanism-design framework that can be applied to a multitude of models. A standard mechanism design setting is composed of agents with private information -LRB- their `` types '' -RRB-, and a social planner, who wishes to implement a social choice function, c -- a function that maps any profile of the agents ' types into a chosen alternative. A classic result in this setting says that under some monotonicity assumption on the agents ' preferences -- the `` single-crossing '' assumption -LRB- see definition below -RRB- -- a social-choice function is implementable in dominant strategies if and only if it is monotone in the players ' types. However, in environments with restricted action spaces, the social planner can not typically implement every social-choice function due to inherent informational constraints. That is, for some realizations of the players ' types, the decision of the social planner will be incompatible with the social-choice function c. In order to quantitatively measure how well bounded-action mechanisms can approximate the original social-choice functions, we follow a standard assumption that the social choice function is derived from a social-value function, g, which assigns a real value for every alternative A and realization of the players ' types. The social-choice function c will therefore choose an alternative that maximizes the social value function, given the type \u2192 \u2212 \u03b8 = -LRB- \u03b81,. . Observe that the social-value function is not necessarily the social welfare function -- the social welfare function is a special case of g in which g is defined to be the sum of the players ' valuations for the chosen alternative. Following are several simple examples of social-value functions : \u2022 Public goods. A government wishes to build a bridge only if the sum of the benefits that agents gain from it exceeds its construction cost C. The social value functions in a 2-player game will therefore be : g -LRB- \u03b81, \u03b82, `` build '' -RRB- = \u03b81 + \u03b82-C and g -LRB- \u03b81, \u03b82, `` do not build '' -RRB- = 0. \u2022 Routing in networks. Consider a network that is composed of two links in parallel. Each link has a secret probability pi of transferring a message successfully. A sender wishes to send his message through the network only if the probability of success is greater than, say, 90 percent - the known probability in an alternate network. \u2022 Single-item auctions. Consider a 2-player auction, where the auctioneer wishes to allocate the item to the player who values it the most. The social choice function is given by : g -LRB- \u03b81, \u03b82, `` player 1 wins '' -RRB- = \u03b81 and for the second alternative is g -LRB- \u03b81, \u03b82, `` player 2 wins '' -RRB- = \u03b82. 1.1 Our Contribution In this paper, we present a general framework for the study of mechanism design in environments with a limited number of actions. We assume a Bayesian model where players have one-dimensional private types, independently distributed on some real interval. The main question we ask is : when agents are only allowed to use k different actions, which mechanisms achieve the optimal expected social-value? Note that this question is actually composed of two separate questions. The first question is an information-theoretic question : what is the optimal result achievable when the players can only reveal information using these k actions -LRB- recall that their type space may be continuous -RRB-. The other question involves gametheoretic considerations : what is the best result achievable with k actions, where this result should be achieved in a dominant-strategy equilibrium. These questions raise the question about the `` price of implementation '' : can the optimal information-theoretic result always be implemented in a dominant-strategy equilibrium? And if not, to what extent does the dominant-strategy requirement degrades the optimal result? Our first contribution is the characterization of sufficient conditions for implementing the optimal informationtheoretic social-choice rule in dominant strategies. We show that for the family of multilinear social-value functions -LRB- that Theorem : Given any multilinear single-crossing socialvalue function, and for any number of alternatives and players, the social choice rule that is information-theoretically optimal is implementable in dominant strategies. Multilinear social-value functions capture many important and well-studied models, and include, for instance, the routing example given above, and any social welfare function in which the players ' valuations are linear in their types -LRB- such as public-goods and auctions -RRB-. The implementability of the information-theoretically optimal mechanisms enables us to use a standard routine in Mechanism Design and first determine the optimal socialchoice rule, and then calculate the appropriate payments that ensure incentive compatibility. To show this result, we prove a useful lemma that gives another characterization for social-choice functions whose `` price of implementation '' is zero. We show that for any social-choice function, incentive compatibility in action-bounded mechanisms is equivalent to the property that the optimal expected social value is achieved with non-decreasing strategies -LRB- or threshold strategies -RRB-.1 In other words, this lemma implies that one can always implement, with dominant strategies, the best socialchoice rule that is achievable with non-decreasing strategies. Our second contribution is in characterizing the optimal action-bounded mechanisms. We identify some necessary conditions for the optimality of mechanisms in general, and using these conditions, we fully characterize the optimal mechanisms in environments with two players and two alternatives. We complete the characterization of the optimal mechanisms with the depiction of the optimal strategies -- strategies that are `` mutually maximizers ''. Since the payments in a dominantstrategy implementation are uniquely defined by a monotone allocation and a profile of strategies, this also defines the payments in the mechanism. We give an intuitive proof for the optimality of such strategies, generalizing the concept of optimal `` mutually-centered '' strategies from -LSB- 4 -RSB-. Surprisingly, as opposed to the optimal auctions in -LSB- 4 -RSB-, for some non-trivial social-value functions, the optimal `` diagonal '' mechanism may not utilize all the k available actions. Theorem : For any multilinear single-crossing social-value function over two alternatives, the informationally optimal 2-player k-action mechanism is diagonal, and the optimal dominant strategies are mutually-maximizers. Achieving a full characterization of the optimal actionbounded mechanism for multi-player or multi-alternative environments seems to be harder. To support this claim, we observe that the number of mechanisms that satisfy the necessary conditions above is growing exponentially in the number of players. 1The restriction to non-decreasing strategies is very common in the literature. One remarkable result by Athey -LSB- 1 -RSB- shows that when a non-decreasing strategy is a best response for any other profile of non-decreasing strategies, a pure Bayesian-Nash equilibrium must exist. Our next result compares the expected social-value in k-action mechanisms to the optimal expected social value when the action space is unrestricted. For any number of players or alternatives, and for any profile of independent distribution functions, we construct mechanisms that are nearly optimal -- up to an additive difference of O -LRB- k21 -RRB-. This result is achieved in dominant strategies. Theorem : For any multilinear social-value function, the optimal k-action mechanism incurs an expected social loss of O -LRB- k21 -RRB-. Note that there are social-choice functions that can be implemented with k actions with no loss at all -LRB- for example, the rule `` always choose alternative A '' -RRB-. However, we know that in some settings -LRB- e.g., auctions -LSB- 5 -RSB- -RRB- the optimal loss may be proportional to 1k2, thus a better general upper bound is impossible. Finally, we present our results in the context of several natural applications. First, we give an explicit solution for a public-good game with k-actions. This is a natural application in our context since education levels are often discrete -LRB- e.g., B.A, M.A and PhD -RRB-. The latter example illustrates how our results apply to settings where the goal of the social planner is not welfare maximization -LRB- nor variants of it like `` affine maximizers '' -RRB-. The rest of the paper is organized as follows : our model and notations are described in Section 2. We then describe our general results regarding implementation in multi-player and multi-alternative environments in Section 3, including the asymptotic analysis of the social-value loss. In Section 4, we fully characterize the optimal mechanisms for 2player environments with two alternative. In Section 5, we conclude with applying our general results to several wellstudied models. 5. EXAMPLES Our results apply to a variety of economic, computational and networked settings. In this section, we demonstrate the applicability of our results to public-good models, signaling games and routing applications. 5.1 Application 1 : Public Goods The public-good model deals with a social planner -LRB- e.g., government -RRB- that needs to decide whether to supply a public good, such as building a bridge. Let Yes and No denote the respective alternatives of building and not building the bridge. v = v1,..., vn is the vector of the players ' types -- the values they gain from using the bridge. The decision that maximizes the social welfare is to build the bridge if and only if P is built, the social welfare is P i vi is greater than its cost, denoted by C. The utility of player i under payment pi is ui = vi -- pi if the bridge is built, and 0 otherwise. It is well-known that under no restriction on the action space, it is possible to induce truthful revelation by VCG mechanisms, therefore full efficiency can be achieved. Obviously, when the action set is limited to k actions, we can not achieve full efficiency due to the informational constraints. Hence, the information-theoretically optimal kaction mechanism is implementable in dominant strategies. Moreover, as Theorem 3 suggests, in the k-action 2-player public-good game, we can fully characterize the optimal mechanisms. In the proof of Theorem 3, we saw that when for both players g -LRB- \u03b8i, \u03b8i, A -RRB- = g -LRB- \u03b8i, \u03b8i, B -RRB-, the mechanism is non-degenerate with respect to both players.6 This condition clearly holds here -LRB- 1 + 0 -- C = 0 + 1 -- C -RRB-, therefore the optimal mechanisms will use all k actions. 1. Allocation : Build the bridge if j '' b1 + b2 > k. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1, 2. Allocation : Build the bridge if j '' b1 + b2 > k -- 1. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1 : Recall that we define the optimal mechanisms by their allocation scheme and by the optimal strategies for the players. It is well known, that the allocation scheme in monotone mechanisms uniquely defines the payments that ensure incentive-compatibility. In public-good games, these payments satisfy the rule that a player pays his lowest value for which the bridge is built, when the action of the other player is fixed. Therefore, the payments for the players 1 and 2 reporting the actions b1 and b2 are as follows : in mechanism 1 from Proposition 3, p1 = xb2 and p2 = yb1 ; in mechanism 2 from Proposition 3, p1 = xb2 -- 1 and p2 = yb1 -- 1. We now show a more specific example that assumes uniform distributions. The example shows how the optimal mechanism is determined by the cost C : for low costs, mechanism of type 1 is optimal, and for high costs the optimal mechanism is of type 2. An additional interesting feature of the optimal mechanisms in the example is that they are symmetric with respect to the players. This come as opposed to the optimal mechanisms in the auction model -LSB- 5 -RSB- that are asymmetric -LRB- even when the players ' values are drawn from identical distributions -RRB-. Figure 2 : Optimal mechanisms in a 2-player, 2-alternative, 2-action public-goods game, when the types are uniformly distributed in -LSB- 0, 1 -RSB-. The mechanism on the left is optimal when C < 1 and the other is optimal when C > 1. EXAMPLE 1. Suppose that the types of both players are uniformly distributed on -LSB- 0, 1 -RSB-. Figure 2 illustrates the optimal mechanisms for k = 2, and shows how both the allocation scheme and the payments depend on the construction cost C. Then, the welfare-maximizing mechanisms are : 5.2 Application 2 : Signaling We now study a signaling model in labor markets. In this model, the type of each worker, \u03b8i \u2208 -LSB- \u03b8, \u03b8 -RSB-, describes the worker 's productivity level. The firm wants to make her hiring decisions according to a decision function f -LRB- \u2212 \u2192 \u03b8 -RRB-. For example, the firm may want to hire the most productive worker -LRB- like the auction model -RRB-, or hire a group of workers only if their sum of productivities is greater than some threshold -LRB- similar to the public-good model -RRB-. However, the worker 's productivity is invisible to the firm ; the firm only observes the worker 's education level e that should convey signals about her productivity level. Note that the assumption here is that acquiring education, at any level, does not affect the productivity of the worker, but only signals about the worker 's skills. A main component in this model, is the fact that as the worker is more productive, it is easier for him to acquire high-level education. In addition, the cost of acquiring education increases with the education level. More formally, a continuous function C -LRB- e, \u03b8 -RRB- describes the cost to a worker from acquiring each education level as a function of his productivity. An action for a worker in this game is the education level he chooses to acquire. In standard models, this action space is continuous, and then a `` fully separating equilibrium '' exists -LRB- under the single-crossing conditions on the cost function -RRB-. That is, there exists an equilibrium in which every type is mapped into a different education level ; thus, the firm can induce the exact productivity levels of the workers by this signaling mechanism. However, it is hard to imagine a world with a continuum of education levels. It is usually the case that there are only several discrete education levels -LRB- e.g., BSc, MSc, PhD -RRB-. With k education levels, the firm may not be able to exactly follow the decision function f. For achieving the best result in k actions, the firm may want the workers to play according to specific threshold strategies. It turns out that the standard condition, the single-crossing condition on the cost function, suffices for ensuring that these threshold strategies will be dominant for the players. COROLLARY 4. Consider a multilinear decision function f, and a single-crossing cost function for the players. With k education levels, the firm can implement in dominant strategies a decision function that incurs a loss of O -LRB- k21 -RRB- compared with the decision function f. 5.3 Application 3 : Routing In our last example, we show the applicability of our results to routing in lossy networks. In such systems, a sender needs to decide through which network to transmit his message. In this example, we focus on parallel-path networks. The edges in these networks are controlled by different selfish agents, and each edge appears only in one of the networks. Suppose that the sender, who wishes to send a message from the source to the sink, knows the topology of each network, but the probability of success on each link, pi, is the link 's private information. The problem of the sender is to decide whether to send a message through the network N1 or through an alternate network N2. Obviously, the sender wishes to send the message through N1 only if the total probability of success in N1 is greater than the success probability in N2. Let f N -LRB- \u2212 \u2192 p -RRB- denote the probability of success in network N with a successprobability vector \u2192 \u2212 p. The social choice function in this example is thus : c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figure 3 : An example for a parallel-path network, where each link has a probability pi for transmission success. We show that the overall probability of success in such networks is multilinear in pi, and thus the optimal k-action social-choice function is dominant-strategy implementable. In this example, we assume that every agent has a singlecrossing valuation function over the alternatives. That is, each player wishes that the message will be sent through his network, and his benefit is positively correlated with his secret data -LRB- e.g., the valuation of player i may be exactly pi -RRB-. We would like to emphasize that the social planner in this example -LRB- the sender -RRB- does not aim to maximize the social welfare. That is, the social value is not the sum of the players ' types nor any weighted sum of the types -LRB- `` affine maximizer '' -RRB-. The success probability of sending a message through a parallel-path network is multilinear, since it can be expressed by the following multilinear formula -LRB- where P denotes the set of all paths between the source and the sink -RRB- : Note that for every link i, the partial derivative in pi of the success probability written in Equation 3 is positive. In all the other networks, that do not contain link i, the partial derivative is clearly zero. Therefore, the social-value function is single crossing and our general results can be applied. COROLLARY 5. For any social-choice function that maximizes the success probability over parallel-path networks, the informationally optimal k-action social-choice function is implementable -LRB- for any k -RRB-. Acknowledgment. The work of the second author is also supported by the Lady Davis Trust Fellowship.", "keyphrases": ["bound action space", "implement", "domin strategi", "social-choic function", "decis function", "singl-cross condit", "multilinear function", "optim mechan", "action-bound mechan", "probabl of success"]}
{"file_name": "C-19", "text": "Service Interface : A New Abstraction for Implementing and Composing Protocols * ABSTRACT In this paper we compare two approaches to the design of protocol frameworks -- tools for implementing modular network protocols. The most common approach uses events as the main abstraction for a local interaction between protocol modules. We argue that an alternative approach, that is based on service abstraction, is more suitable for expressing modular protocols. It also facilitates advanced features in the design of protocols, such as dynamic update of distributed protocols. We then describe an experimental implementation of a service-based protocol framework in Java. 1. INTRODUCTION They allow complex protocols to be implemented by decomposing them into several modules cooperating together. This approach facilitates code reuse and customization of distributed protocols in order to fit the needs of different applications. Moreover, protocol modules can be plugged in to the system dynamically. All these features of protocol frameworks make them an interesting enabling technology for implementing adaptable systems -LSB- 14 -RSB- - an important class of applications. Most protocol frameworks are based on events -LRB- all frameworks cited above are based on this abstraction -RRB-. Events are used for asynchronous communication between different modules on the same machine. For instance, the composition of modules may require connectors to route events, which introduces burden for a protocol composer -LSB- 4 -RSB-. Protocol frameworks such as Appia and Eva extend the event-based approach with channels. However, in our opinion, this solution is not satisfactory since composition of complex protocol stacks becomes more difficult. In this paper, we propose a new approach for building modular protocols, that is based on a service abstraction. We compare this new approach with the common, event-based approach. We show that protocol frameworks based on services have several advantages, e.g. allow for a fairly straightforward protocol composition, clear implementation, and better support of dynamic replacement of distributed protocols. To validate our claims, we have implemented SAMOA -- an experimental protocol framework that is purely based on the service-based approach to module composition and implementation. The framework allowed us to compare the service - and event-based implementations of an adaptive group communication middleware. Section 2 defines general notions. Section 3 presents the main characteristics of event-based frameworks, and features that are distinct for each framework. Section 4 describes our new approach, which is based on service abstraction. Section 5 discusses the advantages of a service-based protocol framework compared to an event-based protocol framework. The description of our experimental implementation is presented in Section 6. Finally, we conclude in Section 7. 7. CONCLUSION In the paper, we proposed a new approach to the protocol composition that is based on the notion of Service Interface, instead of events. We believe that the service-based framework has several advantages over event-based frameworks. A prototype implementation allowed us to validate our ideas.", "keyphrases": ["protocol framework", "distribut algorithm", "distribut system", "servic interfac", "network", "commun", "event-base framework", "stack", "modul", "request", "repli"]}
{"file_name": "I-21", "text": "Interactions between Market Barriers and Communication Networks in Marketing Systems ABSTRACT We investigate a framework where agents search for satisfying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents. Local behaviour incorporates a satisficing model of choice, a set of rules governing the interactions between agents, including learning about the trustworthiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products. We use various degree distributions dictating the extent of connectivity, and incorporate both small-world effects and the notion of preferential attachment in our network models. We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effectiveness changes with the imposition of market barriers. 1. INTRODUCTION Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product -LSB- 14 -RSB- and the availability of more attractive alternatives -LSB- 17 -RSB-. However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves -LRB- in the guise of formal or informal contracts -RRB-, or other so-called ` switching costs ' or market barriers -LSB- 12, 5 -RSB-. The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. While the level of satisfaction with a currently-used product will largely be a function of one 's own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 2. BACKGROUND 2.1 Word-of-mouth communication The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. Simulation-based investigations of wordof-mouth -LSB- 6, 13 -RSB- have focused on developing strategies for ensuring that a system reaches an equilibrium level where all agents are satisfied, largely by learning about the effectiveness of others ' referrals or by varying the degree of inertia in individual behaviour. The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in -LSB- 6 -RSB- in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects. 2.2 Market barriers The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. A useful typology of market barriers distinguishes ` transactional ' barriers associated with the monetary cost of changing -LRB- e.g. in financial services -RRB-, ` learning ' barriers associated with deciding to replace well-known existing products, and ` contractual ' barriers imposing legal constraints for the term of the contract -LSB- 12 -RSB-. A different typology -LSB- 5 -RSB- introduces the additional aspect of ` relational ' barriers arising from personal relationships that may be interwoven with the use of a particular product. There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. Burnham et al. -LSB- 5 -RSB- find that perceived market barriers account for nearly twice the variance in intention to stay with a product than that explained by satisfaction with the product -LRB- 30 % and 16 % respectively -RRB-, and that so-called relational barriers are considerably more influential than either transactional or learning barriers. Further, they find that switching costs are perceived by consumers to exist even in markets which are fluid and where barriers would seem to be weak. Simply put, market barriers appear to play a greater role in what people do than satisfaction ; and their presence may be more pervasive than is generally thought. 5. CONCLUSIONS AND RELATED WORK Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components : networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world. It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving. In poorly-connected networks, the effect of barriers on market shares is slight. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share. Products with small market share -LRB- which, in the context of our simulations, is generally due to the product offering poor performance -RRB- are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case. Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two ` modules ' comprising network structure and local behaviour. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions.", "keyphrases": ["referr system", "purchas behaviour", "word-of-mouth commun", "market system", "defect behaviour", "psycholog affin", "switch behaviour", "agent-base model", "social psycholog", "market barrier", "consum choic", "switch cost"]}
{"file_name": "C-34", "text": "Researches on Scheme of Pairwise Key Establishment for Distributed Sensor Networks ABSTRACT Security schemes of pairwise key establishment, which enable sensors to communicate with each other securely, play a fundamental role in research on security issue in wireless sensor networks. A new kind of cluster deployed sensor networks distribution model is presented, and based on which, an innovative Hierarchical Hypercube model - H -LRB- k, u, m, v, n -RRB- and the mapping relationship between cluster deployed sensor networks and the H -LRB- k, u, m, v, n -RRB- are proposed. By utilizing nice properties of H -LRB- k, u, m, v, n -RRB- model, a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed, which combines the idea of KDC -LRB- Key Distribution Center -RRB- and polynomial pool schemes. Furthermore, the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected. Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key, compared with previous related works. 1. INTRODUCTION Security communication is an important requirement in many sensor network applications, so shared secret keys are used between communicating nodes to encrypt data. As one of the most fundamental security services, pairwise key establishment enables the sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the sensor nodes ' limited computational capabilities, battery energy, and available memory, it is not feasible for them to use traditional pairwise key establishment techniques such as public key cryptography and key distribution center -LRB- KDC -RRB-. Several alternative approaches have been developed recently to perform pairwise key establishment on resource-constrained sensor networks without involving the use of traditional cryptography -LSB- 14 -RSB-. Eschenauer and Gligor proposed a basic probabilistic key predistribution scheme for pairwise key establishment -LSB- 1 -RSB-. In the scheme, each sensor node randomly picks a set of keys from a key pool before the deployment so that any two of the sensor nodes have a certain probability to share at least one common key. Chan et al. further extended this idea and presented two key predistribution schemes : a q-composite key pre-distribution scheme and a random pairwise keys scheme. The q-composite scheme requires any two sensors share at least q pre-distributed keys. The random scheme randomly picks pair of sensors and assigns each pair a unique random key -LSB- 2 -RSB-. Based on such a framework, they presented two pairwise key pre-distribution schemes : a random subset assignment scheme and a grid-based scheme. A polynomial pool is used in those schemes, instead of using a key pool in the previous techniques. The random subset assignment scheme assigns each sensor node the secrets generated from a random subset of polynomials in the polynomial pool. The gridbased scheme associates polynomials with the rows and the columns of an artificial grid, assigns each sensor node to a unique coordinate in the grid, and gives the node the secrets generated from the corresponding row and column polynomials. Based on this grid, each sensor node can then identify whether it can directly establish a pairwise key with another node, and if not, what intermediate nodes it can contact to indirectly establish the pairwise key. A similar approach to those schemes described by Liu et al was independently developed by Du et a. -LSB- 5 -RSB-. Rather than on Blundo 's scheme their approach is based on Blom 's scheme -LSB- 6 -RSB-. All of those schemes above improve the security over the basic probabilistic key pre-distribution scheme. However, the pairwise key establishment problem in sensor networks is still not well solved. For the basic probabilistic and the q-composite key predistribution schemes, as the number of compromised nodes increases, the fraction of affected pairwise keys increases quickly. As a result, a small number of compromised nodes may affect a large fraction of pairwise keys -LSB- 3 -RSB-. Though the random pairwise keys scheme doses not suffer from the above security problem, it incurs a high memory overhead, which increases linearly with the number of nodes in the network if the level of security is kept constant -LSB- 2 -RSB- -LSB- 4 -RSB-. For the random subset assignment scheme, it suffers higher communication and computation overheads. In 2004, Liu proposed a new hypercube-based pairwise key predistribution scheme -LSB- 7 -RSB-, which extends the grid-based scheme from a two dimensional grid to a multi-dimensional hypercube. The analysis shows that hypercube-based scheme keeps some attractive properties of the grid-based scheme, including the guarantee of establishing pairwise keys and the resilience to node compromises. Also, when perfect security against node compromise is required, the hypercube-based scheme can support a larger network by adding more dimensions instead of increasing the storage overhead on sensor nodes. Though hypercube-based scheme -LRB- we consider the grid-based scheme is a special case of hypercube-based scheme -RRB- has many attractive properties, it requires any two nodes in sensor networks can communication directly with each other. This strong assumption is impractical in most of the actual applications of the sensor networks. In this paper, we present a kind of new cluster-based distribution model of sensor networks, and for which, we propose a new pairwise key pre-distribution scheme. Based on the topology, we propose a novel cluster distribution based hierarchical hypercube model to establish the pairwise key. We develop a kind of new pairwise key establishment algorithm with our hierarchical hypercube model. The structure of this paper is arranged as follows : In section 3, a new distribution model of cluster deployed sensor networks is presented. In section 4, a new Hierarchical Hypercube model is proposed. In section 5, the mapping relationship between the clusters deployed sensor network and Hierarchical Hypercube model is discussed. In section 6 and section 7, new pairwise key establishment algorithm are designed based on the Hierarchical Hypercube model and detailed analyses are described. Finally, section 8 presents a conclusion. 2. PRELIMINARY Definition 1 -LRB- Key Predistribution -RRB- : The procedure, which is used to encode the corresponding encryption and decryption algorithms in sensor nodes before distribution, is called Key Predistribution. Definition 2 -LRB- Pairwise Key -RRB- : For any two nodes A and B, if they have a common key E, then the key E is called a pairwise key between them. 8. CONCLUSION A new hierarchical hypercube model named H -LRB- k, u, m, v, n -RRB- is proposed, which can be used for pairwise key predistribution for cluster deployed sensor networks. And Based on the H -LRB- k, u, m, v, n -RRB- model, an innovative pairwise key predistribution scheme and algorithm are designed respectively, by combing the good properties of the Polynomial Key and Key Pool encryption schemes. So, the traditional pairwise key predistribution algorithm based on hypercube model -LSB- 7 -RSB- is only a special case of the new algorithm proposed in this paper. Theoretical and experimental analyses show that the newly proposed algorithm is an efficient pairwise key establishment algorithm that is suitable for the cluster deployed sensor networks.", "keyphrases": ["sensor network", "kei pool", "kei predistribut", "hierarch hypercub model", "secur", "pairwis kei establish algorithm", "cluster-base distribut model", "polynomi kei", "encrypt", "node code", "high fault-toler"]}
{"file_name": "H-32", "text": "Interesting Nuggets and Their Impact on Definitional Question Answering ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a `` Human Interest Model '' from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering. 1. DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003. The Definition questions, also called Other questions in recent years, are defined as follows. Given a question topic X, the task of a definitional QA system is akin to answering the question `` What is X? '' . The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic. Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as `` informative nuggets ''. Each informative nugget is a sentence fragment that describe some factual information about the topic. From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets can not simply be described as informative nuggets. Rather, these topic nuggets have a trivia-like quality associated with them. Typically, these are out of the ordinary pieces of information about a topic that can pique a human reader 's interest. For this reason, we decided to define answer nuggets that can evoke human interest as `` interesting nuggets ''. In essence, interesting nuggets answer the questions `` What is X famous for? '' , `` What defines X? '' . We now have two very different perspective as to what constitutes an answer to Definition questions. An answer can be some important factual information about the topic or some novel and interesting aspect about the topic. This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of `` George Foreman ''. Certain answer nuggets are more informative while other nuggets are more interesting in nature. Informative Nuggets - Became oldest world champion in boxing history. Interesting Nuggets - Returned to boxing after 10 yr hiatus. As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets. In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets. A '' Human Interest Model '' definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system. We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2. RELATED WORK There are currently two general methods for Definitional Question Answering. The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. -LSB- 1 -RSB- and Xu et al. -LSB- 14 -RSB-. Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets. For example, Xu et al. used 40 manually defined `` structured patterns '' in their 2003 definitional question answering system. Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created. A recent system by Harabagiu et al. -LSB- 6 -RSB- created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains. As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information. Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences. Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a person 's birthdate, or the name of a company 's CEO. However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations. This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities. For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being. Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets. This leads to the exploration of the second relevance-based approach that has been used in definitional question answering. Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets -LSB- 1 -RSB-. A similar approach has also been used as a baseline system for TREC 2003 -LSB- 14 -RSB-. More recently, Chen et al. -LSB- 3 -RSB- adapted a bi-gram or bi-term language model for definitional Question Answering. Generally, the relevance-based approach requires a `` definitional corpus '' that contain documents highly relevant to the topic. The baseline system in TREC 2003 simply uses the topic words as its definitional corpus. Blair-Goldensohn et al. -LSB- 1 -RSB- uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional. Chen et al. -LSB- 3 -RSB- collect snippets from Google to build its definitional corpus. From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected. This centroid vector or set of centroid words is taken to be highly indicative of the topic. Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic. BlairGoldensohn et al. -LSB- 1 -RSB- uses Cosine similarity to rank sentences by `` centrality ''. As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus. However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences. Thus such methods identify relevant sentences and not sentences containing definitional nuggets. Yet, the TREC 2003 baseline system -LSB- 14 -RSB- outperformed all but one other system. The bi-term language model -LSB- 3 -RSB- is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach. At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin -LSB- 7 -RSB-. We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords. This may explain why relevance-based method can perform competitively in definitional question answering. However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner. Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets. We will describe how we expand upon such methods to identify interesting nuggets in the next section. 7. CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets. Interesting nuggets are uncommon pieces of information about the topic that can evoke a human reader 's curiosity. The notion of an '' average human reader '' is an important consideration in our approach. This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering. Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings. Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems. We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers. What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic. Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic. Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features. As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model. We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers. Although the methods we used are simple, they have been shown experimentally to be effective. Our approach may also provide some insight into a few anomalies in past definitional question answering 's trials. For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets. We suspect the main contributor to the system 's performance Table 3 : TREC 2005 Topics Grouped by Entity Type In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers.", "keyphrases": ["us of linguist", "extern knowledg", "comput of human interest", "new corpu", "question topic", "inform nugget", "sentenc fragment", "human reader", "interest", "interest nugget", "uniqu qualiti", "surpris factor", "lexic pattern", "manual labor", "baselin system"]}
{"file_name": "I-4", "text": "Meta-Level Coordination for Solving Negotiation Chains in Semi-Cooperative Multi-Agent Systems ABSTRACT A negotiation chain is formed when multiple related negotiations are spread over multiple agents. In order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility, we present an extension to a singleagent concurrent negotiation framework. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. We introduce a pre-negotiation phase that allows agents to transfer meta-level information. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context. The agent can also use this information to allocate appropriate time for each negotiation, hence to find a good ordering of all related negotiations. The experimental data shows that these mechanisms improve the agents ' and the system 's overall performance significantly. 1. INTRODUCTION Sophisticated negotiation for task and resource allocation is crucial for the next generation of multi-agent systems -LRB- MAS -RRB- applications. Groups of agents need to efficiently negotiate over multiple related issues concurrently in a complex, distributed setting where there are deadlines by which the negotiations must be completed. This is an important research area where there has been very little work done. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. There is no single global goal in such systems, either because each agent represents a different organization/user, or because it is difficult/impossible to design one single global goal. This issue arises due to multiple concurrent tasks, resource constrains and uncertainties, and thus no agent has sufficient knowledge or computational resources to determine what is best for the whole system -LSB- 11 -RSB-. To accomplish tasks continuously arriving in the virtual organization, cooperation and sub-task relocation are needed and preferred. There is no single global goal since each agent may be involved in multiple virtual organizations. Meanwhile, the performance of each individual agent is tightly related to other agents ' cooperation and the virtual organization 's overall performance. The negotiation in such systems is not a zero-sum game, a deal that increases both agents ' utilities can be found through efficient negotiation. Additionally, there are multiple encounters among agents since new tasks are arriving all the time. In such negotiations, price may or may not be important, since it can be fixed resulting from a long-term contract. Other factors like quality and delivery time are important too. Reputation mechanisms in the system makes cheating not attractive from a long term viewpoint due to multiple encounters among agents. Another major difference between this work and other work on negotiation is that negotiation, here, is not viewed as a stand-alone process. Rather it is one part of the agent 's activity which is tightly interleaved with the planning, scheduling and executing of the agent 's activities, which also may relate to other negotiations. Based on this recognition, this work on negotiation is concerned more about the meta-level decision-making process in negotiation rather than the basic protocols or languages. the negotiations be performed. These macro-strategies are different from those micro-strategies that direct the individual negotiation thread, such as whether the agent should concede and how much the agent should concede, etc -LSB- 3 -RSB-. In this paper we extend a multi-linked negotiation model -LSB- 10 -RSB- from a single-agent perspective to a multi-agent perspective, so that a group of agents involved in chains of interrelated negotiations can find nearly-optimal macro negotiation strategies for pursuing their negotiations. Section 2 describes the basic negotiation process and briefly reviews a single agent 's model of multi-linked negotiation. Section 3 introduces a complex supply-chain scenario. Section 4 details how to solve those problems arising in the negotiation chain. Section 5 reports on the experimental work. Section 6 discusses related work and Section 7 presents conclusions and areas of future work. 2. BACKGROUND ON MULTI-LINKED NEGOTIATION This process can go back and forth until an agreement is reached or the agents decide to stop. If an agreement is reached and one agent can not fulfill the commitment, it needs to pay the other party a decommitment penalty as specified in the commitment. A negotiation starts with a proposal, which announces that a task -LRB- t -RRB- needs to be performed includes the following attributes : 1. deadline -LRB- dl -RRB- : the latest finish time of the task ; the task needs to be finished before the deadline dl. 3. minimum quality requirement -LRB- minq -RRB- : the task needs to be finished with a quality achievement no less than minq. 4. regular reward -LRB- r -RRB- : if the task is finished as the contract requested, the contractor agent will get reward r. 5. early finish reward rate -LRB- e -RRB- : if the contractor agent can finish the task earlier than dl, it will get the extra early finish reward proportional to this rate. 6. The multi-linked negotiation problem has two dimensions : the negotiations, and the subjects of negotiations. The negotiations are interrelated and the subjects are interrelated ; the attributes of negotiations and the attributes of the subjects are interrelated as well. This two-dimensional complexity of interrelationships distinguishes it from the classic project management problem or scheduling problem, where all tasks to be scheduled are local tasks and no negotiation is needed. 1. negotiation duration -LRB- \u03b4 -LRB- v -RRB- -RRB- : the maximum time allowed for negotiation v to complete, either reaching an agreed upon proposal -LRB- success -RRB- or no agreement -LRB- failure -RRB-. 2. negotiation start time -LRB- \u03b1 -LRB- v -RRB- -RRB- : the start time of negotiation v. \u03b1 -LRB- v -RRB- is an attribute that needs to be decided by the agent. 3. negotiation deadline -LRB- e -LRB- v -RRB- -RRB- : negotiation v needs to be finished before this deadline e -LRB- v -RRB-. The negotiation is no longer valid after time e -LRB- v -RRB-, which is the same as a failure outcome of this negotiation. 4. It depends on a set of attributes, including both attributes-in-negotiation -LRB- i.e. reward, flexibility, etc. -RRB- and attributes-of-negotiation -LRB- i.e. negotiation start time, negotiation deadline, etc. -RRB-. An agent involved in multiple related negotiation processes needs to reason on how to manage these negotiations in terms of ordering them and choosing the appropriate values for features. This is the multi-linked negotiation problem -LSB- 10 -RSB- : \u03c1 -LRB- v -RRB- -RRB-, which describes the relationship between negotiation v and its children. The AND relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires all its children nodes have successful accomplishments. The OR relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires at least one child node have successful accomplishment, where the multiple children nodes represent alternatives to accomplish the same goal. Multi-linked negotiation problem is a local optimization problem. To solve a multi-linked negotiation problem is to find a negotiation solution -LRB- 0, \u03d5 -RRB- with optimized expected utility Elf -LRB- 0, \u03d5 -RRB-, which is defined as : A negotiation ordering 0 defines a partial order of all negotiation issues. A feature assignment \u03d5 is a mapping function that assigns a value to each attribute that needs to be decided in the negotiation. A negotiation outcome \u03c7 for a set of negotiations -LCB- vj 1, -LRB- j = 1,..., n -RRB- specifies the result for each negotiation, either success or failure. There are a total of 2n different outcomes for n negotiations : -LCB- chii1, -LRB- i = 1,..., 2n -RRB-. P -LRB- \u03c7i, \u03d5 -RRB- denotes the probability of the outcome \u03c7i given the feature assignment \u03d5, which is calculated based on the success probability of each negotiation. The Sixth Intl.. Joint Conf. Figure 1 : A Complex Negotiation Chain Scenario A heuristic search algorithm -LSB- 10 -RSB- has been developed to solve the single agent 's multi-linked negotiation problem that produces nearly-optimal solutions. This algorithm is used as the core of the decision-making for each individual agent in the negotiation chain scenario. In the rest of the paper, we present our work on how to improve the local solution of a single agent in the global negotiation chain context. 6. RELATED WORK Fatima, Wooldridge and Jennings -LSB- 1 -RSB- studied the multiple issues in negotiation in terms of the agenda and negotiation procedure. However, this work is limited since it only involves a single agent 's perspective without any understanding that the agent may be part of a negotiation chain. Mailler and Lesser -LSB- 4 -RSB- have presented an approach to a distributed resource allocation problem where the negotiation chain scenario occurs. It models the negotiation problem as a distributed constraint optimization problem -LRB- DCOP -RRB- and a cooperative mediation mechanism is used to centralize relevant portions of the DCOP. In our work, the negotiation involves more complicated issues such as reward, penalty and utility ; also, we adopt a distribution approach where no centralized control is needed. A mediator-based partial centralized approach has been applied to the coordination and scheduling of complex task network -LSB- 8 -RSB-, which is different from our work since the system is a complete cooperative system and individual utility of single agent is not concerned at all. A combinatorial auction -LSB- 2, 9 -RSB- could be another approach to solving the negotiation chain problem. However, in a combinatorial auction, the agent does not reason about the ordering of negotiations. This would lead to a problem similar to those we discussed when the same-deadline policy is used. 7. CONCLUSION AND FUTURE WORK In this paper, we have solved negotiation chain problems by extending our multi-linked negotiation model from the perspective of a single agent to multiple agents. Instead of solving the negotiation chain problem in a centralized approach, we adopt a distributed approach where each agent has an extended local model and decisionmaking process. We have introduced a pre-negotiation phase that allows agents to transfer meta-level information on related negotiation issues. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing the appropriate negotiation solution. The experimental data shows that these mechanisms improve the agent 's and the system 's overall performance significantly. In future extension of this work, we would like to develop mechanisms to verify how reliable the agents are.", "keyphrases": ["multipl agent", "negoti framework", "negoti chain", "semi-cooper multi-agent system", "pre-negoti", "multi-link negoti", "agent", "distribut set", "multipl concurr task", "virtual organ", "sub-task reloc", "reput mechan", "complex suppli-chain scenario"]}
{"file_name": "I-14", "text": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems ABSTRACT The dominant existing routing strategies employed in peerto-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions. Specifically, agents maintain estimates on the downstream agents ' abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies. 1. INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems -LSB- 6, 13, 14, 15 -RSB-. In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents. In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches -LSB- 6, 13, 14, 15 -RSB-. While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors. Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms. In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms. Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' capabilities of providing relevant documents for specific types of incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. This process is conducted in an iterative manner. The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents. This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time. Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network -LRB- agent organization -RRB- based on the contentsimilarity measure among agents ' document collections in a bottom-up fashion. In the past work, we have shown that this organization improves search performance significantly. The intention of the reinforcement learning is to adapt the agents ' routing decisions to the dynamic network situations and learn from past search sessions. Specifically, the contributions of this paper include : -LRB- 1 -RRB- a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents ; -LRB- 2 -RRB- two strategies to speed up the learning process. The remainder of this paper is organized as follows : Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology. Section 3 describes a reinforcement learning based approach to direct the routing process ; Section 4 details the experimental settings and analyze the results. Section 5 discusses related studies and Section 6 concludes the paper. 5. RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks. In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process. IP-level Routing problems have been attacked from the reinforcement learning perspective -LSB- 2, 5, 11, 12 -RSB-. There are two major classes of adaptive, distributed packet routing algorithms in the literature : distance-vector algorithms and link-state algorithms. While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks. Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors. A variant of Q-Learning techniques is deployed The Sixth Intl.. Joint Conf. to update the estimations to converge to the real distances. It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies -LSB- 3 -RSB-. The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property. This is because the users ' traffic and query patterns can reduce the state space and speed up the learning process. Related work in taking advantage of this property include -LSB- 7 -RSB-, where the authors attempted to address this problem by user modeling techniques. 6. CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' ability to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents modify their routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time.", "keyphrases": ["peer-to-peer inform retriev system", "reinforc learn", "distribut search algorithm", "rout decis", "util", "network", "learn algorithm", "rout polici", "queri"]}
{"file_name": "C-23", "text": "Implementation of a Dynamic Adjustment Mechanism with Efficient Replica Selection in Data Grid Environments ABSTRACT The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers. Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers. In this paper, we propose a dynamic coallocation scheme, namely Recursive-Adjustment Co-Allocation scheme, to improve the performance of data transfer in Data Grids. Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide an effective scheme for reducing the cost of reassembling data blocks. 1. INTRODUCTION Data Grids aggregate distributed resources for solving large-size dataset management problems. Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment. Certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access. Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity. One way to improve download speeds is to determine the best replica locations using replica selection techniques -LSB- 19 -RSB-. This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet. Another way is to use co-allocation technology -LSB- 17 -RSB- to download data. Co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel. Several co-allocation strategies were provided in previous work -LSB- 17 -RSB-. An idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block. Therefore, it is important to reduce the differences in finishing time among replica servers. In this paper, we propose a dynamic co-allocation scheme based on co-allocation Grid data transfer architecture called RecursiveAdjustment Co-Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance -LSB- 24 -RSB-. Experimental results show that our approach is superior to previous methods and achieved the best overall performance. We also discuss combination cost and provide an effective scheme for reducing it. Related background review and studies are presented in Section 2 and the co-allocation architecture and related work are introduced in Section 3. In Section 4, an efficient replica selection service is proposed by us. Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6. Section 7 concludes this research paper. 2. BACKGROUND 2.1 Data Grid Data Grids -LSB- 1, 2, 16 -RSB- federate a lot of storage resources. Large collections of measured or computed data are emerging as important resources in many data intensive applications. 2.1.1 Replica Management Replica management involves creating or removing replicas at a data grid site -LSB- 19 -RSB-. In other words, the role of a replica manager is to create or delete replicas, within specified storage systems. Most often, these replicas are exact copies of the original files, created only to harness certain performance benefits. A replica manager typically maintains a replica catalog containing replica site addresses and the file instances. The replica management service is responsible for managing the replication of complete and partial copies of datasets, defined as collections of files. The replica management service is just one component in a Data Grid environment that provides support for high-performance, data-intensive applications. A replica or location is a subset of a collection that is stored on a particular physical storage system. There may be multiple possibly overlapping subsets of a collection stored on multiple storage systems in a Data Grid. These Grid storage systems may use a variety of underlying storage technologies and data movement protocols, which are independent of replica management. 2.1.2 Replica Catalog As mentioned above, the purpose of the replica catalog is to provide mappings between logical names for files or collections and one or more copies of the objects on physical storage systems. The replica catalog includes optional entries that describe individual logical files. Logical files are entities with globally unique names that may have one or more physical instances. The catalog may optionally contain one logical file entry in the replica catalog for each logical file in a collection. A Data Grid may contain multiple replica catalogs. For example, a community of researchers interested in a particular research topic might maintain a replica catalog for a collection of data sets of mutual interest. It is possible to create hierarchies of replica catalogs to impose a directory-like structure on related logical collections. In addition, the replica manager can perform access control on entire catalogs as well as on individual logical files. 2.1.3 Replica Selection The purpose of replica selection -LSB- 16 -RSB- is to select a replica from among the sites which constitute a Data Grid -LSB- 19 -RSB-. The criteria of selection depend on characteristics of the application. By using this mechanism, users of the Data Grid can easily manage replicas of data sets at their sites, with better performance. Much previous effort has been devoted to the replica selection problem. The common process of replica selection consists of three steps : data preparation, preprocessing and prediction. Then, applications can select a replica according to its specific attributes. Replica selection is important to data-intensive applications, and it can provide location transparency. When a user requests for accessing a data set, the system determines an appropriate way to deliver the replica to the user. 2.2 Globus Toolkit and GridFTP The Globus Project -LSB- 9, 11, 16 -RSB- provides software tools collectively called The Globus Toolkit that makes it easier to build computational Grids and Grid-based applications. Many organizations use the Globus Toolkit to build computational Grids to support their applications. The composition of the Globus Toolkit can be pictured as three pillars : Resource Management, Information Services, and Data Management. GRAM implements a resource management protocol, MDS implements an information services protocol, and GridFTP implements a data transfer protocol. The Globus alliance proposed a common data transfer and access protocol called GridFTP that provides secure, efficient data movement in Grid environments -LSB- 3 -RSB-. This protocol, which extends the standard FTP protocol, provides a superset of the features offered by the various Grid storage systems currently in use. In order to solve the appearing problems, the Data Grid community tries to develop a secure, efficient data transport mechanism and replica management services. GridFTP is a reliable, secure and efficient data transport protocol which is developed as a part of the Globus project. There is another key technology from Globus project, called replica catalog -LSB- 16 -RSB- which is used to register and manage complete and partial copies of data sets. The replica catalog contains the mapping information from a logical file or collection to one or more physical files. 2.3 Network Weather Service The Network Weather Service -LRB- NWS -RRB- -LSB- 22 -RSB- is a generalized and distributed monitoring system for producing short-term performance forecasts based on historical performance measurements. The goal of the system is to dynamically characterize and forecast the performance deliverable at the application level from a set of network and computational resources. 2.4 Sysstat Utilities The Sysstat -LSB- 15 -RSB- utilities are a collection of performance monitoring tools for the Linux OS. The Sysstat package incorporates the sar, mpstat, and iostat commands. The sar command collects and reports system activity information, which can also be saved in a system activity file for future inspection. The iostat command reports CPU statistics and I/O statistics for tty devices and disks. 7. CONCLUSIONS The co-allocation architecture provides a coordinated agent for assigning data blocks. A previous work showed that the dynamic co-allocation scheme leads to performance improvements. However, it can not handle the idle time of faster servers, which must wait for the slowest server to deliver its final block. We proposed the Recursive-Adjustment Co-Allocation scheme to improve data transfer performances using the co-allocation architecture in -LSB- 17 -RSB-. In this approach, the workloads of selected replica servers are continuously adjusted during data transfers, and we provide a function that enables users to define a final block threshold, according to their data grid environment. Experimental results show the effectiveness of our proposed technique in improving transfer time and reducing overall idle time spent waiting for the slowest server. We also discussed the re-combination cost and provided an effective scheme for reducing it.", "keyphrases": ["distribut resourc", "data grid applic", "replic", "co-alloc", "larg dataset", "resourc manag protocol", "replica", "co-alloc strategi", "server", "perform"]}
{"file_name": "J-28", "text": "Approximately-Strategyproof and Tractable Multi-Unit Auctions ABSTRACT We present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem. The bidding language in our auctions allows marginal-decreasing piecewise constant curves. First, we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem, which computes a -LRB- 1 + e -RRB- approximation in worst-case time T = O -LRB- n3/e -RRB-, given n bids each with a constant number of pieces. Second, we embed this approximation scheme within a Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism and compute payments to n agents for an asymptotic cost of O -LRB- T log n -RRB-. The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by e / -LRB- 1 + e -RRB- V, where V is the total surplus in the efficient outcome. 1. INTRODUCTION In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce ; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services -LRB- e.g. circuit boards, power suppliers, toner cartridges -RRB-, with flexible-sized lots. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. We consider the computational complexity of implementing the Vickrey-Clarke-Groves -LSB- 22, 5, 11 -RSB- mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer can not be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design -LSB- 17 -RSB-. The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to -LRB- weakly -RRB- intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the 1However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a -LRB- 1 + ~ -RRB- - approximation in worst-case time T = O -LRB- n3 / \u03b5 -RRB-, where each bid has a fixed number of piecewise constant pieces. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O -LRB- nT -RRB-. This upper-bound tends to 1 as the number of sellers increases. The approximate VCG mechanism is -LRB- \u03b5 1 + \u03b5 -RRB- - strategyproof for an approximation to within -LRB- 1 + ~ -RRB- of the optimal allocation. This means that a bidder can gain at most -LRB- \u03b5 1 + \u03b5 -RRB- V from a nontruthful bid, where V is the total surplus from the efficient allocation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about \u03b5-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme. Section 4 defines the approximation scheme for the payments in the VCG mechanism. Section 5 concludes. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem -LRB- CAP -RRB- is both NP-complete and inapproximable -LRB- e.g. -LSB- 6 -RSB- -RRB-. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language -- the bid taker in our setting is allowed to accept at most one point on the bid curve. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. For instance, Lehmann et al. -LSB- 15 -RSB- propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Feigenminimum-lot size constraints from the buyers. baum & Shenker -LSB- 8 -RSB- have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Eso et al. -LSB- 7 -RSB- have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. Kalagnanam et al. -LSB- 12 -RSB- address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different : it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel -LSB- 1 -RSB- has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values -LSB- 1 -RSB-, with an interpretation as a primal-dual algorithm -LSB- 2 -RSB-. 5. CONCLUSIONS We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language. Our scheme is both approximately efficient and approximately strategyproof within any specified factor \u03b5 > 0. As such it is an example of computationally tractable \u03b5-dominance result, as well as an example of a non-trivial but approximable allocation problem. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O -LRB- T log n -RRB-, where T is the time complexity to compute the solution to a single allocation problem.", "keyphrases": ["approxim-effici and approximatelystrategyproof auction mechan", "singl-good multi-unit alloc problem", "fulli polynomi-time approxim scheme", "vickrei-clark-grove", "forward auction", "revers auction", "equilibrium", "margin-decreas piecewis constant curv", "bid languag", "dynam program"]}
{"file_name": "C-40", "text": "Edge Indexing in a Grid for Highly Dynamic Virtual Environments \u2217 ABSTRACT Newly emerging game -- based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real -- time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client -- server based grid subspace division where the virtual worlds are partitioned into manageable sub -- worlds. In each sub -- world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user 's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real -- time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments. The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real -- time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next -- generation virtual frameworks that may merge into existing web -- based services in the near future. \u2217 This research has been funded in part by NSF grants EEC9529152 -LRB- IMSC ERC -RRB- and IIS-0534761, and equipment gifts from Intel Corporation, Hewlett-Packard, Sun Microsystems and Raptor Networks Technology. Categories and Subject Descriptors : C. 2.4 -LSB- Computer -- Com 1. INTRODUCTION Recently, Massively Multiplayer Online Games -LRB- MMOGs -RRB- have been studied as a framework for next -- generation virtual environments. In this paper, we mainly focus on the first two requirements. Dynamic extensibility allows regular game -- users to deploy their own created content. This is a powerful concept, but unfortunately, user -- created content tends to create imbalances among the existing scene complexity, causing system -- wide performance problems. Another important requirement is scalability. By carefully partitioning the world into multiple sub -- worlds or replicating worlds at geographically dispersed locations, massive numbers of concurrent users can be supported. Second Life -LSB- 4 -RSB- is the first successfully deployed MMOG system that meets both requirements. But we acknowledge that these requirements are also valid for new virtual environments. Figure 1 : Object popping occurred as a user moves forward -LRB- screenshots from Second Life -RRB- where \u0394 = 2 seconds. employs a client/server based 3D object streaming model -LSB- 5 -RSB-. In this model, a server continuously transmits both update events and geometry data to every connected user. As a result, this extensible gaming environment has accelerated the deployment of user -- created content and provides users with unlimited freedom to pursue a navigational experience in its space. One of the main operations in MMOG applications that stream 3D objects is to accurately calculate all objects that are visible to a user. The traditional visibility determination approach, however, has an object popping problem. For example, a house outside a user 's visible range is not drawn at time t, illustrated in Figure 1 -LRB- a -RRB-. As the user moves forward, the house will suddenly appear at time -LRB- t + \u0394 -RRB- as shown in Figure 1 -LRB- b -RRB-. The visibility calculation for each user not only needs to be accurate, but also fast. This challenge is illustrated by the fact that the maximum number of concurrent users per server of Second Life is still an order of magnitude smaller than for stationary worlds. To address these challenges, we propose a method that identifies the most relevant visible objects from a given geometry database -LRB- view model -RRB- and then put forth a fast indexing method that computes the visible objects for each user -LRB- spatial indexing -RRB-. Our two novel methods represent the main contributions of this work. Section 2 presents related work. Section 3 describes our new view method. In Section 4, we present assumptions on our target application and introduce a new spatial indexing method designed to support real -- time visibility computations. We also discuss its optimization issues. Section 5 reports on the quantitative analysis and Section 6 presents preliminary results of our simulation based experiments. Finally, we conclude and address future research directions in Section 7. 2. RELATED WORK Visibility determination has been widely explored in the field of 3D graphics. Various local rendering algorithms have been proposed to eliminate unnecessary objects before rendering or at any stage in the rendering pipeline. However, these algorithms assume that all the candidate visible objects have been stored locally. If the target objects are stored on remote servers, the clients receive the geometry items that are necessary for rendering from the server databases. However, these online optimization algorithms fail to address performance issue at the server in highly crowded environments. On the other hand, our visibility computation model, a representative of this category, is based on different assumptions on the data representation of virtual entities. In the graphics area, there has been little work on supporting real -- time visibility computations for a massive number of moving objects and users. Here we recognize that such graphics related issues have a very close similarity to spatial database problems. Recently, a number of publications have addressed the scalability issue on how to support massive numbers of objects and queries in highly dynamic environments. To support frequent updates, two partitioning policies have been studied in depth : -LRB- 1 -RRB- R-tree based spatial indexing, and -LRB- 2 -RRB- grid -- based spatial indexing. The grid -- based partitioning model is a special case of fixed partitioning. Recently, it has been re -- discovered since it can be efficient in highly dynamic environments. Q-Index -LSB- 13, 11 -RSB- is one of the earlier work that re -- discovers the usefulness of grid -- based space partitioning for emerging moving object environments. In contrast to traditional spatial indexing methods that construct an index on the moving objects, it builds an index on the continuous range queries, assuming that the queries move infrequently while the objects move freely. The basic idea of the Q+R tree -LSB- 14 -RSB- is to separate indexing structures for quasi -- stationary objects and moving objects : fast -- moving objects are indexed in a Quadtree and quasi -- stationary objects are stored in an R \u2217 - tree. SINA -LSB- 10 -RSB- was proposed to provide efficient query evaluations for any combination of stationary/moving objects and stationary/moving queries. Specifically, this approach only detects newly discovered -LRB- positive -RRB- or no longer relevant -LRB- negative -RRB- object updates efficiently. Unlike other spatial indexing methods that focus on reducing the query evaluation cost, Hu et al. -LSB- 12 -RSB- proposed a general framework that minimizes the communication cost for location updates by maintaining a rectangular area called a safe region around moving objects. As long as any object resides in this region, all the query results are guaranteed to be valid in the system. If objects move out of their region, location update requests should be delivered to the database server and the affected queries are re -- evaluated on the fly. Our indexing method is very similar to the above approaches. The major difference is that we are more concentrating on real -- time visibility determination while others assume loose timing constraints. 6. EVALUATION This section presents two simulation setups and their performance results. Section 6.1 examines whether our new view approach is superior to existing view models, in spite of its higher indexing complexity. Section 6.2 discusses the degree of practicality and scalability of our indexing method that is designed for our new view model. 6.1 Justification of Object-initiated View Model 6.1.1 Evaluation Metrics P is the ratio of relevant, retrieved items to all retrieved items. A lower value of P implies that the query result set contains a large number of unnecessary objects that do not have to be delivered to a client. A higher P value means a higher network traffic load than required. R is the ratio of relevant, retrieved items to all relevant items. A lower R value means that more objects that should be recognized are ignored. From the R measure, we can quantitatively estimate the occurrence of object popping. In addition to the P and R metrics, we use a standardized single -- valued query evaluation metric that combines P and R, called E -- measure -LSB- 15 -RSB-. The E -- measure is defined as : If \u03b2 is less than 1, P becomes more important. Otherwise, R will affect the E -- measure significantly. A lower E -- measure value implies that the tested view model has a higher quality. The best E -- measure value is zero, where the best values for P and R are both ones. 6.1.2 Simulation Setup We tested four query processing schemes, which use either a user -- initiated or an object -- initiated view model : \u2022 User-initiated visibility computation -- RQ -- OP : Region Query -- Object Point \u2022 Object-oriented visibility computation -- PQ-OR : Point Query -- Object Region -- RQ-OR : Region Query -- Object Region -- ACQ-OR : Approximate Cell Query -- Object Region RQ -- OP is the typical computation scheme that collects all objects whose location is inside a user defined AOI. PQ -- OR collects a set of objects whose AOI intersects with a given user point, formally -LCB- o | q.P \u2208 o.R -RCB-. RQ -- OR, an imaginary computation scheme, is the combination of RQ -- OP and PQ -- OR where the AOI of an object intersects with that of a user, -LCB- o | o.R \u2229 q.R = ~ \u2205 -RCB-. Lastly, ACQ -- OR, an approximate visibility computation model, is a special scheme designed for grid -- based space partitioning, which is our choice of cell evaluation methodology for edge indexing. If a virtual space is partitioned into tiled cells and a user point belongs to one of the cells, the ACQ -- OR searches the objects whose AOI Table 5 : P and R computations of different visibility determination schemes. Table 6 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a slowly moving environment would intersect with the region of the corresponding grid cell. It identifies any object o satisfying the condition c.R n o.R _ ~ 0 where the cell c satisfies q.P E c.R as well. Our simulation program populated 100K object entities and 10K user entities in a 2D unit space, -LSB- 0, 1 -RRB- x -LSB- 0, 1 -RRB-. The populated entities are uniformly located in the unit space. The program performs intersection tests between all user and all object entities exhaustively and computes the P, R, and E -- measure values -LRB- shown in Table 5 -RRB-. 6.1.3 Experimental Results Distribution of P and R measure : Figure 7 shows the distribution of P and R for RQ -- OP. We can observe that P and R are roughly inversely proportional to each other when varying a user AOI range. A smaller side length leads to higher accuracy but lower comprehensiveness. For example, 5 % of the side length of a user AOI detects all objects whose side length of the AOI is at least 5 %. Thus, every object retrieved by RQ -- OP is guaranteed to be all rendered at the client. But RQ -- OP can not detect the objects outside the AOI of the user, thus suffering from too many missing objects that should be rendered. Similarly, the user whose AOI is wider than any other AOI can not miss any objects that should be rendered, but detects too many unnecessary objects. To remove any object popping problem, the side length of any AOI should be greater than or equal to the maximum visible distance of any object in the system, which may incur significant system degradation. E-measure Distribution : Figure 8 reveals two trends. First, the precision values of RQ -- OP lie in between those of ACQ -- OR -LRB- 100 x 100 grid -RRB- and RQ -- OR. Second, the tendency curve of the Precision -- to -- E -- measure plot of RQ -- OR shows resemblance to that of ACQ -- OR. Effect of Different Grid Size : Figure 9 shows the statistical difference of E -- measure values of seven different grid partitioning schemes -LRB- using ACQ -- OR -RRB- and one RQ -- OP model. We use a box -- and -- whisker plot to show both median values and the variances of E-measure distributions and the outliers of each scheme. We also draw the median value of the RQ -- OP E -- measures -LRB- green line -RRB- for comparison purposes. While the ACQ -- OR schemes have some outliers, their E-measure values are heavily concentrated around the median values, thus, they are less sensitive to object AOI. As expected, fine-grained grid partitioning showed a smaller E-measure value. The RQ -- OP scheme showed a wider variance of its quality than other schemes, which is largely attributable to different user side lengths. As the R measure becomes more important, the query quality of ACQ -- OR is improved more evidently than that of RQ -- OP. From Figure 9, the 20x20 grid scheme had a better E-measure Table 7 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a highly dynamic environment -LRB- value in a prioritized environment than in an equal-prioritized environment. As a result, we can roughly anticipate that at least the 20x20 grid cell partitioning retrieves a higher quality of visible sets than the RQ -- OP. 6.2 Evaluation of Edge Indexing In this section, we present the preliminary results of the simulations that examine the applicability of our edge indexing implementation. To estimate the degree of real -- time support of our indexing method, we used the total elapsed time of updating all moving entities and computing visible sets for every cell. We also experimented with different grid partitioning policies and compared them with exhaustive search solutions. 6.2.1 Simulation Setup We implemented edge indexing algorithms in C and ran the experiments on a 64-bit 900MHz Itanium processor with 8 GBs of memory. We implemented a generalized hash table mechanism to store node and edge structures. 6.2.2 Experimental Results Periodic Monitoring Cost : Tables 6 and 7 show the performance numbers of different edge indexing methods by varying v. The moving speed of entities was also uniformly assigned between 0 and v. However, the two -- table method showed a slightly higher evaluation time than the two single -- table methods because of its sequential token removal. Table 7 exemplified the elapsed time of index updates and cell evaluations in a highly dynamic environment where slowly moving and dynamically moving objects co -- exist. Compared with the results shown in Table 6, the two -- table approach produced similar performance numbers regardless of the underlying moving environments. However, the performance gain obtained by the incremental policy of the single -- table is decreased compared with that in the slowly moving environment. Effect of Different Grid Size : How many object updates and cell evaluations can be supported in a given time period is an important performance metric to quantify system throughput. In this section, we evaluate the performance results of three different visibility computation models : two computation -- driven exhaustive search methods ; and one two -- table edge indexing method with different grid sizes. Figure 7 : Distribution of P and R measured by RQ -- OP. Figure 8 : E -- measure value as a function of Figure 9 : E -- measure value as a function of ACQ -- QR grid partitioning scheme when Figure 10 : Total elapsed time of different indexing schemes. Exhaustive search methods do not maintain any intermediate results. They simply compute whether a given user point is inside a given object AOI. They can tolerate unpredictable behavior of object movement. Figure 10 reveals the performance difference between the exhaustive solutions and the two -- table methods, a difference of up to two orders of magnitude. As shown in Section 5, the total elapsed time of object updates and cell evaluations is linear with respect to the average side length of object AOI. Because the side length is represented by cell units, an increase in the number of cells increases the side lengths proportionally. Figure 10 illustrates that the measured simulation results roughly match the expected performance gain computed from the analysis. 7. CONCLUSION AND FUTURE WORK To support dynamic extensibility and scalability in highly dynamic environments, we proposed a new view paradigm, the object-initiated view model, and its efficient indexing method, edge indexing. Compared with the traditional view model, our new view model promises to eliminate any object popping problem that can easily be observed in existing virtual environments at the expense of increased indexing complexity. Our edge indexing model, however, can overcome such higher indexing complexity by indexing spatial extensions at edge -- level not at node -- level in a grid partitioned sub -- world and was validated through quantitative analyses and simulations. However, for now our edge indexing still retains a higher complexity, even in a two -- dimensional domain. Currently, we are developing another edge indexing method to make the indexing complexity constant. Once indexing complexity becomes constant, we plan to index 3D spatial extensions and multi -- resolutional geometry data. We expect that our edge indexing can contribute to successful deployment of next -- generation gaming environments.", "keyphrases": ["edg index", "dynam virtual environ", "game-base applic", "mutabl virtual content", "spatial databas", "spatial index method", "real-time visibl test", "object-initi view model", "object pop", "3d spatial extens"]}
{"file_name": "C-86", "text": "Addressing Strategic Behavior in a Deployed Microeconomic Resource Allocator ABSTRACT While market-based systems have long been proposed as solutions for distributed resource allocation, few have been deployed for production use in real computer systems. Towards this end, we present our initial experience using Mirage, a microeconomic resource allocation system based on a repeated combinatorial auction. Mirage allocates time on a heavily-used 148-node wireless sensor network testbed. In particular, we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects. Based on these results, we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. Finally, we propose refinements to the system 's current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction. 1. INTRODUCTION Market-based systems have long been proposed as solutions for resource allocation in distributed systems including computational Grids -LSB- 2, 20 -RSB-, wide-area network testbeds -LSB- 9 -RSB-, and peer-to-peer systems -LSB- 17 -RSB-. Yet, while the theoretical underpinnings of market-based schemes have made significant strides in recent years, practical integration of market-based mechanisms into real computer systems and empirical observations of such systems under real workloads has remained an elusive goal. Towards this end, we have designed, implemented, and deployed a microeconomic resource allocation system called Mirage -LSB- 3 -RSB- for scheduling testbed time on a 148-node wireless sensor network -LRB- SensorNet -RRB- testbed at Intel Research. The system, which employs a repeated combinatorial auction -LSB- 5, 14 -RSB- to schedule allocations, has been in production use for over four months and has scheduled over 312,148 node hours across 11 research projects to date. In designing and deploying Mirage, we had three primary goals. First, we wanted to validate whether a market-based resource allocation scheme was necessary at all. An economic problem only exists when resources are scarce. Therefore, a key goal was to first measure both resource contention and the range of underlying valuations users place on the resources during periods of resource scarcity. Second, we wanted to observe how users would actually behave in a market-based environment. With Mirage, we wanted to observe to what extent rationality held and in what ways users would attempt to strategize and game the system. Finally, we wanted to identify what other practical problems would emerge in a deployment of a market based system. In this paper, we report briefly on our first goal while focusing primarily on the second. In deploying Mirage, we made the early decision to base the system on a repeated combinatorial auction known not to be strategyproof. That is, self-interested users could attempt to increase their personal gain, at the expense of others, by not revealing their true value to the system. We made this decision mainly because designing a strategyproof mechanism remains an open, challenging problem and we wanted to deploy a working system and gain experience with real users to address our three goals in a timely manner. Deploying a non-strategyproof mechanism also had the benefit of testing rationality and seeing how and to what extent users would try to game the system. The key contribution of this paper is an analysis of such strategic behavior as observed over a four-month time period and proposed refinements for mitigating such behavior en route to building an approximately strategyproof repeated combinatorial auction. The rest of this paper is organized as follows. In Section 2, we present an overview of Mirage including high-level observations on usage over a four-month period. In Section 3, we examine strategic user behavior, focusing on the four primary types of strategies employed by users in the system. Based on these results, Section 4 presents a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. As a first step in addressing some of these challenges, we describe refinements to Mirage 's current auction scheme that mitigate the strategies observed to date and also comment on some initial steps towards building an approximately strategyproof repeated combinatorial auction for Mirage. Finally, in Section 5, we conclude the paper. 5. CONCLUSION Despite initially using a repeated combinatorial auction known not to be strategyproof, Mirage has shown significant promise as a vehicle for SensorNet testbed allocation. Fully realizing these gains, however, requires addressing key problems in strategyproof mechanism design and combinatorial optimization. The temporal nature of computational resources and the combinatorial resource demands of distributed applications adds an additional layer of complexity.", "keyphrases": ["resourc alloc system", "combinatori auction", "market-base system", "distribut system", "strateg behavior", "ration", "auction-base scheme", "mirag system", "sensornet testb", "node-hour price", "usabl overhead", "batch schedul", "distribut applic"]}
