{"file_name": "C-38", "text": "Un framework per l'architettura di overlay peer-to-peer guidati dal ricevitore ABSTRACT Questo documento presenta un framework semplice e scalabile per l'architettura di overlay peer-to-peer chiamato Peer-to-peer Receiverdriven Overlay -LRB- o PRO -RRB-. PRO \u00e8 progettato per applicazioni di streaming non interattive e il suo obiettivo di progettazione principale \u00e8 massimizzare la larghezza di banda fornita -LRB- e quindi fornire qualit\u00e0 -RRB- ai peer con larghezza di banda eterogenea e asimmetrica. Per raggiungere questo obiettivo, PRO adotta un approccio guidato dal ricevitore in cui ciascun ricevitore -LRB- o peer partecipante -RRB- -LRB- i -RRB- scopre indipendentemente altri peer nell'overlay attraverso il pettegolezzo e -LRB- ii -RRB- egoisticamente determina il miglior sottoinsieme di peer principali attraverso cui connettersi all'overlay per massimizzare la propria larghezza di banda fornita. I peer partecipanti formano un overlay non strutturato che \u00e8 intrinsecamente robusto per un tasso di abbandono elevato. Inoltre, ciascun ricevitore sfrutta la larghezza di banda controllata dalla congestione dei suoi genitori come segnale implicito per rilevare e reagire ai cambiamenti a lungo termine nella rete o nelle condizioni di sovrapposizione senza alcun coordinamento esplicito con gli altri peer partecipanti. La selezione indipendente dei genitori da parte di singoli pari convergono dinamicamente in un'efficiente struttura di sovrapposizione. 1. INTRODUZIONE eterogeneit\u00e0 e asimmetria della connettivit\u00e0 della larghezza di banda tra i peer partecipanti -LSB- 19 -RSB-. Affrontare le variazioni di larghezza di banda, l'eterogeneit\u00e0 e l'asimmetria sono particolarmente importanti nella progettazione di sovrapposizioni peer-to-peer per applicazioni di streaming poich\u00e9 la qualit\u00e0 fornita a ciascun peer \u00e8 direttamente determinata dalla sua connettivit\u00e0 di larghezza di banda a -LRB- altro peer -LRB- s -RRB- su -RRB- la sovrapposizione. Questo documento presenta un semplice framework per l'architettura di un overlay peer-to-peer basato sul ricevitore, chiamato PRO. La filosofia di progettazione principale di PRO \u00e8 che a ciascun peer dovrebbe essere consentito di determinare in modo indipendente ed egoistico il modo migliore per connettersi all'overlay al fine di massimizzare la propria qualit\u00e0 fornita. A tal fine, ciascun peer pu\u00f2 connettersi alla topologia overlay in pi\u00f9 punti -LRB-, ovvero ricevere contenuto attraverso pi\u00f9 peer principali -RRB-. Pertanto, i peer partecipanti formano un overlay non strutturato in grado di far fronte con grazia ad un elevato tasso di abbandono -LSB- 5 -RSB-. Inoltre, avere pi\u00f9 peer genitori soddisfa l\u2019eterogeneit\u00e0 e l\u2019asimmetria della larghezza di banda, migliorando al tempo stesso la resilienza rispetto alle dinamiche di partecipazione dei peer. PRO \u00e8 costituito da due componenti chiave: -LRB- i -RRB- Peer Discovery basato su gossip: ciascun peer scambia periodicamente messaggi -LRB- ovvero pettegolezzi -RRB- con altri peer noti per conoscere progressivamente un sottoinsieme di peer partecipanti nell'overlay che probabilmente saranno dei buoni genitori. -LRB- ii -RRB- Selezione dei genitori guidata dal destinatario: date le informazioni raccolte sugli altri pari partecipanti tramite il meccanismo di pettegolezzo,ciascun peer -LRB- o ricevitore -RRB- migliora gradualmente la propria qualit\u00e0 fornita selezionando dinamicamente un sottoinsieme appropriato di peer genitori che collettivamente massimizzano la larghezza di banda fornita al ricevitore. Poich\u00e9 la larghezza di banda disponibile da diversi peer partecipanti a un ricevitore -LRB- e la possibile correlazione tra loro -RRB- pu\u00f2 essere misurata solo su quel ricevitore, un approccio guidato dal ricevitore \u00e8 la soluzione naturale per massimizzare la larghezza di banda disponibile per peer eterogenei. Inoltre, la larghezza di banda disponibile dai peer principali funge da segnale implicito affinch\u00e9 un ricevitore possa rilevare e reagire ai cambiamenti nella rete o nelle condizioni di overlay senza alcun coordinamento esplicito con gli altri peer partecipanti. La selezione indipendente dei genitori da parte dei singoli peer porta a una sovrapposizione efficiente che massimizza la qualit\u00e0 fornita a ciascun peer. PRO incorpora diverse funzioni di smorzamento per garantire la stabilit\u00e0 del rivestimento nonostante le azioni non coordinate di diversi colleghi. PRO fa parte di un'architettura pi\u00f9 ampia che abbiamo sviluppato per lo streaming peer-to-peer. Pertanto, PRO e PALS sono entrambi guidati dal ricevitore ma si completano a vicenda. Pi\u00f9 specificamente, PRO determina un sottoinsieme appropriato di peer principali che massimizzano collettivamente la larghezza di banda fornita a ciascun ricevitore mentre PALS coordina lo streaming \"in-time\" di diversi segmenti di contenuto multimediale da questi genitori nonostante le variazioni imprevedibili nella loro larghezza di banda disponibile. Questa divisione delle funzionalit\u00e0 offre una grande flessibilit\u00e0 perch\u00e9 disaccoppia la costruzione della sovrapposizione dal meccanismo di consegna. In questo articolo ci concentreremo principalmente sul meccanismo di costruzione overlay, o PRO. Il resto di questo documento \u00e8 organizzato come segue: Nella Sezione 2, rivisitiamo il problema della costruzione dell'overlay per lo streaming peerto-peer e identifichiamo i suoi due componenti chiave ed esploriamo il loro spazio di progettazione. Presentiamo il nostro quadro proposto nella Sezione 3. Nelle Sezioni 4 e 5, i componenti chiave del nostro quadro sono descritti in maggiore dettaglio. Infine, la Sezione 6 conclude il documento e presenta i nostri piani futuri. 6. CONCLUSIONI E LAVORO FUTURO In questo articolo, abbiamo presentato un semplice framework guidato dal ricevitore per l'architettura di strutture overlay peer-to-pee chiamato PRO. PRO consente a ciascun peer di determinare egoisticamente e in modo indipendente il modo migliore per connettersi all'overlay per massimizzarne le prestazioni. Pertanto, PRO dovrebbe essere in grado di massimizzare la qualit\u00e0 fornita ai peer con connettivit\u00e0 a larghezza di banda eterogenea e asimmetrica. Sia la scoperta dei pari che la selezione dei pari in questo framework sono scalabili. Inoltre, PRO utilizza la larghezza di banda controllata dalla congestione come segnale implicito per rilevare i colli di bottiglia condivisi tra i genitori esistenti, nonch\u00e9 i cambiamenti nelle condizioni di rete o di sovrapposizione per rimodellare adeguatamente la struttura. Abbiamo descritto la struttura di base e i suoi componenti chiave e abbiamo abbozzato le nostre soluzioni di paglia. Questo \u00e8 un punto di partenza per il nostro lavoro su PRO. Attualmente stiamo valutando vari aspetti di questo quadro tramite simulazione,ed esplorare lo spazio di progettazione dei componenti chiave. Stiamo anche prototipando questo quadro per condurre esperimenti nel mondo reale sul Planet-Lab in un prossimo futuro.", "keyphrases": ["flusso peer-to-peer", "controllo della congestione", "approccio orientato alla ricezione", "sovrapposizione guidata dalla ricezione", "sistema di distribuzione", "progetto", "misura", "struttura efficace sovrapposta", "pro", "sottoinsieme appropriato del peer genitore", "scoperta tra pari basata su gossip", "selezione genitore basata sulla ricezione"]}
{"file_name": "H-17", "text": "Politiche di potatura per indici invertiti a due livelli con garanzia di correttezza ABSTRACT I motori di ricerca Web mantengono indici invertiti su larga scala che vengono interrogati migliaia di volte al secondo da utenti desiderosi di informazioni. Per far fronte all'enorme quantit\u00e0 di carichi di query, i motori di ricerca riducono il proprio indice per conservare i documenti che probabilmente verranno restituiti come risultati migliori e utilizzano questo indice ridotto per calcolare i primi gruppi di risultati. Sebbene questo approccio possa migliorare le prestazioni riducendo la dimensione dell'indice, se calcoliamo i migliori risultati solo dall'indice ridotto potremmo notare un significativo degrado nella qualit\u00e0 del risultato: se un documento dovrebbe essere tra i primi risultati ma non \u00e8 stato incluso in l'indice sfoltito, verr\u00e0 posizionato dietro i risultati calcolati dall'indice sfoltito. Considerata la forte concorrenza nel mercato della ricerca online, questo fenomeno \u00e8 chiaramente indesiderabile. In questo articolo studiamo come evitare qualsiasi degrado della qualit\u00e0 dei risultati dovuto all'ottimizzazione delle prestazioni basata sulla potatura, realizzandone comunque la maggior parte dei vantaggi. Il nostro contributo consiste in una serie di modifiche alle tecniche di potatura per la creazione dell'indice sfoltito e un nuovo algoritmo di calcolo dei risultati che garantisce che le pagine con la corrispondenza pi\u00f9 elevata siano sempre posizionate nei primi risultati di ricerca, anche se stiamo calcolando il primo batch dall'indice sfoltito. indice per la maggior parte del tempo. Mostriamo anche come determinare la dimensione ottimale di un indice ridotto e valutiamo sperimentalmente i nostri algoritmi su una raccolta di 130 milioni di pagine Web. 1. INTRODUZIONE Secondo un recente studio -LSB- 13 -RSB-, si stima che \u2217 Lavoro svolto mentre l'autore era presso il Dipartimento di Informatica dell'UCLA. \u2020 Questo lavoro \u00e8 parzialmente supportato dalle sovvenzioni NSF, IIS-0534784, IIS0347993 e CNS-0626702. A causa di questa immensa quantit\u00e0 di informazioni disponibili, gli utenti stanno diventando sempre pi\u00f9 dipendenti dai motori di ricerca Web per individuare informazioni rilevanti sul Web. In genere, i motori di ricerca Web, analogamente ad altre applicazioni di recupero delle informazioni, utilizzano una struttura dati denominata indice invertito. Un indice invertito consente il recupero efficiente dei documenti -LRB- o delle pagine Web -RRB- che contengono una determinata parola chiave. Nella maggior parte dei casi, una query eseguita dall'utente pu\u00f2 contenere migliaia o addirittura milioni di documenti corrispondenti. Per evitare di sovraccaricare gli utenti con una quantit\u00e0 enorme di risultati, i motori di ricerca presentano i risultati in gruppi di 10-20 documenti rilevanti. L'utente esamina quindi il primo gruppo di risultati e, se non trova la risposta che sta cercando, potrebbe potenzialmente richiedere di visualizzare il gruppo successivo o decidere di inviare una nuova query. Uno studio recente -LSB- 16 -RSB- ha indicato che circa l'80% degli utenti esamina al massimo i primi 3 lotti di risultati. Cio\u00e8, l'80% degli utenti in genere visualizza al massimo dai 30 ai 60 risultati per ogni query che invia a un motore di ricerca. Allo stesso tempo, date le dimensioni del Web,l'indice invertito mantenuto dai motori di ricerca pu\u00f2 diventare molto grande. Una soluzione naturale a questo problema \u00e8 creare un piccolo indice su un sottoinsieme dei documenti che probabilmente verranno restituiti come risultati migliori -LRB- utilizzando, ad esempio, le tecniche di potatura in -LSB- 7, 20 -RSB- -RRB- e calcola il primo gruppo di risposte utilizzando l'indice ridotto. Anche se \u00e8 stato dimostrato che questo approccio offre un miglioramento significativo delle prestazioni, porta anche a un notevole degrado della qualit\u00e0 dei risultati della ricerca, poich\u00e9 le risposte migliori vengono calcolate solo dall'indice ridotto -LSB- 7, 20 -RSB-. Cio\u00e8, anche se una pagina dovesse essere posizionata come la pagina con la corrispondenza pi\u00f9 alta secondo la metrica di ranking di un motore di ricerca, la pagina potrebbe essere posizionata dietro quelle contenute nell'indice sfoltito se la pagina non \u00e8 diventata parte dell'indice sfoltito per vari motivi -LSB- 7, 20 -RSB-. Data la forte concorrenza tra i motori di ricerca di oggi, questo degrado \u00e8 chiaramente indesiderabile e deve essere affrontato, se possibile. In questo documento studieremo come evitare qualsiasi degrado della qualit\u00e0 della ricerca dovuto all'ottimizzazione delle prestazioni di cui sopra, realizzandone comunque la maggior parte dei vantaggi. Cio\u00e8, presentiamo una serie di semplici modifiche -LRB- ma importanti -RRB- nelle tecniche di potatura per la creazione dell'indice pruned. Il nostro contributo principale \u00e8 un nuovo algoritmo di calcolo delle risposte che garantisce che le pagine con la migliore corrispondenza -LRB- secondo la metrica di ranking del motore di ricerca -RRB- siano sempre posizionate in cima ai risultati di ricerca, anche se stiamo calcolando la prima batch di risposte dall'indice ridotto per la maggior parte del tempo. Queste tecniche di potatura avanzate e gli algoritmi di calcolo delle risposte vengono esplorati nel context dell'architettura cluster comunemente impiegata dai motori di ricerca odierni. Infine, studieremo e presenteremo come i motori di ricerca possono ridurre al minimo il costo operativo della risposta alle domande fornendo allo stesso tempo risultati di ricerca di alta qualit\u00e0. Figura 1: -LRB- a -RRB- Il motore di ricerca replica il suo intero indice IF per aumentare la capacit\u00e0 di risposta alle query. -LRB- b -RRB- Nel primo livello, i piccoli IP pindex gestiscono la maggior parte delle query. Quando l'IP non pu\u00f2 rispondere a una query, viene reindirizzato al 2\u00b0 livello, dove viene utilizzato l'indice completo IF per calcolare la risposta. 6. LAVORI CORRELATI -LSB- 3, 30 -RSB- forniscono una buona panoramica dell'indicizzazione inversa nei motori di ricerca Web e nei sistemi IR. Studi sperimentali e analisi di vari schemi di partizionamento per un indice invertito sono presentati in -LSB- 6, 23, 33 -RSB-. Gli algoritmi di potatura presentati in questo articolo sono indipendenti dallo schema di partizionamento utilizzato. Tuttavia, -LSB- 1, 5, 7, 27 -RSB- non considerano alcuna qualit\u00e0 indipendente dalla query -LRB- come PageRank -RRB- nella funzione di classificazione. -LSB- 32 -RSB- presenta un quadro generico per il calcolo delle risposte top-k approssimative con alcuni limiti probabilistici sulla qualit\u00e0 dei risultati. Il nostro lavoro si estende essenzialmente -LSB- 1, 2, 4, 7, 20, 27,31 -RSB- proponendo meccanismi per fornire la garanzia di correttezza dei risultati top-k calcolati. I motori di ricerca utilizzano vari metodi di memorizzazione nella cache come mezzo per ridurre i costi associati alle query -LSB- 18, 19, 21, 31 -RSB-. Questo filo di lavoro \u00e8 anche ortogonale al nostro perch\u00e9 uno schema di memorizzazione nella cache pu\u00f2 funzionare sopra il nostro p-index per ridurre al minimo il costo di calcolo della risposta. Le esatte funzioni di classificazione utilizzate dagli attuali motori di ricerca sono segreti gelosamente custoditi. In generale, tuttavia, le classifiche si basano sulla pertinenza dipendente dalla query e sulla \"qualit\u00e0\" del documento indipendente dalla query. '' Allo stesso modo, ci sono una serie di lavori che misurano la `` qualit\u00e0 '' dei documenti, tipicamente catturati attraverso l'analisi basata sui collegamenti -LSB- 17, 28, 26 -RSB-. Poich\u00e9 il nostro lavoro non assume una forma particolare di funzione di classificazione, \u00e8 complementare a questo corpus di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.\u00e8 complementare a questo corpo di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.\u00e8 complementare a questo corpo di lavoro. \u00c8 stato svolto un grande lavoro sul calcolo dei risultati top-k. 7. OSSERVAZIONI CONCLUSIVE I motori di ricerca Web in genere eliminano i loro indici invertiti su larga scala per adattarsi a enormi carichi di query. Anche se questo approccio pu\u00f2 migliorare le prestazioni, calcolando i risultati migliori da un indice ridotto potremmo notare un significativo peggioramento della qualit\u00e0 dei risultati. In questo articolo abbiamo fornito una struttura per nuove tecniche di potatura e algoritmi di calcolo delle risposte che garantiscono che le pagine pi\u00f9 corrispondenti siano sempre posizionate in cima ai risultati di ricerca nell'ordine corretto. Abbiamo studiato due tecniche di potatura, vale a dire la potatura basata su keyphrases e quella basata su documenti, nonch\u00e9 la loro combinazione. I nostri risultati sperimentali hanno dimostrato che i nostri algoritmi possono essere utilizzati efficacemente per eliminare un indice invertito senza compromettere la qualit\u00e0 dei risultati. In particolare, un indice eliminato per keyphrases pu\u00f2 garantire il 73% delle query con una dimensione pari al 30% dell'indice completo, mentre un indice eliminato per documenti pu\u00f2 garantire il 68% delle query con la stessa dimensione. Quando combiniamo i due algoritmi di potatura possiamo garantire il 60% delle query con una dimensione dell'indice del 16%. La nostra speranza \u00e8 che il nostro lavoro aiuti i motori di ricerca a sviluppare indici migliori, pi\u00f9 veloci e pi\u00f9 efficienti e quindi a fornire agli utenti una migliore esperienza di ricerca sul Web.", "keyphrases": ["motore di ricerca web", "indice invertito su larga scala", "carico di domanda", "indice di prugna", "mercato della ricerca online", "degrado della qualit\u00e0 dei risultati", "la base di prugna ha prestazioni ottimali", "tecnica della prugna", "algoritmo di calcolo dei risultati", "pagina con la corrispondenza migliore", "risultato della ricerca migliore", "dimensione ottimale"]}
{"file_name": "J-25", "text": "Scommesse in stile booleano: una struttura per la negoziazione di titoli basata su formule logiche ABSTRACT Sviluppiamo una struttura per la negoziazione di titoli composti: strumenti finanziari che pagano in base ai risultati di dichiarazioni arbitrarie in logica proposizionale. L'acquisto o la vendita di titoli - che possono essere considerati come scommesse su o contro un particolare risultato futuro - consente agli agenti sia di coprire il rischio che di trarre profitto -LRB- in aspettativa -RRB- su previsioni soggettive. Un mercato mobiliare composto consente agli agenti di piazzare scommesse su combinazioni booleane arbitrarie di eventi, consentendo loro di raggiungere pi\u00f9 da vicino la loro esposizione ottimale al rischio e consentendo al mercato nel suo insieme di raggiungere pi\u00f9 da vicino l\u2019ottimo sociale. Il compromesso per consentire tale espressivit\u00e0 sta nella complessit\u00e0 dei problemi di ottimizzazione degli agenti e del banditore. Sviluppiamo e motiviamo il concetto di mercato mobiliare composto, presentando il quadro attraverso una serie di definizioni formali ed esempi. Analizziamo poi nel dettaglio il problema di abbinamento del banditore. Mostriamo che, con n eventi, il problema del matching \u00e8 co-NP-completo nel caso divisibile e \u03a3p2-completo nel caso indivisibile. Mostriamo che quest'ultimo risultato di durezza vale anche in presenza di severe restrizioni linguistiche sulle offerte. Con eventi log n, il problema \u00e8 polinomiale nel caso divisibile e NP-completo nel caso indivisibile. Discuteremo brevemente gli algoritmi di abbinamento e i casi speciali trattabili. 1. INTRODUZIONE I mercati mobiliari consentono effettivamente ai trader di piazzare scommesse sui risultati di proposte future incerte. Il valore economico dei mercati mobiliari \u00e8 duplice. In primo luogo, consentono ai trader di coprire i rischi o di assicurarsi contro risultati indesiderati. Ad esempio, il proprietario di un titolo potrebbe acquistare un'opzione put -LRB- il diritto di vendere il titolo a un prezzo particolare -RRB- per assicurarsi contro una flessione del titolo. In secondo luogo, i mercati mobiliari consentono agli operatori di speculare o di ottenere un profitto soggettivo atteso quando i prezzi di mercato non riflettono la loro valutazione della probabilit\u00e0 di risultati futuri. Ad esempio, un trader potrebbe acquistare un'opzione call se ritiene che vi sia un'elevata probabilit\u00e0 che il prezzo del titolo sottostante salga, indipendentemente dall'esposizione al rischio derivante dalle variazioni del prezzo del titolo. Poich\u00e9 i trader possono ottenere un profitto se riescono a effettuare valutazioni probabilistiche efficaci, spesso i prezzi nei mercati finanziari forniscono previsioni aggregate molto accurate di eventi futuri -LSB- 10, 29, 27, 28 -RSB-. I mercati dei titoli reali hanno strutture di rendimento complesse con vari fattori scatenanti. Tuttavia, questi possono tutti essere modellati come raccolte di titoli Arrow-Debreu pi\u00f9 elementari o atomici -LSB- 1, 8, 20 -RSB-. Una unit\u00e0 di un titolo Arrow-Debreu paga un dollaro se e solo se -LRB- se e solo -RRB- si verifica un evento binario corrispondente; non paga nulla se l'evento non si verifica. Quindi, ad esempio, un'unit\u00e0 di un titolo denominata -LRB- Acme100 -RRB- potrebbe pagare $ 1 se e solo se le azioni di Acme fossero superiori a $ 100 il 4 gennaio,2004. Un'opzione su azioni Acme, come verrebbe definita in una borsa finanziaria, pu\u00f2 essere considerata come un portafoglio di tali titoli atomici.1 In questo articolo, sviluppiamo e analizziamo una struttura per la negoziazione in mercati di titoli composti con profitti condizionati da combinazioni logiche di eventi, compresi i condizionali. Ad esempio, dati gli eventi binari A, B e C, un trader potrebbe fare un'offerta per acquistare tre unit\u00e0 di un titolo indicato -LRB- A n B \u00af VC -RRB- che paga $ 1 se e solo l'evento composto A n B \u00af VC avviene per trenta centesimi ciascuno. Dato un insieme di tali offerte, il banditore deve affrontare un complesso problema di abbinamento per decidere quali offerte sono accettate per quante unit\u00e0 e a quale prezzo. In genere, il banditore non cerca di assumersi alcun rischio, ma si limita a confrontare le transazioni accettabili tra gli offerenti, ma consideriamo anche formulazioni alternative in cui il banditore agisce come un market maker disposto ad accettare alcuni rischi. Esaminiamo la complessit\u00e0 computazionale del problema di abbinamento del banditore. Sia la lunghezza della descrizione di tutti i titoli disponibili O -LRB- n -RRB-. Con n eventi, il problema di abbinamento \u00e8 co-NP-completo nel caso divisibile ed Ep2-completo nel caso indivisibile. Questa durezza completa Ep2 rimane anche quando la lingua delle offerte \u00e8 significativamente limitata. Con eventi log n, il problema \u00e8 polinomiale nel caso divisibile e NP-completo nel caso indivisibile. La sezione 2 presenta alcune informazioni di base, motivazioni e lavoro correlato necessari. La sezione 3 descrive formalmente il nostro quadro per i titoli composti e definisce il problema di abbinamento del banditore. La sezione 4 discute brevemente gli algoritmi naturali per risolvere il problema dell'abbinamento. La sezione 5 dimostra i nostri risultati centrali sulla complessit\u00e0 computazionale. La sezione 6 discute la possibilit\u00e0 di casi speciali trattabili. La sezione 7 si conclude con una sintesi e alcune idee sulle direzioni future. 2. PRELIMINARI 2.1 Context e notazione In questo mondo semplice ci sono quattro possibili stati futuri -- tutte le possibili combinazioni dei risultati degli eventi binari: colpito n acme100, colpito n acme100, colpito n acme100, colpito n acme100. Il rischio di copertura pu\u00f2 essere pensato come un\u2019azione di spostamento di denaro tra vari possibili stati futuri. Ad esempio, insur1Tecnicamente, un'opzione \u00e8 un portafoglio di infiniti titoli atomici, sebbene possa essere modellato approssimativamente con un numero finito. Trasferire denaro dalla propria casa dai futuri stati in cui \u00e8 colpito non \u00e8 vero per gli stati in cui si trova. La vendita di un titolo denominato -LRB-acme100 -RRB- - che paga $ 1 se e solo se si verifica l'evento acme100 - trasferisce denaro dagli stati futuri in cui il prezzo di Acme \u00e8 superiore a $ 100 il 4 gennaio agli stati in cui non lo \u00e8. La speculazione \u00e8 anche un atto di trasferimento di denaro tra stati futuri, sebbene solitamente associato alla massimizzazione del rendimento atteso piuttosto che alla riduzione del rischio. Ad esempio, scommettere su una squadra di calcio sposta denaro dallo stato \"la squadra perde\" allo stato \"la squadra vince\".Tutti i possibili risultati futuri formano uno spazio degli stati \u03a9, costituito da stati mutuamente esclusivi ed esaustivi \u03c9 E \u03a9. Spesso un modo pi\u00f9 naturale di pensare ai possibili risultati futuri \u00e8 come uno spazio degli eventi A di eventi AEA linearmente indipendenti che possono sovrapporsi arbitrariamente. Quindi nel nostro esempio del giocattolo colpito n acme100 \u00e8 uno dei quattro stati disgiunti, mentre colpito \u00e8 uno dei due eventi. Si noti che un insieme di n eventi linearmente indipendenti definisce uno spazio degli stati \u03a9 di dimensione 2 '' costituito da tutte le possibili combinazioni di risultati di eventi. Al contrario, qualsiasi spazio di stato \u03a9 pu\u00f2 essere fattorizzato in eventi -LSB- log l\u03a9ll. Supponiamo che A copra in modo esaustivo tutti i risultati futuri significativi -LRB-, cio\u00e8 copra tutte le eventualit\u00e0 contro le quali gli agenti potrebbero voler proteggersi e/o speculare su -RRB-. Quindi l'esistenza di 2 titoli linearmente indipendenti \u2013 chiamati mercato completo \u2013 consente agli agenti di distribuire arbitrariamente la loro ricchezza tra gli stati futuri.2 Un agente pu\u00f2 creare qualsiasi copertura o speculazione desideri. In condizioni classiche, gli agenti che commerciano in un mercato completo formano un equilibrio in cui il rischio \u00e8 allocato in modo Pareto ottimale. Se il mercato \u00e8 incompleto, cio\u00e8 consiste di meno di 2 '' titoli linearmente indipendenti, allora in generale gli agenti non possono costruire coperture arbitrarie e le allocazioni di equilibrio potrebbero non essere ottimali -LSB- 1, 8, 19, 20 -RSB-. Negli scenari del mondo reale, il numero di eventi significativi n \u00e8 elevato e quindi il numero di garanzie richieste per la completezza \u00e8 intrattabile. Non esiste n\u00e9 esister\u00e0 mai un mercato veramente completo. Una delle motivazioni alla base dei mercati mobiliari composti \u00e8 quella di fornire un meccanismo che supporti il \u200b\u200bmassimo trasferimento del rischio utilizzando il minor numero di transazioni possibile. I titoli composti consentono un elevato grado di espressivit\u00e0 nella costruzione delle offerte. Il compromesso per una maggiore espressivit\u00e0 \u00e8 una maggiore complessit\u00e0 computazionale, sia dal punto di vista dell'offerente che del banditore. 2.2 Lavoro correlato La ricerca per ridurre il numero di strumenti finanziari necessari per supportare un'allocazione ottimale del rischio risale al lavoro originale di Arrow -LSB- 1 -RSB-. Il requisito sopra indicato di \"solo\" 2 \"titoli linearmente indipendenti \u00e8 di per s\u00e9 una riduzione della formulazione pi\u00f9 semplice. In un'economia con k beni standard, il mercato completo pi\u00f9 semplice contiene k \u2022 2 titoli, ciascuno dei quali frutta in un bene con una realizzazione statale. Arrow -LSB- 1 -RSB- ha dimostrato che \u00e8 completo anche un mercato in cui titoli e beni sono sostanzialmente separati, con 2'' titoli che pagano in un unico bene numerario pi\u00f9 k mercati spot nei beni standard. Per i nostri scopi, dobbiamo considerare solo il mercato dei titoli. 2Per titoli linearmente indipendenti intendiamo che i vettori dei payoff in tutti gli stati futuri di questi titoli sono linearmente indipendenti. Varian -LSB- 34 -RSB- mostra che un mercato completo pu\u00f2 essere costruito utilizzando meno di 2n titoli,sostituzione dei titoli mancanti con opzioni. Tuttavia, il numero di strumenti finanziari linearmente indipendenti \u2013 titoli pi\u00f9 opzioni \u2013 deve essere 2n per garantire la completezza. Gli autori mostrano che in alcuni casi il mercato pu\u00f2 essere strutturato e \u201ccompattato\u201d in analogia alle rappresentazioni di rete bayesiana delle distribuzioni di probabilit\u00e0 congiunte -LSB- 23 -RSB-. Essi mostrano che, se le indipendenze neutrali al rischio di tutti gli agenti concordano con le indipendenze codificate nella struttura del mercato, allora il mercato \u00e8 operativamente completo. Per insiemi di agenti tutti con costante avversione assoluta al rischio, \u00e8 sufficiente un accordo sulle indipendenze di Markov. Bossaerts, Fine e Ledyard -LSB- 2 -RSB- sviluppano un meccanismo che chiamano negoziazione a valore combinato -LRB- CVT -RRB- che consente ai trader di ordinare un portafoglio arbitrario di titoli in un'unica offerta, anzich\u00e9 suddividere l'ordine in una sequenza di offerte su singoli titoli. Se l'ordine di portafoglio viene accettato, tutte le negoziazioni implicite sui singoli titoli vengono eseguite simultaneamente, eliminando cos\u00ec il cosiddetto rischio di esecuzione che i prezzi cambino nel mezzo di una sequenza pianificata di ordini. Gli autori conducono esperimenti di laboratorio dimostrando che, anche nei mercati sottili in cui il normale trading sequenziale fallisce, la CVT supporta una determinazione dei prezzi e un\u2019allocazione efficienti. Si noti che la CVT differisce in modo significativo dalla negoziazione di titoli composti. CVT consente la negoziazione istantanea di qualsiasi combinazione lineare di titoli, mentre i titoli composti consentono titoli pi\u00f9 espressivi in \u200b\u200bgrado di codificare combinazioni booleane non lineari di eventi. Ad esempio, CVT pu\u00f2 consentire a un agente di ordinare titoli -LRB- A -RRB- e -LRB- B -RRB- in un pacchetto che ripaga come una combinazione lineare di A e B,3 ma CVT non consentir\u00e0 il costruzione di un titolo composto -LRB- A n B -RRB- che paga $ 1 se e solo se si verificano sia A che B, oppure un titolo composto -LRB- AIB -RRB-. Le aste combinatorie consentono agli offerenti di assegnare valori distinti a tutti i possibili pacchetti di beni anzich\u00e9 solo ai singoli beni. I titoli composti differiscono dalle aste combinatorie per concetto e complessit\u00e0. I titoli composti consentono agli offerenti di costruire una scommessa arbitraria su uno qualsiasi dei 22n possibili eventi composti esprimibili come funzioni logiche degli n eventi base, condizionata a qualsiasi altro dei 22n eventi composti. Gli agenti ottimizzano in base alle proprie probabilit\u00e0 soggettive e all'attitudine al rischio -LRB- e, in generale, alle loro convinzioni sulle convinzioni e utilit\u00e0 di altri agenti, ad infinitum -RRB-. Il problema centrale del banditore \u00e8 identificare le opportunit\u00e0 di arbitraggio: cio\u00e8 abbinare le scommesse senza assumersi alcun rischio. Le aste combinatorie, invece, consentono offerte su uno qualsiasi dei 2n panieri di n beni. l\u2019incertezza \u2013 e quindi il rischio \u2013 non viene considerata. Il problema centrale del banditore \u00e8 massimizzare il benessere sociale. Si noti inoltre che i problemi risiedono in diverse classi di complessit\u00e0.Mentre la chiusura di un'asta combinatoria \u00e8 polinomiale nel caso divisibile e NP-completa nel caso indivisibile, l'abbinamento in un mercato di titoli composto \u00e8 NP-completo nel caso divisibile ed Ep2-completo nel caso indivisibile. Infatti, anche il problema di decidere se due offerte su titoli composti coincidono, anche nel caso divisibile, \u00e8 NP-completo -LRB- vedi Sezione 5.2 -RRB-. In un certo senso \u00e8 possibile tradurre il nostro problema di abbinamento per titoli composti in un problema analogo per la compensazione di scambi combinatori bilaterali -LSB- 31 -RSB- di dimensione esponenziale. Nello specifico, se consideriamo il profitto in un particolare stato come un bene, allora i titoli composti possono essere visti come pacchetti di quantit\u00e0 frazionarie -LRB- di tali beni -RRB-. Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Il vincolo di bilancio materiale che il banditore combinatorio deve affrontare corrisponde a una restrizione secondo la quale il banditore della sicurezza composta non pu\u00f2 assumere alcun rischio. Si noti che questa traduzione non \u00e8 affatto utile per affrontare il problema dell\u2019abbinamento dei titoli composti, poich\u00e9 lo scambio combinatorio risultante ha un numero esponenziale di beni. Hanson -LSB- 15 -RSB- sviluppa un meccanismo di mercato chiamato regola del punteggio di mercato che \u00e8 particolarmente adatto per consentire scommesse su un numero combinatorio di risultati. Il meccanismo mantiene una distribuzione di probabilit\u00e0 congiunta su tutti i 2n stati, esplicitamente o implicitamente utilizzando una rete bayesiana o altra rappresentazione compatta. Nel limite di un singolo trader, il meccanismo si comporta come una regola di punteggio, atta a interrogare un singolo agente per la sua distribuzione di probabilit\u00e0. Nel limite di molti trader, produce una stima combinata. Poich\u00e9 il mercato ha essenzialmente sempre una serie completa di prezzi pubblicati per tutti i possibili risultati, il meccanismo evita il problema dei mercati sottili, o illiquidit\u00e0, che necessariamente affligge qualsiasi mercato contenente un numero esponenziale di investimenti alternativi. Le offerte per titoli composti possono essere pensate come espressioni di disuguaglianze probabilistiche: ad esempio, un'offerta per acquistare -LRB- A n B -RRB- al prezzo 0,3 \u00e8 un'affermazione che la probabilit\u00e0 di A n B \u00e8 maggiore di 0,3. Se un insieme di offerte di singole unit\u00e0 corrisponde a un insieme di disuguaglianze probabilistiche incoerenti, allora esiste una corrispondenza. Affrontiamo questi problemi di seguito.Affrontiamo questi problemi di seguito.Affrontiamo questi problemi di seguito.", "keyphrases": ["scommessa combinatoria", "effetto probabile valutare", "combinazione logica arbitraria", "composto sicuro", "rete bayesiana", "commercio a valore combinato", "algoritmo approssimativo", "vettore di profitto", "caso trattabile", "base sicura"]}
{"file_name": "J-15", "text": "Scomposizione generalizzata del valore e aste multiattributo strutturate ABSTRACT I meccanismi di asta multiattributo generalmente rimangono agnostici riguardo alle preferenze dei trader o presuppongono forme altamente restrittive, come la piena additivit\u00e0. Le preferenze reali spesso mostrano dipendenze tra attributi, ma possono possedere una struttura che pu\u00f2 essere utilmente sfruttata per snellire la comunicazione e semplificare il funzionamento di un'asta multiattributo. Sviluppiamo tale struttura utilizzando la teoria delle funzioni di valore misurabili, una rappresentazione dell'utilit\u00e0 cardinale basata su un ordine sottostante sulle differenze di preferenza. Un insieme di relazioni di indipendenza condizionale locale su tali differenze supporta una rappresentazione generalizzata delle preferenze additive, che scompone l\u2019utilit\u00e0 attraverso gruppi sovrapposti di attributi correlati. Introduciamo un meccanismo d'asta iterativo che mantiene i prezzi su cluster locali di attributi piuttosto che sull'intero spazio delle configurazioni congiunte. Quando le preferenze dei trader sono coerenti con la struttura additiva generalizzata dell'asta, il meccanismo produce allocazioni approssimativamente ottimali, a prezzi VCG approssimativi. 1. INTRODUZIONE I meccanismi di negoziazione multiattributo estendono i tradizionali meccanismi basati esclusivamente sul prezzo facilitando la negoziazione su una serie di attributi predefiniti che rappresentano vari aspetti non legati al prezzo dell'operazione. Invece di negoziare su un bene o servizio completamente definito, un meccanismo multiattributo ritarda l\u2019impegno verso configurazioni specifiche finch\u00e9 non vengono identificati i candidati pi\u00f9 promettenti. Ad esempio, l'ufficio acquisti di un'azienda pu\u00f2 utilizzare un'asta multiattributo per selezionare un fornitore di dischi rigidi. Per tenere conto delle preferenze dei trader, il meccanismo dell'asta deve estrarre informazioni valutative su un dominio complesso di configurazioni multidimensionali. Costruire e comunicare una specificazione completa delle preferenze pu\u00f2 rappresentare un grave onere anche per un numero moderato di attributi, pertanto le aste pratiche multiattributo devono accogliere specifiche parziali o supportare l'espressione compatta delle preferenze assumendo una forma semplificata. La forma multiattributo di gran lunga pi\u00f9 popolare da adottare \u00e8 la pi\u00f9 semplice: una rappresentazione additiva in cui il valore complessivo \u00e8 una combinazione lineare di valori associati a ciascun attributo. Ad esempio, diverse proposte recenti per aste iterative multiattributo -LSB- 2, 3, 8, 19 -RSB- richiedono rappresentazioni di preferenze aggiuntive. Tale additivit\u00e0 riduce esponenzialmente la complessit\u00e0 della specificazione delle preferenze -LRB- rispetto al caso discreto generale -RRB-, ma preclude l'espressione di qualsiasi interdipendenza tra gli attributi. In pratica, tuttavia, le interdipendenze tra gli attributi naturali sono abbastanza comuni. In tali casi una funzione di valore additivo potrebbe non essere in grado di fornire nemmeno un\u2019approssimazione ragionevole delle preferenze reali. D\u2019altro canto, i modelli pienamente generali sono intrattabili,ed \u00e8 ragionevole aspettarsi che le preferenze multiattributo mostrino una certa struttura. Il nostro obiettivo, quindi, \u00e8 identificare le rappresentazioni strutturate pi\u00f9 sottili ma pi\u00f9 ampiamente applicabili e sfruttare queste propriet\u00e0 delle preferenze nei meccanismi di scambio. Proponiamo un meccanismo d'asta iterativo basato proprio su tale struttura di preferenza flessibile. Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo periodo di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo termine di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 delle preferenze. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il nostro approccio si ispira alla progettazione di un\u2019asta iterativa di appalto multiattributo per preferenze additive, dovuta a Parkes e Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone due tipi di aste iterative: la prima -LRB- NLD -RRB- non fa ipotesi sulle preferenze dei trader e consente ai venditori di fare offerte sull'intero spazio degli attributi multidimensionali. Poich\u00e9 NLD mantiene una struttura dei prezzi esponenziale, \u00e8 adatto solo per domini di piccole dimensioni. L'altra asta -LRB- AD -RRB- presuppone funzioni di valutazione additiva dell'acquirente e di costo del venditore. Raccoglie le offerte di vendita per livello di attributo e per un singolo termine di sconto. Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Il prezzo di una configurazione \u00e8 definito come la somma dei prezzi dei livelli di attributo scelti meno lo sconto. L'asta che proponiamo supporta anche spazi di prezzo compatti, anche se per livelli di cluster di attributi piuttosto che di singleton. Date le sue radici nella teoria dell\u2019utilit\u00e0 multiattributo -LSB- 13 -RSB-, la condizione GAI \u00e8 definita rispetto alla funzione di utilit\u00e0 attesa. Applicarlo per modellare i valori per determinati risultati, quindi, richiede una reinterpretazione della preferenza in condizioni di certezza. A tal fine, sfruttiamo il fatto che i risultati dell\u2019asta sono associati a prezzi continui, che forniscono una scala naturale per valutare l\u2019entit\u00e0 della preferenza. Per prima cosa definiamo un quadro di rappresentazione delle preferenze che cattura, oltre al semplice ordinamento tra i valori di configurazione degli attributi, la differenza nella disponibilit\u00e0 a pagare -LRB- wtp -RRB- per ciascuno. Successivamente, costruiamo un collegamento diretto e formalmente giustificato dalle dichiarazioni di preferenza rispetto ai risultati prezzati a una scomposizione additiva generalizzata della funzione wtp. Dopo aver predisposto questa infrastruttura, utilizziamo questo strumento di rappresentazione per lo sviluppo di un meccanismo di asta iterativa multiattributo che consente ai trader di esprimere le loro preferenze complesse in formato GAI. Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.Studieremo poi le propriet\u00e0 allocative, computazionali e pratiche dell'asta. Nella Sezione 2 presentiamo il background essenziale del nostro quadro di rappresentazione, la funzione del valore misurabile -LRB- MVF -RRB-. La sezione 3 sviluppa nuove strutture multiattributo per MVF, supportando scomposizioni additive generalizzate. Successivamente, mostriamo l'applicabilit\u00e0 del quadro teorico alle preferenze nel trading. Il resto del documento \u00e8 dedicato al meccanismo d'asta proposto.", "keyphrases": ["asta", "asta multiattributo", "preferisco l'handl", "teoria della funzione valore misura", "meccanismo dell'asta iter", "mvf", "gau", "asta base gai"]}
{"file_name": "I-7", "text": "Impegno ed estorsione * ABSTRACT Assumere impegni, ad esempio attraverso promesse e minacce, consente a un giocatore di sfruttare i punti di forza della propria posizione strategica cos\u00ec come i punti deboli di quella dei suoi avversari. Quali impegni un giocatore pu\u00f2 assumere con credibilit\u00e0 dipende dalle circostanze. In alcuni, un giocatore pu\u00f2 impegnarsi solo nell'esecuzione di un'azione, in altri pu\u00f2 impegnarsi in modo condizionale rispetto alle azioni degli altri giocatori. Alcune situazioni consentono addirittura impegni su impegni o impegni verso azioni randomizzate. Esploriamo le propriet\u00e0 formali di questi tipi di impegno condizionato -LRB- -RRB- e le loro interrelazioni. Per evitare incoerenze tra gli impegni condizionati, assumiamo un ordine in cui i giocatori assumono i propri impegni. Centrale nelle nostre analisi \u00e8 la nozione di estorsione, che definiamo, per un dato ordine di giocatori, come un profilo che contiene, per ciascun giocatore, un impegno ottimale dati gli impegni dei giocatori che si sono impegnati in precedenza. Su questa base, indaghiamo per diversi tipi di impegno se sia vantaggioso impegnarsi prima piuttosto che dopo, e in che modo i risultati ottenuti attraverso le estorsioni si collegano all\u2019induzione a ritroso e all\u2019efficienza paretiana. 1. INTRODUZIONE Da un lato, il minimo che ci si pu\u00f2 aspettare dalla teoria dei giochi \u00e8 che fornisca una risposta alla domanda: quali azioni massimizzano l'utilit\u00e0 attesa di un agente in situazioni di processo decisionale interattivo. Da questa prospettiva, il modello formale di un gioco in forma strategica delinea solo le caratteristiche strategiche di una situazione interattiva. Oltre alla semplice scelta ed esecuzione di un'azione da una serie di azioni, possono esserci anche altri percorsi aperti a un agente. Ad esempio, la situazione strategica del territorio pu\u00f2 essere tale che una promessa, una minaccia o una combinazione di entrambe sarebbero pi\u00f9 conduttrici ai suoi fini. Allo stesso modo, una minaccia riesce a scoraggiare un agente solo se si pu\u00f2 far credere a quest\u2019ultimo che il minacciatore \u00e8 tenuto a mettere in atto la minaccia, nel caso in cui questa venga ignorata. In questo senso, le promesse e le minacce implicano essenzialmente un impegno da parte di chi le fa, limitando cos\u00ec di proposito la sua libert\u00e0 di scelta. Promesse e minacce sintetizzano uno dei fenomeni fondamentali e a prima vista forse pi\u00f9 sorprendenti della teoria dei giochi: pu\u00f2 accadere che un giocatore possa migliorare la propria posizione strategica limitando la propria libert\u00e0 di azione. Per impegni intendiamo tali limitazioni del proprio spazio di azione. L\u2019azione stessa potrebbe essere vista come l\u2019impegno finale. Compiere una particolare azione significa farlo escludendo tutte le altre azioni. Gli impegni assumono forme diverse e pu\u00f2 dipendere dalle circostanze quali possono essere assunti in modo credibile e quali no. Oltre a impegnarsi semplicemente a compiere un'azione, un agente potrebbe subordinare il suo impegno alle azioni di altri agenti, come fa, ad esempio, il rapitore, quando promette di liberare un ostaggio dietro pagamento di un riscatto,minacciando di tagliargli un altro dito, altrimenti. Alcune situazioni consentono addirittura impegni su impegni o impegni verso azioni randomizzate. Concentrandosi sulla selezione delle azioni piuttosto che sugli impegni, potrebbe sembrare che la concezione della teoria dei giochi come mera teoria delle decisioni interattive sia troppo ristretta. A questo riguardo, il punto di vista di Schelling potrebbe sembrare mostrare una comprensione pi\u00f9 completa di ci\u00f2 che la teoria dei giochi cerca di realizzare. Si potrebbe obiettare che gli impegni potrebbero essere visti come le azioni di un gioco pi\u00f9 ampio. -LSB-... -RSB- Quello che vogliamo \u00e8 una teoria che sistematizzi lo studio dei vari ingredienti universali che compongono la struttura-mossa dei giochi; un modello troppo astratto li mancher\u00e0. -LSB- 9, pp. 156-7 -RSB- La nostra preoccupazione riguarda queste tattiche di impegno, anche se la nostra analisi \u00e8 limitata a situazioni in cui i giocatori possono impegnarsi in un dato ordine e dove assumiamo gli impegni che i giocatori possono assumere sono dati. Nonostante l'avvertimento di Schelling per un quadro troppo astratto, il nostro approccio si baser\u00e0 sulla nozione formale di estorsione, che proporremo nella Sezione 4 come tattica uniforme per una classe completa di situazioni in cui gli impegni possono essere assunti in sequenza. Su questa base affrontiamo questioni come l'utilit\u00e0 di certi tipi di impegno in diverse situazioni -LRB- giochi strategici -RRB- o se sia meglio impegnarsi presto piuttosto che tardi. Forniamo anche un quadro per la valutazione di questioni pi\u00f9 generali di teoria dei giochi come la relazione tra le estorsioni e l\u2019induzione all\u2019indietro o l\u2019efficienza paretiana. Ad esempio, si \u00e8 sostenuto che gli impegni siano importanti per l'interazione degli agenti software nonch\u00e9 per la progettazione dei meccanismi. Nel primo caso, l\u2019incapacit\u00e0 di riprogrammare un agente software al volo pu\u00f2 essere vista come un impegno a rispettarne le specifiche e quindi sfruttata per rafforzare la propria posizione strategica in un context multiagente. Un meccanismo, d'altro canto, potrebbe essere visto come un insieme di impegni che orientano il comportamento dei giocatori in un certo modo desiderato -LRB- vedi, ad esempio, -LSB- 2 -RSB- -RRB-. Questi giochi analizzano situazioni in cui un leader si impegna in una strategia pura o mista e un numero di seguaci, che poi agiscono simultaneamente. Dopo aver discusso brevemente il lavoro correlato nella Sezione 2, presentiamo il quadro formale della teoria dei giochi, in cui definiamo le nozioni di tipo di impegno cos\u00ec come gli impegni condizionati e incondizionati -LRB- Sezione 3 -RRB-. Nella sezione 4 proponiamo il concetto generico di estorsione, che per ciascuna tipologia di impegno coglie l'idea di un profilo di impegno ottimale. La sezione 5 esamina brevemente alcune altre tipologie di impegno, come gli impegni condizionali induttivi, misti e misti. 2. LAVORI CORRELATI L'impegno \u00e8 un concetto centrale nella teoria dei giochi. La possibilit\u00e0 di assumere impegni distingue la teoria dei giochi cooperativa da quella non cooperativa -LSB- 4, 6 -RSB-. I giochi di leadership, come accennato nell\u2019introduzione,analizzare l'impegno verso strategie pure o miste in quello che \u00e8 essenzialmente un context a due giocatori -LSB- 15, 16 -RSB-. In modo informale, Schelling -LSB- 9 -RSB- ha sottolineato l'importanza delle promesse, delle minacce e simili per una corretta comprensione dell'interazione sociale. A un livello pi\u00f9 formale, le minacce sono presenti anche nella teoria della contrattazione. Il gioco delle minacce di Nash -LSB- 5 -RSB- e le minacce razionali di Harsanyi -LSB- 3 -RSB- sono due importanti primi esempi. Inoltre, gli impegni hanno giocato un ruolo significativo nella teoria della selezione dell'equilibrio -LRB- vedi, ad esempio, -LSB- 13 -RSB-. Negli ultimi anni, la teoria dei giochi \u00e8 diventata quasi indispensabile come strumento di ricerca per l\u2019informatica e la ricerca su agenti -LRB- multi -RRB-. Gli impegni non sono affatto passati inosservati -LRB- vedi Figura 1: Impegnarsi in una strategia dominata pu\u00f2 essere vantaggioso. ad esempio, -LSB- 1, 11 -RSB- -RRB-. Recentemente anche gli aspetti strategici degli impegni hanno attirato l'attenzione degli informatici. Pertanto, Conitzer e Sandholm -LSB- 2 -RSB- hanno studiato la complessit\u00e0 computazionale del calcolo della strategia ottimale a cui impegnarsi in giochi in forma normale e bayesiani. Sandholm e Lesser -LSB- 8 -RSB- impiegano impegni livellati per la progettazione di sistemi multiagente in cui gli accordi contrattuali non sono completamente vincolanti. Un altro collegamento tra impegni e informatica \u00e8 stato sottolineato da Samet -LSB- 7 -RSB- e Tennenholtz -LSB- 12 -RSB-. Il loro punto di partenza \u00e8 la constatazione che i programmi possono essere utilizzati per formulare impegni condizionati ai programmi di altri sistemi. Il nostro approccio \u00e8 simile all'impostazione Stackleberg in quanto presupponiamo un ordine in cui i giocatori si impegnano. Noi, tuttavia, consideriamo diverse tipologie di impegno, tra cui gli impegni condizionati, e proponiamo un concetto di soluzione generico. 6. SOMMARIO E CONCLUSIONE In alcune situazioni gli agenti possono rafforzare la loro posizione strategica impegnandosi in una particolare linea d'azione. Esistono vari tipi di impegno, ad esempio puro, misto e condizionato. Il tipo di impegno che un agente \u00e8 in grado di assumere dipende essenzialmente dalla situazione considerata. Se gli agenti commettono in un ordine particolare, esiste una tattica comune all'assunzione di impegni di qualsiasi tipo, che abbiamo formalizzato mediante il concetto di estorsione. Questo concetto generico di estorsione pu\u00f2 essere analizzato in abstracto. Inoltre, sulla base di esso \u00e8 possibile confrontare formalmente e sistematicamente le diverse tipologie di impegno. Abbiamo visto che il tipo di impegno che un agente pu\u00f2 assumere ha un profondo impatto su ci\u00f2 che un agente pu\u00f2 ottenere in una situazione simile a un gioco. In alcune situazioni un giocatore \u00e8 molto aiutato se \u00e8 nella posizione di impegnarsi condizionatamente, mentre in altre sarebbero pi\u00f9 proficui impegni misti. Ci\u00f2 solleva la questione delle caratteristiche formali caratteristiche delle situazioni in cui \u00e8 vantaggioso per un giocatore poter assumere impegni di un determinato tipo.Un altro problema che lasciamo per la ricerca futura \u00e8 la complessit\u00e0 computazionale nel trovare un\u2019estorsione per i diversi tipi di impegno.", "keyphrases": ["commettere", "credibile", "teoria del gioco", "decidere", "posizione strategica", "libert\u00e0 di azione", "sistema multiag", "distribuire il calcolo", "mercato degli elettroni", "estorcere", "insieme di Stackleberg", "impegno in condizioni ottimali", "tipo di commit sequenziale", "indurre ipotesi", "pareto efficace", "pareto effici condit extort"]}
{"file_name": "J-10", "text": "Comprendere il comportamento degli utenti nella segnalazione di feedback online SOMMARIO Le recensioni online sono diventate sempre pi\u00f9 popolari come mezzo per giudicare la qualit\u00e0 di vari prodotti e servizi. Il lavoro precedente ha dimostrato che resoconti contraddittori e pregiudizi degli utenti sottostanti rendono difficile giudicare il vero valore di un servizio. In questo documento, esaminiamo i fattori sottostanti che influenzano il comportamento degli utenti quando riportano il feedback. Esaminiamo due fonti di informazione oltre alle valutazioni numeriche: prove linguistiche provenienti dal commento testuale che accompagna una revisione e modelli nella sequenza temporale dei resoconti. Innanzitutto mostriamo che i gruppi di utenti che discutono ampiamente di una determinata funzionalit\u00e0 hanno maggiori probabilit\u00e0 di concordare una valutazione comune per quella funzionalit\u00e0. In secondo luogo, mostriamo che la valutazione di un utente riflette in parte la differenza tra la qualit\u00e0 reale e le aspettative di qualit\u00e0 precedenti desunte dalle recensioni precedenti. Entrambi ci forniscono un modo meno rumoroso per produrre stime di valutazione e rivelare le ragioni alla base dei pregiudizi degli utenti. Le nostre ipotesi sono state convalidate da prove statistiche provenienti dalle recensioni degli hotel sul sito TripAdvisor. 1. MOTIVAZIONI considerano attentamente il feedback online quando prendono decisioni di acquisto e sono disposti a pagare premi di reputazione per prodotti o servizi che hanno una buona reputazione. Analisi recenti, tuttavia, sollevano importanti questioni riguardanti la capacit\u00e0 dei forum esistenti di riflettere la reale qualit\u00e0 di un prodotto. In assenza di incentivi chiari, gli utenti con una prospettiva moderata non si preoccuperanno di esprimere le proprie opinioni, il che porta a un campione di recensioni non rappresentativo. In queste circostanze, utilizzare la media aritmetica per prevedere la qualit\u00e0 -LRB- come fa effettivamente la maggior parte dei forum -RRB- fornisce all'utente tipico uno stimatore con un'elevata varianza che spesso \u00e8 falsa. Migliorare il modo in cui aggreghiamo le informazioni disponibili dalle recensioni online richiede una profonda comprensione dei fattori sottostanti che influenzano il comportamento di valutazione degli utenti. Hu et al. -LSB- 12 -RSB- propongono il \"modello Brag-and-Moan\" in cui gli utenti valutano solo se la loro utilit\u00e0 del prodotto -LRB- ricavata da una distribuzione normale -RRB- cade al di fuori di un intervallo mediano. Gli autori concludono che il modello spiega la distribuzione empirica dei report e offre spunti su modi pi\u00f9 intelligenti per stimare la reale qualit\u00e0 del prodotto. Nel presente articolo estendiamo questa linea di ricerca e tentiamo di spiegare ulteriori fatti sul comportamento degli utenti quando riportano feedback online. Utilizzando le recensioni effettive degli hotel dal sito TripAdvisor2, prendiamo in considerazione due ulteriori fonti di informazioni oltre alle valutazioni numeriche di base inviate dagli utenti. La prima \u00e8 una semplice evidenza linguistica derivante dalla revisione testuale che solitamente accompagna le valutazioni numeriche. Abbiamo scoperto che gli utenti che commentano pi\u00f9 spesso la stessa funzionalit\u00e0 hanno maggiori probabilit\u00e0 di concordare una valutazione numerica comune per quella particolare funzionalit\u00e0. Intuitivamente, lunghi commenti rivelano all'utente l'importanza della funzionalit\u00e0.Poich\u00e9 le persone tendono ad essere pi\u00f9 informate sugli aspetti che considerano importanti, si potrebbe presumere che gli utenti che discutono una determinata funzionalit\u00e0 in modo pi\u00f9 dettagliato abbiano maggiore autorit\u00e0 nella valutazione di tale funzionalit\u00e0. Successivamente esaminiamo la relazione tra una recensione. Figura 1: la pagina di TripAdvisor che mostra le recensioni per un famoso hotel di Boston. Il nome dell'hotel e gli annunci pubblicitari sono stati deliberatamente cancellati. e le recensioni che lo hanno preceduto. Un esame delle recensioni online mostra che le valutazioni fanno spesso parte dei thread di discussione, in cui un post non \u00e8 necessariamente indipendente dagli altri post. Si possono vedere, ad esempio, utenti che si sforzano di contraddire o di concordare con veemenza con le osservazioni degli utenti precedenti. Analizzando la sequenza temporale dei report, concludiamo che le revisioni passate influenzano i report futuri, poich\u00e9 creano alcune aspettative precedenti riguardo alla qualit\u00e0 del servizio. La percezione soggettiva dell'utente \u00e8 influenzata dal divario tra l'aspettativa precedente e l'effettiva prestazione del servizio -LSB- 17, 18, 16, 21 -RSB- che si rifletter\u00e0 successivamente nella valutazione dell'utente. Proponiamo un modello che cattura la dipendenza dei rating dalle aspettative precedenti e lo convalida utilizzando i dati empirici che abbiamo raccolto. Entrambi i risultati possono essere utilizzati per migliorare il modo in cui i meccanismi di reputazione aggregano le informazioni provenienti dalle singole recensioni. Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.Il nostro primo risultato pu\u00f2 essere utilizzato per determinare una stima della qualit\u00e0 caratteristica per caratteristica, dove per ciascuna caratteristica viene considerato un diverso sottoinsieme di recensioni -LRB-, ovvero quelle con commenti lunghi su quella caratteristica -RRB-. Il secondo porta ad un algoritmo che produce una stima pi\u00f9 precisa della qualit\u00e0 reale.", "keyphrases": ["recensione in linea", "meccanico di reputazione", "stima della qualit\u00e0 caratteristica per caratteristica", "assenza di chiaro incentivo", "utilit\u00e0 del prodotto", "modello di vantarsi e lamentarsi", "valutare", "ottimo probabilmente bimodale", "distribuzione a forma di U", "orientamento semantico della valutazione del prodotto", "correlare", "ampio arco di tempo"]}
{"file_name": "C-1", "text": "Discovery scalabile dei servizi di rete basato su UDDI * ABSTRACT Il rilevamento efficiente dei servizi di rete \u00e8 essenziale per il successo del grid computing. La standardizzazione delle griglie basate sui servizi web ha comportato la necessit\u00e0 di implementare meccanismi scalabili di scoperta dei servizi web nelle griglie. Anche se UDDI \u00e8 stato di fatto lo standard industriale per la scoperta dei servizi web, ha imposto requisiti di stretta replica tra i registri e la mancanza del controllo autonomo ne ha gravemente ostacolato la diffusione e l\u2019utilizzo diffuso. Con l'avvento del grid computing, il problema della scalabilit\u00e0 dell'UDDI diventer\u00e0 un ostacolo che ne impedir\u00e0 l'implementazione nelle reti. In questo articolo presentiamo la nostra architettura distribuita di rilevamento dei servizi Web, denominata DUDE -LRB- Distributed UDDI Deployment Engine -RRB-. DUDE sfrutta DHT -LRB- Distributed Hash Tables -RRB- come meccanismo di incontro tra pi\u00f9 registri UDDI. DUDE consente ai consumatori di interrogare pi\u00f9 registri, consentendo allo stesso tempo alle organizzazioni di avere un controllo autonomo sui propri registri. . Sulla base del prototipo preliminare su PlanetLab, riteniamo che l'architettura DUDE possa supportare una distribuzione efficace dei registri UDDI rendendo cos\u00ec l'UDDI pi\u00f9 robusto e risolvendo anche i suoi problemi di ridimensionamento. Inoltre, l'architettura DUDE per la distribuzione scalabile pu\u00f2 essere applicata oltre UDDI a qualsiasi meccanismo di Grid Service Discovery. 1. INTRODUZIONE La scoperta efficiente dei servizi di rete \u00e8 essenziale per il successo del grid computing. meccanismi di scoperta da implementare nelle griglie. I servizi di Grid Discovery offrono la possibilit\u00e0 di monitorare e scoprire risorse e servizi sulle griglie. Forniscono la possibilit\u00e0 di interrogare e sottoscrivere informazioni su risorse/servizi. Lo stato dei dati deve essere mantenuto in uno stato soft in modo che le informazioni pi\u00f9 recenti siano sempre disponibili. Le informazioni raccolte devono essere fornite a una variet\u00e0 di sistemi allo scopo di utilizzare la griglia o fornire informazioni sintetiche. Tuttavia, il problema fondamentale \u00e8 la necessit\u00e0 di essere scalabili per gestire enormi quantit\u00e0 di dati provenienti da pi\u00f9 fonti. La comunit\u00e0 dei servizi web ha affrontato la necessit\u00e0 di rilevamento dei servizi, prima che le reti fossero anticipate, tramite uno standard industriale chiamato UDDI. Tuttavia, anche se UDDI \u00e8 stato di fatto lo standard industriale per il rilevamento dei servizi web, ha imposto requisiti di stretta replica tra i registri e la mancanza di controllo autonomo, tra le altre cose ha gravemente ostacolato la sua diffusione e utilizzo diffuso -LSB- 7 -RSB- . Con l'avvento del grid computing, il problema della scalabilit\u00e0 con UDDI diventer\u00e0 un ostacolo che ne impedir\u00e0 l'implementazione nelle reti. Questo documento affronta il problema della scalabilit\u00e0 e un modo per trovare servizi su pi\u00f9 registri in UDDI sviluppando un'architettura di rilevamento dei servizi Web distribuiti. La distribuzione della funzionalit\u00e0 UDDI pu\u00f2 essere ottenuta in diversi modi e forse utilizzando diverse infrastrutture/piattaforme di calcolo distribuite -LRB- ad esempio CORBA, DCE, ecc. -RRB-.In questo articolo esploriamo come sfruttare la tecnologia Distributed Hash Table -LRB- DHT -RRB- per sviluppare un'architettura scalabile di rilevamento dei servizi Web distribuiti. Un DHT \u00e8 un sistema distribuito peer-to-peer -LRB- P2P -RRB- che forma un overlay strutturato che consente un routing pi\u00f9 efficiente rispetto alla rete sottostante. Il primo fattore motivante \u00e8 la semplicit\u00e0 intrinseca dell'astrazione put/get fornita dai DHT, che semplifica la creazione rapida di applicazioni sui DHT. Altre piattaforme/middleware di elaborazione distribuita, pur fornendo pi\u00f9 funzionalit\u00e0, hanno un sovraccarico e una complessit\u00e0 molto pi\u00f9 elevati. Il secondo fattore motivante deriva dal fatto che i DHT sono uno strumento relativamente nuovo per costruire applicazioni distribuite e vorremmo testarne il potenziale applicandolo al problema della distribuzione di UDDI. Nella sezione successiva, forniamo una breve panoramica dei servizi di informazione di rete, UDDI e le sue limitazioni, seguita da una panoramica dei DHT nella sezione 3. La sezione 4 descrive la nostra architettura proposta con dettagli sui casi d'uso. Nella Sezione 5, l'Articolo 2 descrive la nostra attuale implementazione, seguita dai nostri risultati nella Sezione 6. La Sezione 7 discute il lavoro correlato in quest'area e la Sezione 8 contiene le nostre osservazioni conclusive. 2. BACKGROUND 2.1 Grid Service Discovery Il grid computing \u00e8 basato su standard che utilizzano la tecnologia dei servizi web. Nell'architettura presentata in -LSB-6 -RSB-, la funzione di rilevamento del servizio \u00e8 assegnata ad un servizio Grid specializzato chiamato Registro. La sua funzione di base lo rende simile al registro UDDI. Per ottenere la scalabilit\u00e0, i servizi Index di diversi contenitori Globus possono registrarsi tra loro in modo gerarchico per aggregare i dati. Nello specifico, questo approccio non si adatta bene ai sistemi che cercano di sfruttare la convergenza del grid e del peer-to-peer computing -LSB- 5 -RSB-. 2.2 UDDI Al di l\u00e0 del grid computing, il problema del service discovery deve essere affrontato pi\u00f9 in generale nella comunit\u00e0 dei servizi web. Ancora una volta, la scalabilit\u00e0 \u00e8 una delle principali preoccupazioni poich\u00e9 milioni di acquirenti alla ricerca di servizi specifici hanno bisogno di trovare tutti i potenziali venditori del servizio che possano soddisfare le loro esigenze. Sebbene esistano diversi modi per farlo, i comitati per gli standard dei servizi Web soddisfano questo requisito attraverso una specifica denominata UDDI -LRB- Universal Description, Discovery, and Integration -RRB-. Un registro UDDI consente a un'azienda di inserire tre tipi di informazioni in un registro UDDI: pagine bianche, pagine gialle e pagine verdi. L'intento di UDDI \u00e8 quello di funzionare come un registro per i servizi proprio come le Pagine Gialle sono un registro per le imprese. Proprio come nelle Pagine Gialle, le aziende registrano se stesse e i loro servizi in diverse categorie. In UDDI, le Pagine Bianche sono un elenco delle entit\u00e0 aziendali. Le pagine verdi rappresentano le informazioni tecniche necessarie per usufruire di un determinato servizio. Pertanto, esplorando un registro UDDI,uno sviluppatore dovrebbe essere in grado di individuare un servizio e un'azienda e scoprire come richiamare il servizio. Quando fu inizialmente offerto l'UDDI, offriva molto potenziale. Tuttavia, oggi scopriamo che l'UDDI non \u00e8 stato ampiamente utilizzato in Internet. In effetti, gli unici usi conosciuti dell'UDDI sono quelli conosciuti come registri UDDI privati \u200b\u200ball'interno dei confini di un'azienda. I lettori possono fare riferimento a -LSB- 7 -RSB- per un articolo recente che discute le carenze di UDDI e le propriet\u00e0 di un registro di servizio ideale. Il miglioramento dello standard UDDI continua a pieno ritmo e la versione 3 dell'UDDI -LRB- V3 -RRB- \u00e8 stata recentemente approvata come standard OASIS. Tuttavia, l\u2019UDDI oggi presenta problemi che non sono stati affrontati, come la scalabilit\u00e0 e l\u2019autonomia dei singoli registri. UDDI V3 fornisce un supporto pi\u00f9 ampio per ambienti multi-registro basati sulla portabilit\u00e0 delle chiavi Consentendo di registrare nuovamente le chiavi in \u200b\u200bpi\u00f9 registri, viene effettivamente abilitata la capacit\u00e0 di collegare registri in varie topologie. Tuttavia, a questo punto non viene fornita alcuna descrizione normativa di queste topologie nella specifica UDDI. I miglioramenti all'interno di UDDI V3 che consentono il supporto per ambienti multiregistro sono significativi e aprono la possibilit\u00e0 di ulteriori ricerche su come possono essere distribuiti ambienti multiregistro. Uno scenario di distribuzione consigliato proposto dalla specifica UDDI V3.0.2 consiste nell'utilizzare i registri aziendali UDDI come registri root ed \u00e8 possibile abilitarlo utilizzando la nostra soluzione. 2.3 Tabelle Hash Distribuite Una Tabella Hash Distribuita -LRB- DHT -RRB- \u00e8 un sistema distribuito peer-to-peer -LRB- P2P -RRB- che forma un overlay strutturato che consente un routing pi\u00f9 efficiente rispetto alla rete sottostante. Mantiene una raccolta di coppie chiave-valore sui nodi che partecipano a questa struttura grafica. Per la nostra distribuzione, una chiave \u00e8 l'hash di una parola chiave dal nome o dalla descrizione di un servizio. Ci saranno pi\u00f9 valori per questa chiave, uno per ciascun servizio contenente la parola chiave. Proprio come qualsiasi altra struttura dati della tabella hash, fornisce una semplice interfaccia composta da operazioni put -LRB- -RRB- e get -LRB- -RRB-. Ci\u00f2 deve essere fatto con robustezza a causa della natura transitoria dei nodi nei sistemi P2P. Le chiavi DHT sono ottenute da un ampio spazio identificatore. Una funzione hash, come MD5 o SHA-1, viene applicata al nome di un oggetto per ottenere la sua chiave DHT. I nodi in un DHT vengono anche mappati nello stesso spazio degli identificatori applicando la funzione hash al loro identificatore, come l'indirizzo IP e il numero di porta o la chiave pubblica. Lo spazio degli identificatori viene assegnato ai nodi in modo distribuito e deterministico, in modo che l'instradamento e la ricerca possano essere eseguiti in modo efficiente. I nodi di un DHT mantengono collegamenti con alcuni degli altri nodi del DHT. Lo schema di questi collegamenti \u00e8 noto come geometria del DHT. Ad esempio, nel Bamboo DHT -LSB- 11 -RSB-, e nel Pastry DHT -LSB- 8 -RSB- su cui si basa il Bamboo,i nodi mantengono i collegamenti ai nodi vicini e ad altri nodi distanti trovati in una tabella di instradamento. La tabella di routing consente un routing overlay efficiente. Per ottenere un routing o una ricerca coerente, una chiave DHT deve essere instradata al nodo con l'identificatore numericamente pi\u00f9 vicino. Per i dettagli su come vengono costruite e mantenute le tabelle di routing si rimanda il lettore a -LSB- 8, 11 -RSB-. 5. LAVORI CORRELATI Un quadro per la scoperta di servizi basati su QoS nelle griglie \u00e8 stato proposto in -LSB- 18 -RSB-. UDDIe, un registro UDDI esteso per la pubblicazione e la scoperta di servizi basati su parametri QoS, \u00e8 proposto in -LSB- 19 -RSB-. Il nostro lavoro \u00e8 complementare poich\u00e9 ci concentriamo su come federare i registri UDDI e affrontare il problema della scalabilit\u00e0 con UDDI. Il proxy DUDE pu\u00f2 pubblicare le propriet\u00e0 del servizio supportate da UDDIe nel DHT e supportare query di intervallo utilizzando le tecniche proposte per tali query su DHT. Quindi potremo offrire i vantaggi di scalabilit\u00e0 della nostra attuale soluzione ai registri UDDI e UDDIe. La scoperta dei servizi che soddisfano i requisiti di QoS e di prezzo \u00e8 stata studiata nel context di un'economia di rete, in modo che i pianificatori della rete possano utilizzare vari modelli di mercato come i mercati delle materie prime e le aste. A questo scopo \u00e8 stata proposta la Grid Market Directory -LSB- 20 -RSB-. Le descrizioni delle risorse e delle richieste sono espresse in RDF Schema, un linguaggio di markup semantico. Le regole del matchmaking sono espresse in TRIPLE, un linguaggio basato sulla logica del corno. Sebbene la nostra attuale implementazione si concentri sulla versione 2 di UDDI, in futuro prenderemo in considerazione estensioni semantiche a UDDI, WS-Discovery -LSB- 16 -RSB- e altri standard di Grid computing come Monitoring and Discovery Service -LRB- MDS -RRB- -LSB - 10 -RSB-. Quindi l'estensione pi\u00f9 semplice del nostro lavoro potrebbe comportare l'utilizzo del DHT per effettuare una ricerca iniziale basata sulla sintassi per identificare i registri locali che devono essere contattati. La convergenza del grid computing e del P2P \u00e8 stata esplorata in -LSB- 5 -RSB-. Un servizio UDDI federato -LSB- 4 -RSB- \u00e8 stato costruito sul sistema di pubblicazione-sottoscrizione PlanetP -LSB- 3 -RSB- per comunit\u00e0 P2P non strutturate. Il focus di questo lavoro \u00e8 stato la gestibilit\u00e0 del servizio federato. Il servizio UDDI \u00e8 trattato come un servizio applicativo ai sensi dell'Articolo 2 da gestire nel loro framework. Pertanto non affrontano il problema della scalabilit\u00e0 in UDDI e utilizzano invece la semplice replica. In -LSB- 21 -RSB-, gli autori descrivono un sistema di estensione UDDI -LRB- UX -RRB- che lancia una query federata solo se i risultati trovati localmente non sono adeguati. Sebbene il server UX sia posizionato come intermediario in modo simile al proxy UDDI descritto nel nostro framework DUDE, si concentra maggiormente sul framework QoS e non tenta di implementare un meccanismo di federazione senza soluzione di continuit\u00e0 come il nostro approccio basato su DHT. In -LSB- 22 -RSB- D2HT descrive un framework di scoperta costruito su DHT. Tuttavia, abbiamo scelto di utilizzare UDDI oltre a DHT. 6. CONCLUSIONI E LAVORI FUTURI In questo documento,abbiamo descritto un'architettura distribuita per supportare la scoperta su larga scala di servizi web. La nostra architettura consentir\u00e0 alle organizzazioni di mantenere un controllo autonomo sui propri registri UDDI e allo stesso tempo consentir\u00e0 ai clienti di interrogare pi\u00f9 registri contemporaneamente. Sulla base dei test iniziali del prototipo, riteniamo che l'architettura DUDE possa supportare una distribuzione efficace dei registri UDDI, rendendo cos\u00ec l'UDDI pi\u00f9 robusto e risolvendo anche i suoi problemi di ridimensionamento. Il documento ha risolto i problemi di scalabilit\u00e0 con UDDI ma non preclude l'applicazione di questo approccio ad altri meccanismi di rilevamento dei servizi. Un esempio di un altro meccanismo di rilevamento dei servizi che potrebbe trarre vantaggio da un simile approccio \u00e8 MDS di Globus Toolkit. Inoltre, intendiamo indagare altri aspetti della scoperta del servizio di rete che estendono questo lavoro. Inoltre, prevediamo di rivisitare le API del servizio per una soluzione Grid Service Discovery sfruttando le soluzioni e le specifiche disponibili, nonch\u00e9 il lavoro presentato in questo documento.", "keyphrases": ["scoperta del servizio di rete", "uddi", "distribuire il servizio web discoveri architecture", "dht base uddi registri gerarchici", "distribuire il problema", "codice dht di bamb\u00f9", "ricerca senza distinzione tra maiuscole e minuscole", "queri", "prefisso dell'avail pi\u00f9 lungo", "servizio qo-base discoveri", "controllo autonomo", "registro uddi", "problema scalabile", "stato molle"]}
{"file_name": "J-22", "text": "Scommesse sulle permutazioni ABSTRACT Consideriamo uno scenario di scommesse sulle permutazioni, in cui le persone scommettono sull'ordine finale di n candidati: ad esempio, il risultato di una corsa di cavalli. Esaminiamo il problema del banditore di abbinare senza rischi le scommesse o, equivalentemente, di trovare opportunit\u00e0 di arbitraggio tra le scommesse proposte. Richiedere agli offerenti di elencare esplicitamente gli ordini su cui vorrebbero scommettere \u00e8 sia innaturale che intrattabile, perch\u00e9 il numero di ordinamenti \u00e8 n! e il numero di sottoinsiemi di ordinamenti \u00e8 2n! . Proponiamo due linguaggi espressivi per le scommesse che sembrano naturali per gli offerenti ed esaminiamo in ciascun caso la complessit\u00e0 computazionale del problema del banditore. Le scommesse sui sottoinsiemi consentono ai trader di scommettere che un candidato finir\u00e0 classificato in un sottoinsieme di posizioni nell'ordine finale, ad esempio \"il cavallo A finir\u00e0 nelle posizioni 4, 9 o 13-21\", oppure che una posizione sar\u00e0 preso da un sottoinsieme di candidati, ad esempio \"il cavallo A, B o D finir\u00e0 nella posizione 2\". Per le scommesse sui sottoinsiemi, mostriamo che il problema del banditore pu\u00f2 essere risolto in tempo polinomiale se gli ordini sono divisibili. Le scommesse di coppia consentono ai trader di scommettere se un candidato finir\u00e0 per classificarsi pi\u00f9 in alto rispetto a un altro candidato, ad esempio \"il cavallo A batter\u00e0 il cavallo B\". Dimostriamo che il problema del banditore diventa NP-difficile per le scommesse sulle coppie. Identifichiamo una condizione sufficiente per l'esistenza di un match di scommesse di coppia che pu\u00f2 essere verificato in tempo polinomiale. Mostriamo anche che un algoritmo greedy naturale fornisce una scarsa approssimazione per ordini indivisibili. 1. INTRODUZIONE Acquistare o vendere un titolo finanziario \u00e8 in effetti una scommessa sul valore del titolo. Ad esempio, acquistare un'azione significa scommettere che il valore dell'azione \u00e8 maggiore del suo prezzo corrente. Ogni trader valuta il profitto atteso per decidere la quantit\u00e0 da acquistare o vendere in base alle proprie informazioni e alla valutazione soggettiva della probabilit\u00e0. L'interazione collettiva di tutte le scommesse porta ad un equilibrio che riflette l'aggregazione di tutte le informazioni e le convinzioni dei trader. Considera l'acquisto di un titolo al prezzo di cinquantadue centesimi, che paga 1 dollaro se e solo se un democratico vince le elezioni presidenziali americane del 2008. In questo caso di un titolo contingente all'evento, il prezzo \u2013 il valore di mercato del titolo \u2013 corrisponde direttamente alla probabilit\u00e0 stimata dell'evento. Quasi tutte le borse finanziarie e di scommesse esistenti accoppiano partner commerciali bilaterali. Ad esempio, un trader disposto ad accettare una perdita di x dollari se un democratico non vince in cambio di un profitto di y dollari se un democratico vince viene abbinato a un secondo trader disposto ad accettare il contrario. Tuttavia in molti scenari, anche se non esistono accordi bilaterali tra i commercianti, potrebbero essere possibili accordi multilaterali. Proponiamo uno scambio in cui i trader hanno una notevole flessibilit\u00e0 per esprimere le loro scommesse in modo naturale e conciso,ed esaminare la complessit\u00e0 computazionale del conseguente problema di abbinamento del banditore nell'identificazione di accordi bilaterali e multilaterali. In particolare, ci concentreremo su un context in cui i trader scommettono sull'esito di una competizione tra n candidati. Ad esempio, supponiamo che ci siano n candidati in un'elezione -LRB- oppure n cavalli in una corsa, ecc. -RRB- e quindi n! eventuali ordinamenti dei candidati dopo il conteggio dei voti finali. Come vedremo, il problema del matching pu\u00f2 essere impostato come un programma lineare o intero, a seconda che gli ordini siano rispettivamente divisibili o indivisibili. Tentare di ridurre il problema a un problema di corrispondenza bilaterale creando esplicitamente n! titoli, uno per ogni eventuale ordinamento finale, risulta essere macchinoso per gli operatori e computazionalmente impraticabile anche per n. Inoltre, l'attenzione dei trader sarebbe distribuita tra n! scelte indipendenti, facendo sembrare remota la probabilit\u00e0 che due trader convergano nello stesso momento e nello stesso luogo. Esiste un compromesso tra l'espressivit\u00e0 del linguaggio di offerta e la complessit\u00e0 computazionale del problema di abbinamento. Vogliamo offrire ai trader il linguaggio di offerta pi\u00f9 espressivo possibile mantenendo la fattibilit\u00e0 computazionale. Esploriamo due linguaggi di offerta che sembrano naturali dal punto di vista del trader. Le scommesse sui sottoinsiemi, descritte nella Sezione 3.2, consentono ai trader di scommettere su quali posizioni nella classifica cadr\u00e0 un candidato, ad esempio \"il candidato D finir\u00e0 nella posizione 1, 3-5 o 10\". Simmetricamente, i trader possono anche scommettere su quali candidati cadranno in una determinata posizione. Nella Sezione 4, deriviamo un algoritmo tempo-polinomiale per abbinare le scommesse del sottoinsieme -LRB- divisibili -RRB-. Le scommesse di coppia, descritte nella Sezione 3.3, consentono ai trader di scommettere sulla classifica finale di due candidati qualsiasi, ad esempio \"il candidato D sconfigger\u00e0 il candidato R\". Nella Sezione 5, mostriamo che l'abbinamento ottimale delle scommesse sulla coppia -LRB- divisibile o indivisibile -RRB- \u00e8 NP-difficile, attraverso una riduzione dal problema dell'insieme dell'arco di feedback minimo non ponderato. Forniamo anche una condizione sufficiente polinomialmente verificabile per l'esistenza di una coppia di scommesse e mostriamo che un algoritmo greedy offre una scarsa approssimazione per le scommesse su coppie indivisibili. 2. BACKGROUND E LAVORO CORRELATO Consideriamo le scommesse a permutazione, ovvero le scommesse sull'esito di una competizione tra n candidati. Il risultato finale o ES dello stato \u00e8 una classifica ordinale degli n candidati. Ad esempio, i candidati potrebbero essere i cavalli in una corsa e il risultato l'elenco dei cavalli in ordine crescente in base al tempo finale. Lo spazio degli stati S contiene tutti gli n! permutazioni mutuamente esclusive ed esaustive dei candidati. In pratica in pista, ciascuno di questi diversi tipi di scommesse viene elaborato in pool o gruppi separati. Descriviamo invece uno scambio centrale in cui tutte le scommesse sul risultato vengono elaborate insieme, aggregando cos\u00ec la liquidit\u00e0 e garantendo che l'inferenza informativa avvenga automaticamente. Idealmente,vorremmo consentire ai trader di scommettere su qualsiasi propriet\u00e0 dell'ordine finale che preferiscono, espressa esattamente nella lingua che preferiscono. In pratica, consentire un linguaggio troppo flessibile crea un onere computazionale per il banditore che tenta di abbinare i commercianti disponibili. Esploriamo il compromesso tra l'espressivit\u00e0 del linguaggio di offerta e la complessit\u00e0 computazionale del problema di abbinamento. Consideriamo un quadro in cui le persone propongono di acquistare titoli che pagano 1 dollaro se e solo se alcune propriet\u00e0 dell'ordine finale sono vere. I trader dichiarano il prezzo che sono disposti a pagare per azione e il numero di azioni che vorrebbero acquistare. Un ordine divisibile consente al trader di ricevere meno azioni di quelle richieste, purch\u00e9 venga rispettato il vincolo di prezzo; un ordine indivisibile \u00e8 un ordine tutto o niente. titoli, uno per ogni stato s ES -LRB- o di fatto qualsiasi insieme di n! titoli linearmente indipendenti -RRB-. Questo \u00e8 il cosiddetto mercato completo dei titoli Arrow-Debreu -LSB- 1 -RSB- per il nostro context. In pratica, i trader non vogliono avere a che fare con la specificazione di basso livello di ordini completi: le persone pensano in modo pi\u00f9 naturale in termini di propriet\u00e0 di alto livello degli ordini. Inoltre, operando n! titoli \u00e8 irrealizzabile in pratica da un punto di vista computazionale al crescere di n. Un linguaggio di offerta molto semplice potrebbe consentire ai trader di scommettere solo su chi vince la competizione, come avviene nel pool di \"vincite\" negli ippodromi. Il corrispondente problema di abbinamento \u00e8 polinomiale, tuttavia il linguaggio non \u00e8 molto espressivo. Un trader che crede che A sconfigger\u00e0 B, ma che nessuno dei due vincer\u00e0 a titolo definitivo, non pu\u00f2 impartire utilmente le sue informazioni al mercato. Lo spazio dei prezzi del mercato rivela le stime collettive delle probabilit\u00e0 di vincita ma nient\u2019altro. Il nostro obiettivo \u00e8 trovare linguaggi quanto pi\u00f9 espressivi e intuitivi possibile e rivelare quante pi\u00f9 informazioni possibili, pur mantenendo la fattibilit\u00e0 computazionale. Il nostro lavoro \u00e8 in diretta analogia con il lavoro di Fortnow et. Mentre esploriamo la combinatoria delle permutazioni, Fortnow et. al. esplorare la combinatoria booleana. Gli autori considerano uno spazio degli stati dei 2n possibili risultati di n variabili binarie. I trader esprimono le scommesse secondo la logica booleana. Gli autori mostrano che l'abbinamento divisibile \u00e8 co-NP-completo e l'abbinamento indivisibile \u00e8 p2-completo. Hanson -LSB- 9 -RSB- descrive un meccanismo di regole di punteggio di mercato che pu\u00f2 consentire di scommettere su un numero combinatorio di risultati. Il mercato inizia con una distribuzione di probabilit\u00e0 congiunta su tutti i risultati. Funziona come una versione sequenziale di una regola di punteggio. Qualsiasi trader pu\u00f2 modificare la distribuzione di probabilit\u00e0 purch\u00e9 accetti di pagare il trader pi\u00f9 recente in base alla regola del punteggio. Il market maker paga l\u2019ultimo trader. Pertanto, si assume il rischio e pu\u00f2 subire perdite. I meccanismi delle regole di punteggio di mercato hanno la caratteristica di limitare la perdita del market maker nel caso peggiore. Tuttavia, gli aspetti computazionali su come far funzionare il meccanismo non sono stati completamente esplorati.I nostri meccanismi hanno un banditore che non si assume alcun rischio e abbina solo gli ordini. Le aste combinatorie consentono agli offerenti di attribuire valori distinti a pacchetti di beni piuttosto che solo a singoli beni. L\u2019incertezza e il rischio in genere non vengono presi in considerazione e il problema centrale del banditore \u00e8 massimizzare il benessere sociale. I nostri meccanismi consentono ai trader di costruire scommesse per un evento con n! risultati. Vengono presi in considerazione l'incertezza e il rischio e il problema del banditore \u00e8 esplorare le opportunit\u00e0 di arbitraggio e abbinare le scommesse senza rischi. 6. CONCLUSIONE Consideriamo uno scenario di scommesse a permutazione, in cui i trader scommettono sull'ordine finale di n candidati. Sebbene sia innaturale e intrattabile consentire ai trader di scommettere direttamente sul n! diversi ordinamenti finali, proponiamo due linguaggi espressivi per le scommesse, le scommesse sui sottoinsiemi e le scommesse sulle coppie. In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppie. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppia. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.In un mercato di scommesse su sottoinsiemi, i trader possono scommettere su un sottoinsieme di posizioni in cui si trova un candidato o su un sottoinsieme di candidati che occupano una posizione specifica nell'ordine finale. Le scommesse di coppia consentono ai trader di scommettere sul fatto che un dato candidato si classifichi pi\u00f9 in alto di un altro dato candidato. Esaminiamo il problema del banditore di abbinare gli ordini senza incorrere in rischi. Troviamo che in un mercato di scommesse sottoinsieme un banditore pu\u00f2 trovare l'insieme ottimale e la quantit\u00e0 di ordini da accettare in modo tale che il suo profitto nel caso peggiore sia massimizzato in tempo polinomiale se gli ordini sono divisibili. La complessit\u00e0 cambia radicalmente per le scommesse sulle coppie. Dimostriamo che il problema di abbinamento ottimale per il banditore \u00e8 NP-difficile per le scommesse sulle coppie con ordini sia indivisibili che divisibili attraverso riduzioni al problema dell'insieme dell'arco di feedback minimo. Identifichiamo una condizione sufficiente per l'esistenza di una corrispondenza, che pu\u00f2 essere verificata in tempo polinomiale. \u00c8 stato dimostrato che un algoritmo avido naturale fornisce una scarsa approssimazione per le scommesse sulle coppie indivisibili. Interessanti domande aperte per le nostre scommesse con permutazione includono la complessit\u00e0 computazionale dell'abbinamento indivisibile ottimale per le scommesse sui sottoinsiemi e la condizione necessaria per l'esistenza di una corrispondenza nei mercati delle scommesse a coppia. Siamo interessati ad esplorare ulteriormente algoritmi di approssimazione migliori per i mercati delle scommesse a coppie.", "keyphrases": ["scommessa permuta", "scommessa del sottoinsieme", "partner commerciale bilaterale", "algoritmo polinomi-tempo", "informare aggreg", "combinatore di permutazione", "mercato delle scommesse a coppie", "grafico bipartito", "feedback minimo", "algoritmo greedi", "Trasformata polinomistica complessa"]}
{"file_name": "I-31", "text": "Ragionare sul giudizio e sull'aggregazione delle preferenze \u25e6 ABSTRACT Gli agenti che devono raggiungere accordi con altri agenti devono ragionare su come le loro preferenze, giudizi e credenze potrebbero essere aggregati con quelli degli altri attraverso i meccanismi di scelta sociale che governano le loro interazioni. Il campo dell'aggregazione del giudizio emergente di recente studia l'aggregazione da una prospettiva logica e considera come pi\u00f9 insiemi di formule logiche possono essere aggregati in un unico insieme coerente. Come caso speciale, si pu\u00f2 vedere che l'aggregazione dei giudizi sussume l'aggregazione delle preferenze classica. Presentiamo una logica modale che intende supportare il ragionamento sugli scenari di aggregazione dei giudizi -LRB- e quindi, come caso speciale, sull'aggregazione delle preferenze -RRB-: il linguaggio logico \u00e8 interpretato direttamente nelle regole di aggregazione dei giudizi. Presentiamo una solida e completa assiomatizzazione di tali regole. Mostriamo che la logica pu\u00f2 esprimere regole di aggregazione come il voto a maggioranza; propriet\u00e0 delle regole come l'indipendenza; e risultati come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet - che sono derivabili come teoremi formali della logica. La logica \u00e8 parametrizzata in modo tale da poter essere utilizzata come quadro generale per confrontare le propriet\u00e0 logiche di diversi tipi di aggregazione, inclusa l'aggregazione delle preferenze classica. 1. INTRODUZIONE In questo articolo, siamo interessati ai formalismi di rappresentazione della conoscenza per sistemi in cui gli agenti hanno bisogno di aggregare le loro preferenze, giudizi, credenze, ecc.. Ad esempio, un agente potrebbe aver bisogno di ragionare sul voto a maggioranza in un gruppo di cui fa parte. un membro di. L'aggregazione delle preferenze - che combina le relazioni di preferenza degli individui su un insieme di alternative in una relazione di preferenza che rappresenta le preferenze congiunte del gruppo mediante le cosiddette funzioni di benessere sociale - \u00e8 stata ampiamente studiata nella teoria della scelta sociale -LSB-2 -RSB- . Il campo recentemente emergente dell'aggregazione dei giudizi studia l'aggregazione da una prospettiva logica e discute come, dato un insieme coerente di formule logiche per ciascun agente, che rappresentano le credenze o i giudizi dell'agente, possiamo aggregarli in un unico insieme coerente di formule. A tal fine sono state sviluppate diverse regole di aggregazione dei giudizi. Come caso speciale, si pu\u00f2 vedere che l'aggregazione dei giudizi sussume l'aggregazione delle preferenze -LSB- 5 -RSB-. In questo articolo presentiamo una logica, chiamata Logica di Aggregazione del Giudizio -LRB-jal -RRB-, per ragionare sull'aggregazione dei giudizi. Le formule della logica vengono interpretate come affermazioni sulle regole di aggregazione dei giudizi e diamo un'assiomatizzazione solida e completa di tutte queste regole. L'assiomatizzazione \u00e8 parametrizzata in modo tale da poterla istanziare per ottenere una gamma di diverse logiche di aggregazione del giudizio. Ad esempio, un esempio \u00e8 un'assiomatizzazione, nel nostro linguaggio, di tutte le funzioni di benessere sociale - quindi otteniamo anche una logica di aggregazione delle preferenze classica.E questo \u00e8 uno dei principali contributi di questo articolo: identifichiamo le propriet\u00e0 logiche dell'aggregazione del giudizio e possiamo confrontare le propriet\u00e0 logiche di diverse classi di aggregazione del giudizio - e dell'aggregazione generale del giudizio e dell'aggregazione delle preferenze in particolare. Naturalmente una logica \u00e8 interessante solo finch\u00e9 \u00e8 espressiva. Uno degli obiettivi di questo articolo \u00e8 indagare le capacit\u00e0 rappresentazionali e logiche di cui un agente ha bisogno per il giudizio e l'aggregazione delle preferenze; cio\u00e8, che tipo di linguaggio logico potrebbe essere utilizzato per rappresentare e ragionare sull'aggregazione dei giudizi? Il linguaggio di rappresentazione della conoscenza di un agente dovrebbe essere in grado di esprimere: regole comuni di aggregazione come il voto a maggioranza; propriet\u00e0 comunemente discusse delle regole di aggregazione del giudizio e delle funzioni di benessere sociale come l'indipendenza; paradossi comunemente usati per illustrare l'aggregazione dei giudizi e l'aggregazione delle preferenze, vale a dire. rispettivamente il paradosso discorsivo e il paradosso di Condorcet ; e altre propriet\u00e0 importanti come il teorema di Arrow. Da questo esempio sembra che un linguaggio formale per i SWF dovrebbe essere in grado di esprimere: \u2022 Propriet\u00e0 delle relazioni di preferenza per agenti diversi e propriet\u00e0 di diverse relazioni di preferenza per lo stesso agente nella stessa formula. \u2022 Confronto tra diverse relazioni di preferenza. \u2022 La relazione di preferenza risultante dall'applicazione di un SWF ad altre relazioni di preferenza. Da questi punti potrebbe sembrare che un tale linguaggio sarebbe piuttosto complesso -LRB- in particolare, questi requisiti sembrano escludere una logica modale proposizionale standard -RRB-. Nella sezione successiva esamineremo le basi dell'aggregazione dei giudizi e dell'aggregazione delle preferenze e menzioneremo alcune propriet\u00e0 comunemente discusse delle regole di aggregazione dei giudizi e delle funzioni di benessere sociale. Le formule di JAL sono interpretate direttamente dalle regole di aggregazione dei giudizi e quindi ne rappresentano le propriet\u00e0. Nella Sezione 4 dimostriamo che la logica pu\u00f2 esprimere propriet\u00e0 comunemente discusse delle regole di aggregazione del giudizio, come il paradosso discorsivo. Diamo un'assiomatizzazione solida e completa della logica nella Sezione 5, partendo dal presupposto che l'agenda su cui gli agenti esprimono giudizi sia finita. Come accennato in precedenza, l'aggregazione delle preferenze pu\u00f2 essere vista come un caso speciale di aggregazione del giudizio e nella Sezione 6 introduciamo un'interpretazione alternativa delle formule JAL direttamente nelle funzioni di benessere sociale. Otteniamo anche una solida e completa assiomatizzazione della logica per l'aggregazione delle preferenze. Le sezioni 7 e 8 discutono il lavoro correlato e concludono. 7. LAVORI CORRELATI Le logiche formali legate alla scelta sociale si sono concentrate principalmente sulla rappresentazione logica delle preferenze quando l'insieme di alternative \u00e8 ampio e sulle propriet\u00e0 computazionali del calcolo delle preferenze aggregate per una data rappresentazione -LSB- 6, 7, 8 -RSB- . Un'eccezione notevole e recente \u00e8 un quadro logico per l'aggregazione dei giudizi sviluppato da Marc Pauly in -LSB- 10 -RSB-,al fine di poter caratterizzare le relazioni logiche tra diverse regole di aggregazione dei giudizi. La logica modale della freccia -LSB- 11 -RSB- \u00e8 progettata per ragionare su qualsiasi oggetto che pu\u00f2 essere rappresentato graficamente come una freccia e dispone di vari operatori modali per esprimere propriet\u00e0 e relazioni tra queste frecce. Nella logica di aggregazione delle preferenze jal -LRB- LK -RRB- abbiamo interpretato le formule in coppie di alternative - che possono essere viste come frecce. Pertanto, -LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.-LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.-LRB- almeno -RRB- la variante di aggregazione delle preferenze della nostra logica \u00e8 correlata alla logica delle frecce. Tuttavia, sebbene gli operatori modali della logica delle frecce possano esprimere propriet\u00e0 delle relazioni di preferenza come la transitivit\u00e0, non possono esprimere direttamente la maggior parte delle propriet\u00e0 discusse in questo articolo. Tuttavia, la relazione con la logica delle frecce potrebbe essere ulteriormente studiata in lavori futuri. In particolare, la logica delle frecce si \u00e8 solitamente rivelata completa. un'algebra. Ci\u00f2 potrebbe significare che potrebbe essere possibile utilizzare tali algebre come struttura sottostante per rappresentare le preferenze individuali e collettive. Quindi, cambiare il profilo delle preferenze ci porta da un'algebra all'altra, e un SWF determina la preferenza collettiva, in ciascuna delle algebre. 8. DISCUSSIONE Abbiamo presentato un jal logico valido e completo per rappresentare e ragionare sull'aggregazione dei giudizi. jal \u00e8 espressivo: pu\u00f2 esprimere regole di aggregazione del giudizio come il voto a maggioranza; propriet\u00e0 complicate come l'indipendenza; e risultati importanti come il paradosso discorsivo, il teorema di Arrow e il paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.Teorema di Arrow e paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.Teorema di Arrow e paradosso di Condorcet. Sosteniamo che questi risultati mostrano esattamente di quali capacit\u00e0 logiche ha bisogno un agente per poter ragionare sull'aggregazione dei giudizi. Forse \u00e8 sorprendente che un linguaggio relativamente semplice offra queste capacit\u00e0. L'assiomatizzazione descrive i principi logici dell'aggregazione dei giudizi e pu\u00f2 anche essere istanziata per ragionare su istanze specifiche di aggregazione dei giudizi, come l'aggregazione delle preferenze arroviane classiche. Pertanto il nostro quadro fa luce sulle differenze tra i principi logici alla base dell\u2019aggregazione del giudizio generale da un lato e l\u2019aggregazione delle preferenze classica dall\u2019altro. Nel lavoro futuro sarebbe interessante allentare i requisiti di completezza e coerenza degli insiemi di giudizi e provare invece a caratterizzarli nel linguaggio logico, come propriet\u00e0 degli insiemi di giudizi generali.", "keyphrases": ["la conoscenza rappresenta formale", "funzione di assistenza sociale", "assiomatis completa", "sintassi e semantica di jal", "parla del paradosso", "regola del giudizio aggregato", "teorema della freccia", "esprimere", "non dittatura", "unanime", "preferire l'aggregato", "logica della freccia", "jal"]}
{"file_name": "C-27", "text": "Un sistema di localizzazione ad alta precisione e basso costo per reti di sensori wireless SOMMARIO Il problema della localizzazione dei nodi di sensori wireless \u00e8 stato a lungo considerato molto difficile da risolvere, se si considerano le realt\u00e0 degli ambienti del mondo reale. In questo articolo descriviamo, progettiamo, implementiamo e valutiamo formalmente un nuovo sistema di localizzazione, chiamato Spotlight. Il nostro sistema utilizza le propriet\u00e0 spazio-temporali di eventi ben controllati nella rete -LRB-, ad esempio la luce -RRB-, per ottenere le posizioni dei nodi sensore. Dimostriamo che \u00e8 possibile ottenere un'elevata precisione nella localizzazione senza l'ausilio di hardware costoso sui nodi sensore, come richiesto da altri sistemi di localizzazione. Valutiamo le prestazioni del nostro sistema nelle implementazioni di mote Mica2 e XSM. Attraverso le valutazioni delle prestazioni di un sistema reale distribuito all'aperto, otteniamo un errore di localizzazione di 20 cm. Una rete di sensori, con qualsiasi numero di nodi, dispiegata in un\u2019area di 2500 m2, pu\u00f2 essere localizzata in meno di 10 minuti, utilizzando un dispositivo che costa meno di 1000 dollari. Per quanto ne sappiamo, questo \u00e8 il primo rapporto di un sub- errore di localizzazione del misuratore, ottenuto in un ambiente esterno, senza dotare i nodi sensore wireless di hardware di rilevamento specializzato. 1. INTRODUZIONE Recentemente, i sistemi di reti di sensori wireless sono stati utilizzati in molte applicazioni promettenti tra cui la sorveglianza militare, il monitoraggio degli habitat, il monitoraggio della fauna selvatica, ecc. -LSB- 12 -RSB- -LSB- 22 -RSB- -LSB- 33 -RSB- -LSB - 36 -RSB-. Sebbene molti servizi middleware, per supportare queste applicazioni, siano stati progettati e implementati con successo, la localizzazione, ovvero l'individuazione della posizione dei nodi sensore, rimane una delle sfide di ricerca pi\u00f9 difficili da risolvere praticamente. Un GPS di bordo -LSB- 23 -RSB- \u00e8 una tipica soluzione di fascia alta, che richiede hardware sofisticato per ottenere la sincronizzazione temporale ad alta risoluzione con i satelliti. I vincoli in termini di potenza e costi per i piccoli nodi di sensori ne impediscono l\u2019adozione come soluzione praticabile. Altre soluzioni richiedono dispositivi per nodo in grado di eseguire la scansione tra i nodi vicini. Le difficolt\u00e0 di questi approcci sono duplici. Innanzitutto, a causa dei vincoli di forma e alimentazione, la portata effettiva di tali dispositivi \u00e8 molto limitata. Ad esempio, la portata effettiva dei trasduttori a ultrasuoni utilizzati nel sistema Cricket \u00e8 inferiore a 2 metri quando l'emettitore e il ricevitore non sono uno di fronte all'altro -LSB- 26 -RSB-. In secondo luogo, poich\u00e9 la maggior parte dei nodi sensori sono statici, ovvero non \u00e8 previsto che la posizione cambi, non \u00e8 conveniente dotare questi sensori di circuiti speciali solo per una localizzazione una tantum. Per superare queste limitazioni, sono stati proposti molti schemi di localizzazione senza raggio d'azione. La maggior parte di questi schemi stimano la posizione dei nodi sensore sfruttando le informazioni di connettivit\u00e0 radio tra i nodi vicini. Questi approcci eliminano la necessit\u00e0 di hardware specializzato ad alto costo, a scapito di una localizzazione meno accurata. Inoltre,le caratteristiche della propagazione radio variano nel tempo e dipendono dall'ambiente, imponendo quindi elevati costi di calibrazione per gli schemi di localizzazione senza portata. La nostra risposta a questa sfida \u00e8 un sistema di localizzazione chiamato Spotlight. Questo sistema utilizza un'architettura asimmetrica, in cui i nodi sensore non necessitano di hardware aggiuntivo, oltre a quello di cui dispongono attualmente. Tutto l'hardware e i calcoli sofisticati risiedono su un singolo dispositivo Spotlight. Il dispositivo Spotlight utilizza una sorgente di luce laser orientabile, illuminando i nodi sensore posizionati all'interno di un terreno noto. Allo stesso tempo, poich\u00e9 \u00e8 necessario un solo sofisticato dispositivo per localizzare l\u2019intera rete, il costo ammortizzato \u00e8 molto inferiore al costo per aggiungere componenti hardware ai singoli sensori. 2. LAVORI CORRELATI Il problema della localizzazione \u00e8 un problema di ricerca fondamentale in molti settori. Gli errori di localizzazione segnalati sono dell'ordine di decine di centimetri, quando si utilizza hardware specializzato, ad esempio telemetro laser o ultrasuoni. A causa del costo elevato e del fattore di forma non trascurabile dell'hardware, queste soluzioni non possono essere semplicemente applicate alle reti di sensori. L'RSSI \u00e8 stata una soluzione interessante per stimare la distanza tra il mittente e il destinatario. Il sistema RADAR -LSB- 2 -RSB- utilizza l'RSSI per costruire un archivio centralizzato di intensit\u00e0 del segnale in varie posizioni rispetto a un insieme di nodi beacon. La posizione di un utente mobile \u00e8 stimata entro pochi metri. In un approccio simile, MoteTrack -LSB- 17 -RSB- distribuisce i valori RSSI di riferimento ai nodi beacon. Sono state proposte anche soluzioni che utilizzano RSSI e non richiedono nodi beacon -LSB- 5 -RSB- -LSB- 14 -RSB- -LSB- 24 -RSB- -LSB- 26 -RSB- -LSB- 29 -RSB-. Tutti condividono l'idea di utilizzare un beacon mobile. I nodi sensore che ricevono i beacon applicano diversi algoritmi per dedurre la loro posizione. In -LSB- 29 -RSB-, Sichitiu propone una soluzione in cui i nodi che ricevono il beacon costruiscono, in base al valore RSSI, un vincolo sulla loro stima di posizione. In -LSB-24 -RSB-, Pathirana et al. formulare il problema della localizzazione come stima on-line in un sistema dinamico non lineare e proporre un filtro di Kalman esteso robusto per risolverlo. Elnahrawy -LSB- 8 -RSB- fornisce una prova evidente dei limiti intrinseci dell'accuratezza della localizzazione utilizzando RSSI, in ambienti interni. Una tecnica di misurazione pi\u00f9 precisa utilizza la differenza di tempo tra un segnale radio e un'onda acustica, per ottenere distanze a coppie tra i nodi sensore. Questo approccio produce errori di localizzazione minori, al costo di hardware aggiuntivo. Il sistema di supporto alla localizzazione Cricket -LSB- 25 -RSB- pu\u00f2 raggiungere una granularit\u00e0 di localizzazione di decine di centimetri con ricetrasmettitori a ultrasuoni a corto raggio. AHLoS, proposto da Savvides et al. -LSB- 27 -RSB-, utilizza tecniche di misurazione del tempo di arrivo -LRB- ToA -RRB- che richiedono hardware esteso e la risoluzione di sistemi di equazioni non lineari relativamente grandi.In -LSB- 30 -RSB-, Simon et al. implementare un sistema distribuito -LRB- utilizzando il raggio acustico -RRB- che localizza un cecchino in un terreno urbano. La gamma acustica per la localizzazione \u00e8 utilizzata anche da Kwon et al. -LSB-15 -RSB-. Gli errori di localizzazione riportati variano da 2,2 m a 9,5 m, a seconda del tipo -LRB- centralizzato o distribuito -RRB- dell'algoritmo Least Square Scaling utilizzato. Per le reti di sensori wireless, la distanza \u00e8 un'opzione difficile. Tuttavia, l'elevata precisione di localizzazione ottenibile da questi schemi \u00e8 molto desiderabile. Per superare le sfide poste dagli schemi di localizzazione basati sul raggio d'azione, quando applicati alle reti di sensori, in passato \u00e8 stato proposto e valutato un approccio diverso. Questo approccio \u00e8 chiamato range-free e tenta di ottenere informazioni sulla posizione dalla vicinanza a un insieme di nodi beacon noti. Bulusu et al. proporre in -LSB- 4 -RSB- uno schema di localizzazione, chiamato Centroide, in cui ciascun nodo si localizza al centroide dei suoi nodi beacon prossimi. Il Global Coordinate System -LSB-20 -RSB-, sviluppato al MIT, utilizza la conoscenza a priori della densit\u00e0 dei nodi nella rete, per stimare la distanza media del salto. La famiglia DV - * di schemi di localizzazione -LSB- 21 -RSB-, utilizza il conteggio dei salti dai nodi beacon noti ai nodi della rete per dedurre la distanza. La maggior parte degli schemi di localizzazione senza raggio d'azione sono stati valutati in simulazioni o ambienti controllati. Langendoen e Reijers presentano uno studio dettagliato e comparativo di diversi schemi di localizzazione in -LSB- 16 -RSB-. Per quanto ne sappiamo, Spotlight \u00e8 il primo sistema di localizzazione senza portata che funziona molto bene in un ambiente esterno. Il nostro sistema richiede una linea di vista tra un singolo dispositivo e i nodi dei sensori e la mappa del terreno in cui si trova il campo dei sensori. Il sistema Spotlight ha una portata effettiva lunga -LRB- 1000 metri -RRB- e non richiede alcuna infrastruttura o hardware aggiuntivo per i nodi sensore. Il sistema Spotlight unisce i vantaggi e non soffre degli svantaggi delle due classi di localizzazione. 7. CONCLUSIONI E LAVORO FUTURO In questo articolo abbiamo presentato la progettazione, l'implementazione e la valutazione di un sistema di localizzazione per reti di sensori wireless, chiamato Spotlight. La nostra soluzione di localizzazione non richiede alcun hardware aggiuntivo per i nodi sensore, oltre a quello gi\u00e0 esistente. Tutta la complessit\u00e0 del sistema \u00e8 racchiusa in un unico dispositivo Spotlight. Il nostro sistema di localizzazione \u00e8 riutilizzabile, ovvero i costi possono essere ammortizzati attraverso diverse implementazioni e le sue prestazioni non sono influenzate dal numero di nodi di sensori nella rete. I nostri risultati sperimentali, ottenuti da un sistema reale distribuito all'aperto, mostrano che l'errore di localizzazione \u00e8 inferiore a 20 cm. Questo errore \u00e8 attualmente lo stato dell'arte,anche per i sistemi di localizzazione basati sulla distanza ed \u00e8 inferiore del 75% rispetto all'errore ottenuto quando si utilizzano dispositivi GPS o quando l'implementazione manuale dei nodi sensore \u00e8 un'opzione fattibile -LSB- 31 -RSB-. Come lavoro futuro, vorremmo esplorare l'autocalibrazione e l'autotuning del sistema Spotlight. La precisione del sistema pu\u00f2 essere ulteriormente migliorata se viene riportata la distribuzione dell'evento, invece di una singola marcatura temporale. Una generalizzazione potrebbe essere ottenuta riformulando il problema come un problema di stima angolare che fornisce gli elementi costitutivi per tecniche di localizzazione pi\u00f9 generali.", "keyphrases": ["rete di sensori wireless", "Locale", "locale della base di chiamata", "schema senza chiamata", "trasmettere", "eseguire", "accurati", "errore locale", "rete di sensori", "sistema di faretti", "tecnica locale", "distribuire"]}
{"file_name": "C-17", "text": "Problemi di implementazione di un sistema di conferenza VoIP in un ambiente di conferenza virtuale ABSTRACT I servizi in tempo reale sono stati supportati in generale su reti a commutazione di circuito. Le tendenze recenti favoriscono i servizi portati su reti a commutazione di pacchetto. Per le conferenze audio, dobbiamo considerare molti problemi: scalabilit\u00e0, qualit\u00e0 dell'applicazione per conferenze, controllo della sala e carico sui client/server, solo per citarne alcuni. In questo articolo descriviamo un framework di servizi audio progettato per fornire un ambiente di conferenza virtuale -LRB- VCE -RRB-. Il sistema \u00e8 progettato per accogliere un gran numero di utenti finali che parlano contemporaneamente e si diffondono su Internet. Il framework \u00e8 basato su Conference Server -LSB- 14 -RSB-, che facilitano la gestione dell'audio, mentre sfruttiamo le capacit\u00e0 SIP per scopi di segnalazione. La selezione del cliente si basa su un quantificatore recente chiamato \"Numero di volume\" che aiuta a imitare una conferenza fisica faccia a faccia. Trattiamo i problemi di implementazione della soluzione proposta sia in termini di scalabilit\u00e0 che di interattivit\u00e0, spiegando al contempo le tecniche che utilizziamo per ridurre il traffico. Abbiamo implementato un'applicazione Conference Server -LRB- CS -RRB- su una rete a livello di campus presso il nostro Istituto. 1. INTRODUZIONE Internet di oggi utilizza la suite di protocolli IP che \u00e8 stata progettata principalmente per il trasporto di dati e fornisce il miglior sforzo di consegna dei dati. I vincoli di ritardo e le caratteristiche separano i dati tradizionali da un lato dalle applicazioni voce e video dall'altro. Pertanto, man mano che su Internet vengono distribuite applicazioni voce e video sempre pi\u00f9 sensibili al fattore tempo, l'inadeguatezza di Internet viene messa a nudo. Inoltre, cerchiamo di portare i servizi telefonici su Internet. Tra questi, la struttura per conferenze virtuali -LRB- teleconferenza -RRB- \u00e8 all'avanguardia. Le conferenze audio e video su Internet sono popolari -LSB- 25 -RSB- per i numerosi vantaggi che comportano -LSB- 3,6 -RSB-. Chiaramente la larghezza di banda necessaria per una teleconferenza su Internet aumenta rapidamente con il numero dei partecipanti; ridurre la larghezza di banda senza compromettere la qualit\u00e0 audio \u00e8 una sfida nella telefonia Internet. C'\u00e8 molta discussione tra la comunit\u00e0 HCI e CSCW sull'uso dell'etnometodologia per la progettazione di applicazioni CSCW. L'approccio di base \u00e8 quello di fornire una larghezza di banda pi\u00f9 ampia, pi\u00f9 strutture e meccanismi di controllo pi\u00f9 avanzati, in attesa di una migliore qualit\u00e0 dell'interazione. Questo approccio ignora l'utilit\u00e0 funzionale dell'ambiente utilizzato per la collaborazione. Pertanto, \u00e8 necessario adottare un approccio che consideri entrambi gli aspetti: quello tecnico e quello funzionale. In questo lavoro non parliamo di videoconferenza; la sua inclusione non apporta vantaggi significativi alla qualit\u00e0 della conferenza -LSB- 4 -RSB-. La nostra attenzione \u00e8 rivolta agli ambienti audio virtuali. Descriviamo innanzitutto le sfide incontrate nelle conferenze audio virtuali. Poi esamineremo le motivazioni seguite dalla letteratura rilevante. Nella sezione 5,spieghiamo l'architettura del nostro sistema. La sezione 6 comprende la descrizione dei vari algoritmi utilizzati nella nostra configurazione. Affrontiamo i problemi di distribuzione. Segue una discussione sulle prestazioni. Concludiamo affrontando alcune questioni attuative. 4. LAVORI CORRELATI Lo standard SIP definito in RFC 3261 -LSB- 22 -RSB- e nelle estensioni successive come -LSB- 21 -RSB- non offre servizi di controllo delle conferenze come il controllo della sala o la votazione e non prescrive come una Fig 1. Esempio di conferenza: 3 domini contenenti le entit\u00e0 necessarie affinch\u00e9 la conferenza possa aver luogo. la conferenza deve essere gestita. Tuttavia SIP pu\u00f2 essere utilizzato per avviare una sessione che utilizza qualche altro protocollo di controllo della conferenza. La specifica SIP principale supporta molti modelli per conferenze -LSB- 26, 23 -RSB-. Nei modelli basati su server, un server mescola i flussi multimediali, mentre in una conferenza senza server, la miscelazione viene effettuata sui sistemi finali. SDP -LSB- 7 -RSB- pu\u00f2 essere utilizzato per definire le capacit\u00e0 multimediali e fornire altre informazioni sulla conferenza. Considereremo ora alcuni modelli di conferenza in SIP che sono stati proposti di recente -LSB- 23 -RSB-. Innanzitutto, esaminiamo i modelli senza server. Nell'End-System Mixing, solo un client -LRB- SIP UA -RRB- gestisce la segnalazione e il mixaggio multimediale per tutti gli altri, il che chiaramente non \u00e8 scalabile e causa problemi quando quel particolare client lascia la conferenza. Ci\u00f2 porta a un numero crescente di hop per le foglie remote e non \u00e8 scalabile. Un'altra opzione potrebbe essere quella di utilizzare il multicast per le conferenze, ma il multicast non \u00e8 abilitato su Internet ed \u00e8 attualmente possibile solo su una LAN. Tra i modelli basati su server, in una conferenza Dial-In, gli UA si connettono a un server centrale che gestisce tutto il mixaggio. Questo modello non \u00e8 scalabile poich\u00e9 \u00e8 limitato dalla potenza di elaborazione del server e dalla larghezza di banda della rete. Le conferenze centralizzate ad hoc e i server per conferenze in uscita presentano meccanismi e problemi simili. I modelli ibridi che coinvolgono segnalazione centralizzata e media distribuiti, con questi ultimi che utilizzano unicast o multicast, sollevano problemi di scalabilit\u00e0 come prima. Tuttavia un vantaggio \u00e8 che il controllo della conferenza pu\u00f2 essere una soluzione di terze parti. La perdita di spazialismo quando si mescolano e l'aumento della larghezza di banda quando non lo fanno sono problemi aperti. Uno studio correlato -LSB- 19 -RSB- dello stesso autore propone un'architettura di conferenza per ambienti virtuali collaborativi -LRB- CVE -RRB- ma non fornisce l'angolo di scalabilit\u00e0 senza la disponibilit\u00e0 del multicasting. Tenendo presenti i limiti dei sistemi di conferenza proposti, descriveremo ora nei dettagli la nostra proposta. 9. CONCLUSIONE In questo articolo abbiamo presentato una discussione su un ambiente di conferenza virtuale esclusivamente vocale. Abbiamo sostenuto che la natura distribuita della distribuzione la rende scalabile. L'interattivit\u00e0 si ottiene adattando un recente schema di selezione del flusso basato sul Loudness Number. Pertanto, vi \u00e8 un utilizzo significativamente efficace della larghezza di banda.In questo modo il discorso improvvisato in una teleconferenza virtuale tramite VoIP diventa realt\u00e0, come in una vera conferenza faccia a faccia. Il traffico nella WAN -LRB- Internet -RRB- \u00e8 limitato superiormente dal quadrato del numero di domini, ulteriormente ridotto utilizzando algoritmi euristici, che \u00e8 molto inferiore al numero totale di client nella conferenza. Ci\u00f2 \u00e8 dovuto all'utilizzo di un Conference Server locale per ciascun dominio. Le tecniche VAD aiutano a ridurre ulteriormente il traffico. L'utilizzo dello standard SIP per la segnalazione rende questa soluzione altamente interoperabile. Abbiamo implementato un'applicazione CS su una rete a livello di campus. Riteniamo che questa nuova generazione di ambienti di conferenza virtuale guadagner\u00e0 maggiore popolarit\u00e0 in futuro poich\u00e9 la loro facilit\u00e0 di implementazione \u00e8 garantita grazie a tecnologie prontamente disponibili e strutture scalabili.", "keyphrases": ["sistema di conferenza voip", "rete a commutazione di pacchetto", "struttura dei servizi audio", "ambiente di conferenza virtuale", "conferire server", "numero forte", "miscela parziale", "rilevamento attivo vocale", "sono sufficienti tre altoparlanti simultanei", "tecnica vad"]}
{"file_name": "H-30", "text": "Espansione dei concetti latenti utilizzando campi casuali di Markov ABSTRACT L'espansione delle query, sotto forma di feedback di pseudo-rilevanza o feedback di pertinenza, \u00e8 una tecnica comune utilizzata per migliorare l'efficacia del recupero. La maggior parte degli approcci precedenti hanno ignorato questioni importanti, come il ruolo delle caratteristiche e l\u2019importanza della modellazione delle dipendenze dei termini. In questo articolo proponiamo una robusta tecnica di espansione delle query basata sul modello di campo casuale di Markov per il recupero delle informazioni. La tecnica, chiamata espansione dei concetti latenti, fornisce un meccanismo per modellare le dipendenze dei termini durante l'espansione. Inoltre, l\u2019uso di caratteristiche arbitrarie all\u2019interno del modello fornisce un potente quadro per andare oltre le semplici caratteristiche di occorrenza dei termini che sono implicitamente utilizzate dalla maggior parte delle altre tecniche di espansione. Valutiamo la nostra tecnica rispetto ai modelli di pertinenza, una tecnica di espansione delle query di modellazione del linguaggio all'avanguardia. Il nostro modello dimostra miglioramenti coerenti e significativi nell'efficacia del recupero su diversi set di dati TREC. Descriviamo anche come la nostra tecnica pu\u00f2 essere utilizzata per generare concetti multitermine significativi per attivit\u00e0 quali suggerimento/riformulazione di query. 1. INTRODUZIONE possibilmente una narrazione pi\u00f9 lunga. Una grande quantit\u00e0 di informazioni viene persa durante il processo di conversione dal bisogno informativo alla query effettiva. Per questo motivo c\u2019\u00e8 stato un forte interesse verso le tecniche di espansione delle query. Tali tecniche vengono utilizzate per aumentare la query originale per produrre una rappresentazione che rifletta meglio il bisogno informativo sottostante. Le tecniche di espansione delle query sono state ben studiate per vari modelli in passato e hanno dimostrato di migliorare significativamente l'efficacia sia nel feedback di pertinenza che nel feedback di pseudo-rilevanza -LSB- 12, 21, 28, 29 -RSB-. Il modello MRF generalizza l'unigramma, il bigramma e altri vari modelli di dipendenza -LSB- 14 -RSB-. La maggior parte dei modelli di dipendenza a termine del passato non sono riusciti a mostrare miglioramenti consistenti e significativi rispetto alle linee di base dell'unigramma, con poche eccezioni -LSB- 8 -RSB-. Fino ad ora, il modello \u00e8 stato utilizzato esclusivamente per classificare i documenti in risposta a una determinata query. In questo lavoro, mostriamo come il modello pu\u00f2 essere esteso e utilizzato per l'espansione delle query utilizzando una tecnica che chiamiamo espansione dei concetti latenti -LRB- LCE -RRB-. Ci sono tre contributi principali del nostro lavoro. Innanzitutto, LCE fornisce un meccanismo per combinare la dipendenza dai termini con l'espansione delle query. Le precedenti tecniche di espansione delle query si basano su modelli \"bag of word\". Pertanto, eseguendo l'espansione della query utilizzando il modello MRF, siamo in grado di studiare la dinamica tra la dipendenza dai termini e l'espansione della query. Successivamente, come mostreremo, il modello MRF consente di utilizzare caratteristiche arbitrarie all'interno del modello. In passato le tecniche di espansione delle query utilizzavano implicitamente solo le funzionalit\u00e0 di occorrenza dei termini. Utilizzando set di funzionalit\u00e0 pi\u00f9 robusti, \u00e8 possibile produrre termini di espansione migliori che discriminino meglio tra documenti rilevanti e non rilevanti. Finalmente,il nostro approccio proposto fornisce senza soluzione di continuit\u00e0 un meccanismo per generare concetti sia a termine singolo che a termine multiplo. La maggior parte delle tecniche precedenti, per impostazione predefinita, generano termini in modo indipendente. Ci sono stati diversi approcci che fanno uso di concetti generalizzati, tuttavia tali approcci erano in qualche modo euristici e realizzati al di fuori del modello -LSB- 19, 28 -RSB-. Il nostro approccio \u00e8 sia formalmente motivato che una naturale estensione del modello sottostante. Nella Sezione 2 descriviamo i relativi approcci di espansione delle query. La sezione 3 fornisce una panoramica del modello MRF e descrive in dettaglio la nostra tecnica di espansione dei concetti latenti proposta. Nella Sezione 4 valutiamo il nostro modello proposto e analizziamo i risultati. 2. LAVORI CORRELATI Uno degli approcci classici e pi\u00f9 ampiamente utilizzati per l'espansione delle query \u00e8 l'algoritmo di Rocchio -LSB- 21 -RSB-. L'approccio di Rocchio, che \u00e8 stato sviluppato all'interno del modello dello spazio vettoriale, ripondera il vettore di query originale spostando i pesi verso l'insieme di documenti rilevanti o pseudo-rilevanti e lontano dai documenti non rilevanti. Sfortunatamente, non \u00e8 possibile applicare formalmente l'approccio di Rocchio a un modello di recupero statistico, come la modellazione del linguaggio per il recupero delle informazioni. Sono state sviluppate numerose tecniche formalizzate di espansione delle query per il framework di modellazione del linguaggio, incluso il feedback basato su modelli di Zhai e Lafferty e i modelli di rilevanza di Lavrenko e Croft -LSB- 12, 29 -RSB-. Entrambi gli approcci tentano di utilizzare documenti pseudo-rilevanti o rilevanti per stimare un modello di query migliore. Il feedback basato sul modello trova il modello che meglio descrive i documenti rilevanti prendendo in considerazione un modello di rumore di fondo -LRB- -RRB-. Ci\u00f2 separa il modello di contenuto dal modello di sfondo. Il modello di contenuto viene quindi interpolato con il modello di query originale per formare la query espansa. L\u2019altra tecnica, i modelli di rilevanza, \u00e8 pi\u00f9 strettamente legata al nostro lavoro. Entriamo quindi nei dettagli del modello. Proprio come il feedback basato su modelli, i modelli di pertinenza stimano un modello di query migliorato. L\u2019unica differenza tra i due approcci \u00e8 che i modelli di rilevanza non modellano esplicitamente i documenti rilevanti o pseudo-rilevanti. Invece, modellano una nozione di rilevanza pi\u00f9 generalizzata, come mostreremo ora. Data una query Q, un modello di rilevanza \u00e8 una distribuzione multinomiale, P -LRB- \u00b7 | Q -RRB-, che codifica la verosimiglianza di ciascun termine data la query come prova. Viene calcolato come: dove RQ \u00e8 l'insieme di documenti rilevanti o pseudorilevanti per interrogare Q. Questi presupposti blandi rendono il calcolo del posteriore bayesiano pi\u00f9 pratico. Dopo che il modello \u00e8 stato stimato, i documenti vengono classificati ritagliando il modello di pertinenza scegliendo i k termini pi\u00f9 probabili da P -LRB- \u00b7 | Q -RRB-. Questa distribuzione ritagliata viene quindi interpolata con il modello di query originale di massima verosimiglianza -LSB- 1 -RSB-. Questo pu\u00f2 essere pensato come un'espansione della query originale di k termini ponderati. Per tutto il resto di questo lavoro,ci riferiamo a questa istanziazione dei modelli di rilevanza come RM3. \u00c8 stato svolto relativamente poco lavoro nell'area dell'espansione delle query nel context dei modelli di dipendenza -LSB-9 -RSB-. Tuttavia, ci sono stati diversi tentativi di espansione utilizzando concetti multitermine. Il metodo di analisi del context locale di Xu e Croft -LRB- LCA -RRB- combinava il recupero a livello di passaggio con l'espansione dei concetti, dove i concetti erano singoli termini e frasi -LSB- 28 -RSB-. I concetti di espansione sono stati scelti e ponderati utilizzando una metrica basata su statistiche di co-occorrenza. Papka e Allan studiano l'utilizzo del feedback sulla pertinenza per eseguire l'espansione del concetto multi-termine per l'instradamento dei documenti -LSB- 19 -RSB-. I risultati hanno mostrato che la combinazione di concetti a termine singolo e multitermine ad ampia finestra ha migliorato significativamente l'efficacia. 5. CONCLUSIONI In questo articolo abbiamo proposto una robusta tecnica di espansione delle query chiamata espansione dei concetti latenti. La tecnica ha dimostrato di essere un'estensione naturale del modello del campo casuale di Markov per il recupero delle informazioni e una generalizzazione dei modelli di rilevanza. Abbiamo dimostrato che la tecnica pu\u00f2 essere utilizzata per produrre concetti di espansione multi-termine di alta qualit\u00e0, ben formati e rilevanti per l'attualit\u00e0. I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.I concetti generati possono essere utilizzati in un modulo di suggerimento di query alternative. Abbiamo anche dimostrato che il modello \u00e8 altamente efficace. In effetti, ottiene miglioramenti significativi nella precisione media media rispetto ai modelli di rilevanza attraverso una selezione di set di dati TREC. \u00c8 stato anche dimostrato che il modello MRF stesso, senza alcuna espansione di query, supera i modelli di pertinenza su set di dati web di grandi dimensioni. Infine, abbiamo ribadito l'importanza di scegliere termini di espansione che modellino la pertinenza, piuttosto che i documenti rilevanti, e abbiamo mostrato come LCE catturi le dipendenze sia sintattiche che semantiche sul lato query. Il lavoro futuro esaminer\u00e0 anche l'integrazione delle dipendenze lato documento.", "keyphrases": ["la robusta queri espande la tecnica", "modello linguistico queri espande la tecnica", "feedback rilevante", "feedback pseudo-rilevante", "informare il recupero", "approccio basato sul modello linguistico", "ricerca sul web", "queri si espande", "mrf", "algoritmo di rocchio", "quadro del modello linguistico", "rm3", "percorso dei documenti", "recupero ad hoc", "modello MRF", "distribuzione rilev"]}
{"file_name": "I-10", "text": "SMILE: apprendimento incrementale multi-agente valido ;--RRB- * ABSTRACT Questo articolo affronta il problema dell'apprendimento collaborativo in un sistema multi-agente. Qui ogni agente pu\u00f2 aggiornare in modo incrementale le sue credenze B -LRB- la rappresentazione concettuale -RRB- in modo che sia in un certo senso mantenuta coerente con l'intero insieme di informazioni K -LRB- gli esempi -RRB- che ha ricevuto dall'ambiente o altri agenti. Estendiamo questa nozione di consistenza -LRB- o solidit\u00e0 -RRB- all'intera MAS e discutiamo come ottenere che, in ogni momento, una stessa rappresentazione concettuale coerente sia presente in ciascun agente. Il protocollo corrispondente viene applicato all'apprendimento supervisionato dei concetti. Il metodo risultante SMILE -LRB- che sta per Sound Multiagent Incremental LEarning -RRB- \u00e8 qui descritto e sperimentato. Sorprendentemente, alcune formule booleane difficili vengono apprese meglio, a parit\u00e0 di set di apprendimento, da un sistema multi-agente piuttosto che da un singolo agente. 1. INTRODUZIONE Questo articolo affronta il problema dell'apprendimento collaborativo di concetti in un sistema multi-agente. -LSB- 6 -RSB- introduce una caratterizzazione dell'apprendimento nel sistema multi-agente in base al livello di consapevolezza degli agenti. Al livello 1, gli agenti imparano * L'autore principale di questo articolo \u00e8 uno studente. nel sistema senza tener conto della presenza di altri agenti, se non attraverso la modificazione apportata all'ambiente dalla loro azione. Il livello 2 implica l'interazione diretta tra gli agenti in quanto possono scambiarsi messaggi per migliorare il proprio apprendimento. In questo articolo ci concentreremo sul livello 2, studiando l'interazione diretta tra gli agenti coinvolti in un processo di apprendimento. Si presuppone che ogni agente sia in grado di apprendere in modo incrementale dai dati che riceve, il che significa che ogni agente pu\u00f2 aggiornare il suo insieme di credenze B per mantenerlo coerente con l'intero insieme di informazioni K che ha ricevuto dall'ambiente o da altri agenti. Inoltre, supponiamo che almeno una parte Bc delle credenze di ciascun agente sia comune a tutti gli agenti e debba rimanere tale. Pertanto, un aggiornamento di questo insieme comune Bc da parte dell'agente r deve provocare un aggiornamento di Bc per l'intera comunit\u00e0 di agenti. Ci porta a definire quale sia la mass-coerenza di un agente rispetto alla comunit\u00e0. Il processo di aggiornamento delle convinzioni della comunit\u00e0 quando uno dei suoi membri riceve nuove informazioni pu\u00f2 quindi essere definito come il processo di mantenimento della coerenza che garantisce che ogni agente nella comunit\u00e0 rimanga macoerente. Questo processo di mantenimento della coerenza di massa di un agente che ottiene nuove informazioni gli conferisce il ruolo di colui che apprende e implica la comunicazione con altri agenti che agiscono come critici. Tuttavia, gli agenti non sono specializzati e possono a loro volta essere studenti o critici, nessuno di loro \u00e8 vincolato a un ruolo specifico. Le informazioni vengono distribuite tra gli agenti, ma possono essere ridondanti. Non esiste una memoria centrale. Il lavoro qui descritto ha la sua origine in un precedente lavoro riguardante l'apprendimento in un sistema multi-agente intenzionale utilizzando un formalismo BDI -LSB- 6 -RSB-. In quell'opera,gli agenti avevano dei piani, ciascuno dei quali era associato a un context che definiva in quali condizioni poteva essere attivato. I piani -LRB- ciascuno dei quali aveva il proprio context -RRB- erano comuni all'intero insieme di agenti della comunit\u00e0. Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.Gli agenti dovevano adattare i contesti dei loro piani a seconda del fallimento o del successo dei piani eseguiti, utilizzando un meccanismo di apprendimento e chiedendo ad altri agenti esempi -LRB- piani di successo o fallimenti -RRB-. Tuttavia in questo lavoro mancava un protocollo di apprendimento collettivo che consentisse una reale autonomia del sistema multi-agente. Lo studio di tale protocollo \u00e8 oggetto del presente lavoro. Nella sezione 2 definiamo formalmente la mas-consistenza di un meccanismo di aggiornamento per l'intero MAS e proponiamo un meccanismo di aggiornamento generico che si \u00e8 dimostrato mas coerente. Nella sezione 3 descriviamo SMILE, un concept learner incrementale multi-agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.uno studente incrementale di concetti multi agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.uno studente incrementale di concetti multi agente che applica il nostro meccanismo di aggiornamento pi\u00f9 coerente all'apprendimento collaborativo dei concetti. La sezione 4 descrive vari esperimenti su SMILE e discute varie questioni, incluso il modo in cui variano l'accuratezza e la semplicit\u00e0 dell'ipotesi attuale quando si confrontano l'apprendimento con agente singolo e l'apprendimento mas. Nella sezione 5 presentiamo brevemente alcuni lavori correlati e poi concludiamo nella sezione 6 discutendo ulteriori indagini sull'apprendimento pi\u00f9 coerente. 5. LAVORI CORRELATI Dal 96 -LSB- 15 -RSB-, sono stati eseguiti vari lavori sull'apprendimento in MAS, ma piuttosto pochi sull'apprendimento dei concetti. In -LSB- 11 -RSB- il MAS esegue una forma di apprendimento d'insieme in cui gli agenti sono studenti pigri -LRB- non viene mantenuta alcuna rappresentazione esplicita -RRB- e vendono esempi inutili ad altri agenti. In -LSB- 10 -RSB- ogni agente osserva tutti gli esempi ma percepisce solo una parte della loro rappresentazione. Nell'apprendimento reciproco di concetti online -LSB- 14 -RSB- gli agenti convergono verso un'ipotesi unica, ma ciascun agente produce esempi dalla propria rappresentazione concettuale, risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento di concetti. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento concettuale. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.risultando cos\u00ec in una sorta di sincronizzazione piuttosto che in un puro apprendimento concettuale. 6. CONCLUSIONE Abbiamo qui presentato e sperimentato un protocollo per l'apprendimento online dei concetti MAS. Tuttavia, la nostra struttura \u00e8 aperta, cio\u00e8 gli agenti possono uscire o entrare nel sistema mentre viene preservato il meccanismo di coerenza. Ad esempio, se introduciamo un meccanismo di timeout, anche quando un agente critico si blocca o omette di rispondere, viene coinvolta la coerenza con gli altri critici -LRB- all'interno dei restanti agenti -RRB-. Ulteriore lavoro riguarda in primo luogo l'accoppiamento di induzione e abduzione al fine di eseguire un apprendimento collaborativo di concetti quando gli esempi vengono osservati solo parzialmente da ciascun agente e, in secondo luogo, lo studio dell'apprendimento parziale della memoria: come l'apprendimento viene preservato ogni volta che un agente o l'intero MAS dimentica alcuni esempi selezionati.", "keyphrases": ["apprendimento multi-agente", "il concetto di collaborazione impara", "processo di apprendimento", "conoscenza", "ma-consistono", "incrementare l'apprendimento", "agente", "aggiornamento meccanico", "sincronizzato"]}
{"file_name": "H-31", "text": "Uno studio sul modello di generazione di query di Poisson per il recupero delle informazioni ABSTRACT Sono state proposte molte varianti di modelli linguistici per il recupero delle informazioni. La maggior parte dei modelli esistenti si basa sulla distribuzione multinomiale e assegna un punteggio ai documenti in base alla probabilit\u00e0 delle query calcolata sulla base di un modello probabilistico di generazione delle query. In questo articolo proponiamo e studiamo una nuova famiglia di modelli di generazione di query basati sulla distribuzione di Poisson. Mostriamo che mentre nelle loro forme pi\u00f9 semplici, la nuova famiglia di modelli e i modelli multinomiali esistenti sono equivalenti, si comportano diversamente per molti metodi di livellamento. Mostriamo che il modello di Poisson presenta diversi vantaggi rispetto al modello multinomiale, tra cui il livellamento per termine naturalmente accomodante e la possibilit\u00e0 di una modellazione dello sfondo pi\u00f9 accurata. Presentiamo diverse varianti del nuovo modello corrispondenti a diversi metodi di livellamento e le valutiamo su quattro raccolte di test TREC rappresentative. I risultati mostrano che mentre i loro modelli di base hanno prestazioni comparabili, il modello di Poisson pu\u00f2 sovraperformare il modello multinomiale con livellamento per termine. Le prestazioni possono essere ulteriormente migliorate con lo smoothing a due stadi. 1. INTRODUZIONE Essendo un nuovo tipo di modelli di recupero probabilistico, i modelli linguistici hanno dimostrato di essere efficaci per molti compiti di recupero -LSB- 21, 28, 14, 4 -RSB-. Possiamo quindi classificare i documenti in base alla probabilit\u00e0 di generare la query. Praticamente tutti i modelli linguistici di generazione delle query esistenti sono basati sulla distribuzione multinomiale -LSB- 19, 6, 28 -RSB- o sulla distribuzione multivariata di Bernoulli -LSB- 21, 18 -RSB-. La distribuzione multinomiale \u00e8 particolarmente apprezzata e si \u00e8 dimostrata piuttosto efficace. Si noti che l'assenza di termini viene catturata indirettamente anche in un modello multinomiale attraverso il vincolo che tutte le probabilit\u00e0 dei termini devono sommarsi a 1. In questo articolo proponiamo e studiamo una nuova famiglia di modelli di generazione di query basati sulla distribuzione di Poisson. In questa nuova famiglia di modelli, modelliamo la frequenza di ciascun termine in modo indipendente con una distribuzione di Poisson. Per assegnare un punteggio a un documento, stimeremmo prima un modello di Poisson multivariato basato sul documento, quindi gli daremmo un punteggio in base alla probabilit\u00e0 della query data dal modello di Poisson stimato. In un certo senso, il modello di Poisson combina il vantaggio del multinomiale nel modellare la frequenza dei termini e il vantaggio del modello Bernoulli multivariato nell\u2019accettare il livellamento per termine. Come nel lavoro esistente sui modelli linguistici multinomiali, lo smoothing \u00e8 fondamentale per questa nuova famiglia di modelli. Deriviamo diversi metodi di livellamento per il modello di Poisson in parallelo a quelli utilizzati per le distribuzioni multinomiali e confrontiamo i corrispondenti modelli di recupero con quelli basati su distribuzioni multinomiali. Abbiamo scoperto che mentre con alcuni metodi di livellamento, il nuovo modello e il modello multinomiale portano esattamente alla stessa formula, con altri metodi di livellamento divergono e il modello di Poisson apporta maggiore flessibilit\u00e0 per il livellamento. In particolare,una differenza fondamentale \u00e8 che il modello di Poisson pu\u00f2 naturalmente adattarsi al livellamento pertermine, cosa difficile da ottenere con un modello multinomiale senza una svolta euristica della semantica di un modello generativo. Sfruttiamo questo potenziale vantaggio per sviluppare un nuovo algoritmo di livellamento dipendente dal termine per il modello di Poisson e dimostrare che questo nuovo algoritmo di livellamento pu\u00f2 migliorare le prestazioni rispetto agli algoritmi di livellamento indipendenti dal termine utilizzando il modello di Poisson o quello multinomiale. Questo vantaggio \u00e8 evidente sia per lo smoothing a uno stadio che per quello a due stadi. Un altro potenziale vantaggio del modello di Poisson \u00e8 che il corrispondente modello di fondo per lo smussamento pu\u00f2 essere migliorato utilizzando un modello misto che ha una formula in forma chiusa. \u00c8 stato dimostrato che questo nuovo modello di sfondo supera le prestazioni del modello di sfondo standard e riduce la sensibilit\u00e0 delle prestazioni di recupero al parametro di livellamento. Nella Sezione 2, introduciamo la nuova famiglia di modelli di generazione di query con distribuzione di Poisson e presentiamo vari metodi di livellamento che portano a diverse funzioni di recupero. Nella Sezione 3 confrontiamo analiticamente il modello linguistico di Poisson con il modello linguistico multinomiale, dal punto di vista del recupero. Progettiamo quindi esperimenti empirici per confrontare le due famiglie di modelli linguistici nella Sezione 4. Discutiamo il lavoro correlato nel paragrafo 5 e concludiamo nel paragrafo 6. 5. LAVORO CORRELATO Per quanto ne sappiamo, non \u00e8 stato effettuato alcuno studio sui modelli di generazione di query basati sulla distribuzione di Poisson. I modelli linguistici hanno dimostrato di essere efficaci per molti compiti di recupero -LSB- 21, 28, 14, 4 -RSB-. Il pi\u00f9 popolare e fondamentale \u00e8 il modello linguistico di generazione di query -LSB- 21, 13 -RSB-. Tutti i modelli linguistici di generazione delle query esistenti si basano sulla distribuzione multinomiale -LSB- 19, 6, 28, 13 -RSB- o sulla distribuzione multivariata di Bernoulli -LSB- 21, 17, 18 -RSB-. Introduciamo una nuova famiglia di modelli linguistici, basati sulla distribuzione di Poisson. La distribuzione di Poisson \u00e8 stata precedentemente studiata nei modelli di generazione di documenti -LSB- 16, 22, 3, 24 -RSB-, portando allo sviluppo di una delle formule di recupero pi\u00f9 efficaci BM25 -LSB- 23 -RSB-. -LSB- 24 -RSB- studia la derivazione parallela di tre diversi modelli di recupero che \u00e8 legata al nostro confronto tra Poisson e multinomiale. Tuttavia, il modello di Poisson nel loro articolo rientra ancora nel quadro della generazione dei documenti e inoltre non tiene conto della variazione della lunghezza del documento. -LSB- 26 -RSB- introduce un modo per cercare empiricamente un modello esponenziale per i documenti. Le miscele di Poisson -LSB- 3 -RSB- come 2-Poisson -LSB- 22 -RSB-, multinomiale negativo e KMixture -LSB- 9 -RSB- di Katz hanno dimostrato di essere efficaci per modellare e recuperare documenti. Ancora una volta, nessuno di questo lavoro esplora la distribuzione di Poisson nel framework di generazione delle query. Il livellamento del modello linguistico -LSB- 2, 28, 29 -RSB- e le strutture di fondo -LSB- 15, 10, 25, 27 -RSB- sono stati studiati con modelli linguistici multinomiali.-LSB- 7 -RSB- mostra analiticamente che il livellamento specifico del termine potrebbe essere utile. Mostriamo che il modello linguistico di Poisson \u00e8 naturale per accogliere il livellamento per termine senza torsione euristica della semantica di un modello generativo, ed \u00e8 in grado di modellare in modo efficiente e migliore lo sfondo misto, sia analiticamente che empiricamente. 6. CONCLUSIONI Presentiamo una nuova famiglia di modelli linguistici di generazione di query per il recupero basati sulla distribuzione di Poisson. Deriviamo diversi metodi di smoothing per questa famiglia di modelli, incluso lo smoothing a stadio singolo e lo smoothing a due stadi. Confrontiamo i nuovi modelli con i popolari modelli di recupero multinomiale sia analiticamente che sperimentalmente. La nostra analisi mostra che, sebbene i nostri nuovi modelli e i modelli multinomiali siano equivalenti sotto alcuni presupposti, sono generalmente diversi con alcune importanti differenze. In particolare, mostriamo che Poisson ha un vantaggio rispetto al multinomiale nello smoothing per termine naturalmente accomodante. Sfruttiamo questa propriet\u00e0 per sviluppare un nuovo algoritmo di livellamento per termine per i modelli linguistici di Poisson, che ha dimostrato di sovraperformare il livellamento indipendente dai termini sia per i modelli di Poisson che per quelli multinomiali. Inoltre, mostriamo che un modello di fondo misto per Poisson pu\u00f2 essere utilizzato per migliorare le prestazioni e la robustezza rispetto al modello di fondo di Poisson standard. Il nostro lavoro apre molte direzioni interessanti per ulteriori esplorazioni in questa nuova famiglia di modelli. Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.Esplorare ulteriormente le flessibilit\u00e0 rispetto ai modelli linguistici multinomiali, come la normalizzazione della lunghezza e lo pseudo-feedback, potrebbe essere un buon lavoro futuro.", "keyphrases": ["distribuzione multinomi", "modello probabilistico del queri gener", "distribuzione del veleno", "liscio a due stadi", "multivari distribuzione bernoullu", "riconoscimento vocale", "termine frequenza", "perterm liscio", "nuovo algoritmo uniforme dipendente dai termini", "insieme di vocabolari", "processo di poisson omogeneo", "singolo pseudo termine"]}
{"file_name": "I-35", "text": "Gestione distribuita delle norme nei sistemi multi-agente regolamentati * SOMMARIO Le norme sono ampiamente riconosciute come mezzo per coordinare i sistemi multi-agente. La gestione distribuita delle norme \u00e8 una questione impegnativa e osserviamo una mancanza di realizzazioni computazionali realmente distribuite di modelli normativi. Per regolare il comportamento degli agenti autonomi che prendono parte a molteplici attivit\u00e0 correlate, proponiamo un modello normativo, la Struttura Normativa -LRB- NS -RRB-, un artefatto che si basa sulla propagazione di posizioni normative -LRB- obblighi , divieti, permessi -RRB-, come conseguenze delle azioni degli agenti. All'interno di un NS possono sorgere conflitti a causa della natura dinamica del MAS e della concorrenza delle azioni degli agenti. Tuttavia, garantire la libert\u00e0 da conflitti di un NS in fase di progettazione \u00e8 computazionalmente intrattabile. Lo dimostriamo formalizzando la nozione di conflitto, fornendo una mappatura delle NS nelle reti di Petri colorate e prendendo in prestito risultati teorici ben noti da quel campo. Poich\u00e9 \u00e8 necessaria la risoluzione dei conflitti online, presentiamo un algoritmo trattabile da impiegare in modo distribuito. Dimostreremo quindi che questo algoritmo \u00e8 fondamentale per l'attuazione distribuita di una NS. 1. INTRODUZIONE Una caratteristica fondamentale dei sistemi multi-agente aperti e regolamentati in cui interagiscono agenti autonomi, \u00e8 che gli agenti partecipanti sono tenuti a rispettare le convenzioni del sistema. Le norme possono essere utilizzate per modellare tali convenzioni e quindi come mezzo per regolare il comportamento osservabile degli agenti -LSB- 6, 29 -RSB-. Sul tema delle norme sono numerosi i contributi di sociologi, filosofi e logici -LRB- ad es. -LSB- 15, 28 -RSB- -RRB-. Tuttavia, ci sono pochissime proposte per la realizzazione computazionale di modelli normativi - il modo in cui le norme possono essere integrate nella progettazione e nell'esecuzione dei MAS. Per quanto ne sappiamo, nessuna proposta supporta veramente l'attuazione distribuita di ambienti normativi. Nel nostro articolo affrontiamo questo problema e proponiamo mezzi per gestire gli impegni contrastanti in sistemi aperti, regolamentati e multiagente in modo distribuito. Il tipo di MAS regolamentato che immaginiamo consiste in attivit\u00e0 multiple, simultanee e correlate in cui gli agenti interagiscono. Ogni agente pu\u00f2 partecipare contemporaneamente a diverse attivit\u00e0 e passare da un'attivit\u00e0 all'altra. Le azioni di un agente all'interno di un'attivit\u00e0 possono avere conseguenze sotto forma di posizioni normative -LRB- cio\u00e8 obblighi, permessi e divieti -RRB- -LSB- 26 -RSB- che possono limitare il suo comportamento futuro. Partiamo dal presupposto che gli agenti possano scegliere di non adempiere a tutti i loro obblighi e quindi possano essere sanzionati dalla MAS. Si noti che, quando le attivit\u00e0 vengono distribuite, le posizioni normative devono fluire dalle attivit\u00e0 in cui vengono generate a quelle in cui hanno effetto. Poich\u00e9 in un MAS aperto e regolamentato non \u00e8 possibile incorporare aspetti normativi nella progettazione degli agenti,adottiamo l'idea che la MAS dovrebbe essere integrata con un insieme separato di norme che regolino ulteriormente il comportamento degli agenti partecipanti. Per modellare la separazione delle preoccupazioni tra il livello di coordinamento -LRB- interazioni degli agenti -RRB- e il livello normativo -LRB- propagazione di posizioni normative -RRB-, proponiamo un artefatto chiamato Struttura normativa -LRB- NS -RRB -. All'interno di un NS possono sorgere conflitti a causa della natura dinamica del MAS e della concorrenza delle azioni degli agenti. Ad esempio, un agente pu\u00f2 essere obbligato e vietato di compiere la stessa azione in un'attivit\u00e0. Tuttavia, garantire la libert\u00e0 da conflitti di un NS in fase di progettazione \u00e8 computazionalmente intrattabile. Dimostriamo questo formalizzando la nozione di conflitto, fornendo una mappatura delle NS in reti di Petri colorate -LRB-CPN -RRB- e prendendo in prestito risultati teorici ben noti dal campo dei CPN. Riteniamo che sia necessario il rilevamento e la risoluzione dei conflitti online. Quindi, presentiamo un algoritmo trattabile per la risoluzione dei conflitti. Questo algoritmo \u00e8 fondamentale per l'attuazione distribuita di una NS. Il documento \u00e8 organizzato come segue. Nella Sezione 2 descriviamo in dettaglio uno scenario che servir\u00e0 da esempio per tutto il documento. Successivamente, nella Sezione 3 definiamo formalmente l'artefatto della struttura normativa. Successivamente, nella Sezione 4 formalizziamo la nozione di conflitto per analizzare successivamente la complessit\u00e0 del rilevamento dei conflitti in termini di CPN nella Sezione 5. La Sezione 6 descrive la gestione computazionale dei NS descrivendo la loro attuazione e presentando un algoritmo per la risoluzione dei conflitti. Infine, commenteremo il lavoro correlato, trarremo conclusioni e riferiremo sul lavoro futuro nella Sezione 7. 7. LAVORO CORRELATO E CONCLUSIONI I nostri contributi in questo documento sono triplici. In primo luogo, introduciamo un approccio per la gestione e il ragionamento sulle norme in modo distribuito. A nostra conoscenza, c\u2019\u00e8 poco lavoro pubblicato in questa direzione. In -LSB- 8, 21 -RSB-, vengono presentate due lingue per l'applicazione distribuita delle norme in MAS. Tuttavia, in entrambi i lavori, ciascun agente dispone di un'interfaccia di messaggi locale che inoltra messaggi legali secondo una serie di norme. Poich\u00e9 queste interfacce sono locali per ciascun agente, le norme possono essere espresse solo in termini di azioni di quell\u2019agente. Questo \u00e8 un grave svantaggio, ad esempio quando \u00e8 necessario attivare un obbligo verso un agente a causa di un determinato messaggio di un altro. Il secondo contributo \u00e8 la proposta di un assetto normativo. La nozione \u00e8 fruttuosa perch\u00e9 consente la separazione delle preoccupazioni normative e procedurali. L'impianto normativo da noi proposto rende evidente la somiglianza tra la propagazione delle posizioni normative e la propagazione 642 Sesta Intl.. Conf. congiunta. su Agenti Autonomi e Sistemi Multi-Agente -LRB- AAMAS 07 -RRB- di token in Reti di Petri Colorate. Questa somiglianza suggerisce prontamente una mappatura tra i due, e fornisce le basi per un\u2019opportuna trattazione analitica della struttura normativa, in generale,e la complessit\u00e0 del rilevamento dei conflitti, in particolare. In -LSB-5 -RSB-, le conversazioni vengono prima progettate e analizzate a livello di CPN e successivamente tradotte in protocolli. Lin et al. -LSB- 20 -RSB- mappa gli schemi di conversazione sui CPN. A nostra conoscenza, l\u2019uso di questa rappresentazione a supporto del rilevamento dei conflitti nei MAS regolamentati non \u00e8 stato segnalato altrove. Infine, presentiamo un meccanismo distribuito per risolvere i conflitti normativi. Sartor -LSB- 25 -RSB- tratta i conflitti normativi dal punto di vista della teoria giuridica e suggerisce un modo per ordinare le norme coinvolte. La sua idea \u00e8 implementata in -LSB- 12 -RSB- ma richiede una risorsa centrale per il mantenimento della norma. L'approccio al rilevamento e alla risoluzione dei conflitti \u00e8 un adattamento ed estensione del lavoro sui grafici di istanziazione riportati in -LSB- 17 -RSB- e un algoritmo correlato in -LSB- 27 -RSB-. I tre contributi che presentiamo in questo articolo aprono molte possibilit\u00e0 per il lavoro futuro. Ci aspettiamo che tale accoppiamento doter\u00e0 le istituzioni elettroniche di un ambiente normativo pi\u00f9 flessibile e pi\u00f9 espressivo. Dal punto di vista teorico, intendiamo utilizzare tecniche di analisi dei CPN per caratterizzare classi di CPN -LRB- ad esempio, aciclici, simmetrici, ecc. -RRB- corrispondenti a famiglie di strutture normative che sono suscettibili di rilevamento di conflitti offline trattabili. La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.La combinazione di queste tecniche insieme ai nostri meccanismi di risoluzione dei conflitti online ha lo scopo di dotare i progettisti MAS della capacit\u00e0 di incorporare le norme nei loro sistemi in modo basato su principi.", "keyphrases": ["algoritmo", "attivo", "scenario", "presupposto normativo", "protocollo", "scena normale", "regola di transito normativo", "struttura della norma", "grafico bipartito", "vietare", "consentire la sovrapposizione", "gettone", "conflitto"]}
{"file_name": "I-33", "text": "Un percorso formale dalle norme istituzionali alle strutture organizzative ABSTRACT Finora, il modo in cui le istituzioni e le organizzazioni sono state utilizzate nello sviluppo di sistemi aperti non \u00e8 spesso andato oltre un'utile euristica. Per sviluppare sistemi che implementino effettivamente istituzioni e organizzazioni, i metodi formali dovrebbero prendere il posto di quelli euristici. L'articolo presenta una semantica formale per la nozione di istituzione e le sue componenti -LRB- norme astratte e concrete, empowerment degli agenti, ruoli -RRB- e definisce una relazione formale tra istituzioni e strutture organizzative. Di conseguenza, viene mostrato come le norme istituzionali possano essere raffinate in costrutti \u2013 strutture organizzative \u2013 che sono pi\u00f9 vicini a un sistema implementato. Viene inoltre mostrato come tale processo di raffinamento possa essere pienamente formalizzato e sia quindi suscettibile di verifica rigorosa. 1. INTRODUZIONE L\u2019opportunit\u00e0 di un \u201ctrasferimento tecnologico\u201d dal campo della teoria organizzativa e sociale all\u2019intelligenza artificiale distribuita e ai sistemi multiagente -LRB- MASs -RRB- \u00e8 stata a lungo sostenuta -LRB- -LSB- 8 -RSB- -RRB -. Nei MAS l'applicazione delle metafore organizzative e istituzionali alla progettazione dei sistemi si \u00e8 rivelata utile per lo sviluppo di metodologie e strumenti. In molti casi, tuttavia, l'applicazione di questi apparati concettuali equivale a mere euristiche che guidano la progettazione di alto livello dei sistemi. trattati formalmente, cio\u00e8 una volta che nozioni come norma, ruolo, struttura, ecc. ottengono una semantica formale. Scopo del presente contributo \u00e8 colmare questa lacuna rispetto alla nozione di istituzione fornendo i fondamenti formali per l\u2019applicazione della metafora istituzionale e per la sua relazione con quella organizzativa. Il risultato principale del lavoro consiste nel mostrare come i vincoli astratti -LRB- istituzioni -RRB- possano essere passo dopo passo raffinati in descrizioni strutturali concrete -LRB- strutture organizzative -RRB- del sistema da implementare, colmando cos\u00ec il divario tra norme astratte e specifiche concrete del sistema. Concretamente, nella Sezione 2, viene presentato un quadro logico che fornisce una semantica formale per le nozioni di istituzione, norma, ruolo, e che supporta la spiegazione delle caratteristiche chiave delle istituzioni come la traduzione di norme astratte in norme concrete e implementabili, la empowerment istituzionale degli agenti e alcuni aspetti della progettazione dell\u2019applicazione delle norme. Nella sezione 3 il quadro viene ampliato per trattare la nozione di infrastruttura di un'istituzione. Il quadro esteso viene poi studiato in relazione al formalismo per rappresentare le strutture organizzative presentato in -LSB- 11 -RSB-. Nella Sezione 4 seguono alcune conclusioni. 4. CONCLUSIONI Il contributo mirava a fornire un'analisi formale completa della metafora istituzionale e della sua relazione con quella organizzativa. Lo strumento formale predominante \u00e8 stata la logica descrittiva.I TBox sono stati utilizzati per rappresentare le specifiche delle istituzioni -LRB- Definizione 3 -RRB- e delle loro infrastrutture -LRB- Definizione 6 -RRB-, fornendo quindi una semantica del sistema di transizione per una serie di nozioni istituzionali -LRB- Esempi 1-7 - RRB-. I multigrafi sono stati quindi utilizzati per rappresentare la specificazione delle strutture organizzative -LRB- Definizione 6 -RRB-. L'ultimo risultato presentato riguarda la definizione di una corrispondenza formale tra le specifiche dell'istituzione e dell'organizzazione -LRB- Definizione 7 -RRB-, che fornisce un modo formale per il passaggio tra i due paradigmi. Tutto sommato, questi risultati forniscono un modo per mettere in relazione le specifiche astratte del sistema -LRB- cio\u00e8 le istituzioni come insiemi di norme -RRB- con le specifiche che sono pi\u00f9 vicine a un sistema implementato -LRB- cio\u00e8 le strutture organizzative -RRB-.", "keyphrases": ["metodo formale", "norma dell'istituto", "vincolo astratto", "formale per repres organiz structur", "entiti", "propriet\u00e0", "logica descrittiva", "logica dinamica", "assioma terminologico", "ruolo", "infrastruttura"]}
{"file_name": "C-36", "text": "Controllo degli accessi basato sulla crittografia nelle reti di pubblicazione/sottoscrizione dinamiche multidominio ABSTRACT I sistemi di pubblicazione/sottoscrizione forniscono un'infrastruttura di comunicazione distribuita su vasta area efficiente, basata sugli eventi e su un'ampia area. \u00c8 probabile che i sistemi di pubblicazione/iscrizione su larga scala utilizzino componenti della rete di trasporto di eventi di propriet\u00e0 di organizzazioni cooperanti ma indipendenti. Con l\u2019aumento del numero di partecipanti alla rete, la sicurezza diventa una preoccupazione crescente. Questo documento estende il lavoro precedente per presentare e valutare un'infrastruttura di pubblicazione/sottoscrizione multidominio sicura che supporta e applica un controllo di accesso capillare sui singoli attributi dei tipi di eventi. L'aggiornamento delle chiavi ci consente di garantire la sicurezza in avanti e all'indietro quando i broker di eventi si uniscono e lasciano la rete. Dimostriamo che i costi di tempo e spazio possono essere ridotti al minimo mediante un'attenta considerazione delle tecniche di crittografia e mediante l'uso della memorizzazione nella cache per ridurre le decrittografie non necessarie. Mostriamo che il nostro approccio ha un sovraccarico di comunicazione complessivo inferiore rispetto agli approcci esistenti per ottenere lo stesso grado di controllo sulla sicurezza nelle reti di pubblicazione/sottoscrizione. 1. INTRODUZIONE La pubblicazione/sottoscrizione \u00e8 particolarmente adatta come meccanismo di comunicazione per la creazione di applicazioni distribuite guidate da eventi su scala Internet. dei partecipanti deriva dal disaccoppiamento tra editori e abbonati inserendo tra loro un servizio di distribuzione di eventi asincrono. Nei sistemi di pubblicazione/sottoscrizione realmente su scala Internet, il servizio di distribuzione degli eventi includer\u00e0 un ampio insieme di nodi broker interconnessi che coprono un'ampia area geografica -LRB- e quindi di rete -RRB-. Sebbene le capacit\u00e0 di comunicazione dei sistemi di pubblicazione/sottoscrizione siano ben collaudate, \u00e8 probabile che l'estensione di pi\u00f9 domini amministrativi richieda considerazioni di sicurezza. Poich\u00e9 la sicurezza e il controllo degli accessi sono quasi l\u2019antitesi del disaccoppiamento, finora relativamente poche ricerche di tipo public/subscribe si sono concentrate sulla sicurezza. Il nostro obiettivo generale di ricerca \u00e8 sviluppare reti di pubblicazione/sottoscrizione su scala Internet che forniscano una distribuzione sicura ed efficiente di eventi, tolleranza agli errori e autoriparazione nell'infrastruttura di distribuzione e una comoda interfaccia per gli eventi. In -LSB- 12 -RSB- Pesonen et al. proporre un'architettura di controllo degli accessi multidominio e basata sulle capacit\u00e0 per i sistemi di pubblicazione/sottoscrizione. L'architettura fornisce un meccanismo per autorizzare i client di eventi a pubblicare e sottoscrivere tipi di eventi. I privilegi del client vengono controllati dal broker locale a cui il client si connette per accedere al sistema di pubblicazione/sottoscrizione. L'approccio implementa il controllo degli accessi ai margini della rete dei broker e presuppone che tutti i broker siano affidabili per applicare correttamente le policy di controllo degli accessi. Qualsiasi broker dannoso, compromesso o non autorizzato \u00e8 libero di leggere e scrivere qualsiasi evento che lo attraversa nel suo percorso dagli editori agli abbonati. Proponiamo di applicare il controllo degli accessi all'interno della rete dei broker crittografando il contenuto degli eventi,e quella politica detta i controlli sulle chiavi di crittografia necessarie. Con il contenuto dell'evento crittografato solo i broker autorizzati ad accedere alle chiavi di crittografia sono in grado di accedere al contenuto dell'evento -LRB- ovvero pubblicare, iscriversi o filtrare -RRB-. Spostiamo effettivamente l'applicazione del controllo degli accessi dai broker ai gestori delle chiavi di crittografia. Ci aspettiamo che il controllo degli accessi debba essere applicato in un sistema di pubblicazione/sottoscrizione multidominio quando pi\u00f9 organizzazioni formano un sistema di pubblicazione/sottoscrizione condiviso ma eseguono pi\u00f9 applicazioni indipendenti. Il controllo degli accessi potrebbe essere necessario anche quando una singola organizzazione \u00e8 composta da pi\u00f9 sottodomini che forniscono dati riservati tramite il sistema di pubblicazione/sottoscrizione a livello di organizzazione. Entrambi i casi richiedono il controllo degli accessi perch\u00e9 la distribuzione degli eventi in un'infrastruttura dinamica di pubblicazione/sottoscrizione basata su una rete di broker condivisa potrebbe portare all'instradamento degli eventi attraverso domini non autorizzati lungo i loro percorsi dagli editori agli abbonati. Ci sono due vantaggi particolari nella condivisione dell'infrastruttura di pubblicazione/sottoscrizione, entrambi legati alla rete di broker. Innanzitutto, i broker di condivisione creeranno una rete fisicamente pi\u00f9 grande che fornir\u00e0 una maggiore portata geografica. In secondo luogo, l\u2019aumento dell\u2019interconnettivit\u00e0 dei broker consentir\u00e0 al sistema di pubblicazione/sottoscrizione di fornire una maggiore tolleranza agli errori. La Figura 1 mostra la rete di pubblicazione/sottoscrizione multidominio che utilizziamo come esempio in questo documento. Questo dominio contiene una serie di telecamere a circuito chiuso che pubblicano informazioni sui movimenti dei veicoli nell'area di Londra. Abbiamo incluso il detective Smith come abbonato in questo dominio. Dominio del servizio di tassazione della congestione. Le tariffe applicate quotidianamente ai veicoli che hanno attraversato la zona a traffico limitato di Londra sono emesse da sistemi all'interno di questo dominio. I dati di riconoscimento della targa di origine provengono dalle telecamere del dominio della polizia metropolitana. Il fatto che i CCS siano autorizzati a leggere solo un sottoinsieme dei dati degli eventi del veicolo eserciter\u00e0 alcune delle caratteristiche chiave del controllo di accesso al sistema di pubblicazione/sottoscrizione applicabile presentato in questo documento. Dominio PITO. \u00c8 il proprietario del tipo di evento in questo particolare scenario. La crittografia tutela la riservatezza degli eventi nel caso in cui vengano trasportati attraverso domini non autorizzati. Tuttavia, crittografare interi eventi significa che i broker non autorizzati non possono prendere decisioni di routing efficienti. Il nostro approccio consiste nell'applicare la crittografia ai singoli attributi degli eventi. In questo modo la nostra politica di controllo degli accessi multidominio funziona con una granularit\u00e0 pi\u00f9 precisa: editori e abbonati possono essere autorizzati ad accedere a un sottoinsieme degli attributi disponibili. Nei casi in cui vengono utilizzati eventi non crittografati per l'instradamento, possiamo ridurre il numero totale di eventi inviati attraverso il sistema senza rivelare i valori degli attributi sensibili. Preserviamo cos\u00ec la privacy degli automobilisti consentendo comunque al CCS di svolgere il proprio lavoro utilizzando l'infrastruttura condivisa di pubblicazione/iscrizione.L'investigatore ottiene un'ordinanza del tribunale che la autorizza a iscriversi agli eventi relativi alla targa specifica relativa al suo caso. Gli attuali sistemi di controllo degli accessi di pubblicazione/sottoscrizione rafforzano la sicurezza ai margini della rete di broker dove i client si connettono ad essa. Tuttavia questo approccio spesso non sar\u00e0 accettabile nei sistemi su scala Internet. Proponiamo di rafforzare la sicurezza all'interno della rete dei broker e ai margini a cui si connettono i client degli eventi, crittografando il contenuto dell'evento. Le pubblicazioni verranno crittografate con chiavi di crittografia specifiche del tipo di evento. Controllando l'accesso alle chiavi di crittografia, possiamo controllare l'accesso ai tipi di eventi. L'approccio proposto consente agli event broker di instradare gli eventi anche quando hanno accesso solo a un sottoinsieme delle potenziali chiavi di crittografia. Introduciamo i sistemi di pubblicazione/sottoscrizione decentralizzati e la relativa crittografia nella Sezione 2. Nella Sezione 3 presentiamo il nostro modello per crittografare il contenuto dell'evento sia a livello di evento che di attributo. La sezione 4 tratta la gestione delle chiavi di crittografia nei sistemi di pubblicazione/sottoscrizione multidominio. Infine la Sezione 6 discute il lavoro correlato nella protezione dei sistemi di pubblicazione/sottoscrizione e la Sezione 7 fornisce osservazioni conclusive. 2. BACKGROUND In questa sezione forniamo una breve introduzione ai sistemi di pubblicazione/sottoscrizione decentralizzati. Indichiamo le nostre ipotesi sui sistemi di pubblicazione/sottoscrizione multidominio e descriviamo come queste ipotesi influenzano gli sviluppi che abbiamo apportato dal nostro lavoro precedentemente pubblicato. 2.1 Sistemi di pubblicazione/sottoscrizione decentralizzati Un sistema di pubblicazione/sottoscrizione include editori, abbonati e un servizio eventi. Gli editori pubblicano eventi, gli abbonati si iscrivono agli eventi di loro interesse e il servizio eventi \u00e8 responsabile della fornitura degli eventi pubblicati a tutti gli abbonati i cui interessi corrispondono all'evento specificato. Il servizio eventi in un sistema di pubblicazione/sottoscrizione decentralizzato \u00e8 distribuito su un numero di nodi broker. Insieme, questi broker formano una rete responsabile del mantenimento dei percorsi di instradamento necessari dagli editori agli abbonati. I clienti -LRB-, editori e abbonati -RRB- si connettono a un broker locale, di cui il cliente si fida completamente. Nella nostra discussione ci riferiamo ai client hosting broker come editori hosting broker -LRB- PHB -RRB- o abbonati hosting broker -LRB- SHB -RRB- a seconda che il cliente connesso sia un editore o Figura 1: Una visione d'insieme del nostro distribuzione di pubblicazione/sottoscrizione multidominio rispettivamente a un abbonato. Un broker locale di solito fa parte dello stesso dominio del cliente oppure \u00e8 di propriet\u00e0 di un fornitore di servizi di cui il cliente si fida. Una rete di broker pu\u00f2 avere una topologia statica -LRB- ad esempio Siena -LSB- 3 -RSB- e Gryphon -LSB- 14 -RSB- -RRB- o una topologia dinamica -LRB- ad esempio Scribe -LSB- 4 -RSB- ed Hermes -LSB- 13 -RSB- -RRB-. L\u2019approccio proposto funzioner\u00e0 in entrambi i casi.Una topologia statica consente all'amministratore di sistema di creare domini fidati e in questo modo migliorare l'efficienza del routing evitando crittografie non necessarie -LRB- vedi Sez. Il nostro lavoro si basa sul sistema Hermes. Hermes \u00e8 un middleware di pubblicazione/sottoscrizione basato sui contenuti che include un forte supporto per il tipo di evento. In altre parole, ogni pubblicazione \u00e8 un'istanza di un particolare tipo di evento predefinito. Le pubblicazioni vengono verificate presso l'intermediario locale di ciascun editore. Il nostro schema di crittografia a livello di attributo presuppone che gli eventi siano tipizzati. Hermes utilizza una rete overlay strutturata come trasporto e quindi ha una topologia dinamica. Una pubblicazione Hermes \u00e8 costituita da un identificatore del tipo di evento e da un insieme di coppie di valori di attributo. L'identificatore del tipo \u00e8 l'hash SHA-1 del nome del tipo di evento. Viene utilizzato per instradare la pubblicazione attraverso la rete del broker di eventi. Nasconde convenientemente il tipo di pubblicazione, cio\u00e8 ai broker viene impedito di vedere quali eventi li attraversano a meno che non siano a conoscenza del nome e dell'identificatore specifico del tipo di evento. 2.2 Tipi di eventi sicuri Pesonen et al. ha introdotto tipi di eventi sicuri in -LSB- 11 -RSB-, la cui integrit\u00e0 e autenticit\u00e0 pu\u00f2 essere confermata controllando le firme digitali. Un utile effetto collaterale dei tipi di eventi sicuri sono i tipi di eventi e i nomi degli attributi univoci a livello globale. \u00c8 possibile fare riferimento a questi nomi tramite le policy di controllo degli accessi. In questo documento utilizziamo il nome sicuro del tipo di evento o dell'attributo per fare riferimento alla chiave di crittografia utilizzata per crittografare l'evento o l'attributo. 2.3 Controllo degli accessi basato sulle capacit\u00e0 Pesonen et al. ha proposto un'architettura di controllo degli accessi basata sulle capacit\u00e0 per sistemi di pubblicazione/sottoscrizione multidominio in -LSB- 12 -RSB-. Il modello tratta i tipi di eventi come risorse a cui editori, abbonati e broker di eventi desiderano accedere. Il proprietario del tipo di evento \u00e8 responsabile della gestione del controllo degli accessi per un tipo di evento emettendo certificati di autorizzazione Simple Public Key Infrastructure -LRB-SPKI -RRB- che garantiscono al titolare l'accesso al tipo di evento specificato. Ad esempio, agli editori autorizzati verr\u00e0 rilasciato un certificato di autorizzazione che specifica che l'editore, identificato tramite chiave pubblica, \u00e8 autorizzato a pubblicare istanze del tipo di evento specificato nel certificato. Sfruttiamo il meccanismo di controllo degli accessi sopra menzionato in questo documento controllando l'accesso alle chiavi di crittografia utilizzando gli stessi certificati di autorizzazione. Cio\u00e8, un editore autorizzato a pubblicare un determinato tipo di evento, \u00e8 anche autorizzato ad accedere alle chiavi di crittografia utilizzate per proteggere eventi di quel tipo. 4. 2.4 Modello di minaccia L'obiettivo del meccanismo proposto \u00e8 quello di imporre il controllo dell'accesso per i partecipanti autorizzati al sistema. Nel nostro caso il primo livello di controllo dell'accesso viene applicato quando il partecipante tenta di unirsi alla rete di pubblicazione/sottoscrizione. Ai broker di eventi non autorizzati non \u00e8 consentito unirsi alla rete dei broker. Allo stesso modo, ai client di eventi non autorizzati non \u00e8 consentito connettersi a un broker di eventi.Tutte le connessioni nella rete del broker tra broker di eventi e client di eventi utilizzano Transport Layer Security -LRB- TLS -RRB- -LSB- 5 -RSB- per impedire l'accesso non autorizzato al livello di trasporto. L'architettura del sistema di pubblicazione/sottoscrizione implica che i client degli eventi devono connettersi ai broker di eventi per poter accedere al sistema di pubblicazione/sottoscrizione. Pertanto presumiamo che questi client non costituiscano una minaccia. Il client dell'evento si affida completamente al broker di eventi locale per l'accesso alla rete del broker. Pertanto il client dell'evento non \u00e8 in grado di accedere ad alcun evento senza l'assistenza del broker locale. I broker invece sono in grado di analizzare tutti gli eventi del sistema che li attraversano. Un broker pu\u00f2 analizzare sia il traffico degli eventi sia il numero e i nomi degli attributi che vengono popolati in un evento -LRB- nel caso della crittografia a livello di attributo -RRB-. Esistono approcci praticabili per prevenire l'analisi del traffico inserendo eventi casuali nel flusso di eventi al fine di produrre un modello di traffico uniforme. 6. LAVORI CORRELATI Wang et al. hanno classificato i vari problemi di sicurezza che dovranno essere affrontati in futuro nei sistemi di pubblicazione/sottoscrizione in -LSB- 20 -RSB-. Il documento \u00e8 una panoramica completa dei problemi di sicurezza nei sistemi di pubblicazione/sottoscrizione e come tale cerca di attirare l'attenzione sui problemi piuttosto che fornire soluzioni. Bacon et al. in -LSB- 1 -RSB- esaminare l'uso del controllo degli accessi basato sui ruoli in sistemi di pubblicazione/sottoscrizione distribuiti multidominio. Opyrchal e Prakash affrontano il problema della riservatezza degli eventi nell'ultimo collegamento tra l'abbonato e l'SHB in -LSB- 10 -RSB-. Affermano correttamente che un approccio di comunicazione di gruppo sicuro non \u00e8 fattibile in un ambiente come quello di pubblicazione/sottoscrizione che ha appartenenze a gruppi altamente dinamici. Nel nostro lavoro assumiamo che SHB sia abbastanza potente da mantenere una connessione protetta TLS per ciascun abbonato locale. Sia Srivatsa et al. -LSB- 19 -RSB- e Raiciu et al. -LSB- 16 -RSB- presenti meccanismi per proteggere la riservatezza dei messaggi nelle infrastrutture di pubblicazione/sottoscrizione decentralizzate. Rispetto al nostro lavoro, entrambi i documenti mirano a fornire i mezzi per proteggere l'integrit\u00e0 e la riservatezza dei messaggi, mentre l'obiettivo del nostro lavoro \u00e8 applicare il controllo degli accessi all'interno della rete dei broker. Raiciu et al. presuppongono nel loro lavoro che nessuno dei broker nella rete sia affidabile e quindi tutti gli eventi siano crittografati dall'editore all'abbonato e che tutta la corrispondenza sia basata su eventi crittografati. Al contrario, presupponiamo che alcuni dei broker sul percorso di una pubblicazione siano affidabili per accedere a quella pubblicazione e siano quindi in grado di implementare la corrispondenza degli eventi. Partiamo inoltre dal presupposto che l'editore e i broker di hosting dell'abbonato siano sempre affidabili per accedere alla pubblicazione. Infine, Fiege et al. affrontare l'argomento correlato della visibilit\u00e0 degli eventi in -LSB- 6 -RSB-.Sebbene il lavoro si sia concentrato sull\u2019utilizzo degli ambiti come meccanismo per strutturare sistemi basati su eventi su larga scala, la nozione di visibilit\u00e0 degli eventi \u00e8 in qualche modo in sintonia con il controllo degli accessi. 7. CONCLUSIONI La crittografia del contenuto degli eventi pu\u00f2 essere utilizzata per applicare una politica di controllo degli accessi mentre gli eventi sono in transito nella rete di broker di un sistema di pubblicazione/sottoscrizione multi-dominio. La crittografia a livello di attributo pu\u00f2 essere implementata per applicare politiche di controllo degli accessi a grana fine. Oltre a fornire il controllo dell'accesso a livello di attributo, la crittografia degli attributi consente ai broker parzialmente autorizzati di implementare l'instradamento basato sul contenuto in base agli attributi a loro accessibili.", "keyphrases": ["sistema di pubblicazione/sottoscrizione sicuro", "distribuire il controllo degli accessi", "dominio amministrativo multiplo", "crittografia degli attributi", "multidominio", "spese generali generali", "distribuire l'applicazione di distribuzione del sistema", "eseguire", "crittografare", "servizio tariffazione congestione"]}
{"file_name": "C-29", "text": "Implementazione e valutazione delle prestazioni di CONFLEX-G: programma di ricerca spaziale conformazionale molecolare abilitato alla griglia con OmniRPC ABSTRACT CONFLEX-G \u00e8 la versione abilitata alla griglia di un programma di ricerca spaziale conformazionale molecolare chiamato CONFLEX. Abbiamo implementato CONFLEX-G utilizzando un sistema RPC a griglia chiamato OmniRPC. In questo articolo riportiamo le prestazioni di CONFLEX-G in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare molte conformazioni di grandi biomolecole, CONFLEX-G genera strutture di prova delle molecole e assegna compiti per ottimizzare una struttura di prova con un metodo affidabile di meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato per supportare le applicazioni di ricerca parametrica. In questo modello, quando la procedura di inizializzazione \u00e8 definita nel modulo RPC, il modulo viene inizializzato automaticamente al momento dell'invocazione richiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare comunicazioni e inizializzazioni non necessarie ad ogni chiamata in CONFLEX-G. CONFLEXG pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e pu\u00f2 sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di un cluster di pi\u00f9 cluster nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha raggiunto un'accelerazione di 56,5 volte nel caso della molecola 1BL1, dove la molecola \u00e8 costituita da un gran numero di atomi, e ogni ottimizzazione della struttura di prova richiede tempo significativo. Anche lo squilibrio del carico del tempo di ottimizzazione della struttura di prova pu\u00f2 causare un degrado delle prestazioni. 1. INTRODUZIONE Recentemente, il concetto di griglia computazionale ha iniziato ad attirare un notevole interesse nel campo del network computing ad alte prestazioni. CONFLEX \u00e8 uno dei programmi di ricerca dello spazio conformazionale pi\u00f9 efficienti e affidabili -LSB- 1 -RSB-. Abbiamo applicato questo programma alla parallelizzazione utilizzando il calcolo globale. Le prestazioni del CONFLEX parallelizzato consentono l'esplorazione della regione a bassa energia dello spazio conformazionale di piccoli peptidi entro un tempo trascorso disponibile utilizzando un cluster PC locale. Poich\u00e9 l'ottimizzazione della struttura di prova in CONFLEX viene calcolata tramite la meccanica molecolare, la ricerca nello spazio conformazionale pu\u00f2 essere eseguita rapidamente rispetto a quella utilizzando il calcolo dell'orbitale molecolare. Sebbene sia stata utilizzata la versione parallelizzata di CONFLEX per calcolare in parallelo l'ottimizzazione della struttura, che richiede oltre il 90% dell'elaborazione nella ricerca della conformazione molecolare, con questo metodo da solo non \u00e8 stato possibile ottenere un miglioramento sufficiente nell'accelerazione. Ci\u00f2 richiede le vaste risorse informatiche di un ambiente di grid computing. In questo articolo descriviamo CONFLEX-G, un programma di ricerca conformazionale molecolare abilitato alla griglia, utilizzando OmniRPC e riportiamo le sue prestazioni in una griglia di diversi cluster di PC distribuiti geograficamente. Il prototipo CONFLEX-G assegna l'ottimizzazione delle strutture di prova di calcolo, che \u00e8 un compito che richiede molto tempo, ai nodi di lavoro nell'ambiente di rete al fine di ottenere un throughput elevato.Inoltre, confrontiamo le prestazioni di CONFLEX-G in un cluster di PC locale con quelle in un banco di prova a griglia. OmniRPC -LSB- 2, 3, 4 -RSB- \u00e8 un'implementazione thread-safe di Ninf RPC -LSB- 5, 6 -RSB- che \u00e8 una struttura Grid RPC per il calcolo dell'ambiente grid. Diversi sistemi adottano il concetto di RPC come modello di base per il grid Environment Computing, tra cui Ninf-G -LSB- 7 -RSB-, NetSolve -LSB- 8 -RSB- e CORBA -LSB- 9 -RSB-. Il sistema RPCstyle fornisce un'interfaccia di programmazione intuitiva e facile da usare, che consente agli utenti del sistema grid di creare facilmente applicazioni abilitate alla griglia. Per supportare la programmazione parallela, un client RPC pu\u00f2 inviare richieste di chiamata asincrone a un diverso computer remoto per sfruttare il parallelismo a livello di rete tramite OmniRPC. In questo articolo proponiamo il modello di persistenza OmniRPC a un sistema Grid RPC e ne dimostriamo l'efficacia. Per supportare un'applicazione tipica per un ambiente grid, come un'applicazione di ricerca parametrica, in cui la stessa funzione viene eseguita con parametri di input diversi sullo stesso set di dati. Nell'attuale sistema GridRPC -LSB- 10 -RSB-, i dati impostati dalla chiamata precedente non possono essere utilizzati dalle chiamate successive. Questo articolo dimostra che CONFLEX-G \u00e8 in grado di sfruttare le enormi risorse informatiche di un ambiente grid e di ricercare conformeri molecolari su larga scala. Dimostriamo CONFLEX-G sul nostro banco di prova a griglia utilizzando la proteina reale come molecola campione. La funzionalit\u00e0 OmniRPC del modulo inizializzabile automatico -LRB- AIM -RRB- consente al sistema di calcolare in modo efficiente numerosi conformeri. Inoltre, utilizzando OmniRPC, l'utente pu\u00f2 parallelizzare l'applicazione esistente e spostarsi dal cluster all'ambiente grid senza modificare il codice del programma e compilare il programma. Inoltre, l'utente pu\u00f2 facilmente creare un ambiente di rete privato. Una panoramica Figura 1: Algoritmo di ricerca dello spazio conformazionale nel CONFLEX originale. del sistema CONFLEX \u00e8 presentato nella Sezione 2, e l'implementazione e la progettazione di CONFLEX-G sono descritte nella Sezione 3. Riportiamo i risultati sperimentali ottenuti utilizzando CONFLEX-G e discutiamo le sue prestazioni nella Sezione 4. Nella Sezione 6, presentiamo le conclusioni e discutiamo argomenti per studi futuri. 5. LAVORI CORRELATI Recentemente \u00e8 stato sviluppato un algoritmo che risolve i problemi di parallelizzazione e comunicazione in processori scarsamente connessi da utilizzare per la simulazione. Ci\u00f2 ci ha permesso di simulare il ripiegamento per la prima volta e di esaminare direttamente le malattie correlate al ripiegamento. SETI@home[14] \u00e8 un programma per cercare la vita aliena analizzando i segnali dei radiotelescopi utilizzando i dati dei radiotelescopi in trasformata di Fourier provenienti da telescopi di diversi siti. SETI@home affronta problemi estremamente paralleli, in cui il calcolo pu\u00f2 essere facilmente suddiviso tra pi\u00f9 computer. I blocchi di dati del radiotelescopio possono essere facilmente assegnati a diversi computer. Tuttavia, le competenze e l'impegno necessari per sviluppare un'applicazione grid potrebbero non essere necessari per OmniRPC.Nimrod/G -LSB- 15 -RSB- \u00e8 uno strumento per la modellazione parametrica distribuita e implementa una task farm parallela per simulazioni che richiedono diversi parametri di input variabili. Nimrod \u00e8 stato applicato ad applicazioni tra cui la bioinformatica, la ricerca operativa e la modellazione molecolare per la progettazione di farmaci. NetSolve -LSB- 8 -RSB- \u00e8 una struttura RPC simile a OmniRPC e Ninf, che fornisce un'interfaccia di programmazione simile e un meccanismo di bilanciamento automatico del carico. Matsuoka et al. -LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.-LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.-LSB- 16 -RSB- ha anche discusso diverse questioni di progettazione relative ai sistemi RPC a griglia. 6. CONCLUSIONI E LAVORO FUTURO Abbiamo progettato e implementato CONFLEX-G utilizzando OmniRPC. Abbiamo riportato le sue prestazioni in un banco di prova a griglia di diversi cluster di PC distribuiti geograficamente. Per esplorare la conformazione di grandi biomolecole, CONFLEXG \u00e8 stato utilizzato per generare strutture di prova delle molecole e assegnare compiti per ottimizzarle mediante la meccanica molecolare nella griglia. OmniRPC fornisce un modello di persistenza limitato in modo che il modulo venga automaticamente inizializzato all'invocazione chiamando la procedura di inizializzazione. Ci\u00f2 pu\u00f2 eliminare le comunicazioni non necessarie e l'inizializzazione ad ogni chiamata in CONFLEX-G. CONFLEX-G pu\u00f2 raggiungere prestazioni paragonabili a CONFLEX MPI e sfruttare pi\u00f9 risorse di calcolo consentendo l'uso di pi\u00f9 cluster di PC nella griglia. Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.Il risultato sperimentale mostra che CONFLEX-G ha ottenuto un aumento di velocit\u00e0 di 56,5 volte per la molecola 1BL1, dove la molecola \u00e8 composta da un gran numero di atomi e ogni ottimizzazione della struttura di prova richiede molto tempo. Lo squilibrio del carico delle ottimizzazioni della struttura di prova pu\u00f2 causare un degrado delle prestazioni. Dobbiamo perfezionare l'algoritmo utilizzato per generare la struttura di prova al fine di migliorare l'ottimizzazione del bilanciamento del carico per le strutture di prova in CONFLEX. Gli studi futuri includeranno lo sviluppo di strumenti di implementazione e un esame della tolleranza agli errori. Nell'attuale OmniRPC, la registrazione di un programma di esecuzione su host remoti e le distribuzioni di programmi di lavoro vengono impostate manualmente. Gli strumenti di distribuzione saranno necessari man mano che il numero di host remoti aumenter\u00e0. Negli ambienti grid in cui l'ambiente cambia dinamicamente, \u00e8 anche necessario supportare la tolleranza agli errori. Questa funzionalit\u00e0 \u00e8 particolarmente importante nelle applicazioni su larga scala che richiedono calcoli lunghi in un ambiente di rete. Abbiamo in programma di perfezionare l'algoritmo di ottimizzazione conformazionale in CONFLEX per esplorare la ricerca nello spazio conformazionale di biomolecole pi\u00f9 grandi come la proteasi dell'HIV utilizzando fino a 1000 lavoratori in un ambiente a griglia.", "keyphrases": ["riflesso-g", "omnirpc", "ricerca nello spazio conforme", "biomolecola", "modulo rpc", "procedura initi", "MPU", "gruppo di PC", "calcolo della griglia", "sistema rpc a griglia", "meccanica molecolare", "modulo inizializzazione automatica"]}
{"file_name": "C-9", "text": "EDAS: fornire un ambiente per servizi adattivi decentralizzati ABSTRACT Poich\u00e9 l'idea di virtualizzazione della potenza di calcolo, dello storage e della larghezza di banda diventa sempre pi\u00f9 importante, il grid computing si evolve e viene applicato a un numero crescente di applicazioni. L'ambiente per i servizi adattivi decentralizzati -LRB- EDAS -RRB- fornisce un'infrastruttura simile a una griglia per servizi a lungo termine accessibili agli utenti -LRB- ad es. server web, repository di codice sorgente ecc. -RRB-. Mira a supportare l'esecuzione autonoma e l'evoluzione dei servizi in termini di scalabilit\u00e0 e distribuzione consapevole delle risorse. EDAS offre modelli di servizio flessibili basati su oggetti mobili distribuiti che vanno da uno scenario client-server tradizionale a un approccio completamente basato su peer-to-peer. La gestione automatica e dinamica delle risorse consente un utilizzo ottimizzato delle risorse disponibili riducendo al minimo la complessit\u00e0 amministrativa. 1. INTRODUZIONE Le infrastrutture per il grid computing mirano a virtualizzare un gruppo di computer, server e storage come un unico grande sistema informatico. La gestione delle risorse \u00e8 una questione chiave in tali sistemi, necessaria per una distribuzione efficiente e automatizzata dei compiti sulla rete. Tali infrastrutture di rete sono spesso implementate a livello aziendale, ma progetti come SETI@home -LSB- 1 -RSB- hanno dimostrato la fattibilit\u00e0 anche di reti pi\u00f9 decentralizzate. Le attuali infrastrutture di grid computing non forniscono un supporto sufficiente per l'esecuzione di servizi distribuiti, accessibili agli utenti e a lungo termine poich\u00e9 sono progettate per risolvere compiti ad alta intensit\u00e0 di calcolo o di dati con un insieme di parametri pi\u00f9 o meno fissi. Invece un'infrastruttura per servizi a lungo termine deve collocare i servizi in base alla domanda attuale e ai requisiti futuri stimati. La migrazione, tuttavia, \u00e8 costosa poich\u00e9 deve essere trasferito l'intero stato di un servizio. Inoltre, un servizio non replicato non \u00e8 accessibile durante la migrazione. Pertanto la gestione delle risorse deve evitare la migrazione, se possibile. Inoltre deve essere fornito un concetto di servizio che eviti in primo luogo il sovraccarico e in secondo luogo inibisca l'indisponibilit\u00e0 del servizio se la migrazione non pu\u00f2 essere evitata. EDAS -LSB- 2 -RSB- mira a fornire un'infrastruttura simile a una griglia per servizi a lungo termine accessibili agli utenti che consenta l'adattamento dinamico in fase di esecuzione, fornisce un'infrastruttura di gestione e offre supporto a livello di sistema per scalabilit\u00e0 e guasti tolleranza. I nodi possono unirsi e abbandonare dinamicamente l'infrastruttura e tutte le attivit\u00e0 di gestione, in particolare la gestione delle risorse, sono decentralizzate. L'ambiente \u00e8 basato sulla nostra infrastruttura middleware AspectIX -LSB- 3 -RSB-, che supporta direttamente la riconfigurazione dinamica dei servizi basata su QoS. La gestione delle risorse si concentra sull'esecuzione di servizi che hanno un tempo operativo lungo, potenzialmente infinito. Questi servizi sono organizzati in progetti. Ogni progetto ha un ambito di esecuzione distribuito chiamato ambiente di servizio. Un tale ambiente probabilmente abbraccia pi\u00f9 istituzioni.Ciascuna istituzione rappresenta un dominio amministrativo in grado di supportare un progetto con un insieme fisso di risorse. Il nostro approccio supporta la gestione adattiva delle risorse di tutti i progetti nell'ambito di un'istituzione basata su un algoritmo ispirato agli algoritmi diffusivi per il bilanciamento del carico decentralizzato -LSB- 4 -RSB-. Non \u00e8 noto come suddividere in modo ottimale queste risorse per i servizi poich\u00e9 la domanda di risorse dei servizi pu\u00f2 cambiare nel tempo o addirittura fluttuare frequentemente. Per fornire le risorse necessarie, il nostro approccio ridedica automaticamente in modo uniforme le risorse libere o non necessarie tra le istanze del servizio su progetti e nodi. Nei casi in cui la ridedicazione non \u00e8 possibile, viene avviata la migrazione del servizio impegnativo. In un'infrastruttura di rete di servizi a lungo termine, la replica attiva presenta diversi vantaggi: le repliche possono unirsi e lasciare il gruppo di oggetti e pertanto \u00e8 possibile migrare le repliche senza indisponibilit\u00e0 del servizio. Infine, pu\u00f2 essere tollerata una certa quantit\u00e0 di arresti anomali dei nodi. La Sezione 4 spiega i concetti di autogestione e ridedicazione della gestione distribuita delle risorse adattive. La sezione 5 descrive il quadro per i servizi adattivi decentralizzati. La Sezione 6 descrive il lavoro correlato e infine la Sezione 7 conclude il documento. 6. LAVORI CORRELATI Infrastrutture grid come Globus-Toolkit -LSB- 11 -RSB- forniscono servizi e meccanismi per ambienti eterogenei distribuiti per combinare risorse su richiesta per risolvere compiti ad alta intensit\u00e0 di consumo di risorse e di calcolo. A causa di questo orientamento si concentrano su diversi modelli di servizio e non forniscono alcun supporto per la mobilit\u00e0 degli oggetti, se non addirittura supportando un approccio a oggetti distribuiti. Ma, cosa pi\u00f9 importante, seguono un diverso approccio di gestione delle risorse poich\u00e9 mirano all\u2019esecuzione parallela di un gran numero di attivit\u00e0 a breve e medio termine. Gli oggetti replicati attivamente sono forniti da Jgroup -LSB- 14 -RSB- basato su RMI. Oltre a questo middleware di base \u00e8 stato implementato un livello di gestione della replica chiamato ARM -LSB- 15 -RSB-. JGroup si concentra sulla replica attiva degli oggetti ma manca del supporto per servizi pi\u00f9 flessibili come fa EDAS. ARM pu\u00f2 essere paragonato a EDAS ma non supporta alcuna distribuzione consapevole delle risorse. Fog -LSB- 16 -RSB- e Globe -LSB- 17 -RSB- sono ambienti middleware di base che supportano l'approccio agli oggetti frammentati. Globe considera la replica e la memorizzazione nella cache. Entrambi i sistemi non supportano la distribuzione consapevole delle risorse. 7. CONCLUSIONE E LAVORI IN CORSO Sulla base del modello a oggetti frammentato e dell'architettura dell'ambiente EDAS, i servizi adattivi decentralizzati possono essere facilmente progettati, implementati ed eseguiti. Come descritto, la gestione delle risorse pu\u00f2 essere scomposta in due problemi principali che devono essere risolti. Controllo e gestione dei limiti delle risorse, inclusa la garanzia che le risorse assegnate siano disponibili -LRB- anche nel context di crash del nodo -RRB- e il posizionamento autonomo dei servizi. Per entrambi i problemi offriamo una soluzione,un ambiente di simulazione attualmente implementato ne verificher\u00e0 la fattibilit\u00e0. In una fase successiva la gestione delle risorse sar\u00e0 integrata in un prototipo gi\u00e0 implementato dell'architettura EDAS. Come descritto, abbiamo gi\u00e0 una prima implementazione del quadro per i servizi adattivi decentralizzati. Questo quadro deve essere esteso per interagire agevolmente con la gestione delle risorse e l'architettura EDAS. In una fase finale dobbiamo implementare alcuni servizi che verifichino l'usabilit\u00e0 dell'intero progetto EDAS.", "keyphrases": ["decentrare adattare il servizio", "gestione delle risorse", "ambiente domestico", "infrastruttura", "cliente", "servizio a lungo termine", "eda", "limite locale", "limite globale", "risorsa", "nodo"]}
{"file_name": "H-14", "text": "Studiare l'uso di destinazioni popolari per migliorare l'interazione della ricerca sul Web ABSTRACT Presentiamo una nuova funzionalit\u00e0 di interazione della ricerca sul Web che, per una determinata query, fornisce collegamenti a siti Web visitati frequentemente da altri utenti con esigenze di informazioni simili. Queste destinazioni popolari integrano i risultati di ricerca tradizionali, consentendo la navigazione diretta verso risorse autorevoli per l'argomento della query. Le destinazioni vengono identificate utilizzando la cronologia del comportamento di ricerca e di navigazione di molti utenti per un periodo di tempo prolungato, il cui comportamento collettivo fornisce una base per l'autorit\u00e0 della fonte informatica. Descriviamo uno studio sugli utenti che ha confrontato il suggerimento delle destinazioni con il suggerimento precedentemente proposto di query correlate, nonch\u00e9 con la ricerca Web tradizionale e non assistita. I risultati mostrano che la ricerca potenziata dai suggerimenti di destinazione supera gli altri sistemi per le attivit\u00e0 esplorative, con le migliori prestazioni ottenute dall'estrazione del comportamento passato degli utenti con granularit\u00e0 a livello di query. 1. INTRODUZIONE Il problema del miglioramento delle query inviate ai sistemi di Information Retrieval -LRB- IR -RRB- \u00e8 stato ampiamente studiato nella ricerca IR -LSB- 4 -RSB- -LSB- 11 -RSB-. Formulazioni di query alternative, note come suggerimenti di query, possono essere offerte agli utenti dopo una query iniziale, consentendo loro di modificare le specifiche delle loro esigenze fornite al sistema, con conseguente miglioramento delle prestazioni di recupero. La recente popolarit\u00e0 dei motori di ricerca Web ha consentito suggerimenti di query che si basano sul comportamento di riformulazione delle query di molti utenti per fornire consigli sulle query basati sulle precedenti interazioni dell'utente -LSB- 10 -RSB-. Sfruttare i processi decisionali di molti utenti per la riformulazione delle query ha le sue radici nell'indicizzazione adattiva -LSB- 8 -RSB-. Tuttavia, gli approcci basati sull\u2019interazione per suggerire query possono essere meno potenti quando il bisogno di informazione \u00e8 esplorativo, poich\u00e9 gran parte dell\u2019attivit\u00e0 dell\u2019utente per tali bisogni di informazione pu\u00f2 verificarsi oltre le interazioni con i motori di ricerca. Nei casi in cui la ricerca diretta rappresenta solo una frazione del comportamento di ricerca di informazioni degli utenti, l'utilit\u00e0 dei clic di altri utenti nello spazio dei risultati di primo livello potrebbe essere limitata, poich\u00e9 non copre il successivo comportamento di navigazione. Allo stesso tempo, la navigazione dell'utente che segue le interazioni dei motori di ricerca fornisce l'approvazione implicita delle risorse Web preferite dagli utenti, il che pu\u00f2 essere particolarmente utile per attivit\u00e0 di ricerca esplorativa. Pertanto, proponiamo di sfruttare una combinazione di ricerche passate e comportamenti degli utenti di navigazione per migliorare le interazioni di ricerca Web degli utenti. I plug-in del browser e i log del server proxy forniscono accesso ai modelli di navigazione degli utenti che trascendono le interazioni con i motori di ricerca. In lavori precedenti, tali dati sono stati utilizzati per migliorare il posizionamento dei risultati di ricerca da Agichtein et al. -LSB-1 -RSB-. Radlinski e Joachims -LSB- 13 -RSB- hanno utilizzato tale intelligenza collettiva dell'utente per migliorare la precisione del recupero utilizzando sequenze di riformulazioni di query consecutive,tuttavia il loro approccio non considera le interazioni degli utenti oltre la pagina dei risultati di ricerca. In questo articolo presentiamo uno studio sugli utenti di una tecnica che sfrutta il comportamento di ricerca e navigazione di molti utenti per suggerire pagine Web popolari, da ora in poi denominate destinazioni, oltre ai normali risultati di ricerca. Le destinazioni potrebbero non essere tra i primi risultati, non contenere i termini cercati o addirittura non essere indicizzate dal motore di ricerca. Si tratta invece di pagine in cui altri utenti finiscono frequentemente dopo aver inviato query identiche o simili e poi allontanandosi dai risultati di ricerca inizialmente cliccati. Ipotizziamo che le destinazioni popolari tra un gran numero di utenti possano catturare l'esperienza collettiva dell'utente per esigenze di informazione, e i nostri risultati supportano questa ipotesi. In -LSB- 19 -RSB-, Wexelblat e Maes descrivono un sistema per supportare la navigazione all'interno del dominio in base ai percorsi di navigazione di altri utenti. Tuttavia, non siamo a conoscenza di tali principi applicati alla ricerca sul Web. Forse l'esempio pi\u00f9 vicino al teletrasporto \u00e8 l'offerta da parte dei motori di ricerca di numerose scorciatoie all'interno del dominio sotto il titolo di un risultato di ricerca. Sebbene questi possano essere basati sul comportamento dell'utente e possibilmente sulla struttura del sito, l'utente risparmia al massimo un clic da questa funzione. Al contrario, l'approccio da noi proposto pu\u00f2 trasportare gli utenti in luoghi molti clic oltre il risultato della ricerca, risparmiando tempo e offrendo loro una prospettiva pi\u00f9 ampia sulle informazioni correlate disponibili. Lo studio condotto sugli utenti esamina l'efficacia dell'inclusione di collegamenti a destinazioni popolari come funzionalit\u00e0 aggiuntiva dell'interfaccia nelle pagine dei risultati dei motori di ricerca. Confronteremo due varianti di questo approccio con il suggerimento di query correlate e ricerca sul Web non assistita e cercheremo risposte a domande su: -LRB- i -RRB- preferenza dell'utente ed efficacia della ricerca per elementi noti e attivit\u00e0 di ricerca esplorativa e -LRB- ii -RRB- la distanza preferita tra query e destinazione utilizzata per identificare le destinazioni popolari dai registri di comportamento passati. I risultati indicano che suggerire destinazioni popolari agli utenti che tentano attivit\u00e0 esplorative fornisce i migliori risultati negli aspetti chiave dell\u2019esperienza di ricerca di informazioni, mentre fornire suggerimenti di perfezionamento delle query \u00e8 pi\u00f9 auspicabile per attivit\u00e0 con elementi noti. Nella Sezione 2 descriviamo l'estrazione dei percorsi di ricerca e di navigazione dai registri delle attivit\u00e0 degli utenti e il loro utilizzo nell'identificazione delle principali destinazioni per le nuove query. La Sezione 3 descrive il disegno dello studio sugli utenti, mentre le Sezioni 4 e 5 presentano rispettivamente i risultati dello studio e la loro discussione. 6. CONCLUSIONI Abbiamo presentato un nuovo approccio per migliorare l'interazione degli utenti nella ricerca sul Web fornendo collegamenti a siti Web visitati di frequente da utenti che hanno effettuato ricerche in passato con esigenze di informazioni simili. \u00c8 stato condotto uno studio sugli utenti in cui abbiamo valutato l'efficacia della tecnica proposta rispetto a un sistema di perfezionamento delle query e alla ricerca Web non assistita. I risultati del nostro studio hanno rivelato che:I sistemi -LRB- i -RRB- che suggeriscono perfezionamenti delle query sono stati preferiti per attivit\u00e0 di elementi noti, i sistemi -LRB- ii -RRB- che offrono destinazioni popolari sono stati preferiti per attivit\u00e0 di ricerca esplorativa e le destinazioni -LRB- iii -RRB- dovrebbero essere estratte da la fine dei percorsi delle query, non dei percorsi delle sessioni. Nel complesso, i suggerimenti di destinazioni popolari hanno influenzato strategicamente le ricerche in un modo non ottenibile con gli approcci di suggerimento delle query, offrendo un nuovo modo per risolvere i problemi di informazione e migliorare l'esperienza di ricerca di informazioni per molti ricercatori sul Web.", "keyphrases": ["destinazione popolare", "la ricerca web interagisce", "domande improvvisate", "recuperare eseguire", "riguardare queri", "esperienza di ricerca di informazioni", "sentiero queri", "percorso della sessione", "approccio basato sulla ricerca", "valutazione su base logaritmica"]}
{"file_name": "I-20", "text": "Calcolo dell'indice di potenza di Banzhaf nei giochi di flusso di rete SOMMARIO L'aggregazione delle preferenze viene utilizzata in una variet\u00e0 di applicazioni multiagente e, di conseguenza, la teoria del voto \u00e8 diventata un argomento importante nella ricerca sui sistemi multiagente. Tuttavia, gli indici di potere -LRB- che riflettono quanto \u201cpotere reale\u201d ha un elettore in un sistema di voto ponderato -RRB- hanno ricevuto relativamente poca attenzione, sebbene siano stati a lungo studiati nelle scienze politiche e in economia. L'indice di potenza Banzhaf \u00e8 uno dei pi\u00f9 popolari; \u00e8 anche ben definito per qualsiasi semplice gioco di coalizione. In questo articolo, esaminiamo la complessit\u00e0 computazionale del calcolo dell'indice di potenza di Banzhaf all'interno di un particolare dominio multiagente, un gioco di flusso di rete. Gli agenti controllano i bordi di un grafico; una coalizione vince se riesce a inviare un flusso di una determinata dimensione da un vertice sorgente a un vertice destinazione. Il potere relativo di ciascun edge/agente riflette la sua importanza nel consentire tale flusso e nelle reti del mondo reale potrebbe essere utilizzato, ad esempio, per allocare risorse per la manutenzione di parti della rete. Mostriamo che il calcolo dell'indice di potenza Banzhaf di ciascun agente in questo dominio di flusso di rete \u00e8 #P - completo. Mostriamo anche che per alcuni domini di flusso di rete ristretti esiste un algoritmo polinomiale per calcolare gli indici di potenza Banzhaf degli agenti. 1. INTRODUZIONE Qual \u00e8 la complessit\u00e0 del processo? La complessit\u00e0 pu\u00f2 essere utilizzata per difendersi da fenomeni indesiderati? La complessit\u00e0 del calcolo impedisce l\u2019implementazione realistica di una tecnica? Le applicazioni pratiche del voto tra agenti automatizzati sono gi\u00e0 diffuse. Infatti, per vedere la generalit\u00e0 dello scenario di voto automatizzato -LRB- -RRB-, si consideri la moderna ricerca sul web. In questo articolo, consideriamo un argomento che \u00e8 stato meno studiato nel context del voto automatizzato degli agenti, vale a dire gli indici di potere. Un indice di potere \u00e8 una misura del potere che un sottogruppo, o equivalentemente un elettore in un ambiente di voto ponderato, ha sulle decisioni di un gruppo pi\u00f9 ampio. L\u2019indice di potere di Banzhaf \u00e8 una delle misure pi\u00f9 popolari del potere di voto e, sebbene sia stato utilizzato principalmente per misurare il potere nei giochi di voto ponderato, \u00e8 ben definito per qualsiasi semplice gioco di coalizione. Esaminiamo alcuni aspetti computazionali dell'indice di potenza di Banzhaf in un ambiente specifico, vale a dire un gioco di flusso di rete. In questo gioco, una coalizione di agenti vince se riesce a inviare un flusso di dimensione k da un vertice sorgente s a un vertice bersaglio t, con la potenza relativa di ciascun bordo che riflette il suo significato nel consentire tale flusso. Mostriamo che il calcolo dell'indice di potenza Banzhaf di ciascun agente in questo dominio di flusso della rete generale \u00e8 #P - completo. giochi di nectivity su grafi a strati limitati -RRB-, esiste un algoritmo polinomiale per calcolare l'indice di potenza Banzhaf di un agente. Il documento procede come segue. Nella Sezione 2 forniamo alcune informazioni di base sui giochi di coalizione e sull'indice di potere di Banzhaf, e nella Sezione 3 introduciamo il nostro specifico gioco di flusso di rete.Nella Sezione 4 discutiamo l'indice di potenza di Banzhaf nei giochi di flusso di rete, presentando il nostro risultato di complessit\u00e0 nel caso generale. Nella Sezione 5 consideriamo un caso ristretto del gioco del flusso di rete e presentiamo i risultati. Nella Sezione 6 discuteremo il lavoro correlato e concluderemo nella Sezione 7. 6. LAVORO CORRELATO La misurazione del potere dei singoli giocatori nei giochi di coalizione \u00e8 stata studiata per molti anni. Gli indici pi\u00f9 popolari suggeriti per tale misurazione sono l\u2019indice Banzhaf -LSB- 1 -RSB- e l\u2019indice Shapley-Shubik -LSB- 19 -RSB-. Nel suo articolo fondamentale, Shapley -LSB- 18 -RSB- ha considerato i giochi di coalizione e l'equa allocazione dell'utilit\u00e0 acquisita dalla grande coalizione -LRB- la coalizione di tutti gli agenti -RRB- ai suoi membri. L'indice Shapley-Shubik -LSB- 19 -RSB- \u00e8 l'applicazione diretta del valore di Shapley a semplici giochi di coalizione. L'indice Banzhaf \u00e8 emerso direttamente dallo studio del voto negli organi decisionali. L\u2019indice Banzhaf normalizzato misura la percentuale di coalizioni in cui un giocatore \u00e8 uno swinger, rispetto a tutte le coalizioni vincenti. Questo indice \u00e8 simile all'indice Banzhaf discusso nella Sezione 1, ed \u00e8 definito come: L'indice Banzhaf \u00e8 stato analizzato matematicamente in -LSB- 3 -RSB-, dove \u00e8 stato dimostrato che questa normalizzazione manca di alcune propriet\u00e0 desiderabili, e l'indice Banzhaf pi\u00f9 naturale viene introdotto l'indice Sia l\u2019indice Shapley-Shubik che quello Banzhaf sono stati ampiamente studiati, e Straffin -LSB- 20 -RSB- ha dimostrato che ciascun indice riflette condizioni specifiche in un organo di voto. -LSB- 11 -RSB- considera questi due indici insieme a molti altri e descrive gli assiomi che caratterizzano i diversi indici. L'implementazione ingenua di un algoritmo per calcolare l'indice Banzhaf di un agente i enumera tutte le coalizioni contenenti i. Esistono 2n \u2212 1 coalizioni di questo tipo, quindi la prestazione \u00e8 esponenziale nel numero di agenti. -LSB- 12 -RSB- contiene un'indagine sugli algoritmi per il calcolo degli indici di potere dei giochi a maggioranza ponderata. Deng e Papadimitriou -LSB- 2 -RSB- mostrano che il calcolo del valore di Shapley nei giochi a maggioranza ponderata \u00e8 #P - completo, utilizzando una riduzione da KNAPSACK. Poich\u00e9 il valore di Shapley di qualsiasi gioco semplice ha lo stesso valore del suo indice di Shapley-Shubik, ci\u00f2 dimostra che il calcolo dell'indice di Shapley-Shubik nei giochi a maggioranza ponderata \u00e8 #Pcompleto. Matsui e Matsui -LSB- 13 -RSB- hanno dimostrato che il calcolo degli indici Banzhaf e Shapley-Shubik nei giochi di voto ponderato \u00e8 NP-completo. Il problema degli indici di potenza di calcolo nei giochi semplici dipende dalla rappresentazione scelta del gioco. Poich\u00e9 il numero di possibili coalizioni \u00e8 esponenziale nel numero di agenti, il calcolo degli indici di potere in polinomio temporale nel numero di agenti pu\u00f2 essere ottenuto solo in domini specifici. In questo articolo abbiamo considerato il dominio del flusso di rete, dove una coalizione di agenti deve raggiungere un flusso oltre un certo valore. Il gioco del flusso di rete che abbiamo definito \u00e8 un gioco semplice. -LSB-10,9 -RSB- hanno considerato un dominio di flusso di rete simile, in cui ciascun agente controlla un bordo di un grafo di flusso di rete. Tuttavia, hanno introdotto un gioco non semplice, in cui il valore raggiunto da una coalizione di agenti \u00e8 il flusso totale massimo. Hanno dimostrato che alcune famiglie di giochi a flusso di rete e giochi simili hanno nuclei non vuoti. 7. CONCLUSIONI E DIREZIONI FUTURE Abbiamo considerato giochi di flusso di rete, dove una coalizione di agenti vince se riesce a inviare un flusso di pi\u00f9 di un certo valore k tra due vertici. Abbiamo valutato il potere relativo di ciascun agente in questo scenario utilizzando l'indice Banzhaf. Questo indice di potere pu\u00f2 essere utilizzato per decidere come allocare le risorse di manutenzione nelle reti del mondo reale, al fine di massimizzare la nostra capacit\u00e0 di mantenere un certo flusso di informazioni tra due siti. Sebbene l'indice di Banzhaf ci permetta teoricamente di misurare il potere degli agenti nel gioco del flusso di rete, abbiamo dimostrato che il problema del calcolo dell'indice di Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.abbiamo dimostrato che il problema del calcolo dell'indice Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.abbiamo dimostrato che il problema del calcolo dell'indice Banzhaf in questo dominio in #P - \u00e8 completo. Nonostante questo risultato scoraggiante per il dominio del flusso di rete generale, abbiamo anche fornito un risultato pi\u00f9 incoraggiante per un dominio ristretto. Nel caso dei giochi di connettivit\u00e0 -LRB- dove \u00e8 richiesto solo che una coalizione contenga un percorso dalla sorgente alla destinazione -RRB- giocato su grafici a strati limitati, \u00e8 possibile calcolare l'indice Banzhaf di un agente in tempo polinomiale . Rimane un problema aperto trovare modi per approssimare in modo trattabile l'indice di Banzhaf nel dominio generale del flusso della rete. Potrebbe anche essere possibile trovare altri domini limitati utili dove \u00e8 possibile calcolare esattamente l'indice Banzhaf. Abbiamo considerato solo la complessit\u00e0 del calcolo dell'indice Banzhaf; rimane un problema aperto trovare la complessit\u00e0 del calcolo dello Shapley-Shubik o di altri indici nel dominio del flusso di rete. Infine, riteniamo che esistano molti altri ambiti interessanti oltre ai giochi di voto ponderato e ai giochi di flusso di rete, e varrebbe la pena indagare la complessit\u00e0 del calcolo dell\u2019indice Banzhaf o di altri indici di potere in tali ambiti.", "keyphrases": ["preferire l'aggregato", "applicazione multiag", "voto teorico", "indice di potenza banzhaf", "analisi di algoritmi e problemi complessi", "teoria della scelta sociale", "voto dell'agente automatico", "gioco del flusso di rete", "modello probabilistico", "collegare il gioco"]}
{"file_name": "J-7", "text": "Il ruolo della compatibilit\u00e0 nella diffusione delle tecnologie attraverso i social network ABSTRACT In molti contesti, le tecnologie concorrenti \u2013 ad esempio i sistemi operativi, i sistemi di messaggistica istantanea o i formati di documenti \u2013 possono essere viste adottare un livello limitato di compatibilit\u00e0 tra loro; in altre parole, la difficolt\u00e0 nell\u2019utilizzare pi\u00f9 tecnologie \u00e8 in equilibrio tra i due estremi dell\u2019impossibilit\u00e0 e dell\u2019interoperabilit\u00e0 senza sforzo. Ci sono una serie di ragioni per cui questo fenomeno si verifica, molte delle quali \u2013 basate su considerazioni legali, sociali o aziendali \u2013 sembrano sfidare modelli matematici concisi. Nonostante ci\u00f2, mostriamo che i vantaggi di una compatibilit\u00e0 limitata possono emergere in un modello molto semplice di diffusione nei social network, offrendo cos\u00ec una spiegazione di base di questo fenomeno in termini puramente strategici. Il nostro approccio si basa sul lavoro sulla diffusione delle innovazioni nella letteratura economica, che cerca di modellare come una nuova tecnologia A potrebbe diffondersi attraverso una rete sociale di individui che sono attualmente utenti della tecnologia B. Consideriamo diversi modi per catturare la compatibilit\u00e0 di A e B, concentrandosi principalmente su un modello in cui gli utenti possono scegliere di adottare A, adottare B o, a un costo aggiuntivo, adottare sia A che B. Caratterizziamo come la capacit\u00e0 di A di diffondersi dipenda sia dalla sua qualit\u00e0 relativa a B, e anche questo costo aggiuntivo derivante dall\u2019adozione di entrambe, e trovare alcune sorprendenti propriet\u00e0 di non monotonicit\u00e0 nella dipendenza da questi parametri: in alcuni casi, affinch\u00e9 una tecnologia sopravviva all\u2019introduzione di un\u2019altra, il costo dell\u2019adozione di entrambe le tecnologie deve essere bilanciato entro un intervallo ristretto e intermedio. Estendiamo il quadro anche al caso di pi\u00f9 tecnologie, dove scopriamo che un semplice Questo lavoro \u00e8 stato supportato in parte dai finanziamenti NSF CCF0325453, IIS-0329064, CNS-0403340 e BCS-0537606, un Google Research Grant, un Yahoo ! Research Alliance Grant, l'Istituto per le scienze sociali alla Cornell e la Fondazione John D. e Catherine T. MacArthur. Il modello cattura il fenomeno di due aziende che adottano una \u201calleanza strategica\u201d limitata per difendersi da una nuova, terza tecnologia. 1. INTRODUZIONE Giochi di diffusione e di coordinamento in rete. Tali problemi sorgono, ad esempio, nell'adozione di nuove tecnologie, nell'emergere di nuove norme sociali o convenzioni organizzative, o nella diffusione dei linguaggi umani -LSB- 2, 14, 15, 16, 17 -RSB-. Una linea di ricerca attiva in economia e sociologia matematica si occupa di modellare questi tipi di processi di diffusione come un gioco di coordinazione giocato su una rete sociale -LSB- 1, 5, 7, 13, 19 -RSB-. Inizieremo discutendo uno dei modelli di diffusione pi\u00f9 basilari della teoria dei giochi, proposto in un influente articolo di Morris -LSB- 13 -RSB-, che costituir\u00e0 il punto di partenza per il nostro lavoro qui. Lo descriviamo nei termini del seguente scenario di adozione della tecnologia, sebbene esistano molti altri esempi che potrebbero servire allo stesso scopo.Si noti che A \u00e8 la tecnologia \u201cmigliore\u201d se q < 21, nel senso che i profitti AA supererebbero i guadagni BB, mentre A \u00e8 la tecnologia peggiore se q > 21. Una serie di intuizioni qualitative possono essere derivate da una diffusione modello anche a questo livello di semplicit\u00e0. Nello specifico, consideriamo una rete G e lasciamo che tutti i nodi giochino inizialmente a B. Supponiamo ora che un piccolo numero di nodi inizi invece ad adottare la strategia A. Compatibilit\u00e0, interoperabilit\u00e0 e bilinguismo. Un pezzo importante che probabilmente manca nei modelli di diffusione di base della teoria dei giochi, tuttavia, \u00e8 un quadro pi\u00f9 dettagliato di ci\u00f2 che sta accadendo al confine di coesistenza, dove la forma base del modello presuppone nodi che adottano A collegati a nodi che adottano B. In questi contesti motivanti per i modelli, ovviamente, si vedono molto spesso regioni di interfaccia in cui gli individui diventano essenzialmente \"bilingui\". ''Nel caso della diffusione del linguaggio umano, questa bilinguit\u00e0 \u00e8 intesa letteralmente: le regioni geografiche dove c'\u00e8 una sostanziale interazione con parlanti di due lingue diverse tendono ad avere abitanti che le parlano entrambe. Da questo punto di vista, \u00e8 naturale chiedersi come si comportano i modelli di diffusione una volta estesi in modo che alcuni nodi possano essere bilingui in questo senso molto generale, adottando entrambe le strategie a un certo costo per loro stessi. Cosa potremmo imparare da una simile estensione? Per cominciare, ha il potenziale per fornire una prospettiva preziosa sulla questione della compatibilit\u00e0 e dell\u2019incompatibilit\u00e0 che \u00e8 alla base della concorrenza tra le aziende tecnologiche. Esiste un\u2019ampia letteratura su come la compatibilit\u00e0 tra tecnologie influisca sulla concorrenza tra imprese, e in particolare su come l\u2019incompatibilit\u00e0 possa essere una decisione strategica vantaggiosa per alcuni partecipanti in un mercato -LSB- 3, 4, 8, 9, 12 -RSB-. Mentre questi modelli esistenti di compatibilit\u00e0 catturano gli effetti di rete nel senso che gli utenti sul mercato preferiscono utilizzare una tecnologia pi\u00f9 diffusa, non catturano il fenomeno di rete pi\u00f9 dettagliato rappresentato dalla diffusione \u2013 cio\u00e8 che ogni utente include la sua visione locale in la decisione, in base a ci\u00f2 che stanno facendo i suoi vicini di rete sociale. Un modello di diffusione che incorpori tali estensioni potrebbe fornire informazioni sulla struttura dei confini nella rete tra le tecnologie; potrebbe potenzialmente offrire una base teorica dei grafi su come l\u2019incompatibilit\u00e0 possa avvantaggiare una tecnologia esistente, rafforzando questi confini e prevenendo l\u2019incursione di una tecnologia nuova e migliore. Il presente lavoro: Diffusione con comportamento bilingue. In questo articolo sviluppiamo una serie di modelli di diffusione che incorporano nozioni di compatibilit\u00e0 e bilinguismo e scopriamo che alcuni fenomeni inaspettati emergono anche da versioni molto semplici dei modelli. Inizieremo con il modo forse pi\u00f9 semplice di estendere il modello di Morris discusso sopra per incorporare il comportamento bilingue. Consideriamo ancora l'esempio dei sistemi IM A e B, con la struttura dei payoff come prima,ma supponiamo ora che ciascun nodo possa adottare una terza strategia, denotata AB, in cui decide di utilizzare sia A che B. Infine, chi adotta AB paga una penalit\u00e0 di costo fisso pari a c -LRB- cio\u00e8 -- c viene aggiunto a il suo profitto totale -RRB- rappresenta il costo di dover mantenere entrambe le tecnologie. Pertanto, in questo modello, ci sono due parametri che possono essere variati: le qualit\u00e0 relative delle due tecnologie -LRB- codificate da q -RRB-, e il costo di essere bilingue, che riflette un tipo di incompatibilit\u00e0 -LRB- codificato da c -RRB-. Introduciamo anche un ulteriore bit di notazione che sar\u00e0 utile nelle sezioni successive: definiamo r = c / \u0394, la penalit\u00e0 fissa per l'adozione di AB, scalata in modo che sia un costo per arco. Nel modello di Morris, dove le uniche opzioni strategiche sono A e B, un parametro chiave \u00e8 la soglia di contagio di G, denominata q \u2217 -LRB- G -RRB-: questo \u00e8 l\u2019apice di q per il quale A pu\u00f2 diventare epidemico in G con parametro q nella struttura dei payoff. Un risultato centrale di -LSB- 13 -RSB- \u00e8 che 21 \u00e8 la massima soglia di contagio possibile per qualsiasi grafico: supG q \u2217 -LRB- G -RRB- = 21. In effetti, esistono grafici in cui la soglia di contagio \u00e8 altrettanto grande come 21 -LRB- inclusa la linea infinita -- l'unico grafo 2-regolare infinito connesso -RRB- ; d'altro canto si pu\u00f2 dimostrare che non esiste un grafico con una soglia di contagio maggiore di quella della Figura 1: La regione del piano -LRB- q, r -RRB- per la quale la tecnologia A pu\u00f2 diventare epidemica sulla linea infinita. I nostri risultati. -LRB- Troviamo forme analoghe che diventano ancora pi\u00f9 complesse per altre semplici strutture di grafi infiniti ; si vedano ad esempio le Figure 3 e 4. -RRB- In particolare, ci\u00f2 significa che per valori di q prossimi ma inferiori a 21, la strategia A pu\u00f2 diventare epidemica sulla linea infinita se r \u00e8 sufficientemente piccolo o sufficientemente grande, ma non se r assume valori in qualche intervallo intermedio. In altre parole, la strategia B -LRB- che rappresenta la tecnologia peggiore, poich\u00e9 q < 21 -RRB- sopravviver\u00e0 se e solo se il costo dell'essere bilingue \u00e8 calibrato per rientrare in questo intervallo medio. Ci\u00f2 riflette una compatibilit\u00e0 limitata \u2013 ovvero che potrebbe essere nell\u2019interesse di una tecnologia dominante rendere difficile ma non troppo difficile l\u2019uso di una nuova tecnologia \u2013 e troviamo sorprendente che emerga da un modello di base su tale base. una semplice struttura di rete. Viene naturale chiedersi se esista un'interpretazione qualitativa di come ci\u00f2 emerga dal modello, ed in effetti non \u00e8 difficile dare un'interpretazione del genere, come segue. Quando r \u00e8 molto piccolo, \u00e8 economico per i nodi adottare AB come strategia, e cos\u00ec AB si diffonde attraverso l'intera rete. Una volta che AB \u00e8 ovunque, gli aggiornamenti con la risposta migliore fanno s\u00ec che tutti i nodi passino ad A, poich\u00e9 ottengono gli stessi vantaggi di interazione senza pagare la penalit\u00e0 di r. Quando r \u00e8 molto grande, i nodi all'interfaccia, con un vicino A e uno B, troveranno troppo costoso scegliere AB, quindi sceglieranno A -LRB- la tecnologia migliore -RRB-,e quindi A si diffonder\u00e0 passo dopo passo attraverso la rete. Quando r assume un valore intermedio, un nodo v all'interfaccia, con un vicino A e un vicino B, trover\u00e0 pi\u00f9 vantaggioso adottare AB come strategia. Quindi, questo valore intermedio di r consente la formazione di un \"confine\" di AB tra gli adottanti di A e gli adottanti di B. Ma se ha il giusto equilibrio nel valore di r, allora le adozioni di A arrivano a un fermarsi a un confine bilingue dove i nodi adottano AB. Andando oltre i grafici specifici G, troviamo che questa non convessit\u00e0 vale anche in un senso molto pi\u00f9 generale, considerando la regione epidemica generale \u03a9 = UG\u03a9 -LRB- G -RRB-. Per ogni dato valore di \u0394, la regione \u03a9 \u00e8 un'unione complicata di poligoni limitati e illimitati, e non abbiamo una semplice descrizione in forma chiusa per essa. Tuttavia, possiamo mostrare tramite un argomento di funzione potenziale che nessun punto -LRB- q, r -RRB- con q > 21 appartiene a \u03a9. Inoltre, possiamo dimostrare l'esistenza di un punto -LRB- q, r -RRB- E ~ \u03a9 per il quale q < 21. D'altra parte, considerando la regione epidemica per la linea infinita mostra che -LRB- 21, r -RRB- E \u03a9 per r = 0 e per r sufficientemente grande. Quindi n\u00e9 \u03a9 n\u00e9 il suo complemento sono convessi nel quadrante positivo. Infine, estendiamo anche una caratterizzazione fornita da Morris per la soglia di contagio -LSB- 13 -RSB-, producendo una caratterizzazione un po' pi\u00f9 complessa della regione \u03a9 -LRB- G -RRB-. Nel context di Morris, senza una strategia AB, ha dimostrato che A non pu\u00f2 diventare epidemico con il parametro q se e solo se ogni insieme cofinito di nodi contiene un sottoinsieme S che funziona come una \"comunit\u00e0\" ben connessa: ogni nodo in S ha almeno una frazione -LRB- 1 -- q -RRB- dei suoi vicini in S. In altre parole, le comunit\u00e0 molto unite sono gli ostacoli naturali alla diffusione nel suo ambiente. Con la strategia AB come ulteriore opzione, una struttura pi\u00f9 complessa diventa l'ostacolo: mostriamo che A non pu\u00f2 diventare epidemico con parametri -LRB- q, r -RRB- se e solo se ogni insieme cofinito contiene una struttura costituita da un insieme strettamente comunit\u00e0 -knit con un particolare tipo di \"interfaccia\" di nodi vicini. Mostriamo che tale struttura consente ai nodi di adottare AB all'interfaccia e B all'interno della comunit\u00e0 stessa, impedendo l'ulteriore diffusione di A ; e viceversa, questo \u00e8 l\u2019unico modo per bloccare la diffusione di A. Ulteriori estensioni. Un altro modo per modellare la compatibilit\u00e0 e l'interoperabilit\u00e0 nei modelli di diffusione \u00e8 attraverso i termini \"fuori diagonale\" che rappresentano il profitto per le interazioni tra un nodo che adotta A e un nodo che adotta B. Invece di impostarli su 0, possiamo considerare di impostarli su un valore x < min -LRB- q, 1 -- q -RRB-. Troviamo che nel caso di due tecnologie, il modello non diventa pi\u00f9 generale, in quanto qualsiasi esempio di questo tipo \u00e8 equivalente, mediante un riscalamento di q e r, a uno in cui x = 0. Inoltre,utilizzando la nostra caratterizzazione della regione \u03a9 -LRB- G -RRB- in termini di comunit\u00e0 e interfacce, mostriamo un risultato di monotonicit\u00e0: se A pu\u00f2 diventare epidemico su un grafo G con parametri -LRB- q, r, x -RRB-, e poi x viene aumentato, allora A pu\u00f2 ancora diventare epidemico con i nuovi parametri. Consideriamo anche l'effetto di questi termini fuori diagonale in un'estensione a k > 2 tecnologie concorrenti; per le tecnologie X e Y, sia qX il profitto derivante da un'interazione XX su un bordo e qXY il profitto derivante da un'interazione XY su un bordo. Consideriamo un context in cui due tecnologie B e C, che inizialmente coesistono con qBC = 0, affrontano l'introduzione di una terza, migliore tecnologia A in un insieme finito di nodi. Mostriamo un esempio in cui B e C sopravvivono entrambi in equilibrio se impostano qBC in un particolare intervallo di valori, ma non se impostano qBC troppo basso o troppo alto per rientrare in questo intervallo. Pertanto, anche in un modello di diffusione di base con tre tecnologie, si trovano casi in cui due imprese hanno un incentivo ad adottare una \u201calleanza strategica\u201d limitata, aumentando parzialmente la loro interoperabilit\u00e0 per difendersi da un nuovo concorrente nel mercato. 6. COMPATIBILIT\u00c0 LIMITATA Consideriamo ora alcuni ulteriori modi per modellare la compatibilit\u00e0 e l'interoperabilit\u00e0. Consideriamo innanzitutto due tecnologie, come nelle sezioni precedenti, e introduciamo i payoff \"fuori diagonale\" per acquisire un vantaggio positivo nelle interazioni AB dirette. Troviamo che questo in realt\u00e0 non \u00e8 pi\u00f9 generale del modello con profitti pari a zero per le interazioni AB. Considereremo quindi le estensioni a tre tecnologie, identificando le situazioni in cui due tecnologie dominanti coesistenti possono o meno voler aumentare la loro reciproca compatibilit\u00e0 a fronte di una nuova, terza tecnologia. Due tecnologie. Un rilassamento naturale del modello a due tecnologie consiste nell'introdurre profitti positivi -LRB- piccoli -RRB- per l'interazione AB; cio\u00e8, la comunicazione intertecnologica produce un valore minore per entrambi gli agenti. Possiamo modellare questo utilizzando una variabile xAB che rappresenta il profitto ottenuto da un agente con la tecnologia A quando il suo vicino ha la tecnologia B, e analogamente, una variabile xBA che rappresenta il profitto ottenuto da un agente con B quando il suo vicino ha A. Qui consideriamo la caso speciale in cui questi elementi ``fuori diagonale'' sono simmetrici, cio\u00e8 xAB = xBA = x. Assumiamo anche che x < q < 1 -- q. Mostriamo innanzitutto che il gioco con voci fuori diagonale \u00e8 equivalente a un gioco senza queste voci, con un semplice riscalamento di q e r. Si noti che se ridimensioniamo tutti i guadagni mediante una costante additiva o moltiplicativa, il comportamento del gioco non viene influenzato. Dato un gioco con voci fuori diagonale parametrizzate da q, r e x, considera di sottrarre x da tutti i profitti e di aumentare di un fattore pari a 1 / -LRB- 1 -- 2x -RRB-. Come si pu\u00f2 vedere esaminando la Tabella 1, i guadagni risultanti sono esattamente quelli di un gioco senza voci fuori diagonale,parametrizzato da q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- e r ' = r / -LRB- 1 -- 2x -RRB-. Pertanto l'aggiunta di voci simmetriche fuori diagonale non espande la classe di giochi considerati. La tabella 1 rappresenta i profitti nel gioco di coordinamento in termini di questi parametri. Tuttavia, possiamo ancora chiederci in che modo l\u2019aggiunta di un ingresso fuori diagonale potrebbe influenzare l\u2019esito di un particolare gioco. Come mostra l\u2019esempio seguente, una maggiore compatibilit\u00e0 tra due tecnologie pu\u00f2 consentire a una tecnologia che inizialmente non era epidemica di diventarlo. ESEMPIO 6.1. Consideriamo il gioco del contagio giocato su un grafico a linee spesse -LRB- vedere Sezione 3 -RRB- con r = 5/32 e q = 3/8. In questo caso A non \u00e8 epidemico, come si pu\u00f2 vedere esaminando la Figura 1, poich\u00e9 2r < q e q + r > 1/2. Tuttavia, se inseriamo pagamenti simmetrici fuori diagonale x = 1/4, abbiamo un nuovo gioco, equivalente a un gioco parametrizzato da r ' = 5/16 e q ' = 1/4. Poich\u00e9 q ' < 1/2 e q ' < 2r ', A \u00e8 epidemico in questo gioco, e quindi anche nel gioco con compatibilit\u00e0 limitata. Mostriamo ora che in generale, se A \u00e8 la tecnologia superiore -LRB- cio\u00e8 q < 1/2 -RRB-, l'aggiunta di un termine di compatibilit\u00e0 x pu\u00f2 solo aiutare A a diffondersi. TEOREMA 6.2. Sia G un gioco senza compatibilit\u00e0, parametrizzato da r e q su una particolare rete. Sia G ' lo stesso gioco, ma con l'aggiunta di un termine di compatibilit\u00e0 simmetrica x. Se A \u00e8 epidemico per G, allora A \u00e8 epidemico per G\u2019. PROVA. Mostreremo che qualsiasi struttura bloccante in G ' \u00e8 anche una struttura bloccante in G. Per il nostro teorema di caratterizzazione, Teorema 4.6, ci\u00f2 implica il risultato desiderato. Abbiamo che G ' equivale a un gioco senza compatibilit\u00e0 parametrizzato da q ' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- e r ' = r / -LRB- 1 -- 2x -RRB-. Consideriamo una struttura bloccante -LRB- SB, SAB -RRB- per G '. Quindi pi\u00f9 di due tecnologie. Data la struttura complessa inerente ai giochi di contagio con due tecnologie, la comprensione dei giochi di contagio con tre o pi\u00f9 tecnologie \u00e8 ampiamente aperta. Qui indichiamo alcune delle problematiche tecniche che si presentano con molteplici tecnologie, attraverso una serie di primi risultati. L'assetto di base che studiamo \u00e8 quello in cui due tecnologie dominanti B e C coesistono inizialmente, e una terza tecnologia A, superiore ad entrambe, viene inizialmente introdotta in un insieme finito di nodi. Presentiamo innanzitutto un teorema che afferma che per ogni \u0394 pari, esiste un gioco di contagio su un grafo regolare \u0394 in cui le due tecnologie dominanti B e C possono trovare utile aumentare la loro compatibilit\u00e0 in modo da evitare di essere spazzate via dal nuova tecnologia superiore A. In particolare, consideriamo una situazione in cui inizialmente due tecnologie B e C con compatibilit\u00e0 zero si trovano in uno stato stabile. Per stato stabile intendiamo che nessuna perturbazione finita degli stati attuali pu\u00f2 portare a un\u2019epidemia sia per B che per C. Abbiamo anche una tecnologia A che \u00e8 superiore sia a B che a C,e possono diventare epidemici costringendo un singolo nodo a scegliere A. Tuttavia, aumentando la loro compatibilit\u00e0, B e C possono mantenere la loro stabilit\u00e0 e resistere a un\u2019epidemia da A. Sia qA denotare i profitti di due nodi adiacenti che scelgono entrambi la tecnologia A, e definire qB e qC in modo analogo. Assumeremo qA > qB > qC. Assumiamo inoltre che r, il costo della selezione di tecnologie aggiuntive, sia sufficientemente elevato da garantire che i nodi non adottino mai pi\u00f9 di una tecnologia. Infine, consideriamo un parametro di compatibilit\u00e0 qBC che rappresenta i vantaggi per due nodi adiacenti quando uno seleziona B e l'altro seleziona C. Pertanto il nostro gioco di contagio \u00e8 ora descritto da cinque parametri -LRB- G, qA, qB, qC, qBC -RRB -. PROVA. -LRB- Schizzo. -RRB- Dato \u0394, definisci G iniziando con una griglia infinita e collegando ciascun nodo al suo \u0394 pi\u00f9 vicino - 2 vicini che si trovano nella stessa riga. Lo stato iniziale s assegna la strategia B alle righe pari e la strategia C alle righe dispari. La prima, la terza e la quarta affermazione del teorema possono essere verificate controllando le corrispondenti disuguaglianze. La seconda affermazione deriva dalla prima e dall'osservazione che le file alternate contengono qualsiasi epidemia plausibile che cresca verticalmente. Il teorema precedente mostra che due tecnologie possono entrambe essere in grado di sopravvivere all\u2019introduzione di una nuova tecnologia aumentando il loro livello di compatibilit\u00e0 reciproca. Come ci si potrebbe aspettare, Tabella 1: I guadagni nel gioco di coordinazione. La voce -LRB- x, y -RRB- nella riga i, colonna j indica che il giocatore della riga ottiene un profitto di x e il giocatore della colonna ottiene un profitto di y quando il giocatore della riga gioca la strategia i e il giocatore della colonna gioca la strategia j. ci sono casi in cui una maggiore compatibilit\u00e0 tra due tecnologie aiuta una tecnologia a scapito dell'altra. Sorprendentemente, per\u00f2, ci sono anche casi in cui la compatibilit\u00e0 \u00e8 di fatto dannosa per entrambe le parti; l'esempio successivo considera una configurazione iniziale fissa con le tecnologie A, B e C che \u00e8 all'equilibrio quando qBC = 0. Tuttavia, se questo termine di compatibilit\u00e0 viene aumentato sufficientemente, l'equilibrio viene perso e A diventa epidemica. ESEMPIO 6.4. Consideriamo l'unione di un grafico a griglia bidimensionale infinito con nodi u -LRB- x, y -RRB- e un grafico a linee infinite con nodi v -LRB- y -RRB-. Aggiungi un bordo tra u -LRB- 1, y -RRB- e v -LRB- y -RRB- per ogni y. Per questa rete, consideriamo la configurazione iniziale in cui tutti i nodi v -LRB- y -RRB- selezionano A, e il nodo u -LRB- x, y -RRB- seleziona B se x < 0 e seleziona C altrimenti. Definiamo ora i parametri di questo gioco come segue. Si verifica facilmente che per questi valori la configurazione iniziale sopra riportata \u00e8 un equilibrio. Supponiamo ora per\u00f2 di aumentare il termine di coordinazione, ponendo qBC = 0,9. Questo non \u00e8 un equilibrio, poich\u00e9 ogni nodo della forma u -LRB- 0, y -RRB- ha ora un incentivo a passare da C -LRB- generando un payoff di 3,9 -RRB- a B -LRB- generando cos\u00ec un payoff di 3,95 -RRB-. Tuttavia,una volta che questi nodi hanno adottato B, la risposta migliore per ciascun nodo della forma u -LRB- 1, y -RRB- \u00e8 A -LRB- A genera un profitto di 4 mentre B genera solo un profitto di 3,95 -RRB- . Da qui non \u00e8 difficile dimostrare che A si diffonde direttamente attraverso l'intera rete.", "keyphrases": ["processo diffuso", "modello diffuso della teoria dei giochi", "strategia incompatibile", "bilingue", "limite compat", "interoperabile", "propriet\u00e0 non convesse", "carattere", "teorema di morri", "soglia di contagio", "gioco del contagio", "funzione potente"]}
{"file_name": "I-34", "text": "Risolvere conflitti e incoerenze nelle organizzazioni virtuali regolate da norme ABSTRACT Le organizzazioni virtuali governate da norme definiscono, governano e facilitano la condivisione coordinata delle risorse e la risoluzione dei problemi nelle societ\u00e0 di agenti. Con un resoconto esplicito delle norme, \u00e8 possibile raggiungere l\u2019apertura nelle organizzazioni virtuali: nuovi componenti, progettati da varie parti, possono essere integrati senza soluzione di continuit\u00e0. Ci concentriamo su organizzazioni virtuali realizzate come sistemi multi-agente, in cui agenti umani e software interagiscono per raggiungere obiettivi individuali e globali. Tuttavia, qualsiasi spiegazione realistica delle norme dovrebbe affrontare la loro natura dinamica: le norme cambieranno man mano che gli agenti interagiscono tra loro e con il loro ambiente. A causa della natura mutevole delle norme o a causa delle norme derivanti da diverse organizzazioni virtuali, ci saranno situazioni in cui un'azione \u00e8 contemporaneamente consentita e vietata, cio\u00e8 sorge un conflitto. Allo stesso modo, ci saranno situazioni in cui un'azione \u00e8 sia obbligata che vietata, ovvero si verifica un'incoerenza. Introduciamo un approccio, basato sull'unificazione del primo ordine, per individuare e risolvere tali conflitti e incoerenze. Nella soluzione proposta, annotiamo una norma con l'insieme di valori che le sue variabili non dovrebbero avere per evitare un conflitto o un'incoerenza con un'altra norma. Il nostro approccio si adatta perfettamente alle interrelazioni dipendenti dal dominio tra le azioni e ai conflitti/incoerenze indiretti che queste possono causare. Pi\u00f9 in generale, possiamo acquisire un'utile nozione di delega inter-agente -LRB- e inter-ruolo -RRB- di azioni e norme ad esse associate, e usarla per affrontare conflitti/incoerenze causati dalla delega dell'azione. Illustriamo il nostro approccio con un esempio di e-Science in cui gli agenti supportano i servizi Grid. 1. INTRODUZIONE Le organizzazioni virtuali -LRB- VO -RRB- facilitano la condivisione coordinata delle risorse e la risoluzione dei problemi coinvolgendo varie parti geograficamente remote -LSB- 9 -RSB-. Le VO definiscono e regolano le interazioni -LRB- facilitando cos\u00ec il coordinamento -RRB- tra software e/o agenti umani che comunicano per raggiungere obiettivi individuali e globali -LSB- 16 -RSB-. Le VO sono realizzate come sistemi multi-agente e una caratteristica pi\u00f9 desiderabile di tali sistemi \u00e8 l'apertura grazie alla quale i nuovi componenti progettati da altre parti vengono integrati senza soluzione di continuit\u00e0. Le norme regolano il comportamento osservabile di agenti software egoistici ed eterogenei, progettati da varie parti che potrebbero non fidarsi completamente l'una dell'altra -LSB- 3, 24 -RSB-. Tuttavia, le VO regolamentate da norme possono incontrare problemi quando le norme assegnate ai loro agenti sono in conflitto -LRB- cio\u00e8, un'azione \u00e8 contemporaneamente proibita e consentita -RRB- o incoerenti -LRB- cio\u00e8, un'azione \u00e8 contemporaneamente vietata e obbligata -RRB- . Proponiamo un mezzo per rilevare e risolvere automaticamente conflitti e incoerenze nelle VO regolamentate dalle norme. Usiamo l'unificazione dei termini del primo ordine -LSB- 8 -RSB- per scoprire se e come le norme si sovrappongono nella loro influenza -LRB- cio\u00e8,gli agenti e i valori dei parametri nelle azioni degli agenti che le norme possono influenzare -RRB-. Ci\u00f2 consente una soluzione a grana fine in cui l\u2019influenza di norme contrastanti o incoerenti viene ridotta per particolari insiemi di valori. Ad esempio, le norme `` all'agente x \u00e8 consentito inviare un'offerta -LRB- ag1, 20 -RRB- '' e `` all'agente ag2 \u00e8 vietato inviare un'offerta -LRB- y, z -RRB- '' -LRB- dove x, y, z sono variabili e ag1, ag2, 20 sono costanti -RRB- sono in conflitto perch\u00e9 i loro agenti, azioni e termini -LRB- all'interno delle azioni -RRB- si unificano. Risolviamo il conflitto annotando le norme con insiemi di valori che le loro variabili non possono avere, limitando cos\u00ec la loro influenza. Nel nostro esempio, il conflitto viene evitato se richiediamo che la variabile y non possa essere ag1 e che z non possa essere 20. Nella sezione successiva forniremo una definizione minimalista per le VO regolate dalle norme. Nella sezione 3 definiamo formalmente i conflitti tra norme e spieghiamo come vengono rilevati e risolti. Nella sezione 4 descriviamo come i meccanismi della sezione precedente possono essere adattati per rilevare e risolvere le incongruenze normative. Nella sezione 5 descriviamo come le nostre norme ridotte vengono utilizzate nelle societ\u00e0 di agenti consapevoli delle norme. Nella sezione 6 spieghiamo come i nostri macchinari possono essere utilizzati per rilevare e risolvere conflitti/incoerenze indiretti, cio\u00e8 quelli causati attraverso le relazioni tra le azioni; estendiamo e adattiamo i macchinari per accogliere la delega delle norme. Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.Nella sezione 7 illustriamo il nostro approccio con un esempio di agenti software regolamentati da norme al servizio della Grid.", "keyphrases": ["organo virtuale", "sistema multiagente", "norma-regola vo", "agente", "conflitto di norme", "il conflitto vieta", "la norma \u00e8 inconsistente", "agente esterno", "agente del governatore"]}
{"file_name": "J-31", "text": "Calcolare la strategia ottimale a cui impegnarsi \u2217 ABSTRACT Nei sistemi multiagente, le impostazioni strategiche vengono spesso analizzate partendo dal presupposto che i giocatori scelgano le loro strategie simultaneamente. Tuttavia, questo modello non \u00e8 sempre realistico. In molti contesti, un giocatore \u00e8 in grado di impegnarsi in una strategia prima che l'altro giocatore prenda una decisione. Tali modelli sono sinonimi di leadership, impegno o modelli Stackelberg e il gioco ottimale in tali modelli \u00e8 spesso significativamente diverso dal gioco ottimale nel modello in cui le strategie vengono selezionate simultaneamente. Il recente aumento di interesse per le soluzioni informatiche della teoria dei giochi ha finora ignorato i modelli di leadership -LRB- con l'eccezione dell'interesse per la progettazione di meccanismi, dove il progettista \u00e8 implicitamente in una posizione di leadership -RRB-. In questo articolo, studiamo come calcolare le strategie ottimali per impegnarsi sia nell'impegno per le strategie pure che nell'impegno per le strategie miste, sia nei giochi in forma normale che in quelli bayesiani. Forniamo sia risultati positivi -LRB- algoritmi efficienti -RRB- sia risultati negativi -LRB- risultati di durezza NP -RRB-. 1. INTRODUZIONE Nei sistemi multiagente con agenti autointeressati -LRB- inclusa la maggior parte dei contesti economici -RRB-, l'azione ottimale che un agente intraprende dipende dalle azioni intraprese dagli altri agenti. Per analizzare come dovrebbe comportarsi un agente in tali contesti, \u00e8 necessario applicare gli strumenti della teoria dei giochi. Tipicamente, quando un\u2019impostazione strategica \u00e8 modellata nel quadro della teoria dei giochi, si presuppone che i giocatori scelgano le loro strategie simultaneamente. Ci\u00f2 \u00e8 particolarmente vero quando l'ambientazione \u00e8 modellata come un gioco in forma normale, che specifica solo l'utilit\u00e0 di ciascun agente in funzione del vettore di strategie che gli agenti scelgono, e non fornisce alcuna informazione sull'ordine in cui gli agenti effettuano le loro decisioni e ci\u00f2 che gli agenti osservano riguardo alle decisioni precedenti di altri agenti. Dato che il gioco \u00e8 modellato in forma normale, viene tipicamente analizzato utilizzando il concetto di equilibrio di Nash. Un equilibrio di Nash specifica una strategia per ciascun giocatore, in modo tale che nessun giocatore abbia un incentivo a deviare individualmente da questo profilo di strategie. -LRB- In genere, le strategie possono essere miste, ovvero distribuzioni di probabilit\u00e0 sulle strategie originali -LRB- pure -RRB-. -RRB- Una -LRB- strategia mista -RRB- L'equilibrio di Nash \u00e8 garantito nei giochi finiti -LSB- 18 -RSB-, ma un problema \u00e8 che potrebbero esserci pi\u00f9 equilibri di Nash. Ci\u00f2 porta al problema della selezione dell\u2019equilibrio: come un agente pu\u00f2 sapere quale strategia giocare se non sa quale equilibrio deve essere giocato. Quando l'ambientazione \u00e8 modellata come un gioco in forma estesa, \u00e8 possibile specificare che alcuni giocatori ricevano alcune informazioni sulle azioni intraprese da altri nelle fasi precedenti del gioco prima di decidere la loro azione. Tuttavia, in generale, i giocatori non sanno tutto quello che \u00e8 successo all'inizio del gioco. A causa di ci\u00f2,questi giochi vengono tipicamente ancora analizzati utilizzando un concetto di equilibrio, in cui si specifica una strategia mista per ciascun giocatore e si richiede che la strategia di ciascun giocatore sia la migliore risposta alle strategie degli altri. -LRB- In genere viene ora imposto un vincolo aggiuntivo sulle strategie per garantire che i giocatori non giochino in un modo irrazionale rispetto alle informazioni che hanno ricevuto finora. Ci\u00f2 porta a perfezionamenti dell'equilibrio di Nash come il sottogioco perfetto e l'equilibrio sequenziale. -RRB- Tuttavia, in molti contesti del mondo reale, le strategie non vengono selezionate in modo cos\u00ec simultaneo. Spesso, un giocatore -LRB- il leader -RRB- \u00e8 in grado di impegnarsi in una strategia prima di un altro giocatore -LRB- il follower -RRB-. Ci\u00f2 pu\u00f2 essere dovuto a una serie di motivi. Ad esempio, uno dei giocatori potrebbe arrivare al sito in cui si svolger\u00e0 la partita prima di un altro agente -LRB-; ad esempio, in contesti economici, un giocatore potrebbe entrare prima in un mercato e impegnarsi in un modo di fare affari -RRB -. Tale potere di impegno ha un profondo impatto sul modo in cui il gioco dovrebbe essere giocato. Ad esempio, il leader potrebbe trovarsi meglio a giocare una strategia che \u00e8 dominata nella rappresentazione in forma normale del gioco. In generale, se \u00e8 possibile impegnarsi in strategie miste, allora -LRB- sotto presupposti minori -RRB- non fa mai male, e spesso aiuta, impegnarsi in una strategia -LSB- 26 -RSB-. Essere costretti a impegnarsi in una strategia pura a volte aiuta, a volte fa male -LRB-, ad esempio, impegnarsi in una strategia pura in sasso-carta-forbici prima che la decisione dell'altro giocatore si tradurr\u00e0 naturalmente in una perdita -RRB-. In questo articolo assumeremo che l'impegno sia sempre forzato; in caso contrario, il giocatore che ha la scelta se impegnarsi pu\u00f2 semplicemente confrontare il risultato dell'impegno con il risultato del non impegno -LRB- con movimento simultaneo -RRB-. I modelli di leadership sono particolarmente importanti in contesti con pi\u00f9 agenti software egoisti. Una volta finalizzato il codice per un agente -LRB- o per un team di agenti -RRB- e l'agente viene distribuito, l'agente si impegna a riprodurre la strategia -LRB- possibilmente randomizzata -RRB- prescritta dal codice. Infine, esiste anche una situazione di leadership implicita nel campo della progettazione dei meccanismi, in cui un giocatore -LRB- e il progettista -RRB- scelgono le regole del gioco a cui poi giocano gli altri giocatori. In effetti, il progettista del meccanismo pu\u00f2 trarre vantaggio dall\u2019impegnarsi in una scelta che, se le restanti azioni degli agenti -RRB- fossero fissate, sarebbe subottimale. Tuttavia, il calcolo della strategia ottimale da adottare in una situazione di leadership \u00e8 stato ignorato. Teoricamente, le situazioni di leadership possono essere semplicemente pensate come un gioco in forma estesa in cui un giocatore sceglie prima una strategia -LRB- per il gioco originale -RRB-. Il numero di strategie in questo gioco in forma estesa, tuttavia, pu\u00f2 essere estremamente elevato. Ad esempio, se il leader \u00e8 in grado di impegnarsi in una strategia mista nel gioco originale,quindi ciascuna delle strategie miste -LRB- del continuum -RRB- costituisce una strategia pura nella rappresentazione in forma estesa della situazione di leadership. -LRB- Notiamo che un impegno per una distribuzione non \u00e8 la stessa cosa di una distribuzione sugli impegni. -RRB- Inoltre, se il gioco originale \u00e8 esso stesso un gioco in forma estesa, il numero di strategie nella rappresentazione in forma estesa della situazione di leadership -LRB- che \u00e8 un diverso gioco in forma estesa -RRB- diventa ancora maggiore. Per questo motivo, di solito non \u00e8 computazionalmente fattibile trasformare semplicemente il gioco originale nella rappresentazione in forma estesa della situazione di leadership; dobbiamo invece analizzare il gioco nella sua rappresentazione originale. In questo articolo studiamo come calcolare la strategia ottimale da seguire, sia nei giochi in forma normale -LRB- Sezione 2 -RRB- che nei giochi bayesiani, che sono un caso speciale di giochi in forma estesa -LRB- Sezione 3 -RRB -. 4. CONCLUSIONI E RICERCHE FUTURE Nei sistemi multiagente, le impostazioni strategiche vengono spesso analizzate partendo dal presupposto che i giocatori scelgano le loro strategie simultaneamente. Ci\u00f2 richiede una certa nozione di equilibrio -LRB- Equilibrio di Nash e i suoi perfezionamenti -RRB-, e spesso porta al problema della selezione dell'equilibrio: non \u00e8 chiaro a ogni singolo giocatore in base a quale equilibrio dovrebbe giocare. Tuttavia, questo modello non \u00e8 sempre realistico. In molti contesti, un giocatore \u00e8 in grado di impegnarsi in una strategia prima che l'altro giocatore prenda una decisione. Ad esempio, un agente pu\u00f2 arrivare prima dell'altro al sito -LRB- reale o virtuale -RRB- del gioco, oppure, nel caso specifico degli agenti software, il codice di un agente pu\u00f2 essere completato e impegnato prima di quello di un altro agente. Tali modelli sono sinonimi di leadership, impegno o modelli Stackelberg e il gioco ottimale in tali modelli \u00e8 spesso significativamente diverso dal gioco ottimale nel modello in cui le strategie vengono selezionate simultaneamente. Nello specifico, se l\u2019impegno verso strategie miste \u00e8 possibile, allora l\u2019impegno -LRB- ottimale -RRB- non danneggia mai il leader, e spesso aiuta. Il recente aumento di interesse per le soluzioni informatiche della teoria dei giochi ha finora ignorato i modelli di leadership -LRB- con l'eccezione dell'interesse per la progettazione di meccanismi, dove il progettista \u00e8 implicitamente in una posizione di leadership -RRB-. In questo articolo, abbiamo studiato come calcolare le strategie ottimali per impegnarsi sia nell'impegno per le strategie pure che nell'impegno per le strategie miste, sia nei giochi in forma normale che in quelli bayesiani. Per i giochi in forma normale, abbiamo dimostrato che la strategia pura ottimale a cui impegnarsi pu\u00f2 essere trovata in modo efficiente per qualsiasi numero di giocatori. Una strategia mista ottimale a cui impegnarsi in un gioco in forma normale pu\u00f2 essere trovata in modo efficiente per due giocatori utilizzando la programmazione lineare -LRB- e non pi\u00f9 efficiente di cos\u00ec, nel senso che qualsiasi programma lineare con un vincolo di probabilit\u00e0 pu\u00f2 essere codificato come tale problema -RRB-.-LRB- Questa \u00e8 una generalizzazione della computabilit\u00e0 in tempo polinomiale delle strategie minimax nei giochi in forma normale. -RRB- Il problema diventa NP-difficile per tre giocatori -LRB- o pi\u00f9 -RRB-. Nei giochi bayesiani, il problema di trovare una strategia pura ottimale a cui impegnarsi \u00e8 NP-difficile anche nei giochi a due giocatori in cui il follower ha un solo tipo, sebbene i giochi a due giocatori in cui il leader abbia un solo tipo possano essere risolti in modo efficiente. Il problema di trovare una strategia mista ottimale a cui impegnarsi in un gioco bayesiano \u00e8 NP-difficile anche nei giochi a due giocatori in cui il leader ha un solo tipo, sebbene i giochi a due giocatori in cui il follower abbia un solo tipo possano essere risolti in modo efficiente utilizzando una generalizzazione dell'approccio di programmazione lineare per giochi in forma normale. Le due tabelle seguenti riassumono questi risultati. Risultati per l\u2019impegno verso strategie miste. -LRB- Con pi\u00f9 di 2 giocatori, il `` follower '' \u00e8 l'ultimo giocatore a impegnarsi, il `` leader '' \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.il \"leader\" \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo Come gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.il \"leader\" \u00e8 il primo. -RRB- La ricerca futura pu\u00f2 prendere diverse direzioni. Possiamo anche studiare il calcolo delle strategie ottimali da adottare in altre1 rappresentazioni concise di giochi in forma normale - ad esempio, nei giochi grafici -LSB- 10 -RSB- o nei giochi con grafo ad effetto locale/azione -LSB- 14, 1 -RSB-. Per i casi in cui calcolare una strategia ottimale a cui impegnarsi \u00e8 NP-difficile, possiamo anche studiare il calcolo di strategie approssimativamente ottimali a cui impegnarsi. Si possono anche studiare modelli in cui pi\u00f9 giocatori -LRB- ma non tutti -RRB- si impegnano allo stesso tempo. Un\u2019altra direzione interessante da perseguire \u00e8 vedere se il calcolo di strategie miste ottimali su cui impegnarsi pu\u00f2 aiutarci o altrimenti far luce sul calcolo degli equilibri di Nash. Spesso, le strategie miste ottimali a cui impegnarsi sono anche strategie di equilibrio di Nash -LRB-, ad esempio, nei giochi a somma zero a due giocatori questo \u00e8 sempre vero -RRB-, anche se non \u00e8 sempre il caso -LRB-, ad esempio, come sappiamo Come gi\u00e0 sottolineato, a volte la strategia ottimale a cui impegnarsi \u00e8 una strategia strettamente dominata, che non pu\u00f2 mai essere una strategia di equilibrio di Nash -RRB-.", "keyphrases": ["strategie ottimali", "sistema multiag", "modo simultaneo", "modello di Stackelberg", "modello di leadership", "pura strategia", "mescolare le strategie", "gioco in forma normale", "gioco bayesiano", "equilibrio di Nash", "np-difficile"]}
{"file_name": "H-13", "text": "L'influenza delle caratteristiche delle didascalie sui modelli di click-through nella ricerca web ABSTRACT I motori di ricerca web presentano elenchi di didascalie, comprendenti titolo, snippet e URL, per aiutare gli utenti a decidere quali risultati di ricerca visitare. Comprendere l'influenza delle caratteristiche di queste didascalie sul comportamento di ricerca sul Web pu\u00f2 aiutare a convalidare algoritmi e linee guida per la loro generazione migliorata. In questo documento sviluppiamo una metodologia per utilizzare i registri dei clic da un motore di ricerca commerciale per studiare il comportamento degli utenti quando interagiscono con le didascalie dei risultati di ricerca. I risultati del nostro studio suggeriscono che caratteristiche relativamente semplici delle didascalie, come la presenza di tutti i termini di ricerca, la leggibilit\u00e0 dello snippet e la lunghezza dell'URL mostrato nella didascalia, possono influenzare in modo significativo il comportamento di ricerca sul Web degli utenti. 1. INTRODUZIONE I principali motori di ricerca Web commerciali presentano tutti i loro risultati pi\u00f9 o meno allo stesso modo. Ogni risultato della ricerca \u00e8 descritto da una breve didascalia, comprendente l'URL della pagina Web associata, un titolo e un breve riepilogo -LRB- o `` snippet '' -RRB- che descrive il contenuto della pagina. Spesso lo snippet viene estratto dalla pagina Web stessa, ma pu\u00f2 anche essere preso da fonti esterne, come i riepiloghi generati dagli esseri umani presenti nelle directory Web. La Figura 1 mostra una tipica ricerca sul Web, con didascalie per i primi tre risultati. Mentre le tre didascalie condividono lo stesso Lo snippet della terza didascalia \u00e8 lungo quasi il doppio di quello della prima, mentre lo snippet manca completamente dalla seconda didascalia. Il titolo della terza didascalia contiene tutti i termini della query in ordine, mentre i titoli della prima e della seconda didascalia contengono solo due dei tre termini. Uno dei termini della query viene ripetuto nella prima didascalia. Tutti i termini della query vengono visualizzati nell'URL della terza didascalia, mentre nessuno appare nell'URL della prima didascalia. Sebbene queste differenze possano sembrare minori, potrebbero anche avere un impatto sostanziale sul comportamento degli utenti. Una delle motivazioni principali per fornire una didascalia \u00e8 aiutare l'utente a determinare la pertinenza della pagina associata senza dover effettivamente fare clic per raggiungere il risultato. Nel caso di una query di navigazione, in particolare quando la destinazione \u00e8 ben nota, il solo URL pu\u00f2 essere sufficiente per identificare la pagina desiderata. Ma nel caso di una richiesta informativa, il titolo e lo snippet potrebbero essere necessari per guidare l'utente nella selezione di una pagina per ulteriori approfondimenti, e l'utente potrebbe giudicare la pertinenza di una pagina solo sulla base della didascalia. Quando questo giudizio \u00e8 corretto, pu\u00f2 accelerare il processo di ricerca consentendo all'utente di evitare materiale indesiderato. Quando fallisce, l'utente pu\u00f2 perdere tempo facendo clic su un risultato inappropriato e scansionando una pagina che contiene poco o nulla di interessante. Ancora peggio, l'utente potrebbe essere indotto in errore a saltare una pagina che contiene le informazioni desiderate. Tutti e tre i risultati nella Figura 1 sono rilevanti, con alcune limitazioni. Il primo risultato si collega ai principali Yahoo Kids! home page,ma poi \u00e8 necessario seguire un collegamento in un menu per trovare la pagina principale dei giochi. Nonostante le apparenze, il secondo risultato si collega a una collezione sorprendentemente ampia di giochi online, principalmente con temi ambientali. Sfortunatamente, queste caratteristiche della pagina non si riflettono interamente nelle didascalie. In questo articolo, esaminiamo l'influenza delle funzionalit\u00e0 dei sottotitoli sul comportamento di ricerca sul Web dell'utente, utilizzando i clic estratti dai registri dei motori di ricerca come principale strumento di indagine. Figura 1: Primi tre risultati per la query: giochi online per bambini. Comprendere questa influenza pu\u00f2 aiutare a convalidare algoritmi e linee guida per una migliore generazione dei risultati. didascalie stesse. Inoltre, queste caratteristiche possono svolgere un ruolo nel processo di deduzione di giudizi di rilevanza dal comportamento dell'utente -LSB- 1 -RSB-. Comprendendo meglio la loro influenza, si possono ottenere giudizi migliori. Diversi algoritmi di generazione dei sottotitoli potrebbero selezionare snippet di diversa lunghezza da diverse aree di una pagina. Gli snippet possono essere generati in modo indipendente dalla query, fornendo un riepilogo della pagina nel suo insieme, o in modo dipendente dalla query, fornendo un riepilogo di come la pagina si collega ai termini della query. La scelta corretta dello snippet pu\u00f2 dipendere da aspetti sia della query che della pagina dei risultati. Per i collegamenti che reindirizzano, potrebbe essere possibile visualizzare URL alternativi. Inoltre, per le pagine elencate nelle directory Web modificate dall'uomo come l'Open Directory Project, potrebbe essere possibile visualizzare titoli e frammenti alternativi derivati \u200b\u200bda questi elenchi. Quando questi frammenti, titoli e URL alternativi sono disponibili, la selezione di una combinazione appropriata per la visualizzazione pu\u00f2 essere guidata dalle loro caratteristiche. Uno snippet da una directory Web pu\u00f2 consistere di frasi complete ed essere meno frammentario di uno snippet estratto. Un titolo estratto dal corpo pu\u00f2 fornire una maggiore copertura dei termini della query. Il lavoro riportato in questo articolo \u00e8 stato intrapreso nel context del motore di ricerca Windows Live. Gli esperimenti riportati nelle sezioni successive si basano sui registri delle query di Windows Live, sulle pagine dei risultati e sui giudizi di pertinenza raccolti come parte della ricerca in corso sulle prestazioni dei motori di ricerca -LSB- 1, 2 -RSB-. Tuttavia, data la somiglianza dei formati dei sottotitoli tra i principali motori di ricerca Web, riteniamo che i risultati siano applicabili a questi altri motori. La query in ` www.dmoz.org figura 1 produce risultati con rilevanza simile sugli altri principali motori di ricerca. Questa e altre query producono didascalie che presentano variazioni simili. Inoltre, riteniamo che la nostra metodologia possa essere generalizzata ad altre applicazioni di ricerca quando saranno disponibili dati sufficienti sui clic. 2. LAVORO CORRELATO Sebbene i motori di ricerca Web commerciali abbiano seguito approcci simili alla visualizzazione dei sottotitoli sin dalla loro genesi, \u00e8 stata pubblicata relativamente poca ricerca sui metodi per generare questi sottotitoli e valutare il loro impatto sul comportamento dell'utente.La maggior parte delle ricerche sulla visualizzazione dei risultati Web hanno proposto modifiche sostanziali all'interfaccia, piuttosto che affrontare i dettagli delle interfacce esistenti. 2.1 Visualizzazione dei risultati web Varadarajan e Hristidis -LSB- 16 -RSB- sono tra i pochi che hanno tentato di migliorare direttamente gli snippet generati dai sistemi di ricerca commerciali, senza introdurre ulteriori modifiche all'interfaccia. Hanno generato frammenti da alberi di grafici di documenti e hanno confrontato sperimentalmente questi frammenti con quelli generati per gli stessi documenti dal sistema di ricerca desktop di Google e dal sistema di ricerca desktop MSN. Hanno valutato il loro metodo chiedendo agli utenti di confrontare frammenti provenienti da varie fonti. 6. CONCLUSIONI Le inversioni click-through costituiscono uno strumento appropriato per valutare l'influenza delle caratteristiche delle didascalie. Utilizzando le inversioni di click-through, abbiamo dimostrato che funzionalit\u00e0 di didascalie relativamente semplici possono influenzare in modo significativo il comportamento degli utenti. A nostra conoscenza, questa \u00e8 la prima metodologia validata per valutare la qualit\u00e0 dei sottotitoli Web attraverso feedback implicito. Speriamo anche di raggiungere direttamente l'obiettivo di prevedere la pertinenza dai clic e da altre informazioni presenti nei log dei motori di ricerca.", "keyphrases": ["modello di clic", "funzione didascalia", "comportamento di ricerca sul web", "fattore umano", "estrarre il riassunto", "frammento", "registro delle query", "queri riformulazione", "parola significativa", "clic inverso", "corrispondenza dei termini della query"]}
{"file_name": "I-5", "text": "Verso un'allocazione auto-organizzata delle risorse basata su agenti in un ambiente multi-server ABSTRACT Le applicazioni distribuite richiedono tecniche distribuite per un'allocazione efficiente delle risorse. Queste tecniche devono tenere conto dell\u2019eterogeneit\u00e0 e della potenziale inaffidabilit\u00e0 delle risorse e dei consumatori di risorse in ambienti distribuiti. In questo articolo proponiamo un algoritmo distribuito che risolve il problema dell'allocazione delle risorse nei sistemi multiagente distribuiti. La nostra soluzione si basa sull'auto-organizzazione degli agenti, che non richiede alcun facilitatore o livello di gestione. L\u2019allocazione delle risorse nel sistema \u00e8 un effetto puramente emergente. Presentiamo i risultati del meccanismo di allocazione delle risorse proposto nell'ambiente multi-server statico e dinamico simulato. 1. INTRODUZIONE In questo senso ogni agente \u00e8 un consumatore di risorse che acquisisce una certa quantit\u00e0 di risorse per l'esecuzione dei suoi compiti. \u00c8 difficile per un meccanismo centrale di allocazione delle risorse raccogliere e gestire le informazioni su tutte le risorse condivise e sui consumatori di risorse per eseguire efficacemente l\u2019allocazione delle risorse. Pertanto, sono necessarie soluzioni distribuite del problema dell\u2019allocazione delle risorse. I ricercatori hanno riconosciuto questi requisiti -LSB- 10 -RSB- e hanno proposto tecniche per l'allocazione distribuita delle risorse. Un tipo promettente di tali approcci distribuiti si basa su modelli di mercato economico -LSB-4 -RSB-, ispirati ai principi dei mercati azionari reali. Anche se questi approcci sono distribuiti, di solito richiedono un facilitatore per la determinazione dei prezzi, la scoperta delle risorse e l\u2019invio dei lavori alle risorse -LSB- 5, 9 -RSB-. Un altro problema principalmente irrisolto di questi approcci \u00e8 la regolazione fine di prezzo e tempo, vincoli di budget per consentire un'allocazione efficiente delle risorse in sistemi grandi e dinamici -LSB- 22 -RSB-. In questo articolo proponiamo una soluzione distribuita del problema dell'allocazione delle risorse basata sull'auto-organizzazione dei consumatori di risorse in un sistema con risorse limitate. Nel nostro approccio, gli agenti assegnano dinamicamente le attivit\u00e0 ai server che forniscono una quantit\u00e0 limitata di risorse. Nel nostro approccio, gli agenti selezionano autonomamente la piattaforma di esecuzione per l'attivit\u00e0 anzich\u00e9 chiedere a un broker di risorse di eseguire l'allocazione. Tutto il controllo necessario per il nostro algoritmo \u00e8 distribuito tra gli agenti nel sistema. Ottimizzano il processo di allocazione delle risorse continuamente nel corso della loro vita in base ai cambiamenti nella disponibilit\u00e0 delle risorse condivise imparando dalle decisioni di allocazione passate. Le uniche informazioni disponibili a tutti gli agenti sono il carico delle risorse e le informazioni sull'esito positivo dell'allocazione derivanti dalle allocazioni di risorse precedenti. Ulteriori informazioni sul carico delle risorse sui server non vengono diffuse. Il meccanismo proposto non richiede un'autorit\u00e0 di controllo centrale, un livello di gestione delle risorse o l'introduzione di comunicazioni aggiuntive tra gli agenti per decidere quale attivit\u00e0 \u00e8 assegnata su quale server.Dimostriamo che questo meccanismo esegue bene sistemi dinamici con un gran numero di compiti e pu\u00f2 essere facilmente adattato a varie dimensioni di sistema. Inoltre, le prestazioni complessive del sistema non vengono influenzate nel caso in cui gli agenti o i server si guastino o diventino non disponibili. L'approccio proposto fornisce un modo semplice per implementare l'allocazione distribuita delle risorse e tiene conto delle tendenze del sistema multi-agente verso l'autonomia, l'eterogeneit\u00e0 e l'inaffidabilit\u00e0 delle risorse e degli agenti. Questa tecnica proposta pu\u00f2 essere facilmente integrata da tecniche per accodare o rifiutare le richieste di allocazione delle risorse degli agenti -LSB- 11 -RSB-. Tali capacit\u00e0 di autogestione degli agenti software consentono un'allocazione affidabile delle risorse anche in un ambiente con fornitori di risorse inaffidabili. Ci\u00f2 pu\u00f2 essere ottenuto mediante le reciproche interazioni tra agenti applicando tecniche tratte dalla teoria dei sistemi complessi. L'autorganizzazione di tutti gli agenti porta ad un'autorganizzazione dei 2. LAVORI CORRELATI L'allocazione delle risorse \u00e8 un problema importante nel campo dell'informatica. In generale, l'allocazione delle risorse \u00e8 un meccanismo o una politica per la gestione efficiente ed efficace dell'accesso a una risorsa o a un insieme di risorse limitate da parte dei suoi consumatori. Nel caso pi\u00f9 semplice, i consumatori di risorse chiedono a un intermediario centrale o a un dispatcher le risorse disponibili a cui verranno allocate il consumatore di risorse. Il broker di solito ha piena conoscenza di tutte le risorse di sistema. In questi approcci, il consumatore di risorse non pu\u00f2 influenzare il processo decisionale di allocazione. Il bilanciamento del carico -LSB- 3 -RSB- \u00e8 un caso speciale del problema di allocazione delle risorse che utilizza un broker che cerca di essere equo con tutte le risorse bilanciando equamente il carico del sistema tra tutti i fornitori di risorse. Questo meccanismo funziona meglio in un sistema omogeneo. Una semplice tecnica distribuita per la gestione delle risorse \u00e8 la pianificazione della capacit\u00e0 rifiutando o accodando gli agenti in entrata per evitare il sovraccarico delle risorse -LSB- 11 -RSB-. Dal punto di vista del proprietario della risorsa, questa tecnica \u00e8 importante per prevenire il sovraccarico della risorsa ma non \u00e8 sufficiente per un'allocazione efficace delle risorse. Questa tecnica pu\u00f2 solo fornire un buon complemento ai meccanismi di allocazione distribuita delle risorse. Questi coordinatori di solito devono avere una conoscenza globale dello stato di tutte le risorse del sistema. Un esempio di algoritmo di allocazione dinamica delle risorse \u00e8 il progetto Cactus -LSB- 1 -RSB- per l'allocazione di lavori computazionalmente molto costosi. Il valore delle soluzioni distribuite per il problema dell'allocazione delle risorse \u00e8 stato riconosciuto dalla ricerca -LSB- 10 -RSB-. Ispirandosi ai principi dei mercati azionari, sono stati sviluppati modelli di mercato economico per lo scambio di risorse per la regolazione della domanda e dell'offerta nella rete. Gli utenti cercano di acquistare le risorse a basso costo necessarie per eseguire il lavoro mentre i fornitori cercano di ottenere il massimo profitto possibile e di utilizzare le risorse disponibili a piena capacit\u00e0.Una raccolta di diverse tecniche di allocazione delle risorse distribuite basate su modelli di mercato \u00e8 presentata in Clearwater -LSB- 10 -RSB-. Buyya et al. ha sviluppato un quadro di allocazione delle risorse basato sulla regolamentazione della domanda e dell'offerta -LSB- 4 -RSB- per Nimrod-G -LSB- 6 -RSB- con particolare attenzione alle scadenze lavorative e ai vincoli di budget. Il modello di allocazione delle risorse basato su agenti -LRB- ARAM -RRB- per le griglie \u00e8 progettato per pianificare lavori computazionalmente costosi utilizzando agenti. Lo svantaggio di questo modello \u00e8 l'uso estensivo dello scambio di messaggi tra agenti per il monitoraggio periodico e lo scambio di informazioni all'interno della struttura gerarchica. Le sottoattivit\u00e0 di un lavoro migrano attraverso la rete finch\u00e9 non trovano una risorsa che soddisfa i vincoli di prezzo. L'itinerario di migrazione del lavoro \u00e8 determinato dalle risorse nel collegarli in diverse topologie -LSB- 17 -RSB-. Il meccanismo proposto in questo documento elimina la necessit\u00e0 di uno scambio periodico di informazioni sui carichi delle risorse e non necessita di una topologia di connessione tra le risorse. Negli ultimi anni \u00e8 stato pubblicato un lavoro considerevole sulle tecniche di allocazione decentralizzata delle risorse utilizzando la teoria dei giochi. \u00c8 un problema decisionale mal definito che presuppone e modella il ragionamento induttivo. In questo gioco decisionale ripetitivo, un numero dispari di agenti deve scegliere tra due risorse sulla base delle informazioni sui successi passati cercando di allocarsi nella risorsa con la minoranza. Galstyan et al. -LSB- 14 -RSB- ha studiato una variazione con pi\u00f9 di due risorse, modificando le capacit\u00e0 delle risorse e le informazioni provenienti dagli agenti vicini. Hanno dimostrato che gli agenti possono adattarsi efficacemente alle mutevoli capacit\u00e0 in questo ambiente utilizzando una serie di semplici tabelle di ricerca -LRB- strategie -RRB- per agente. Un'altra tecnica distribuita utilizzata per risolvere il problema dell'allocazione delle risorse \u00e8 basata sull'apprendimento per rinforzo -LSB- 18 -RSB-. Similmente al nostro approccio, un insieme di agenti competono per un numero limitato di risorse basandosi solo su precedenti esperienze individuali. In questo documento, l'obiettivo del sistema \u00e8 massimizzare la produttivit\u00e0 del sistema garantendo al tempo stesso l'equit\u00e0 delle risorse, misurata come tempo di elaborazione medio per unit\u00e0 di lavoro. Un approccio di allocazione delle risorse per reti di sensori basato su tecniche di auto-organizzazione e apprendimento per rinforzo \u00e8 presentato in -LSB- 16 -RSB- con focus principale sull'ottimizzazione del consumo energetico dei nodi della rete. Noi -LSB- 19 -RSB- abbiamo proposto un approccio di bilanciamento del carico auto-organizzato per un singolo server con particolare attenzione all'ottimizzazione dei costi di comunicazione degli agenti mobili. Un agente mobile rifiuter\u00e0 una migrazione a un server dell'agente remoto, se prevede che il server di destinazione sia gi\u00e0 sovraccaricato da altri agenti o attivit\u00e0 del server. Gli agenti prendono le loro decisioni in base alle previsioni di utilizzo del server. In questo documento viene presentata una soluzione per un ambiente multiserver senza considerare i costi di comunicazione o di migrazione. 6.CONCLUSIONI E LAVORO FUTURO In questo articolo \u00e8 stata presentata una tecnica di allocazione distribuita delle risorse auto-organizzata per sistemi multi-agente. Consentiamo agli agenti di selezionare autonomamente la piattaforma di esecuzione per le proprie attivit\u00e0 prima di ogni esecuzione in fase di esecuzione. Nel nostro approccio gli agenti competono per un'allocazione in uno dei Figura 5: Risultati dell'esperimento 2 in un ambiente server dinamico con una media di oltre 100 ripetizioni. risorsa condivisa disponibile. Gli agenti percepiscono l'ambiente del loro server e adottano la loro azione per competere in modo pi\u00f9 efficiente nel nuovo ambiente creato. Questo processo \u00e8 adattivo e ha un forte feedback poich\u00e9 le decisioni di allocazione influenzano indirettamente le decisioni di altri agenti. L\u2019allocazione delle risorse \u00e8 un effetto puramente emergente. Il nostro meccanismo dimostra che l\u2019allocazione delle risorse pu\u00f2 essere effettuata attraverso la concorrenza effettiva di agenti individuali e autonomi. Non necessitano n\u00e9 di coordinamento n\u00e9 di informazioni da parte di un'autorit\u00e0 superiore, n\u00e9 \u00e8 necessaria un'ulteriore comunicazione diretta tra gli agenti. Questo meccanismo \u00e8 stato ispirato dal ragionamento induttivo e dai principi di razionalit\u00e0 limitata che consentono agli agenti di adattare le loro strategie per competere efficacemente in un ambiente dinamico. Nel caso in cui un server diventi indisponibile, gli agenti possono adattarsi rapidamente a questa nuova situazione esplorando nuove risorse o rimanere sul server principale se un'allocazione non \u00e8 possibile. Soprattutto in ambienti dinamici e scalabili come i sistemi a griglia, \u00e8 necessario un meccanismo robusto e distribuito per l\u2019allocazione delle risorse. Il nostro approccio auto-organizzato all'allocazione delle risorse \u00e8 stato valutato con una serie di esperimenti di simulazione in un ambiente dinamico di agenti e risorse del server. I risultati presentati per questo nuovo approccio per l'ottimizzazione della migrazione strategica sono molto promettenti e giustificano ulteriori indagini in un ambiente di sistema multi-agente reale. Si tratta di una politica distribuita, scalabile e di facile comprensione per la regolamentazione della domanda e dell\u2019offerta di risorse. Tutto il controllo \u00e8 implementato negli agenti. Un semplice meccanismo decisionale basato sulle diverse convinzioni dell\u2019agente crea un comportamento emergente che porta ad un\u2019efficace allocazione delle risorse. Questo approccio pu\u00f2 essere facilmente esteso o supportato da meccanismi di bilanciamento/accodamento delle risorse forniti dalle risorse. Il nostro approccio si adatta ai cambiamenti dell\u2019ambiente ma non \u00e8 evolutivo. Non vi \u00e8 alcuna scoperta di nuove strategie da parte degli agenti. indagato in futuro. Nel prossimo futuro esamineremo se un adattamento automatico del tasso di decadimento delle informazioni storiche \u00e8 possibile nel nostro algoritmo e pu\u00f2 migliorare le prestazioni di allocazione delle risorse. Un gran numero di risorse condivise richiede informazioni storiche pi\u00f9 vecchie per evitare un'esplorazione delle risorse troppo frequente. Al contrario, un ambiente dinamico con capacit\u00e0 variabili richiede informazioni pi\u00f9 aggiornate per fare previsioni pi\u00f9 affidabili.Siamo consapevoli della lunga fase di apprendimento in ambienti con un gran numero di risorse condivise conosciute da ciascun agente. Nel caso in cui gli agenti richiedano pi\u00f9 risorse di quelle fornite da tutti i server, tutti gli agenti esploreranno in modo casuale tutti i server conosciuti. Questo processo di acquisizione delle informazioni sul carico delle risorse su tutti i server pu\u00f2 richiedere molto tempo nel caso in cui non vengano fornite risorse condivise sufficienti per tutte le attivit\u00e0. In questa situazione, \u00e8 difficile per un agente raccogliere in modo efficiente informazioni cronologiche su tutti i server remoti. Questo problema necessita di ulteriori indagini in futuro.", "keyphrases": ["sistema multiagente", "agente", "allocazione delle risorse", "algoritmo di distribuzione", "attivit\u00e0 di allocazione dinamica", "rete di server", "utilit\u00e0 del server", "adattare il processo", "competere", "predittore"]}
{"file_name": "J-14", "text": "Calcolo del buon equilibrio di Nash nei giochi grafici * ABSTRACT Questo articolo affronta il problema della selezione dell'equilibrio giusto nei giochi grafici. Il nostro approccio si basa sulla struttura dei dati chiamata politica di migliore risposta, proposta da Kearns et al. -LSB- 13 -RSB- come modo per rappresentare tutti gli equilibri di Nash di un gioco grafico. In -LSB- 9 -RSB-, \u00e8 stato dimostrato che la politica di risposta migliore ha dimensione polinomiale purch\u00e9 il grafico sottostante sia un percorso. In questo articolo mostriamo che se il grafico sottostante \u00e8 un albero a gradi limitati e la politica di risposta migliore ha dimensione polinomiale, allora esiste un algoritmo efficiente che costruisce un equilibrio di Nash che garantisce determinati guadagni a tutti i partecipanti. Un\u2019altra soluzione interessante \u00e8 l\u2019equilibrio di Nash che massimizza il benessere sociale. Mostriamo che, mentre calcolare esattamente quest'ultimo \u00e8 irrealizzabile -LRB- dimostriamo che la risoluzione di questo problema pu\u00f2 coinvolgere numeri algebrici di grado arbitrariamente alto -RRB-, esiste un FPTAS per trovare un tale equilibrio purch\u00e9 la politica di risposta migliore abbia dimensione polinomiale. Questi due algoritmi possono essere combinati per produrre equilibri di Nash che soddisfano vari criteri di equit\u00e0. 1. INTRODUZIONE Questa \u00e8 l'intuizione alla base dei giochi grafici, introdotti da Kearns, Littman e Singh in -LSB- 13 -RSB- come uno schema di rappresentazione compatto per giochi con molti giocatori. In un gioco grafico con n giocatori, ciascun giocatore \u00e8 associato a un vertice di un grafo sottostante G, e i guadagni di ciascun giocatore dipendono dalla sua azione cos\u00ec come dalle azioni dei suoi vicini nel grafico. Se il grado massimo di G \u00e8 \u0394, e ciascun giocatore ha a disposizione due azioni, allora il gioco pu\u00f2 essere rappresentato utilizzando n2\u0394+1 numeri. Al contrario, abbiamo bisogno di n2n numeri per rappresentare un gioco generale con 2 azioni e n-giocatori, il che \u00e8 pratico solo per piccoli valori di n. Per i giochi grafici con \u0394 costante, la dimensione del gioco \u00e8 lineare in n. Uno dei problemi pi\u00f9 naturali per un gioco grafico \u00e8 quello di trovare un equilibrio di Nash, la cui esistenza deriva dal celebre teorema di Nash -LRB- poich\u00e9 i giochi grafici sono solo un caso speciale di giochi con n-giocatori -RRB-. Il primo tentativo di affrontare questo problema \u00e8 stato fatto in -LSB- 13 -RSB-, dove gli autori considerano giochi grafici con due azioni per giocatore in cui il grafico sottostante \u00e8 un albero di gradi limitato. Propongono un algoritmo generico per trovare equilibri di Nash che pu\u00f2 essere specializzato in due modi: un algoritmo in tempo esponenziale per trovare un equilibrio di Nash -LRB- esatto -RRB-, e uno schema di approssimazione temporale completamente polinomiale -LRB- FPTAS -RRB- per trovare un\u2019approssimazione all\u2019equilibrio di Nash. Per ogni e > 0 questo algoritmo produce un equilibrio e-Nash, che \u00e8 un profilo strategico in cui nessun giocatore pu\u00f2 migliorare il proprio payoff di pi\u00f9 di e modificando unilateralmente la propria strategia. Sebbene gli equilibri di e-Nash siano spesso pi\u00f9 facili da calcolare rispetto agli equilibri di Nash esatti, questo concetto di soluzione presenta diversi inconvenienti. Primo,i giocatori potrebbero essere sensibili a una piccola perdita di payoff, quindi il profilo strategico che \u00e8 un equilibrio e-Nash non sar\u00e0 stabile. In secondo luogo, i profili strategici che si avvicinano agli equilibri di Nash possono essere molto migliori rispetto alle propriet\u00e0 in considerazione rispetto agli equilibri di Nash esatti. Pertanto, l'approssimazione -LRB- al valore -RRB- della migliore soluzione che corrisponde a un equilibrio di e-Nash potrebbe non essere indicativa di ci\u00f2 che pu\u00f2 essere ottenuto con un esatto equilibrio di Nash. Ci\u00f2 \u00e8 particolarmente importante se lo scopo della soluzione approssimata \u00e8 fornire un buon punto di riferimento per un sistema di agenti egoisti, poich\u00e9 il punto di riferimento implicito in un equilibrio e-Nash potrebbe non essere realistico. Per queste ragioni, in questo articolo ci concentreremo sul problema del calcolo degli equilibri di Nash esatti. Basandosi sulle idee di -LSB- 14 -RSB-, Elkind et al. -LSB- 9 -RSB- ha mostrato come trovare un equilibrio di Nash -LRB- esatto -RRB- in tempo polinomiale quando il sottostante. Al contrario, trovare un equilibrio di Nash in un grafico generale limitato ai gradi sembra essere computazionalmente intrattabile: \u00e8 stato mostrato -LRB- vedere -LSB- 5, 12, 7 -RSB- -RRB- da completare per la classe di complessit\u00e0 PPAD. -LSB- 9 -RSB- estende questo risultato di durezza al caso in cui il grafico sottostante ha una larghezza di percorso limitata. Un gioco grafico potrebbe non avere un unico equilibrio di Nash, anzi potrebbe averne esponenzialmente molti. Inoltre, alcuni equilibri di Nash sono pi\u00f9 desiderabili di altri. Piuttosto che avere un algoritmo che trovi semplicemente qualche equilibrio di Nash, vorremmo avere algoritmi per trovare equilibri di Nash con varie propriet\u00e0 socialmente desiderabili, come massimizzare il profitto complessivo o distribuire equamente il profitto. Una propriet\u00e0 utile della struttura dati di -LSB- 13 -RSB- \u00e8 che rappresenta simultaneamente l'insieme di tutti gli equilibri di Nash del gioco sottostante. Se questa rappresentazione ha dimensione polinomiale -LRB- come nel caso dei cammini, come mostrato in -LSB- 9 -RSB- -RRB-, si pu\u00f2 sperare di estrarre da essa un equilibrio di Nash con le propriet\u00e0 desiderate. Infatti, in -LSB- 13 -RSB- gli autori affermano che ci\u00f2 \u00e8 effettivamente possibile se si \u00e8 interessati a trovare un equilibrio -LRB- approssimato -RRB- a-Nash. L\u2019obiettivo di questo articolo \u00e8 estendere questo concetto agli equilibri di Nash esatti. 1.1 I nostri risultati In questo articolo studiamo giochi grafici a 2 azioni con n giocatori su alberi a gradi limitati per i quali la struttura dati di -LSB- 13 -RSB- ha dimensione poly -LRB- n -RRB-. Ci concentreremo sul problema di trovare equilibri di Nash esatti con determinate propriet\u00e0 socialmente desiderabili. In particolare, mostriamo come trovare un equilibrio di Nash che -LRB- quasi -RRB- massimizza il benessere sociale, ovvero la somma dei payoff dei giocatori, e mostriamo come trovare un equilibrio di Nash che -LRB- quasi -RRB - Soddisfa i limiti di pagamento prescritti per tutti i giocatori. I giochi grafici su alberi a gradi limitati hanno una struttura algebrica semplice. Una caratteristica interessante, che deriva da -LSB- 13 -RSB-,\u00e8 che ognuno di questi giochi ha un equilibrio di Nash in cui la strategia di ogni giocatore \u00e8 un numero razionale. La sezione 3 studia la struttura algebrica degli equilibri di Nash che massimizzano il benessere sociale. Mostriamo -LRB- Teoremi 1 e 2 -RRB- che, sorprendentemente, l'insieme degli equilibri di Nash che massimizzano il benessere sociale \u00e8 pi\u00f9 complesso. Sembra essere una caratteristica nuova del context che consideriamo qui, che un equilibrio di Nash ottimale sia difficile da rappresentare, in una situazione in cui \u00e8 facile trovare e rappresentare un equilibrio di Nash. Poich\u00e9 l\u2019equilibrio di Nash che massimizza il benessere sociale pu\u00f2 essere difficile da rappresentare in modo efficiente, dobbiamo accontentarci di un\u2019approssimazione. Tuttavia, la differenza cruciale tra il nostro approccio e quello degli articoli precedenti -LSB- 13, 16, 19 -RSB- \u00e8 che richiediamo al nostro algoritmo di produrre un equilibrio di Nash esatto, sebbene non necessariamente quello ottimale rispetto ai nostri criteri. Nella Sezione 4 descriviamo un algoritmo che soddisfa questo requisito. In particolare, proponiamo un algoritmo che per ogni e > 0 trova un equilibrio di Nash il cui payoff totale \u00e8 compreso tra a e ottimale. Un risultato pi\u00f9 correlato a pre1A in un context diverso \u00e8 stato ottenuto da Datta -LSB- 8 -RSB-, il quale mostra che i giochi a 2 azioni con n giocatori sono universali, nel senso che qualsiasi variet\u00e0 algebrica reale pu\u00f2 essere rappresentata come l'insieme di Nash totalmente misti equilibri di tali giochi. Mostriamo -LRB- Sezione 4.1 -RRB- che, sotto alcune restrizioni sulle matrici dei payoff, l'algoritmo pu\u00f2 essere trasformato in un algoritmo -LRB- veramente -RRB- in tempo polinomiale che produce un equilibrio di Nash il cui payoff totale \u00e8 compreso tra 1 \u2212 e fattore da quello ottimale. Nella Sezione 5, consideriamo il problema di trovare un equilibrio di Nash in cui il payoff atteso di ciascun giocatore Vi supera una soglia prescritta Ti. Usando l'idea della Sezione 4 diamo -LRB- Teorema 5 -RRB- uno schema di approssimazione temporale completamente polinomiale per questo problema. Il tempo di esecuzione dell'algoritmo \u00e8 limitato da un polinomio in n, Pmax ed E. Se l'istanza ha un equilibrio di Nash che soddisfa le soglie prescritte, allora l'algoritmo costruisce un equilibrio di Nash in cui il profitto atteso di ciascun giocatore Vi \u00e8 almeno Ti \u2212 E. Nella Sezione 6, introduciamo altri criteri naturali per selezionare un equilibrio di Nash ``buon'' e mostriamo che gli algoritmi descritti nelle due sezioni precedenti possono essere utilizzati come elementi costitutivi per trovare equilibri di Nash che soddisfano questi criteri. In particolare, nella Sezione 6.1 mostriamo come trovare un equilibrio di Nash che approssimi il massimo benessere sociale, garantendo al tempo stesso che ogni payoff individuale sia vicino ad una soglia prescritta. Nella Sezione 6.2 mostriamo come trovare un equilibrio di Nash in cui -LRB- quasi -RRB- massimizza il payoff individuale minimo. Infine, nella Sezione 6.3 mostriamo come trovare un equilibrio di Nash in cui i payoff individuali dei giocatori sono vicini tra loro. 1.2 Lavori correlati Il nostro schema di approssimazione -LRB- Teorema 3 e Teorema 4 -RRB- mostra un contrasto tra i giochi che studiamo e i giochi n-azione a due giocatori, per i quali i problemi corrispondenti sono solitamente intrattabili. Per i giochi n-azione a due giocatori, il problema di trovare equilibri di Nash con propriet\u00e0 speciali \u00e8 tipicamente NP-difficile. In particolare, questo \u00e8 il caso degli equilibri di Nash che massimizzano il benessere sociale -LSB- 11, 6 -RSB-. Inoltre, \u00e8 probabile che sia difficile anche solo approssimare tali equilibri. In particolare, Chen, Deng e Teng -LSB- 4 -RSB- mostrano che esiste un e, polinomio inverso in n, per il quale il calcolo di un equilibrio e-Nash in giochi a 2 giocatori con n azioni per giocatore \u00e8 PPAD-completo. Lipton e Markakis -LSB- 15 -RSB- studiano le propriet\u00e0 algebriche degli equilibri di Nash e sottolineano che gli algoritmi standard di eliminazione dei quantificatori possono essere utilizzati per risolverli. Si noti che questi algoritmi non sono tempo polinomiale in generale. I giochi che studiamo in questo articolo hanno equilibri di Nash calcolabili in tempo polinomiale in cui tutte le strategie miste sono numeri razionali, ma un equilibrio di Nash ottimale pu\u00f2 necessariamente includere strategie miste con alto grado algebrico. Qualsiasi equilibrio di Nash \u00e8 un EC ma in generale non vale il contrario. In contrasto con gli equilibri di Nash, equilibri correlati possono essere trovati per giochi grafici di basso grado -LRB- cos\u00ec come per altre classi di giochi multiplayer rappresentati in modo conciso -RRB- in tempo polinomiale -LSB- 17 -RSB-. Ma per i giochi grafici \u00e8 NP-difficile trovare un equilibrio correlato che massimizzi il payoff totale -LSB- 18 -RSB-. Tuttavia, i risultati della durezza NP si applicano a giochi pi\u00f9 generali di quello qui considerato, in particolare i grafici non sono alberi. Da -LSB- 2 -RSB- \u00e8 anche noto che esistono giochi a 2 giocatori e 2 azioni per i quali il payoff totale atteso del miglior equilibrio correlato \u00e8 maggiore del miglior equilibrio di Nash, e discuteremo ulteriormente questo problema nella Sezione 7. 7. CONCLUSIONI Abbiamo studiato il problema della selezione dell'equilibrio in giochi grafici su alberi a gradi limitati. Abbiamo considerato diversi criteri per selezionare un equilibrio di Nash, come massimizzare il benessere sociale, garantire un limite inferiore al payoff atteso di ciascun giocatore, ecc. In primo luogo, ci siamo concentrati sulla complessit\u00e0 algebrica di un equilibrio di Nash che massimizza il benessere sociale, e ha dimostrato forti risultati negativi per quel problema. Vale a dire, abbiamo dimostrato che anche per i giochi grafici su percorsi, qualsiasi numero algebrico \u03b1 E -LSB- 0, 1 -RSB- pu\u00f2 essere l'unica strategia disponibile per alcuni giocatori in tutti gli equilibri di Nash che massimizzano il benessere sociale. Ci\u00f2 \u00e8 in netto contrasto con il fatto che i giochi grafici sugli alberi possiedono sempre un equilibrio di Nash in cui tutte le strategie dei giocatori sono numeri razionali. Abbiamo quindi fornito algoritmi di approssimazione per selezionare equilibri di Nash con propriet\u00e0 speciali. Sebbene il problema di trovare equilibri di Nash approssimativi per varie classi di giochi abbia ricevuto molta attenzione negli ultimi anni,la maggior parte del lavoro esistente mira a trovare equilibri E-Nash che soddisfino -LRB- o siano E-vicini a soddisfare -RRB- determinate propriet\u00e0. Il nostro approccio \u00e8 diverso in quanto insistiamo nel fornire un esatto equilibrio di Nash, che sia E-vicino al soddisfacimento di un dato requisito. Come sostenuto nell\u2019introduzione, ci sono diverse ragioni per preferire una soluzione che costituisca un esatto equilibrio di Nash. Mentre dimostriamo i nostri risultati per i giochi su un percorso, essi possono essere generalizzati a qualsiasi albero per il quale le politiche di risposta ottima hanno rappresentazioni compatte come unioni di rettangoli. Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Nella versione completa dell'articolo descriviamo i nostri algoritmi per il caso generale. Ulteriore lavoro in questa direzione potrebbe includere estensioni ai tipi di garanzie ricercate per gli equilibri di Nash, come garantire i payoff totali per sottoinsiemi di giocatori, selezionare equilibri in cui alcuni giocatori ricevono payoff significativamente pi\u00f9 alti rispetto ai loro pari, ecc. Al momento, tuttavia, , \u00e8 forse pi\u00f9 importante indagare se gli equilibri di Nash dei giochi grafici possano essere calcolati in modo decentralizzato, in contrasto con gli algoritmi che abbiamo qui introdotto. Viene naturale chiedersi se i nostri risultati o quelli di -LSB- 9 -RSB- possano essere generalizzati a giochi con tre o pi\u00f9 azioni. Tuttavia, sembra che ci\u00f2 render\u00e0 l\u2019analisi notevolmente pi\u00f9 difficile. In particolare, si noti che si possono considerare i giochi con vincita limitata come un caso speciale molto limitato di giochi con tre azioni per giocatore. Vale a dire, dato un gioco a due azioni con limiti di payoff, consideriamo un gioco in cui ogni giocatore Vi ha una terza azione che gli garantisce un payoff di Ti indipendentemente da quello che fanno tutti gli altri. Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.Quindi verificare se esiste un equilibrio di Nash in cui nessuno dei giocatori assegna una probabilit\u00e0 diversa da zero alla sua terza azione equivale a verificare se esiste un equilibrio di Nash che soddisfa i limiti di payoff nel gioco originale, e la Sezione 5.1 mostra che trovare un equilibrio esatto La soluzione a questo problema richiede nuove idee. In alternativa, potrebbe essere interessante cercare risultati simili nel context degli equilibri correlati -LRB- CE -RRB-, soprattutto perch\u00e9 il miglior CE pu\u00f2 avere un valore pi\u00f9 alto -LRB- payoff totale atteso -RRB- rispetto al migliore NE. \u00c8 noto da -LSB- 1 -RSB- che il valore di mediazione dei giochi a 2 giocatori, 2 azioni con vincite non negative \u00e8 al massimo 43, e mostrano un gioco a 3 giocatori per cui \u00e8 infinito.", "keyphrases": ["gioco grafico", "equilibrio di Nash", "schema approssimativo", "Algoritmo esponenziale-tempo", "approssimativo", "varie propriet\u00e0 socialmente desiderate", "ricompensa complessiva", "distribuire gli utili", "benessere sociale", "gioco grafico a vincita integrale g", "grave inconveniente", "profilo strategico", "grafico legato ai gradi"]}
{"file_name": "I-22", "text": "Modellazione realistica del carico cognitivo per migliorare i modelli mentali condivisi nella collaborazione uomo-agente ABSTRACT I membri del team umano spesso sviluppano aspettative condivise per prevedere i bisogni reciproci e coordinare i loro comportamenti. In questo articolo il concetto di \"Mappa delle credenze condivise\" viene proposto come base per lo sviluppo di aspettative realistiche condivise all'interno di un team di coppie di agenti umani -LRB- HAP -RRB-. La creazione di mappe di credenze condivise si basa sulla condivisione di informazioni tra agenti, la cui efficacia dipende in larga misura dai carichi di elaborazione degli agenti e dai carichi cognitivi istantanei dei loro partner umani. Investighiamo modelli di carico cognitivo basati su HMM per facilitare i membri del team a \"condividere le informazioni giuste con la parte giusta al momento giusto\". Il concetto di mappa delle credenze condivise e i modelli di carico cognitivo/elaborativo sono stati implementati in un'architettura di agenti cognitivi: SMMall. Sono stati condotti una serie di esperimenti per valutare il concetto, i modelli e il loro impatto sull'evoluzione dei modelli mentali condivisi dei team HAP. 1. INTRODUZIONE Il lavoro di squadra multiagente centrato sull'uomo ha quindi attirato crescenti attenzioni nel campo dei sistemi multiagente -LSB- 2, 10, 4 -RSB-. Umani e autonomi In breve, gli esseri umani e gli agenti possono collaborare per ottenere prestazioni migliori, dato che possono stabilire una certa consapevolezza reciproca per coordinare le loro attivit\u00e0 di iniziativa mista. Tuttavia, il fondamento della collaborazione uomo-agente continua a essere messo in discussione a causa di modelli non realistici di consapevolezza reciproca dello stato delle cose. In particolare, pochi ricercatori guardano oltre per valutare i principi di modellazione dei costrutti mentali condivisi tra un essere umano e il suo agente assistente. Inoltre, le relazioni uomo-agente possono andare oltre i partner e raggiungere i team. Pertanto, esiste una chiara richiesta di indagini per ampliare e approfondire la nostra comprensione sui principi della modellazione mentale condivisa tra i membri di un team misto uomo-agente. Esistono linee di ricerca sul lavoro di squadra multi-agente, sia teoricamente che empiricamente. Ad esempio, Joint Intention -LSB- 3 -RSB- e SharedPlans -LSB- 5 -RSB- sono due framework teorici per specificare le collaborazioni degli agenti. Uno degli svantaggi \u00e8 che, sebbene entrambi abbiano una profonda radice filosofica e cognitiva, non consentono di modellare i membri del team umano. Studi cognitivi suggeriscono che i team che hanno modelli mentali condivisi dovrebbero avere aspettative comuni sul compito e sul team, che consentono loro di prevedere il comportamento e le esigenze di risorse dei membri del team in modo pi\u00f9 accurato -LSB- 14, 6 -RSB-. Cannon-Bowers et al. -LSB- 14 -RSB- sostengono esplicitamente che i membri del team dovrebbero possedere modelli compatibili che conducano a \u201caspettative\u201d comuni. Siamo d'accordo su questo e crediamo che la creazione di aspettative condivise tra i membri del team umano e degli agenti sia un passo fondamentale per far avanzare la ricerca sul lavoro di squadra incentrata sull'uomo.Va notato che il concetto di aspettativa condivisa pu\u00f2 includere in generale l\u2019assegnazione dei ruoli e le sue dinamiche, gli schemi e i progressi del lavoro di squadra, i modelli e le intenzioni di comunicazione, ecc. Mentre l\u2019obiettivo a lungo termine della nostra ricerca \u00e8 capire come le strutture cognitive condivise possono 6. CONCLUSIONE L'attenzione della recente ricerca sul lavoro di squadra incentrato sull'uomo richiede fortemente la progettazione di sistemi di agenti come ausili cognitivi in \u200b\u200bgrado di modellare e sfruttare i partner umani. capacit\u00e0 cognitive per offrire aiuto in modo non intrusivo. In questo articolo, abbiamo studiato diversi fattori che circondano il difficile problema dell\u2019evoluzione di modelli mentali condivisi di team composti da coppie di agenti umani. Il contributo principale di questa ricerca include -LRB-1 -RRB- Sono stati proposti modelli di carico basati su HMM per un agente per stimare il carico cognitivo del suo partner umano e i carichi di elaborazione di altri compagni di squadra HAP; -LRB- 2 -RRB- Il concetto di mappa delle credenze condivise \u00e8 stato introdotto e implementato. Permette ai membri del gruppo di rappresentare e ragionare efficacemente su modelli mentali condivisi; -LRB- 3 -RRB- Sono stati condotti esperimenti per valutare i modelli di carico cognitivo/elaborativo basati su HMM e gli impatti della comunicazione multilaterale sull'evoluzione delle SMM del team. Nel corso degli esperimenti \u00e8 stata dimostrata anche l\u2019utilit\u00e0 delle mappe delle credenze condivise.", "keyphrases": ["condividere la mappa delle credenze", "lavoro di squadra multiag", "eurista", "motivo", "risoluzione dei problemi", "collaborare", "lavoro di squadra", "aspettarsi", "schema del lavoro di squadra", "eseguire il team di agenti umani", "Teoria del carico cognitivo", "prestazione umana", "allocazione delle risorse", "compito eseguire", "condivisione di informazioni", "comune multipartitico"]}
{"file_name": "J-23", "text": "Rapporti di frugalit\u00e0 e meccanismi di Truthful migliorati per la copertura dei vertici * Nelle aste a sistema fisso, ci sono diversi team di agenti che si sovrappongono e un compito che pu\u00f2 essere completato da uno qualsiasi di questi team. L'obiettivo del banditore \u00e8 assumere una squadra e pagare il meno possibile. Esempi di questa impostazione includono le aste del percorso pi\u00f9 breve e le aste della copertura dei vertici. Recentemente, Karlin, Kempe e Tamir hanno introdotto una nuova definizione del rapporto di offrugalit\u00e0 per questo problema. Informalmente, il \"rapporto di frugalit\u00e0\" \u00e8 il rapporto tra il pagamento totale di un meccanismo e il limite di pagamento desiderato. Il rapporto cattura la misura in cui il meccanismo paga pi\u00f9 del dovuto, rispetto al costo equo percepito in un'asta veritiera. In questo articolo proponiamo una nuova asta polinomiale veritiera per il problema della copertura dei vertici e ne limitiamo il rapporto di frugalit\u00e0. Mostriamo che la qualit\u00e0 della soluzione \u00e8 con un fattore costante di ottimale e il rapporto di frugalit\u00e0 \u00e8 all'interno di un fattore costante del miglior limite possibile del caso peggiore; questa \u00e8 la prima asta per questo problema ad avere queste propriet\u00e0. Inoltre, mostriamo come trasformare qualsiasi asta veritiera in un'asta frugale preservando il rapporto di approssimazione. Inoltre, consideriamo due modifiche naturali della definizione di Karlin et al., e analizziamo le propriet\u00e0 dei limiti di pagamento risultanti, come monotonicit\u00e0, durezza computazionale e robustezza rispetto alla regola di risoluzione del sorteggio. Studiamo le relazioni tra i diversi limiti di pagamento, sia per sistemi di insiemi generali che per aste di insiemi specifici, come aste di percorso e aste di vertex-cover. Utilizziamo queste nuove definizioni nella dimostrazione del nostro risultato principale per le aste di copertura dei vertici tramite una tecnica di bootstrap, che potrebbe essere di interesse indipendente. 1. INTRODUZIONE In un'asta a sistema fisso c'\u00e8 un unico acquirente e molti venditori che possono fornire vari servizi. Si presuppone che i requisiti dell'acquirente possano essere soddisfatti da vari sottoinsiemi di venditori; questi sottoinsiemi sono detti insiemi ammissibili. Assumiamo che ogni venditore abbia un costo per fornire i propri servizi, ma presenti un'offerta possibilmente pi\u00f9 alta al banditore. Sulla base di queste offerte, il banditore seleziona un possibile sottoinsieme di venditori ed effettua i pagamenti ai venditori in questo sottoinsieme. Ogni venditore selezionato gode di un profitto dal pagamento meno i costi. I venditori vogliono massimizzare il profitto, mentre l\u2019acquirente vuole ridurre al minimo l\u2019importo da pagare. Un obiettivo naturale in questo context \u00e8 progettare un'asta veritiera, in cui i venditori abbiano un incentivo a offrire il loro costo reale. Ci\u00f2 pu\u00f2 essere ottenuto pagando a ciascun venditore selezionato un premio superiore alla sua offerta in modo tale che il venditore non abbia alcun incentivo a fare offerte superiori. Una questione interessante nella progettazione del meccanismo \u00e8 quanto il banditore dovr\u00e0 pagare in eccesso per garantire offerte veritiere. Nel context delle aste dei percorsi questo argomento \u00e8 stato affrontato per la prima volta da Archer e Tardos -LSB- 1 -RSB-.Essi definiscono il rapporto di frugalit\u00e0 di un meccanismo come il rapporto tra il suo pagamento totale e il costo del percorso pi\u00f9 economico disgiunto dal percorso selezionato dal meccanismo. Essi mostrano che, per un\u2019ampia classe di meccanismi veritieri per questo problema, il rapporto di frugalit\u00e0 \u00e8 grande quanto il numero di archi nel percorso pi\u00f9 breve. Talwar -LSB- 21 -RSB- estende questa definizione di rapporto di frugalit\u00e0 ai sistemi di insiemi generali e studia il rapporto di frugalit\u00e0 del meccanismo VCG classico -LSB- 22, 4, 14 -RSB- per molti sistemi di insiemi specifici, come lo spanning minimo alberi e coperture. Sebbene la definizione del rapporto di frugalit\u00e0 proposta da -LSB- 1 -RSB- sia ben motivata e sia stata determinante nello studio dei meccanismi veritieri per i sistemi di insiemi, non \u00e8 completamente soddisfacente. Consideriamo, ad esempio, il grafico della Figura 1 con i costi CAB = CBC = Figura 1: Il grafico del diamante Questo grafico \u00e8 2-connesso e il pagamento VCG al percorso vincente ABCD \u00e8 limitato. Tuttavia, il grafico non contiene alcun percorso A-D disgiunto da ABCD, e quindi il rapporto di frugalit\u00e0 di VCG su questo grafico rimane indefinito. Allo stesso tempo, non esiste alcun monopolio, cio\u00e8 non esiste un venditore che compaia in tutti gli insiemi possibili. Per affrontare questo problema, Karlin et al. -LSB- 16 -RSB- suggeriscono un punto di riferimento migliore, definito per qualsiasi sistema di set privo di monopolio. Sulla base di questa nuova definizione, gli autori costruiscono nuovi meccanismi per il problema del percorso pi\u00f9 breve e mostrano che il pagamento in eccesso di questi meccanismi rientra in un fattore costante di ottimale. 1.1 I nostri risultati Aste di copertura dei vertici Proponiamo un'asta polinomiale veritiera per la copertura dei vertici che produce una soluzione il cui costo rientra in un fattore 2 rispetto a quello ottimale e il cui rapporto di frugalit\u00e0 \u00e8 al massimo 2\u0394, dove \u0394 \u00e8 il grado massimo del grafico -LRB- Teorema 4 -RRB-. Completiamo questo risultato dimostrando -LRB- Teorema 5 -RRB- che per ogni \u0394 e n, ci sono grafici di massimo grado \u0394 e dimensione \u0398 -LRB- n -RRB- per i quali qualsiasi meccanismo veritiero ha un rapporto di frugalit\u00e0 almeno \u0394 / 2. Ci\u00f2 significa che la qualit\u00e0 della soluzione della nostra asta \u00e8 ottimale per un fattore 2 e il rapporto di frugalit\u00e0 rientra per un fattore 4 rispetto al miglior limite possibile per gli input nel caso peggiore. Per quanto ne sappiamo, questa \u00e8 la prima asta per questo problema che gode di queste propriet\u00e0. Inoltre, mostriamo come trasformare qualsiasi meccanismo veritiero per il problema della copertura dei vertici in uno frugale preservando il rapporto di approssimazione. Rapporti di frugalit\u00e0 I nostri risultati sulla copertura dei vertici suggeriscono naturalmente due modifiche della definizione di \u03bd in -LSB- 16 -RSB-. Queste modifiche possono essere apportate indipendentemente l'una dall'altra, risultando in quattro diversi limiti di pagamento TUmax, TUmin, NTUmax e NTUmin, dove NTUmin \u00e8 uguale al limite di pagamento originale \u03bd di in -LSB- 16 -RSB-. Mentre il nostro risultato principale sulle aste di copertura dei vertici -LRB- Teorema 4 -RRB- \u00e8 rispetto a NTUmin = \u03bd, utilizziamo le nuove definizioni confrontando prima il pagamento del nostro meccanismo con un limite pi\u00f9 debole NTUmax,e quindi eseguire il bootstrap da questo risultato per ottenere il limite desiderato. Ispirati da questa applicazione, ci imbarchiamo in un ulteriore studio di questi limiti di pagamento. I nostri risultati qui sono i seguenti: 1. Osserviamo -LRB- Proposizione 1 -RRB- che i quattro limiti di pagamento obbediscono sempre a un ordine particolare che \u00e8 indipendente dalla scelta del sistema di insiemi e del vettore di costo, vale a dire TUmin < NTUmin < NTUmax < TUmax. Forniamo esempi -LRB- Proposizione 5 e Corollari 1 e 2 -RRB- che mostrano che per il problema della copertura dei vertici qualsiasi due limiti consecutivi possono differire di un fattore n \u2212 2, dove n \u00e8 il numero di agenti. Mostriamo poi -LRB- Teorema 2 -RRB- che questa separazione \u00e8 quasi la migliore possibile per sistemi di insiemi generali dimostrando che per qualsiasi sistema di insiemi TUmax/TUmin < n. Al contrario, dimostriamo -LRB- Teorema 3 -RRB- che per aste di percorso TUmax/TUmin < 2. Forniamo esempi -LRB- Proposizioni 2, 3 e 4 -RRB- che mostrano che questo limite \u00e8 stretto. Consideriamo questo come un argomento a favore dello studio delle aste di vertexcover, poich\u00e9 sembrano essere pi\u00f9 rappresentative del problema generale di selezione del team rispetto alle aste di percorso ampiamente studiate. 2. Questa osservazione suggerisce che i quattro limiti di pagamento dovrebbero essere studiati in un quadro unitario; inoltre, ci porta a ritenere che la tecnica di bootstrap del Teorema 4 possa avere altre applicazioni. 3. Valutiamo i limiti di pagamento qui introdotti rispetto a una lista di controllo delle caratteristiche desiderabili. Questo pu\u00f2 essere visto come un argomento a favore dell'utilizzo di limiti NTUmax e TUmax pi\u00f9 deboli ma computabili in modo efficiente. Lavori correlati Le aste delle coperture dei vertici sono state studiate in passato da Talwar -LSB- 21 -RSB- e Calinescu -LSB- 5 -RSB-. Entrambi questi documenti si basano sulla definizione del rapporto di frugalit\u00e0 utilizzata in -LSB- 1 -RSB- ; come accennato in precedenza, ci\u00f2 significa che i loro risultati si applicano solo ai grafi bipartiti. Talwar -LSB- 21 -RSB- mostra che il rapporto di frugalit\u00e0 di VCG \u00e8 al massimo \u0394. Tuttavia, poich\u00e9 trovare la copertura del vertice pi\u00f9 economica \u00e8 un problema NP-difficile, il meccanismo VCG \u00e8 computazionalmente irrealizzabile. Il primo articolo -LRB- e, per quanto ne sappiamo, unico -RRB- per indagare i meccanismi veritieri in tempo polinomiale per la copertura dei vertici \u00e8 -LSB- 5 -RSB-. Questo articolo studia un'asta basata sull'algoritmo di allocazione greedy, che ha un rapporto di approssimazione di log n. Mentre l'obiettivo principale di -LSB- 5 -RSB- \u00e8 il problema pi\u00f9 generale della copertura dell'insieme, i risultati di -LSB- 5 -RSB- implicano un rapporto di frugalit\u00e0 di 2\u03942 per la copertura dei vertici. 2. PRELIMINARI Nella maggior parte di questo articolo si parler\u00e0 di aste per sistemi di insiemi. Nelle aste a sistema fisso, ogni elemento e del set terrestre \u00e8 di propriet\u00e0 di un agente indipendente e ha associato un costo non negativo ce. L'obiettivo del centro \u00e8 selezionare -LRB- acquistare -RRB- un insieme fattibile. Ogni elemento e nell'insieme selezionato comporta un costo di ce. Gli elementi non selezionati non comportano alcun costo. L'asta si svolge come segue: tutti gli elementi del terreno fanno le loro offerte,il centro seleziona un insieme fattibile in base alle offerte ed effettua i pagamenti agli agenti. Formalmente un'asta \u00e8 definita da una regola di allocazione A : R '' _, F e da una regola di pagamento P : R '' _, R ''. La regola di allocazione prende come input un vettore di offerte e decide quale degli insiemi in F deve essere selezionato. La regola di pagamento prende anche come input un vettore di offerte e decide quanto pagare a ciascun agente. I requisiti standard sono la razionalit\u00e0 individuale, ovvero il pagamento a ciascun agente dovrebbe essere almeno pari al costo sostenuto -LRB- 0 per gli agenti non nell'insieme selezionato e ce per gli agenti nell'insieme selezionato -RRB- e compatibilit\u00e0 degli incentivi, o veridicit\u00e0, cio\u00e8 la strategia dominante di ciascun agente \u00e8 quella di puntare il suo vero costo. Una regola di allocazione \u00e8 monotona se un agente non pu\u00f2 aumentare le sue possibilit\u00e0 di essere selezionato aumentando la sua offerta. Data una regola di allocazione monotona A e un vettore di offerta b, l'offerta soglia te di un agente e EA -LRB- b -RRB- \u00e8 l'offerta pi\u00f9 alta di questo agente che vince comunque l'asta, dato che le offerte degli altri partecipanti rimangono le Stesso. \u00c8 noto -LRB- vedi ad es. -LSB- 19, 13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- che qualsiasi asta che abbia una regola di allocazione monotona e paghi ad ogni agente la sua offerta soglia \u00e8 veritiera; al contrario, qualsiasi asta veritiera ha una regola di allocazione monotona. Il meccanismo VCG \u00e8 un meccanismo veritiero che massimizza il \u201cbenessere sociale\u201d e paga 0 agli agenti perdenti. Per le aste con sistema di set, ci\u00f2 significa semplicemente scegliere il set pi\u00f9 economico possibile, pagare a ciascun agente del set selezionato la sua offerta soglia e pagare 0 a tutti gli altri agenti. Si noti, tuttavia, che il meccanismo VCG potrebbe essere difficile da implementare, poich\u00e9 trovare un insieme fattibile pi\u00f9 economico potrebbe essere difficile. Se U \u00e8 un insieme di agenti, c -LRB- U -RRB- denota Ew \u2208 U cw. Allo stesso modo, b -LRB- U -RRB- denota Ew \u2208 U bw.", "keyphrases": ["rapporto frugale", "tecnica di bootstrap", "asta di copertura del vertice", "trasferimento util", "procedere al pagamento vincolato", "regola di allocazione monotona", "bottaio", "tempo polinomiale", "non monotono"]}
{"file_name": "I-11", "text": "Caratterizzazione e previsione degli agenti in tempo reale ABSTRACT Ragionare sugli agenti che osserviamo nel mondo \u00e8 impegnativo. Le nostre informazioni disponibili sono spesso limitate all'osservazione del comportamento esterno dell'agente nel passato e nel presente. Per comprendere queste azioni, dobbiamo dedurre lo stato interno dell'agente, che comprende non solo elementi razionali -LRB- come intenzioni e progetti -RRB-, ma anche emotivi -LRB- come la paura -RRB-. Inoltre, spesso vogliamo prevedere le azioni future dell'agente, che sono vincolate non solo da queste caratteristiche interiori, ma anche dalle dinamiche dell'interazione dell'agente con il suo ambiente. BEE -LRB- Behavior Evolution and Extrapolation -RRB- utilizza un modello ambientale basato su agenti pi\u00f9 veloce del tempo reale per caratterizzare lo stato interno degli agenti mediante evoluzione rispetto al comportamento osservato e quindi prevedere il loro comportamento futuro, tenendo conto delle dinamiche di la loro interazione con l\u2019ambiente. 1. INTRODUZIONE Il ragionamento sugli agenti che osserviamo nel mondo deve integrare due livelli disparati. Le nostre osservazioni sono spesso limitate al comportamento esterno dell'agente, che spesso pu\u00f2 essere riassunto numericamente come una traiettoria nello spazio-tempo -LRB- forse punteggiata da azioni provenienti da un vocabolario abbastanza limitato -RRB-. Tuttavia, questo comportamento \u00e8 guidato dallo stato interno dell'agente, che -LRB- nel caso di un essere umano -RRB- pu\u00f2 coinvolgere concetti psicologici e cognitivi di alto livello come intenzioni ed emozioni. Una sfida centrale in molti domini applicativi \u00e8 il ragionamento a partire dalle osservazioni esterne del comportamento degli agenti fino alla stima del loro stato interno. Tale ragionamento \u00e8 motivato dal desiderio di prevedere il comportamento dell'agente. Questo problema \u00e8 stato tradizionalmente affrontato sotto la rubrica \"riconoscimento del piano\" o \"inferenza del piano\". ''Molti problemi realistici si discostano da queste condizioni. \u2022 L'aumento del numero di agenti porta a un'esplosione combinatoria che pu\u00f2 sommergere l'analisi convenzionale. \u2022 Le dinamiche ambientali possono frustrare le intenzioni degli agenti. \u2022 Gli agenti spesso cercano di nascondere le loro intenzioni -LRB- e perfino la loro presenza -RRB-, piuttosto che condividere intenzionalmente le informazioni. \u2022 Lo stato emotivo di un agente pu\u00f2 essere importante almeno quanto il suo stato razionale nel determinare il suo comportamento. BEE -LRB- Behavioral Evolution and Extrapolation -RRB- \u00e8 un nuovo approccio per riconoscere lo stato razionale ed emotivo di pi\u00f9 agenti interagenti basandosi esclusivamente sul loro comportamento, senza ricorrere a comunicazioni intenzionali da parte loro. Si ispira alle tecniche utilizzate per prevedere il comportamento dei sistemi dinamici non lineari, in cui una rappresentazione del sistema \u00e8 continuamente adattata al suo comportamento passato recente. Per i sistemi dinamici non lineari, la rappresentazione \u00e8 un'equazione matematica in forma chiusa. In BEE, \u00e8 un insieme di parametri che governano il comportamento degli agenti software che rappresentano gli individui analizzati.L'attuale versione di BEE caratterizza e prevede il comportamento degli agenti che rappresentano i soldati impegnati nel combattimento urbano -LSB- 8 -RSB-. La sezione 2 esamina il lavoro precedente pertinente. La sezione 3 descrive l'architettura di BEE. La sezione 4 riporta i risultati degli esperimenti con il sistema. La sezione 5 conclude. 2. IL LAVORO PRECEDENTE BEE \u00e8 paragonabile alla ricerca precedente sul riconoscimento del piano AI -LRB- -RRB-, sui modelli Markov nascosti e sui sistemi dinamici non lineari -LRB- previsione della traiettoria -RRB-. 2.1 Il riconoscimento del piano nell'intelligenza artificiale La teoria degli agenti descrive comunemente lo stato cognitivo di un agente in termini di credenze, desideri e intenzioni -LRB- il cosiddetto modello \"BDI\" -LSB- 5, 20 -RSB- -RRB- . Le credenze di un agente sono proposizioni sullo stato del mondo che considera vero, in base alle sue percezioni. I suoi desideri sono proposizioni sul mondo che vorrebbe fosse vero. I desideri non sono necessariamente coerenti tra loro: un agente potrebbe desiderare di essere ricco e di non lavorare allo stesso tempo. Le intenzioni, o obiettivi, di un agente sono un sottoinsieme dei suoi desideri che ha selezionato, in base alle sue convinzioni, per guidare le sue azioni future. A differenza dei desideri, gli obiettivi devono essere coerenti tra loro -LRB- o almeno ritenuti coerenti dall'agente -RRB-. Gli obiettivi di un agente guidano le sue azioni. Pertanto si dovrebbe essere in grado di imparare qualcosa sugli obiettivi di un agente osservando le sue azioni passate, e la conoscenza degli obiettivi dell'agente a sua volta consente di trarre conclusioni su ci\u00f2 che l'agente potrebbe fare in futuro. Questo processo di ragionamento dalle azioni di un agente ai suoi obiettivi \u00e8 noto come \"riconoscimento del piano\" o \"inferenza del piano\". Il riconoscimento del piano \u00e8 raramente perseguito fine a se stesso. Di solito supporta una funzione di livello superiore. Ad esempio, nelle interfacce uomo-computer, il riconoscimento del piano di un utente pu\u00f2 consentire al sistema di fornire informazioni e opzioni pi\u00f9 appropriate per l'azione dell'utente. In un sistema di tutoraggio, dedurre il piano dello studente \u00e8 il primo passo per identificare i piani difettosi e fornire soluzioni adeguate. In molti casi, la funzione di livello superiore prevede le probabili azioni future dell'entit\u00e0 di cui si deduce il piano. Ci concentriamo sul riconoscimento del piano a supporto della previsione. Il piano di un agente \u00e8 un input necessario per prevedere il suo comportamento futuro, ma difficilmente sufficiente. Devono essere prese in considerazione almeno altre due influenze, una interna e una esterna. L'influenza esterna \u00e8 la dinamica dell'ambiente, che pu\u00f2 includere altri agenti. Le dinamiche del mondo reale impongono vincoli significativi. \u2022 L'ambiente pu\u00f2 interferire con i desideri dell'agente -LSB- 4, 10 -RSB-. \u2022 La maggior parte delle interazioni tra agenti, e tra agenti e il mondo, sono non lineari. Un'analisi razionale degli obiettivi di un agente pu\u00f2 permetterci di prevedere cosa tenter\u00e0, ma qualsiasi piano non banale con diversi passaggi dipender\u00e0 sensibilmente in ogni passaggio dalla reazione dell'ambiente.e la nostra previsione deve tenere conto anche di questa reazione. La simulazione effettiva dei futuri \u00e8 un modo -LRB- l'unico che conosciamo ora -RRB- per affrontare l'impatto delle dinamiche ambientali sulle azioni di un agente. Anche gli agenti umani sono soggetti a un'influenza interna. Lo stato emotivo dell'agente pu\u00f2 modulare il suo processo decisionale e il suo focus di attenzione -LRB- e quindi la sua percezione dell'ambiente -RRB-. In casi estremi, l'emozione pu\u00f2 portare un agente a scegliere azioni che dal punto di vista di un'analisi logica possono apparire irrazionali. Il lavoro attuale sul riconoscimento del piano per la previsione si concentra sul piano razionale e non tiene conto n\u00e9 delle influenze ambientali esterne n\u00e9 dei pregiudizi emotivi interni. BEE integra tutti e tre gli elementi nelle sue previsioni. 2.2 Modelli Markov nascosti BEE \u00e8 superficialmente simile ai modelli Markov nascosti -LRB- HMM -LSB- 19 -RSB- -RRB-. BEE offre due importanti vantaggi rispetto a HMM. Innanzitutto, le variabili nascoste di un singolo agente non soddisfano la propriet\u00e0 di Markov. Cio\u00e8, i loro valori at + 1 dipendono non solo dai loro valori at, ma anche dalle variabili nascoste di altri agenti. Si potrebbe evitare questa limitazione costruendo un singolo HMM sullo spazio degli stati congiunto di tutti gli agenti, ma questo approccio \u00e8 combinatoriamente proibitivo. BEE combina l'efficienza di modellare in modo indipendente i singoli agenti con la realt\u00e0 di tenere conto delle interazioni tra di loro. In secondo luogo, i modelli di Markov presuppongono che le probabilit\u00e0 di transizione siano stazionarie. Il processo evolutivo di BEE aggiorna continuamente le personalit\u00e0 degli agenti sulla base di osservazioni reali e quindi tiene conto automaticamente dei cambiamenti nelle personalit\u00e0 degli agenti. 2.3 Fitting di sistemi non lineari in tempo reale Molti sistemi di interesse possono essere descritti da un vettore di numeri reali che cambia in funzione del tempo. Le dimensioni del vettore definiscono lo spazio degli stati del sistema. La previsione a lungo termine di un tale sistema \u00e8 impossibile. Tuttavia, \u00e8 spesso utile anticipare il comportamento del sistema a breve distanza nel futuro. Questo processo si ripete costantemente, fornendo all'utente una visione anticipata limitata. Questo approccio \u00e8 robusto e ampiamente applicato, ma richiede sistemi che possano essere descritti in modo efficiente con equazioni matematiche. BEE estende questo approccio ai comportamenti degli agenti, adattandoli al comportamento osservato utilizzando un algoritmo genetico. 5. CONCLUSIONI In molti ambiti, \u00e8 importante ragionare a partire dal comportamento osservato di un'entit\u00e0 fino a una stima del suo stato interno, e quindi estrapolare tale stima per prevedere il comportamento futuro dell'entit\u00e0. BEE esegue questo compito utilizzando una simulazione pi\u00f9 veloce del tempo reale degli agenti sciamanti, coordinata tramite feromoni digitali. Questa simulazione integra la conoscenza delle aree di minaccia, un'analisi cognitiva delle credenze, dei desideri e delle intenzioni dell'agente, un modello della disposizione e dello stato emotivo dell'agente,e la dinamica delle interazioni con l'ambiente. Facendo evolvere gli agenti in questo ricco ambiente, possiamo adattare il loro stato interno al comportamento osservato. Nei wargame realistici, il sistema rileva con successo le emozioni deliberatamente riprodotte e fa previsioni ragionevoli sui comportamenti futuri delle entit\u00e0. BEE pu\u00f2 modellare solo variabili di stato interne che influiscono sul comportamento esterno dell'agente. Non pu\u00f2 adattarsi a variabili che l'agente non manifesta esternamente, poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto del comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto tra il comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.poich\u00e9 la base del ciclo evolutivo \u00e8 un confronto tra il comportamento esteriore dell'agente simulato con quello dell'entit\u00e0 reale. Questa limitazione \u00e8 grave se il nostro scopo \u00e8 comprendere lo stato interno dell'entit\u00e0 fine a se stessa. Se lo scopo dell\u2019adattamento degli agenti \u00e8 prevedere il loro comportamento successivo, la limitazione \u00e8 molto meno grave. Le variabili di stato che non influiscono sul comportamento, sebbene invisibili a un'analisi basata sul comportamento, sono irrilevanti per una previsione comportamentale. \u2022 Il nostro repertorio iniziale limitato di emozioni \u00e8 un piccolo sottoinsieme di quelli che sono stati distinti dagli psicologi e che potrebbero essere utili per comprendere e progettare il comportamento. Ci aspettiamo di estendere l'insieme delle emozioni e delle disposizioni di supporto che BEE pu\u00f2 rilevare. \u2022 La mappatura tra lo stato psicologico -LRB-, cognitivo ed emotivo -RRB- di un agente e il suo comportamento esteriore non \u00e8 uno a uno. Diversi stati interni diversi potrebbero essere coerenti con un dato comportamento osservato in una serie di condizioni ambientali, ma potrebbero produrre comportamenti distinti in altre condizioni. Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.Se l\u2019ambiente del recente passato \u00e8 tale da confondere stati interni cos\u00ec distinti, non saremo in grado di distinguerli. Finch\u00e9 l\u2019ambiente rimane in questo stato, le nostre previsioni saranno accurate, qualunque sia lo stato interno che assegniamo all\u2019agente. Se l\u2019ambiente poi si sposta verso un ambiente in cui i diversi stati interni portano a comportamenti diversi, l\u2019utilizzo dello stato interno scelto in precedenza produrr\u00e0 previsioni imprecise. Un modo per affrontare queste preoccupazioni \u00e8 sondare il mondo reale, perturbandolo in modi che stimolerebbero comportamenti distinti da parte di entit\u00e0 il cui stato psicologico sarebbe altrimenti indistinguibile. Tale indagine \u00e8 un'importante tecnica di intelligence. La simulazione pi\u00f9 veloce del tempo reale di BEE potrebbe consentirci di identificare azioni di indagine appropriate, aumentando notevolmente l'efficacia degli sforzi di intelligence.", "keyphrases": ["motivo dell'agente", "comportamento esterno", "stato interno", "prevedere il comportamento dell'agente", "comportamento evoluto ed estrapol", "sistema dinamico non lineare", "obiettivo dell'agente", "emot", "sapore di feromoni", "disporre", "comportamento futuro"]}
{"file_name": "J-9", "text": "Calcolo in un mercato dell'informazione distribuita \u2217 ABSTRACT Secondo la teoria economica \u2013 supportata da prove empiriche e di laboratorio \u2013 il prezzo di equilibrio di un titolo finanziario riflette tutte le informazioni riguardanti il \u200b\u200bvalore del titolo. Indaghiamo il processo computazionale nel percorso verso l'equilibrio, dove le informazioni distribuite tra i trader vengono rivelate passo dopo passo nel tempo e incorporate nel prezzo di mercato. Sviluppiamo un modello semplificato di un mercato dell'informazione, insieme a strategie di trading, al fine di formalizzare le propriet\u00e0 computazionali del processo. Mostriamo che i titoli i cui profitti non possono essere espressi come funzioni di soglia ponderate dei bit di input distribuiti non sono garantiti per convergere al corretto equilibrio previsto dalla teoria economica. D'altro canto, \u00e8 garantita la convergenza dei titoli i cui payoff sono funzioni di soglia, per tutte le distribuzioni di probabilit\u00e0 precedenti. Inoltre, questi valori di soglia convergono al massimo in n round, dove n \u00e8 il numero di bit di informazione distribuita. Dimostriamo anche un limite inferiore, mostrando un tipo di sicurezza a soglia che richiede almeno n/2 round per convergere nel caso peggiore. \u2217 Questo lavoro \u00e8 stato supportato dalla DoD University Research Initiative -LRB- URI -RRB- amministrata dall'Office of Naval Research con la sovvenzione N00014-01-1-0795. \u2020 Supportato in parte dalla sovvenzione ONR N00014-01-0795 e dalle sovvenzioni NSF CCR-0105337, CCR-TC-0208972, ANI-0207399 e ITR-0219018. \u2021 Questo lavoro \u00e8 stato condotto presso NEC Laboratories America, Princeton, NJ. 1. INTRODUZIONE La forma forte dell'ipotesi dei mercati efficienti afferma che i prezzi di mercato incorporano quasi istantaneamente tutte le informazioni disponibili a tutti i trader. Di conseguenza, i prezzi di mercato codificano le migliori previsioni sui risultati futuri sulla base di tutte le informazioni, anche se tali informazioni sono distribuite attraverso molte fonti. Il processo di incorporazione delle informazioni \u00e8, nella sua essenza, un calcolo distribuito. Ogni trader inizia con le proprie informazioni. Man mano che vengono effettuate le negoziazioni, le informazioni sintetiche vengono rivelate attraverso i prezzi di mercato. I trader apprendono o deducono quali informazioni gli altri potrebbero avere osservando i prezzi, quindi aggiornano le proprie convinzioni in base alle loro osservazioni. Nel tempo, se il processo funziona come pubblicizzato, tutte le informazioni vengono rivelate e tutti i trader convergono allo stesso stato informativo. A questo punto, il mercato si trova in quello che viene chiamato equilibrio di aspettative razionali -LSB- 11, 16, 19 -RSB-. Tutte le informazioni a disposizione di tutti i trader si riflettono ora nei prezzi correnti e non sono auspicabili ulteriori operazioni finch\u00e9 non saranno disponibili nuove informazioni. Sebbene la maggior parte dei mercati non siano progettati con l'aggregazione delle informazioni come motivazione primaria, ad esempio i derivati. In questo articolo, indaghiamo la natura del processo computazionale mediante il quale le informazioni distribuite vengono rivelate e combinate nel tempo nei prezzi nei mercati dell'informazione. A tal fine, nella sezione 3,proponiamo un modello di mercato dell'informazione che sia trattabile per l'analisi teorica e, a nostro avviso, catturi gran parte dell'importante essenza dei mercati dell'informazione reali. Dimostriamo che solo i titoli booleani i cui payoff possono essere espressi come funzioni di soglia dei bit di informazione distribuiti in input sono garantiti per convergere come previsto dalla teoria delle aspettative razionali. I titoli booleani con pagamenti pi\u00f9 complessi potrebbero non convergere in alcune distribuzioni precedenti. Forniamo inoltre limiti superiori e inferiori sul tempo di convergenza per questi titoli soglia. Mostriamo che, per tutte le distribuzioni precedenti, il prezzo di un titolo soglia converge al prezzo di equilibrio delle aspettative razionali al massimo in n cicli, dove n \u00e8 il numero di bit di informazione distribuita. Mostriamo che questo limite nel caso peggiore \u00e8 ristretto entro un fattore due, illustrando una situazione in cui un titolo soglia richiede n/2 round per convergere.", "keyphrases": ["teoria economica", "empir e laboratori evid", "prezzo di equilibrio", "finanza sicura", "valore di sicurezza", "processo di calcolo", "percorso verso l\u2019equilibrio", "commerciante", "prezzo di mercato", "modello semplificato", "strategie commerciali", "comput property del processo", "sicuro", "saldare", "funzione di soglia", "distribuzione probabile", "girare", "numero di bit", "distribuire informare", "limite inferiore", "caso peggiore", "informare il mercato"]}
{"file_name": "H-19", "text": "Analisi delle traiettorie delle caratteristiche per il rilevamento degli eventi ABSTRACT Consideriamo il problema dell'analisi delle traiettorie delle parole sia nel dominio del tempo che in quello della frequenza, con l'obiettivo specifico di identificare parole importanti e meno riportate, periodiche e aperiodiche. Un insieme di parole con andamento identico pu\u00f2 essere raggruppato per ricostruire un evento in modo del tutto automatico. La frequenza del documento di ciascuna parola nel tempo viene trattata come una serie temporale, in cui ogni elemento \u00e8 il punteggio frequenza del documento - frequenza inversa del documento -LRB- DFIDF -RRB- in un determinato momento. In questo articolo, abbiamo 1 -RRB- applicato per la prima volta l'analisi spettrale per classificare le caratteristiche di diversi eventi: importanti e meno segnalati, periodici e aperiodici; 2 -RRB- hanno modellato caratteristiche aperiodiche con densit\u00e0 gaussiana e caratteristiche periodiche con densit\u00e0 di miscele gaussiane, e successivamente hanno rilevato l'esplosione di ciascuna caratteristica mediante l'approccio gaussiano troncato; 3 -RRB- ha proposto un algoritmo di rilevamento di eventi greedy non supervisionato per rilevare eventi sia aperiodici che periodici. Tutti i metodi sopra indicati possono essere applicati ai dati delle serie temporali in generale. Abbiamo valutato approfonditamente i nostri metodi sul Reuters News Corpus -LSB- 3 -RSB- a 1 anno e abbiamo dimostrato che erano in grado di scoprire eventi aperiodici e periodici significativi. 1. INTRODUZIONE Ci sono pi\u00f9 di 4.000 fonti di notizie online nel mondo. Monitorarli manualmente tutti per eventi importanti \u00e8 diventato difficile o praticamente impossibile. In effetti, la comunit\u00e0 di rilevamento e tracciamento degli argomenti -LRB- TDT -RRB- ha cercato per molti anni di trovare una soluzione pratica per aiutare le persone a monitorare le notizie in modo efficace. le soluzioni proposte per il rilevamento degli eventi -LSB- 20, 5, 17, 4, 21, 7, 14, 10 -RSB- sono troppo semplicistiche -LRB- basate sulla somiglianza del coseno -LSB- 5 -RSB- -RRB- o poco pratiche a causa alla necessit\u00e0 di mettere a punto un gran numero di parametri -LSB- 9 -RSB-. Pertanto in questo articolo esamineremo le notizie e le tendenze in evidenza dal punto di vista dell'analisi di un segnale verbale in serie temporale. Lavori precedenti come -LSB- 9 -RSB- hanno tentato di ricostruire un evento con le sue caratteristiche rappresentative. Tuttavia, in molti compiti di rilevamento predittivo di eventi -LRB-, ovvero rilevamento retrospettivo di eventi -RRB-, esiste un vasto insieme di caratteristiche potenziali solo per un insieme fisso di osservazioni -LRB-, ovvero i burst evidenti -RRB-. Di queste funzionalit\u00e0, spesso ci si aspetta che solo un piccolo numero sia utile. In particolare, studiamo il nuovo problema dell'analisi delle traiettorie delle caratteristiche per il rilevamento degli eventi, prendendo in prestito una tecnica ben nota dall'elaborazione del segnale: identificare le correlazioni distribuzionali tra tutte le caratteristiche mediante l'analisi spettrale. Per valutare il nostro metodo, proponiamo successivamente un algoritmo di rilevamento di eventi non supervisionati per i flussi di notizie. Figura 1: Correlazione delle caratteristiche -LRB- DFIDF: tempo -RRB- tra a -RRB- Pasqua e aprile b -RRB- Non certificato e terminato. Come esempio illustrativo, si consideri la correlazione tra le parole Pasqua e aprile dal Reuters Corpus '.Dal grafico del loro DFIDF normalizzato nella Figura 1 -LRB- a -RRB-, osserviamo la forte sovrapposizione tra le due parole intorno a 04/1997, il che significa che probabilmente appartengono entrambe allo stesso evento durante quel periodo -LRB- festa di Pasqua -RRB-. In questo esempio, l'evento nascosto della festa di Pasqua \u00e8 un tipico evento aperiodico importante su dati di 1 anno. Un altro esempio \u00e8 fornito dalla Figura 1 -LRB- b -RRB-, dove entrambe le parole Unaudited e Ended `Reuters Corpus sono il set di dati predefinito per tutti gli esempi. mostrano un comportamento simile per periodi di 3 mesi. Queste due parole in realt\u00e0 hanno origine dallo stesso evento periodico, i rapporti sulle perdite di reddito netto, che vengono pubblicati trimestralmente dalle societ\u00e0 quotate in borsa. Altre osservazioni tratte dalla Figura 1 sono: 1 -RRB- il periodo di burst di aprile \u00e8 molto pi\u00f9 lungo di Pasqua, il che suggerisce che aprile possa esistere in altri eventi durante lo stesso periodo; 2 -RRB- Unaudited ha un valore DFIDF medio pi\u00f9 alto rispetto a Ended, il che indica che Unaudited \u00e8 pi\u00f9 rappresentativo dell'evento sottostante. Questi due esempi non sono che la punta dell\u2019iceberg tra tutte le tendenze e le correlazioni di parole nascoste in un flusso di notizie come Reuters. Se un gran numero di essi potesse essere scoperto, ci\u00f2 potrebbe aiutare in modo significativo le attivit\u00e0 TDT. In particolare, indica l'importanza dell'estrazione di caratteristiche di correlazione per il rilevamento di eventi corrispondenti. Per riassumere, postuliamo che: 1 -RRB- Un evento \u00e8 descritto dalle sue caratteristiche rappresentative. Sulla base di queste osservazioni, possiamo estrarre caratteristiche rappresentative dato un evento o rilevare un evento da un elenco di caratteristiche altamente correlate. In questo articolo ci concentreremo su quest'ultimo, ovvero su come le caratteristiche correlate possono essere scoperte per formare un evento in modo non supervisionato. 1.1 Contributi Questo articolo contiene tre contributi principali: 9 Per quanto ne sappiamo, il nostro approccio \u00e8 il primo a categorizzare le caratteristiche delle parole per eventi eterogenei. 9 Proponiamo un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per modellare e rilevare i burst di caratteristiche. 9 Creiamo un algoritmo di rilevamento degli eventi non supervisionati per rilevare eventi sia aperiodici che periodici. Il nostro algoritmo \u00e8 stato valutato su un flusso di notizie reale per dimostrarne l'efficacia. 2. LAVORO CORRELATO Inoltre, la maggior parte della ricerca TDT finora si \u00e8 occupata di raggruppare/classificare documenti in tipi di argomenti, identificare nuove frasi -LSB- 6 -RSB- per nuovi eventi, ecc., senza molta attenzione all'analisi della traiettoria delle parole rispetto al tempo. Swan e Allan -LSB- 18 -RSB- hanno tentato per la prima volta di utilizzare termini co-ricorrenti per costruire un evento. Tuttavia, hanno considerato solo entit\u00e0 nominate e coppie di frasi nominali, senza considerare la loro periodicit\u00e0. Al contrario, il nostro articolo considera tutto quanto sopra. Recentemente, c'\u00e8 stato un notevole interesse nel modellare un evento nei flussi di text come una \"esplosione di attivit\u00e0\" incorporando informazioni temporali. Tuttavia, nessuno dei lavori esistenti identificava specificamente le caratteristiche degli eventi, ad eccezione di Fung et al. -LSB-9 -RSB-,che ha raggruppato le caratteristiche del seno per identificare vari eventi esplosivi. Il nostro lavoro differisce da -LSB- 9 -RSB- in diversi modi: 1 -RRB- analizziamo ogni singola caratteristica, non solo le caratteristiche bursty; 2 -RRB- classifichiamo le caratteristiche lungo due dimensioni categoriche -LRB- periodicit\u00e0 e potenza -RRB-, ottenendo complessivamente cinque tipi di caratteristiche primarie; 3 -RRB- non limitiamo ogni caratteristica ad appartenere esclusivamente ad un solo evento. Le tecniche di analisi spettrale sono state precedentemente utilizzate da Vlachos et al. -LSB- 19 -RSB- per identificare periodicit\u00e0 e burst dai log delle query. Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Il loro obiettivo era rilevare periodicit\u00e0 multiple dal grafico dello spettro di potenza, che sono state poi utilizzate per indicizzare le parole per la ricerca \"query-by-burst\". In questo articolo, utilizziamo l'analisi spettrale per classificare le caratteristiche delle parole lungo due dimensioni, vale a dire la periodicit\u00e0 e lo spettro di potenza, con l'obiettivo finale di identificare eventi bursty sia periodici che aperiodici. 8. CONCLUSIONI Questo articolo ha adottato una prospettiva completamente nuova nell'analisi delle traiettorie delle caratteristiche come segnali nel dominio del tempo. Considerando le frequenze dei documenti verbali sia nel dominio del tempo che in quello della frequenza, siamo stati in grado di ricavare molte nuove caratteristiche sui flussi di notizie che erano precedentemente sconosciute, ad esempio, la diversa distribuzione delle stopword durante i giorni feriali e nei fine settimana. Per la prima volta nell'area del TDT, abbiamo applicato un approccio sistematico per rilevare automaticamente eventi importanti e meno segnalati, periodici e aperiodici. L'idea chiave del nostro lavoro risiede nell'osservazione che gli eventi periodici -LRB- a -RRB- hanno caratteristiche rappresentative periodiche -LRB- a -RRB- e gli eventi importanti -LRB- e -RRB- hanno -LRB- in -RRB- attivi caratteristiche rappresentative, differenziate per i loro spettri di potenza e periodi di tempo. Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.Per affrontare il problema del rilevamento degli eventi reali, \u00e8 stato utilizzato un approccio semplice ed efficace basato sulla densit\u00e0 della miscela per identificare i burst di caratteristiche e i periodi di burst associati. Abbiamo anche progettato un algoritmo greedy non supervisionato per rilevare eventi sia aperiodici che periodici, che \u00e8 riuscito a rilevare eventi reali come mostrato nella valutazione su un flusso di notizie reale. Sebbene non abbiamo effettuato alcun confronto di riferimento con un altro approccio, semplicemente perch\u00e9 non esiste lavoro precedente sul problema affrontato. Tuttavia, riteniamo che il nostro metodo semplice ed efficace sar\u00e0 utile per tutti i professionisti del TDT e sar\u00e0 particolarmente utile per l'analisi esplorativa iniziale dei flussi di notizie.", "keyphrases": ["rilevamento eventi", "traiettorie delle parole", "evento di un periodo", "evento del periodo", "segnale verbale", "analisi spettrale", "rilevamento dell'argomento", "traccia dell'argomento", "flusso di text", "nuovo flusso", "serie temporale"]}
{"file_name": "H-3", "text": "Utilizzo dei contesti di query nel recupero delle informazioni ABSTRACT La query dell'utente \u00e8 un elemento che specifica un bisogno di informazione, ma non \u00e8 l'unico. Gli studi in letteratura hanno rilevato molti fattori contestuali che influenzano fortemente l'interpretazione di una query. Studi recenti hanno cercato di considerare gli interessi dell'utente creando un profilo utente. Tuttavia, un singolo profilo per un utente potrebbe non essere sufficiente per una variet\u00e0 di query dell'utente. In questo studio, proponiamo di utilizzare contesti specifici della query anzich\u00e9 quelli incentrati sull'utente, incluso il context attorno alla query e il context all'interno della query. Il primo specifica l'ambiente di una query come il dominio di interesse, mentre il secondo si riferisce alle parole di context all'interno della query, il che \u00e8 particolarmente utile per la selezione delle relazioni tra termini rilevanti. In questo articolo, entrambi i tipi di context sono integrati in un modello IR basato sulla modellazione del linguaggio. I nostri esperimenti su diverse raccolte TREC mostrano che ciascuno dei fattori di context apporta miglioramenti significativi nell'efficacia del recupero. 1. INTRODUZIONE Le interrogazioni, soprattutto quelle brevi, non forniscono una specificazione completa del bisogno informativo. Molti termini rilevanti possono essere assenti dalle query e i termini inclusi potrebbero essere ambigui. Questi problemi sono stati affrontati in un gran numero di studi precedenti. In questi studi, tuttavia, si \u00e8 generalmente assunto che la query sia l'unico elemento disponibile riguardo al bisogno informativo dell'utente. In realt\u00e0, la query \u00e8 sempre formulata in un context di ricerca. Questi fattori includono, tra molti altri, il dominio di interesse dell'utente, la conoscenza, le preferenze, ecc.. Tutti questi elementi specificano le 8. CONCLUSIONI Gli approcci IR tradizionali solitamente considerano la query come l'unico elemento disponibile per il bisogno di informazioni dell'utente. Molti studi precedenti hanno studiato l\u2019integrazione di alcuni fattori contestuali nei modelli IR, tipicamente incorporando un profilo utente. Analogamente ad alcuni studi precedenti, proponiamo di modellare i domini degli argomenti anzich\u00e9 l'utente. Le precedenti indagini sul context si sono concentrate sui fattori relativi alla query. In questo articolo abbiamo dimostrato che anche i fattori all'interno della query sono importanti: aiutano a selezionare le relazioni tra termini appropriati da applicare nell'espansione della query. Abbiamo integrato i fattori contestuali di cui sopra, insieme al modello di feedback, in un unico modello linguistico. I nostri risultati sperimentali confermano fortemente il vantaggio dell\u2019utilizzo di contesti in IR. Questo lavoro mostra anche che il quadro di modellazione del linguaggio \u00e8 appropriato per integrare molti fattori contestuali. Questo lavoro pu\u00f2 essere ulteriormente migliorato su diversi aspetti, inclusi altri metodi per estrarre relazioni tra termini, per integrare pi\u00f9 parole di context nelle condizioni e per identificare domini di query. Sarebbe interessante anche testare il metodo sulla ricerca Web utilizzando la cronologia delle ricerche degli utenti.", "keyphrases": ["profilo utente", "context specifico della query", "incentrato sull'utente", "dominio di interesse", "fattore di context", "parola sens disambigua", "informare il bisogno", "context di ricerca", "conoscenza del dominio", "util della conoscenza generale", "problema dell'ambiguit\u00e0 della conoscenza", "indipendente dal context", "il context informa", "modello di dominio", "soluzione radic", "ricerca persona su google"]}
{"file_name": "C-28", "text": "PackageBLAST: un servizio di griglia multi-policy adattivo per il confronto di sequenze biologiche * ABSTRACT In questo articolo, proponiamo un quadro di allocazione delle attivit\u00e0 adattivo per eseguire ricerche BLAST in un ambiente di griglia rispetto a segmenti di database di sequenze. Il framework, chiamato PackageBLAST, fornisce un'infrastruttura per scegliere o incorporare strategie di allocazione delle attivit\u00e0. Inoltre, proponiamo un meccanismo per calcolare il peso di esecuzione dei nodi della griglia, adattando la politica di allocazione scelta all'attuale potenza computazionale dei nodi. I nostri risultati presentano accelerazioni molto buone e mostrano anche che nessuna singola strategia di allocazione \u00e8 in grado di raggiungere i tempi di esecuzione pi\u00f9 bassi per tutti gli scenari. 1. INTRODUZIONE SW -LSB- 14 -RSB- \u00e8 un algoritmo esatto che trova il miglior allineamento locale tra due sequenze di dimensione n nel tempo e nello spazio quadratici. Per questo motivo sono state proposte euristiche come BLAST -LSB- 3 -RSB- per ridurre i tempi di esecuzione. CLS. Supportato dall'ACM. La pianificazione delle risorse \u00e8 uno dei componenti pi\u00f9 importanti di un sistema a griglia. La scelta delle migliori risorse per una particolare applicazione \u00e8 chiamata task allocation, che \u00e8 un problema NP-Completo. Le applicazioni di rete di solito non hanno velocit\u00e0 di comunicazione elevate e molte di esse seguono il modello master/slave -LSB- 13 -RSB-. Per programmare le applicazioni master/slave sono state proposte molte politiche di allocazione delle attivit\u00e0 come Self Scheduling -LSB- 15 -RSB- e FAC2 -LSB- 8 -RSB-. La scelta della migliore politica di allocazione dipende dal modello di accesso dell'applicazione e dall'ambiente in cui viene eseguita -LSB- 13 -RSB-. In questo articolo proponiamo PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST in griglie composte da database genetici segmentati. PackageBLAST viene eseguito su Globus 3 -LSB- 4 -RSB- e, per ora, fornisce cinque politiche di allocazione. Inoltre, proponiamo un meccanismo adattivo per assegnare pesi ai nodi della griglia, tenendo conto del loro attuale carico di lavoro. Per quanto ne sappiamo, questo \u00e8 il primo servizio grid che esegue BLAST con policy di attivit\u00e0 multiple con un database segmentato in una piattaforma eterogenea non dedicata. Questo documento \u00e8 organizzato come segue. La Sezione 2 presenta il problema del confronto di sequenze e l'algoritmo BLAST. La sezione 3 descrive le politiche di assegnazione per le reti. La sezione 4 discute il lavoro correlato. La sezione 5 presenta la progettazione di PackageBLAST. I risultati sperimentali sono discussi nella sezione 6. La sezione 7 conclude il documento. 4. LAVORI CORRELATI Innanzitutto, il database genetico viene segmentato. Quindi, le query vengono distribuite uniformemente tra i nodi. Se il nodo non dispone di un frammento di database, viene creata una copia locale. Viene proposto un metodo che associa frammenti di dati ai nodi, cercando di minimizzare il numero di copie. BLAST + + -LSB- 10 -RSB- raggruppa pi\u00f9 sequenze per ridurre il numero di accessi al database. Viene utilizzato un approccio master/slave che assegna le query agli slave secondo la politica fissa -LRB- sezione 3.3 -RRB-. Ogni lavoratore esegue BLAST++ in modo indipendente e, infine,i risultati vengono raccolti e combinati dal master. GridBlast -LSB- 9 -RSB- \u00e8 un'applicazione di rete master/slave che utilizza Globus 2. Distribuisce sequenze tra i nodi della rete utilizzando due politiche di allocazione: FCFS e minmax. Tuttavia, per utilizzare minmax, \u00e8 necessario conoscere il tempo di esecuzione totale di ciascun task BLAST. Dopo aver deciso quali sequenze verranno confrontate da ciascun nodo, GridBlast invia le sequenze, i file eseguibili e l'intero database al nodo prescelto. Al termine della ricerca, i risultati vengono compattati e inviati al master. Grid Blast Toolkit -LRB- GBTK -RRB- -LSB- 12 -RSB- \u00e8 un portale web per eseguire ricerche BLAST in Globus 3. Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Quindi verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Quindi verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.Tutti i database genetici sono posizionati staticamente sui nodi della griglia -LRB- senza replica -RRB-. GBTK \u00e8 un'applicazione master/slave che riceve le sequenze e il nome del database genetico. Successivamente verifica se il nodo che contiene il database \u00e8 disponibile. Se il nodo non \u00e8 disponibile, viene scelto il nodo meno caricato e il database vi viene copiato. Il database viene replicato nei nodi, ma solo una parte viene elaborata in ciascun nodo Figura 2: meccanismo di segmentazione e distribuzione di PackageBLAST. 7. CONCLUSIONE In questo articolo, abbiamo proposto e valutato PackageBLAST, un servizio di griglia multi-policy adattivo per eseguire ricerche BLAST master/slave. PackageBLAST contiene un framework in cui l'utente pu\u00f2 scegliere o incorporare politiche di allocazione. Abbiamo anche definito una strategia, PSS, che adatta la politica scelta a un ambiente di rete eterogeneo e non dedicato. I risultati raccolti eseguendo PackageBLAST con 5 politiche di allocazione in un banco di prova a griglia sono stati molto buoni. Per confrontare una sequenza di DNA reale da 10KBP con il database genetico nr, siamo stati in grado di ridurre il tempo di esecuzione da 30,88 minuti a 2,11 minuti. Inoltre, abbiamo dimostrato che, nel nostro banco di prova, non esiste una politica di allocazione che raggiunga sempre la migliore performance e che renda evidente l\u2019importanza di fornire politiche multiple. Inoltre, abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.abbiamo dimostrato che l\u2019introduzione del PSS ha portato a ottimi miglioramenti in termini di performance per alcune politiche. Come lavoro futuro, intendiamo eseguire PackageBLAST in una rete geograficamente dispersa, per valutare l'impatto delle elevate latenze di rete nelle politiche di allocazione e nel PSS. Inoltre, intendiamo fornire supporto per la sincronizzazione del database genomico e le operazioni dinamiche di join/leave per gli schiavi.", "keyphrases": ["confronto di sequenze biologiche", "adeguare il servizio di rete multipolitica", "allocazione dei compiti", "ricerca esplosiva", "packageblast", "bioinformatica", "calcolo della griglia", "biologia informatica", "progetto genoma", "segmentare i database genet", "piattaforma eterogenea non dedicata", "ambiente della griglia", "p.s", "il peso del pacco si adatta all'autoprogrammazione"]}
{"file_name": "C-14", "text": "Strategia di implementazione dei sensori per il rilevamento dei target ABSTRACT Per monitorare l'attraversamento del traffico di una regione, i sensori possono essere implementati per eseguire il rilevamento collaborativo dei target. Una rete di sensori di questo tipo raggiunge un certo livello di prestazioni di rilevamento con un costo di implementazione associato. Questo documento affronta questo problema proponendo l'esposizione del percorso come misura della bont\u00e0 di una distribuzione e presenta un approccio per la distribuzione sequenziale in fasi. Dimostra che il costo di implementazione pu\u00f2 essere ridotto al minimo per ottenere le prestazioni di rilevamento desiderate scegliendo opportunamente il numero di sensori distribuiti in ogni fase. 1. INTRODUZIONE Tale rete pu\u00f2 essere utilizzata per monitorare l'ambiente, rilevare, classificare e localizzare eventi specifici e tracciare obiettivi su una regione specifica. L'implementazione delle reti di sensori varia a seconda dell'applicazione considerata. Pu\u00f2 essere predeterminato quando l'ambiente \u00e8 sufficientemente conosciuto e sotto controllo, nel qual caso i sensori possono essere posizionati strategicamente a mano. Questo articolo analizza le strategie di implementazione per le reti di sensori che eseguono il rilevamento dei bersagli in una regione di interesse. Poich\u00e9 le osservazioni locali effettuate dai sensori dipendono dalla loro posizione, le prestazioni dell'algoritmo di rilevamento dipendono dall'implementazione. Una possibile misura della bont\u00e0 dell'implementazione per il rilevamento del bersaglio \u00e8 chiamata esposizione del percorso. \u00c8 una misura della probabilit\u00e0 di rilevare un bersaglio che attraversa la regione utilizzando un determinato percorso. Maggiore \u00e8 l'esposizione del percorso, migliore \u00e8 la distribuzione. L'insieme dei percorsi da considerare pu\u00f2 essere vincolato dall'ambiente. Ad esempio, se si prevede che l'obiettivo segua una strada, \u00e8 necessario considerare solo i percorsi costituiti dalle strade. In questo studio si presuppone che l'implementazione sia casuale, il che corrisponde a molte applicazioni pratiche in cui la regione da monitorare non \u00e8 accessibile per il posizionamento preciso dei sensori. L'obiettivo di questo documento \u00e8 determinare il numero di sensori da implementare per effettuare il rilevamento del bersaglio in una regione di interesse. I compromessi risiedono tra le prestazioni della rete, il costo dei sensori distribuiti e il costo di distribuzione dei sensori. Questo documento \u00e8 organizzato come segue. Nella sezione 2 viene proposta una definizione di esposizione del percorso e viene sviluppato un metodo per valutare l'esposizione di un determinato percorso. Nella sezione 3 viene formulato il problema della distribuzione casuale e vengono presentate diverse soluzioni. Il documento si conclude con la sezione 7. 7. CONCLUSIONE Questo documento affronta il problema dell'implementazione del sensore in una regione da monitorare per l'intrusione del bersaglio. Viene proposto e analizzato un meccanismo per la collaborazione dei sensori per eseguire il rilevamento del bersaglio per valutare l'esposizione dei percorsi attraverso la regione. L'esposizione minima viene utilizzata come misura della bont\u00e0 dello spiegamento, con l'obiettivo di massimizzare l'esposizione del percorso meno esposto nella regione. Nel caso in cui i sensori siano posizionati casualmente in una regione da monitorare,viene sviluppato un meccanismo per l'implementazione sequenziale in fasi. La strategia consiste nell'utilizzare un numero limitato di sensori alla volta fino al raggiungimento dell'esposizione minima desiderata. La funzione di costo utilizzata in questo studio dipende dal numero di sensori distribuiti in ogni fase e dal costo di ciascuna distribuzione. Attraverso la simulazione, la distribuzione dell'esposizione minima ottenuta mediante l'implementazione casuale \u00e8 stata valutata per un numero variabile di sensori distribuiti. Questi risultati sono stati utilizzati per valutare il costo di implementazione per un numero variabile di sensori distribuiti in ciascuna fase. Abbiamo scoperto che il numero ottimale di sensori distribuiti in ogni fase varia in base al costo relativo assegnato alla distribuzione e ai sensori. I risultati di questo studio possono essere estesi a regioni pi\u00f9 grandi con parametri target diversi. La soluzione proposta in questo documento pu\u00f2 anche essere migliorata considerando l'implementazione di un numero variabile di sensori in ogni fase e questo problema a variabili multiple richiede ulteriori indagini.", "keyphrases": ["rilevamento del bersaglio", "rete di sensori", "esposizione del percorso", "numero di sensore", "distribuzione sequenziale", "esposizione minima", "posizionamento casuale del sensore", "campo del sensore", "bersaglio decai"]}
{"file_name": "C-6", "text": "Progettazione e implementazione di un sistema di gestione dei contenuti distribuiti ABSTRACT La convergenza dei progressi nelle tecnologie di archiviazione, codifica e rete ci ha portato in un ambiente in cui enormi quantit\u00e0 di contenuti multimediali continui vengono regolarmente archiviati e scambiati tra dispositivi abilitati alla rete. Tenere traccia di -LRB- o gestire -RRB- tali contenuti rimane impegnativo a causa dell'enorme volume di dati. La memorizzazione di media continui \"dal vivo\" -LRB- come contenuti TV o radio -RRB- aumenta la complessit\u00e0 in quanto questo contenuto non ha un inizio o una fine ben definiti ed \u00e8 quindi complicato da gestire. L'archiviazione in rete consente al contenuto che viene logicamente visualizzato come parte della stessa raccolta di essere effettivamente distribuito su una rete, rendendo il compito di gestione dei contenuti quasi impossibile da gestire senza un sistema di gestione dei contenuti. In questo articolo presentiamo la progettazione e l'implementazione del sistema di gestione dei contenuti Spectrum, che gestisce efficacemente i contenuti multimediali in questo ambiente. Spectrum ha un'architettura modulare che ne consente l'applicazione sia a scenari standalone che a vari scenari di rete. Un aspetto unico di Spectrum \u00e8 che richiede uno o pi\u00f9 criteri di conservazione -LRB- da applicare a ogni contenuto archiviato nel sistema. Ci\u00f2 significa che non esistono politiche di sfratto. Il contenuto a cui non \u00e8 pi\u00f9 applicato un criterio di conservazione viene semplicemente rimosso dal sistema. \u00c8 possibile applicare facilmente diverse politiche di conservazione allo stesso contenuto facilitando cos\u00ec naturalmente la condivisione senza duplicazioni. Questo approccio consente inoltre a Spectrum di applicare facilmente ai contenuti policy basate sul tempo, che rappresentano gli elementi fondamentali necessari per gestire l'archiviazione di contenuti multimediali live continui. Non solo descriviamo i dettagli dell'architettura Spectrum ma forniamo anche casi d'uso tipici. 1. INTRODUZIONE La manipolazione e la gestione dei contenuti \u00e8 ed \u00e8 sempre stata una delle funzioni primarie di un computer. Le applicazioni informatiche iniziali includono formattatori di text e compilatori di programmi. Il contenuto era inizialmente gestito dall'interazione esplicita dell'utente attraverso l'uso di file e filesystem. Con l\u2019avanzare della tecnologia, sia i tipi di contenuti che il modo in cui le persone desiderano utilizzarli sono notevolmente cambiati. Nuovi tipi di contenuto, come i flussi multimediali continui, sono diventati comuni grazie alla convergenza dei progressi nelle tecnologie di archiviazione, codifica e rete. Un altro esempio \u00e8 la combinazione della codifica e della tecnologia di rete a banda larga. Questa combinazione ha consentito agli utenti di accedere e condividere contenuti multimediali sia su reti locali che remote, con la rete stessa che funge da enorme archivio di dati. La proliferazione di contenuti di alta qualit\u00e0 resa possibile da questi progressi nella tecnologia di archiviazione, codifica e rete crea la necessit\u00e0 di nuovi modi per manipolare e gestire i dati.Il focus del nostro lavoro \u00e8 sull'archiviazione di contenuti multimediali ricchi e in particolare sull'archiviazione di contenuti multimediali continui in forme preconfezionate o \"live\". \u2022 Sebbene sia vero per tutti i tipi di contenuto, l'archiviazione di contenuti multimediali continui \u00e8 particolarmente problematica. Innanzitutto, i contenuti multimediali continui sono ancora molto impegnativi in \u200b\u200btermini di risorse di archiviazione, il che significa che un approccio senza policy all'archiviazione non funzioner\u00e0 per tutti tranne che per i sistemi pi\u00f9 piccoli. In secondo luogo, la memorizzazione di contenuti \"dal vivo\" come TV o radio \u00e8 intrinsecamente problematica poich\u00e9 questi segnali sono flussi continui senza punti finali. Ci\u00f2 significa che prima ancora che si possa pensare a gestire tali contenuti \u00e8 necessario astrarli in qualcosa che possa essere manipolato e gestito. . Quando si ha a che fare con supporti continui archiviati, \u00e8 necessario gestire tali contenuti sia a livello granulare che aggregato. Ad esempio, un singolo utente PVR che desidera conservare solo i momenti salienti di un particolare evento sportivo non dovrebbe essere tenuto a dover memorizzare il contenuto relativo all'evento completo. . Come indicato sopra, provare a tenere traccia dei contenuti su un sistema autonomo senza un sistema di gestione dei contenuti \u00e8 molto difficile. Tuttavia, quando i dispositivi di archiviazione effettivi sono distribuiti su una rete, il compito di tenere traccia dei contenuti \u00e8 quasi impossibile. Questo scenario \u00e8 sempre pi\u00f9 comune nei sistemi di distribuzione di contenuti basati sulla rete ed \u00e8 probabile che diventi importante anche negli scenari di reti domestiche. Sembrerebbe chiaro quindi che sia necessario un sistema di gestione dei contenuti in grado di gestire in modo efficiente contenuti multimediali, sfruttando allo stesso tempo la capacit\u00e0 di rete dei dispositivi di archiviazione. Questo sistema dovrebbe consentire un'archiviazione efficiente e l'accesso ai contenuti su dispositivi di archiviazione di rete eterogenei in base alle preferenze dell'utente. Il sistema di gestione dei contenuti dovrebbe tradurre le preferenze dell'utente in appropriate politiche di archiviazione di basso livello e dovrebbe consentire che tali preferenze siano espresse ad un livello fine di granularit\u00e0 -LRB- senza richiederlo in generale -RRB-. Il sistema di gestione dei contenuti dovrebbe consentire all'utente di manipolare e ragionare su -LRB- ovvero modificare la politica di archiviazione associata a -RRB- l'archiviazione di parti -LRB- di contenuti multimediali continui -RRB-. Affrontare questo problema di gestione dei contenuti distribuiti \u00e8 difficile a causa del numero di requisiti imposti al sistema. Per esempio :. Il sistema di gestione dei contenuti deve operare su un gran numero di sistemi eterogenei. In alcuni casi il sistema potrebbe gestire il contenuto archiviato su un file system locale, mentre in altri il contenuto potrebbe essere archiviato su un dispositivo di archiviazione di rete separato. Il gestore dei contenuti potrebbe essere responsabile dell'implementazione delle policy utilizzate per fare riferimento al contenuto oppure tale ruolo potrebbe essere delegato a un computer separato. Affinch\u00e9 il sistema di gestione dei contenuti possa fornire un'interfaccia uniforme sono necessari un'interfaccia del programma applicativo -LRB- API -RRB- e i protocolli di rete associati. .Il sistema di gestione dei contenuti dovrebbe essere flessibile ed essere in grado di gestire requisiti diversi per le politiche di gestione dei contenuti. Queste politiche riflettono quali contenuti dovrebbero essere ottenuti, quando dovrebbero essere recuperati, per quanto tempo dovrebbero essere conservati e in quali circostanze dovrebbero essere scartati. Ci\u00f2 significa che il sistema di gestione dei contenuti dovrebbe consentire a pi\u00f9 applicazioni di fare riferimento ai contenuti con un ricco insieme di policy e che tutti dovrebbero funzionare insieme senza problemi. . Il sistema di gestione dei contenuti deve essere in grado di monitorare i riferimenti ai contenuti e utilizzare tali informazioni per collocare i contenuti nella giusta posizione nella rete per un accesso efficiente alle applicazioni. . Il sistema di gestione dei contenuti deve gestire l'interazione tra la popolazione implicita ed esplicita di contenuti ai margini della rete. . Il sistema di contenuti deve essere in grado di gestire in modo efficiente grandi insiemi di contenuti, inclusi flussi continui. Deve essere in grado di impacchettare questo contenuto in modo tale che sia conveniente per gli utenti accedervi. Per affrontare questi problemi abbiamo progettato e implementato l'architettura del sistema di gestione dei contenuti Spectrum. Consente a pi\u00f9 applicazioni di fare riferimento al contenuto utilizzando policy diverse. Si noti che l'architettura Spectrum presuppone l'esistenza di una rete di distribuzione dei contenuti -LRB- CDN -RRB- che pu\u00f2 facilitare la distribuzione efficiente dei contenuti -LRB-, ad esempio l'architettura PRISM CDN -LSB- 2 -RSB- -RRB-. La sezione 2 descrive l'architettura del nostro sistema di gestione dei contenuti. Nella Sezione 3 descriviamo sia la nostra implementazione dell'architettura Spectrum sia esempi del suo utilizzo. 4. LAVORI CORRELATI Diversi autori hanno affrontato il problema della gestione dei contenuti nelle reti distribuite. Gran parte del lavoro si concentra sull\u2019aspetto della gestione delle politiche. Ad esempio in -LSB- 5 -RSB- viene considerato il problema di servire contenuti multimediali tramite server distribuiti. Il contenuto viene distribuito tra le risorse del server in proporzione alla domanda dell'utente utilizzando un protocollo di diffusione della domanda. Le prestazioni del sistema vengono valutate tramite simulazione. In -LSB- 1 -RSB- il contenuto \u00e8 distribuito tra le sottocache. La base di conoscenza di Cache consente l'impiego di policy sofisticate. La simulazione viene utilizzata per confrontare lo schema proposto con algoritmi di sostituzione noti. Il nostro lavoro differisce in quanto prendiamo in considerazione qualcosa di pi\u00f9 degli aspetti di gestione politica del problema. Dopo aver considerato attentamente le funzionalit\u00e0 richieste per implementare la gestione dei contenuti nell'ambiente di rete, abbiamo suddiviso il sistema in tre semplici funzioni, vale a dire Content manager, Policy manager e Storage manager. Ci\u00f2 ci ha permesso di implementare e sperimentare facilmente un sistema prototipo. Altro lavoro correlato riguarda i cosiddetti sistemi di raccomandazione TV che vengono utilizzati nei PVR per selezionare automaticamente i contenuti per gli utenti, ad esempio -LSB- 6 -RSB-. Infine, nei fornitori di ambienti CDN commerciali -LRB- ad esCisco e Netapp -RRB- hanno sviluppato e implementato prodotti e strumenti per la gestione dei contenuti. 5. CONCLUSIONE E LAVORO FUTURO In questo articolo abbiamo presentato la progettazione e l'implementazione dell'architettura di gestione dei contenuti Spectrum. Spectrum consente di applicare policy di archiviazione a grandi volumi di contenuti per facilitare un'archiviazione efficiente. Nello specifico, il sistema consente di applicare policy diverse allo stesso contenuto senza replica. Spectrum pu\u00f2 anche applicare politiche \"time-aware\" che gestiscono efficacemente l'archiviazione di contenuti multimediali continui. Infine, il design modulare dell'architettura Spectrum consente realizzazioni sia stand-alone che distribuite in modo che il sistema possa essere implementato in una variet\u00e0 di applicazioni. Ci sono una serie di questioni aperte che richiederanno lavoro futuro. Alcuni di questi problemi includono: \u2022 Prevediamo che Spectrum sia in grado di gestire i contenuti su sistemi che vanno dai grandi CDN fino agli apparecchi pi\u00f9 piccoli come TiVO -LSB-8 -RSB-. Affinch\u00e9 questi sistemi pi\u00f9 piccoli possano supportare Spectrum, richiederanno una rete e un'API esterna. Quando l'API sar\u00e0 disponibile, dovremo capire come inserirla nell'architettura Spectrum. \u2022 Spectrum nomina i contenuti in base all'URL, ma non abbiamo intenzionalmente definito il formato degli URL Spectrum, il modo in cui vengono ricondotti al nome effettivo del contenuto o il modo in cui i nomi e gli URL devono essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.come si associano al nome effettivo del contenuto o come i nomi e gli URL dovrebbero essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.come si associano al nome effettivo del contenuto o come i nomi e gli URL dovrebbero essere presentati all'utente. \u2022 In questo articolo ci siamo concentrati sulla gestione dei contenuti per oggetti multimediali continui. \u2022 Qualsiasi progetto che consenta di condividere facilmente i contenuti multimediali su Internet dovr\u00e0 superare ostacoli legali prima di poter ottenere un'accettazione diffusa. Adattare Spectrum per soddisfare i requisiti legali richieder\u00e0 probabilmente pi\u00f9 lavoro tecnico.", "keyphrases": ["sistema di gestione dei contenuti dello spettro", "archiviazione multimediale continua", "scenario della rete domestica", "interfaccia del programma applicativo", "rete di distribuzione dei contenuti", "localizzazione uniforme delle risorse", "gestione politica", "abilitazione rete DVR", "sistema di database ad alte prestazioni", "gestione dello spettro di livello carrier"]}
{"file_name": "H-11", "text": "Progettazione ottimale laplaciana per il recupero di immagini ABSTRACT Il feedback sulla pertinenza \u00e8 una tecnica potente per migliorare le prestazioni del recupero di immagini basato sui contenuti -LRB- CBIR -RRB-. Sollecita i giudizi di pertinenza dell'utente sulle immagini recuperate restituite dai sistemi CBIR. L'etichettatura dell'utente viene quindi utilizzata per apprendere un classificatore per distinguere tra immagini rilevanti e irrilevanti. Tuttavia, le immagini restituite pi\u00f9 frequentemente potrebbero non essere quelle pi\u00f9 informative. La sfida \u00e8 quindi determinare quali immagini senza etichetta sarebbero le pi\u00f9 informative -LRB-, ovvero migliorerebbero maggiormente il classificatore -RRB- se fossero etichettate e utilizzate come campioni di addestramento. In questo articolo, proponiamo un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design -LRB- LOD -RRB-, per il recupero di immagini con feedback di pertinenza. Il nostro algoritmo si basa su un modello di regressione che minimizza l'errore minimo quadrato sulle immagini misurate -LRB- o etichettate -RRB- e contemporaneamente preserva la struttura geometrica locale dello spazio dell'immagine. Nello specifico, assumiamo che se due immagini sono sufficientemente vicine l'una all'altra, anche le loro misurazioni -LRB- o le etichette -RRB- sono vicine. Costruendo un grafo del vicino pi\u00f9 vicino, la struttura geometrica dello spazio dell'immagine pu\u00f2 essere descritta dal grafo laplaciano. Discuteremo di come i risultati del campo del disegno sperimentale ottimale possano essere utilizzati per guidare la nostra selezione di un sottoinsieme di immagini, che ci fornisce la maggior quantit\u00e0 di informazioni. I risultati sperimentali sul database Corel suggeriscono che l'approccio proposto raggiunge una maggiore precisione nel recupero delle immagini con feedback di pertinenza. 1. INTRODUZIONE In molte attivit\u00e0 di machine learning e di recupero delle informazioni, non mancano i dati senza etichetta, ma le etichette sono costose. La sfida \u00e8 quindi quella di determinare quali campioni senza etichetta sarebbero i pi\u00f9 informativi -LRB-, vale a dire, migliorerebbero maggiormente il classificatore -RRB- se fossero etichettati e utilizzati come campioni di addestramento. Questo problema \u00e8 tipicamente chiamato apprendimento attivo -LSB- 4 -RSB-. Molte applicazioni del mondo reale possono essere inserite in un framework di apprendimento attivo. In particolare, consideriamo il problema del feedback di pertinenza guidato dal Content-Based Image Retrieval -LRB- CBIR -RRB- -LSB- 13 -RSB-. Il recupero di immagini basato sui contenuti ha attirato notevoli interessi nell'ultimo decennio -LSB- 13 -RSB-. Ci\u00f2 \u00e8 motivato dalla rapida crescita dei database di immagini digitali che, a loro volta, richiedono schemi di ricerca efficienti. Invece di descrivere un'immagine utilizzando il text, in questi sistemi una query di immagine viene descritta utilizzando una o pi\u00f9 immagini di esempio. Le caratteristiche visive di basso livello -LRB- colore, trama, forma, ecc. -RRB- vengono estratte automaticamente per rappresentare le immagini. Per ridurre il divario semantico, nel CBIR -LSB- 12 -RSB- viene introdotto il feedback sulla pertinenza. In molti degli attuali sistemi CBIR basati sul feedback di pertinenza, all'utente \u00e8 richiesto di fornire i propri giudizi di pertinenza sulle immagini principali restituite dal sistema.Le immagini etichettate vengono quindi utilizzate per addestrare un classificatore a separare le immagini che corrispondono al concetto di query da quelle che non lo fanno. Tuttavia, in generale le immagini restituite pi\u00f9 frequentemente potrebbero non essere quelle pi\u00f9 informative. Nel peggiore dei casi, tutte le immagini migliori etichettate dall'utente potrebbero essere positive e quindi le tecniche di classificazione standard non possono essere applicate a causa della mancanza di esempi negativi. A differenza dei problemi di classificazione standard in cui i campioni etichettati vengono preassegnati, nel recupero delle immagini con feedback di pertinenza il sistema pu\u00f2 selezionare attivamente le immagini da etichettare. Pertanto l'apprendimento attivo pu\u00f2 essere introdotto naturalmente nel recupero delle immagini. Nonostante molte tecniche di apprendimento attivo esistenti, Support Vector Machine -LRB- SVM -RRB- apprendimento attivo -LSB- 14 -RSB- e apprendimento attivo basato sulla regressione -LSB- 1 -RSB- hanno ricevuto il maggior interesse. Lo svantaggio principale dell\u2019apprendimento attivo SVM \u00e8 che il limite stimato potrebbe non essere sufficientemente accurato. Inoltre, potrebbe non essere applicato all'inizio del recupero quando non sono presenti immagini etichettate. Alcuni altri algoritmi di apprendimento attivo basati su SVM possono essere trovati in -LSB- 7 -RSB-, -LSB- 9 -RSB-. In statistica, il problema della selezione dei campioni da etichettare viene generalmente definito disegno sperimentale. Il campione x viene definito esperimento e la sua etichetta y viene definita misurazione. Lo studio del disegno sperimentale ottimale -LRB- OED -RRB- -LSB- 1 -RSB- riguarda la progettazione di esperimenti che dovrebbero minimizzare le varianze di un modello parametrizzato. L'intento di un disegno sperimentale ottimale \u00e8 solitamente quello di massimizzare la fiducia in un dato modello, minimizzare le varianze dei parametri per l'identificazione del sistema o minimizzare la varianza dell'output del modello. Gli approcci classici alla progettazione sperimentale includono A-Optimal Design, D-Optimal Design ed E-Optimal Design. Tutti questi approcci si basano su un modello di regressione dei minimi quadrati. Rispetto agli algoritmi di apprendimento attivo basati su SVM, gli approcci di progettazione sperimentale sono molto pi\u00f9 efficienti nel calcolo. Tuttavia, questo tipo di approccio prende in considerazione solo i dati misurati -LRB- o etichettati -RRB- nella loro funzione obiettivo, mentre i dati non misurati -LRB- o non etichettati -RRB- vengono ignorati. Approfittando dei recenti progressi sulla progettazione sperimentale ottimale e sull'apprendimento semi-supervisionato, in questo articolo proponiamo un nuovo algoritmo di apprendimento attivo per il recupero di immagini, chiamato Laplacian Optimal Design -LRB- LOD -RRB-. A differenza dei tradizionali metodi di progettazione sperimentale le cui funzioni di perdita sono definite solo sui punti misurati, la funzione di perdita del nostro algoritmo LOD proposto \u00e8 definita sia sui punti misurati che su quelli non misurati. Nello specifico, introduciamo un regolarizzatore che preserva la localit\u00e0 nella funzione di perdita standard basata sull'errore quadrato. La nuova funzione di perdita mira a trovare un classificatore che sia localmente il pi\u00f9 agevole possibile. In altre parole, se due punti sono sufficientemente vicini tra loro nello spazio di input, ci si aspetta che condividano la stessa etichetta.Una volta definita la funzione di perdita, possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sul disegno sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.possiamo selezionare i punti dati pi\u00f9 informativi che vengono presentati all'utente per l'etichettatura. Sarebbe importante notare che le immagini pi\u00f9 informative potrebbero non essere le migliori immagini restituite. Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il resto del lavoro \u00e8 organizzato come segue. Nella Sezione 2 forniamo una breve descrizione del lavoro correlato. La nostra proposta di algoritmo di progettazione ottimale laplaciano \u00e8 introdotta nella sezione 3. Nella sezione 4, confrontiamo il nostro algoritmo con gli algoritmi all'avanguardia e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.confrontiamo il nostro algoritmo con gli algoritmi pi\u00f9 avanzati e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sul disegno sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.confrontiamo il nostro algoritmo con gli algoritmi pi\u00f9 avanzati e presentiamo i risultati sperimentali sul recupero delle immagini. Infine, forniamo alcune osservazioni conclusive e suggerimenti per il lavoro futuro nella Sezione 5. 2. LAVORO CORRELATO Poich\u00e9 l'algoritmo proposto si basa su un quadro di regressione. Il lavoro pi\u00f9 correlato \u00e8 il design sperimentale ottimale -LSB- 1 -RSB-, che comprende A-Optimal Design, D-Optimal Design e EOptimal Design. In questa sezione diamo una breve descrizione di questi approcci. 2.1 Il problema dell'apprendimento attivo Il problema generico dell'apprendimento attivo \u00e8 il seguente. In altre parole, i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.i punti zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- possono migliorare maggiormente il classificatore se vengono etichettati e utilizzati come punti di allenamento. 2.2 Disegno sperimentale ottimale Consideriamo un modello di regressione lineare Osservazioni diverse hanno errori indipendenti, ma con varianze uguali \u03c32. Pertanto, la stima di massima verosimiglianza per il vettore dei pesi, \u02c6w, \u00e8 quella che minimizza l'errore quadratico della somma. Le tre misure scalari pi\u00f9 comuni della dimensione della matrice di covarianza dei parametri nel disegno sperimentale ottimale sono: \u2022 Disegno D-ottimale: determinante di Hsse . \u2022 Design A-ottimale: traccia di Hsse. \u2022 Design E-ottimale: massimo autovalore di Hsse. Poich\u00e9 il calcolo del determinante e degli autovalori di una matrice \u00e8 molto pi\u00f9 costoso del calcolo della traccia della matrice, il progetto A-ottimale \u00e8 pi\u00f9 efficiente degli altri due. Alcuni lavori recenti sulla progettazione sperimentale possono essere trovati in -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONI E LAVORO FUTURO Questo articolo descrive un nuovo algoritmo di apprendimento attivo, chiamato Laplacian Optimal Design, per consentire un recupero di immagini con feedback di pertinenza pi\u00f9 efficace. Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.Il nostro algoritmo si basa su una funzione obiettivo che contemporaneamente minimizza l'errore empirico e preserva la struttura geometrica locale dello spazio dati. Utilizzando tecniche di progettazione sperimentale, il nostro algoritmo trova le immagini pi\u00f9 informative da etichettare. Queste immagini etichettate e le immagini senza etichetta nel database vengono utilizzate per apprendere un classificatore. I risultati sperimentali sul database Corel mostrano che sia l'apprendimento attivo che l'apprendimento semi-supervisionato possono migliorare significativamente le prestazioni di recupero. In questo articolo, consideriamo il problema del recupero dell'immagine su dati di immagine piccoli, statici e di dominio chiuso. Per la ricerca di immagini sul Web, \u00e8 possibile raccogliere una grande quantit\u00e0 di informazioni sui clic degli utenti. Queste informazioni possono essere naturalmente utilizzate per costruire il grafico di affinit\u00e0 nel nostro algoritmo.", "keyphrases": ["feedback rilevante", "l'immagine rappresenta", "recupero immagini contentbas", "apprendimento attivo", "modello di regresso dei minimi quadrati", "progettazione ottimale dell'esperimento", "immagine ritorno in alto", "tasso preciso", "struttura geometrica intrinseca", "riconoscimento del patten", "etichetta"]}
{"file_name": "J-27", "text": "Imparare dalle preferenze rivelate ABSTRACT Una sequenza di prezzi e domande \u00e8 razionalizzabile se esiste una funzione di utilit\u00e0 concava, continua e monotona tale che le domande massimizzano la funzione di utilit\u00e0 sull'insieme di bilancio corrispondente al prezzo. Afriat -LSB- 1 -RSB- presentava le condizioni necessarie e sufficienti affinch\u00e9 una sequenza finita fosse razionalizzabile. Varian -LSB- 20 -RSB- e successivamente Blundell et al. -LSB- 3, 4 -RSB- ha continuato questa linea di lavoro studiando metodi non parametrici per prevedere la domanda. I loro risultati caratterizzano essenzialmente l'apprendimento di classi degenerate di funzioni di domanda e quindi non riescono a fornire un grado generale di fiducia nelle previsioni. Il presente articolo integra questa linea di ricerca introducendo un modello statistico e una misura di complessit\u00e0 attraverso i quali siamo in grado di studiare l\u2019apprendibilit\u00e0 di classi di funzioni di domanda e ricavare un grado di fiducia nelle previsioni. I nostri risultati mostrano che la classe di tutte le funzioni di domanda ha una complessit\u00e0 illimitata e quindi non \u00e8 apprendibile, ma che esistono classi interessanti e potenzialmente utili che possono essere apprese da campioni finiti. Presentiamo anche un algoritmo di apprendimento che \u00e8 un adattamento di una nuova dimostrazione del teorema di Afriat dovuta a Teo e Vohra -LSB- 17 -RSB-. 1. INTRODUZIONE La relazione di preferenza \u00e8 quindi il fattore chiave per comprendere il comportamento dei consumatori. Uno dei presupposti comuni in questa teoria \u00e8 che la relazione di preferenza sia rappresentata da una funzione di utilit\u00e0 e che gli agenti si sforzino di massimizzare la propria utilit\u00e0 dato un vincolo di bilancio. Questo modello di comportamento \u00e8 l\u2019essenza della domanda e dell\u2019offerta, degli equilibri generali e di altri aspetti della teoria del consumatore. Inoltre, come spiegato nella sezione 2, le osservazioni di base sul comportamento della domanda di mercato suggeriscono che le funzioni di utilit\u00e0 sono monotone e concave. Questo ci porta alla domanda, sollevata per la prima volta da Samuelson -LSB- 18 -RSB-, fino a che punto questa teoria \u00e8 confutabile? Date le osservazioni del prezzo e della domanda, in quali circostanze possiamo concludere che i dati sono coerenti con il comportamento di un agente che massimizza l\u2019utilit\u00e0 dotato di una funzione di utilit\u00e0 concava monotona e soggetto a un vincolo di bilancio? Samuelson ha posto una condizione necessaria ma insufficiente sulla preferenza sottostante nota come assioma debole della preferenza rivelata. Uzawa -LSB- 16 -RSB- e Mas-Colell -LSB- 10, 11 -RSB- hanno introdotto la nozione di reddito-Lipschitz e hanno dimostrato che le funzioni di domanda con questa propriet\u00e0 sono razionalizzabili. Queste propriet\u00e0 non richiedono alcuna ipotesi parametrica e sono tecnicamente confutabili, ma presuppongono la conoscenza dell\u2019intera funzione di domanda e fanno molto affidamento sulle propriet\u00e0 differenziali delle funzioni di domanda. Pertanto, per confutare la teoria \u00e8 necessaria una quantit\u00e0 infinita di informazioni. Spesso accade che, oltre alle osservazioni sulla domanda, ci siano informazioni aggiuntive sul sistema ed \u00e8 sensato fare ipotesi parametriche, vale a dire:stipulare una forma funzionale di utilit\u00e0. La coerenza con la massimizzazione dell\u2019utilit\u00e0 dipenderebbe quindi dalla fissazione dei parametri della funzione di utilit\u00e0 in modo che siano coerenti con le osservazioni e con un insieme di equazioni chiamate equazioni di Slutski. Se tali parametri esistono, concludiamo che la forma di utilit\u00e0 stipulata \u00e8 coerente con le osservazioni. Questo approccio \u00e8 utile quando vi \u00e8 motivo di fare tali stipulazioni, poich\u00e9 fornisce una funzione di utilit\u00e0 esplicita che pu\u00f2 essere utilizzata per fare previsioni precise sulla domanda per prezzi non osservati. Lo svantaggio di questo approccio \u00e8 che i dati della vita reale spesso non sono coerenti con le forme funzionali convenienti. Inoltre, se le osservazioni sono incoerenti, non \u00e8 chiaro se si tratti di una confutazione della forma funzionale stipulata o della massimizzazione dell'utilit\u00e0. Si chiede quando si potr\u00e0 determinare che un insieme finito di osservazioni \u00e8 coerente con la massimizzazione dell'utilit\u00e0 senza fare ipotesi parametriche? Egli mostra che la razionalizzabilit\u00e0 di un insieme finito di osservazioni equivale all'assioma forte della preferenza rivelata. Richter -LSB- 15 -RSB- mostra che l'assioma forte della preferenza rivelata equivale alla razionalizzabilit\u00e0 mediante una funzione di utilit\u00e0 monotona strettamente concava. Afriat -LSB- 1 -RSB- fornisce un altro insieme di condizioni di razionalizzabilit\u00e0 che le osservazioni devono soddisfare. Varian -LSB- 20 -RSB- introduce l'assioma generalizzato della preferenza rivelata -LRB- GARP -RRB-, una forma equivalente della condizione di coerenza di Afriat che \u00e8 pi\u00f9 facile da verificare computazionalmente. Afriat -LSB- 1 -RSB- ha dimostrato il suo teorema costruendo esplicita una funzione di utilit\u00e0 che testimonia la consistenza. Varian -LSB- 20 -RSB- ha compiuto un ulteriore passo avanti passando dalla coerenza alla previsione. L'algoritmo di previsione di Varian esclude sostanzialmente i bundle che si rivelano inferiori ai bundle osservati e trova un bundle dall'insieme rimanente che, insieme alle osservazioni, \u00e8 coerente con GARP. Inoltre, introduce la \"metrica monetaria\" di Samuelson come una funzione di utilit\u00e0 canonica e fornisce funzioni di utilit\u00e0 dell'inviluppo superiore e inferiore per la metrica monetaria. Knoblauch -LSB- 9 -RSB- mostra che questi inviluppi possono essere calcolati in modo efficiente. Un approccio diverso \u00e8 presentato da Blundell et al. -LSB- 3, 4 -RSB-. Questi documenti introducono un modello in cui un agente osserva i prezzi e le curve di Engel per questi prezzi. Ci\u00f2 fornisce un miglioramento rispetto ai limiti originali di Varian, sebbene l'idea di base sia ancora quella di escludere richieste che si rivelano inferiori. Questo modello \u00e8 in un certo senso un ibrido tra gli approcci di Mas-Colell e di Afriat. Il primo richiede informazioni complete per tutti i prezzi, il secondo per un numero finito di prezzi. D'altra parte l'approccio adottato da Blundell et al. richiede informazioni complete solo su un numero finito di traiettorie dei prezzi. Differenti segmenti della popolazione affrontano gli stessi prezzi con budget differenti, e per quanto i dati aggregati possano testimoniare sulle preferenze individuali,mostrare come la domanda varia con il budget. Applicando metodi statistici non parametrici, ricostruiscono una traiettoria dalle richieste osservate di diversi segmenti e la utilizzano per ottenere limiti pi\u00f9 stretti. Entrambi questi metodi fornirebbero molto probabilmente una buona previsione per una funzione di domanda fissa dopo un numero sufficiente di osservazioni, presupponendo che siano distribuiti in modo ragionevole. Tuttavia, questi metodi non considerano la complessit\u00e0 delle funzioni di domanda e non utilizzano alcun modello probabilistico delle osservazioni. Pertanto, non sono in grado di fornire alcuna stima del numero di osservazioni sufficienti per una buona previsione o del grado di fiducia in tale previsione. In questo articolo esaminiamo la fattibilit\u00e0 della previsione della domanda con un alto grado di confidenza utilizzando le condizioni di Afriat. Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.Formuliamo la domanda in termini di se la classe di funzioni di domanda derivate da utilit\u00e0 concave monotone sia efficientemente apprendibile dal PAC. Il nostro primo risultato \u00e8 negativo. Mostriamo, calcolando la dimensione fat shattering, che senza alcuna ipotesi preliminare, l'insieme di tutte le funzioni di domanda indotte da funzioni di utilit\u00e0 concave monotone \u00e8 troppo ricco per essere apprendibile in modo efficiente dal PAC. Tuttavia, sulla base di alcune ipotesi precedenti sull'insieme delle funzioni di domanda, mostriamo che la dimensione della frantumazione del grasso \u00e8 finita e quindi gli insiemi corrispondenti sono apprendibili dal PAC. Nella sezione 2 discuteremo brevemente i presupposti di base della teoria della domanda e le loro implicazioni. Nella sezione 3 presentiamo una nuova dimostrazione del teorema di Afriat che incorpora un algoritmo per generare efficientemente una funzione di previsione dovuta a Teo e Vohra -LSB- 17 -RSB-. Mostriamo che questo algoritmo \u00e8 computazionalmente efficiente e pu\u00f2 essere utilizzato come algoritmo di apprendimento. Nella sezione 4 diamo una breve introduzione all'apprendimento PAC, comprese diverse modifiche all'apprendimento di funzioni con valori vettoriali reali. Tracciamo anche i risultati sui limiti superiori. Nella sezione 5 studiamo l'apprendibilit\u00e0 delle funzioni di domanda e calcoliamo direttamente la dimensione fat-shattering della classe di tutte le funzioni di domanda e di una classe di funzioni di domanda reddito-Lipschitziane con una costante di reddito-Lipschitziana globale limitata.", "keyphrases": ["imparare da rivelare preferire", "problema complesso", "previsione", "probabile approssimazione corretta", "funzione util monoton concav", "funzione della domanda", "razionalizzare", "insieme finito di osservaz", "incom-lipschitz", "dimensioni della frantumazione del grasso"]}
{"file_name": "C-18", "text": "Un'analisi iniziale e una presentazione del malware che presenta un comportamento simile a uno sciame ABSTRACT \u00c8 stato osservato che Slammer, attualmente il worm informatico pi\u00f9 veloce mai registrato nella storia, infetta il 90% di tutti gli host Internet vulnerabili entro 10 minuti. Sebbene l'azione principale intrapresa dal worm Slammer sia una replica relativamente poco sofisticata di se stesso, si diffonde comunque cos\u00ec rapidamente che la risposta umana \u00e8 risultata inefficace. La maggior parte delle strategie di contromisure proposte si basano principalmente su algoritmi di rilevamento e limitazione della velocit\u00e0. Tuttavia, tali strategie vengono progettate e sviluppate per contenere efficacemente i worm i cui comportamenti sono simili a quelli di Slammer. Nel nostro lavoro avanziamo l\u2019ipotesi che i vermi della prossima generazione saranno radicalmente diversi e che potenzialmente tali tecniche si riveleranno inefficaci. Nello specifico, si propone di studiare una nuova generazione di worm denominata ''Swarm Worms'', il cui comportamento si basa sul concetto di ''intelligenza emergente''. L\u2019intelligenza emergente \u00e8 il comportamento dei sistemi, molto simile ai sistemi biologici come le formiche o le api, dove semplici interazioni locali di membri autonomi, con semplici azioni primitive, danno origine a comportamenti globali complessi e intelligenti. In questo manoscritto introdurremo i principi di base dietro l'idea di ''Swarm Worms'', nonch\u00e9 la struttura di base richiesta per essere considerato uno ''Swarm Worms''. Inoltre, presenteremo i risultati preliminari sulle velocit\u00e0 di propagazione di uno di questi vermi sciami, chiamato verme ZachiK. Mostreremo che ZachiK \u00e8 in grado di propagarsi a una velocit\u00e0 2 ordini di grandezza pi\u00f9 veloce rispetto a vermi simili senza capacit\u00e0 di sciame. 1. INTRODUZIONE E LAVORO PRECEDENTE Nelle prime ore del mattino -LRB- 05:30 GMT -RRB- del 25 gennaio 2003 il worm informatico pi\u00f9 veloce mai registrato nella storia inizi\u00f2 a diffondersi su Internet. Dopo Slammer, i ricercatori hanno esplorato i comportamenti dei worm a rapida diffusione e hanno progettato strategie di contromisure basate principalmente sul rilevamento della velocit\u00e0 e su algoritmi di limitazione. Ad esempio, Zou et al., -LSB- 2 -RSB-, hanno proposto uno schema in cui viene utilizzato un filtro di Kalman per rilevare la propagazione precoce di un verme. Cio\u00e8, vengono progettati e sviluppati sistemi per contenere efficacemente worm i cui comportamenti sono simili a quelli di Slammer. Nel lavoro qui descritto, avanziamo l'ipotesi che i worm della prossima generazione saranno diversi e pertanto tali tecniche potrebbero presentare alcune limitazioni significative. Nello specifico, si propone di studiare una nuova generazione di worm denominata ''Swarm Worms'', il cui comportamento si basa sul concetto di ''intelligenza emergente''. Il concetto di intelligenza emergente \u00e8 stato studiato per la prima volta in associazione con i sistemi biologici. In tali studi, i primi ricercatori hanno scoperto una variet\u00e0 di comportamenti interessanti di insetti o animali in natura. Uno stormo di uccelli solca il cielo. Generalmente,questo tipo di movimento aggregato \u00e8 stato chiamato \"comportamento dello sciame\". Biologi e scienziati informatici nel campo dell'intelligenza artificiale hanno studiato tali sciami biologici e hanno tentato di creare modelli che spieghino come gli elementi di uno sciame interagiscono, raggiungono obiettivi ed evolvono. I concetti di base che sono stati sviluppati negli ultimi dieci anni per spiegare gli \u201csciami e il comportamento degli sciami\u201d comprendono quattro componenti fondamentali. Questi sono: 1. Semplicit\u00e0 di logica e di azioni: uno sciame \u00e8 composto da N agenti la cui intelligenza \u00e8 limitata. Gli agenti dello sciame utilizzano semplici regole locali per governare le loro azioni. Alcuni modelli chiamano queste azioni o comportamenti primitivi; 2. Meccanismi di comunicazione locale: gli agenti interagiscono con gli altri membri dello sciame tramite semplici meccanismi di comunicazione \"locale\". Ad esempio, un uccello in uno stormo rileva la posizione dell'uccello adiacente e applica una semplice regola di evitamento e di inseguimento. 3.4. \u201cIntelligenza emergente\u201d: il comportamento aggregato di agenti autonomi risulta in comportamenti \u201cintelligenti\u201d complessi; compresa l\u2019autorganizzazione\u2019\u2019. Per comprendere appieno il comportamento di tali sciami \u00e8 necessario costruire un modello che spieghi il comportamento di quelli che chiameremo vermi generici. Questo modello, che estende il lavoro di Weaver -LSB- 5 -RSB- \u00e8 presentato qui nella sezione 2. Inoltre, intendiamo estendere detto modello in modo tale che spieghi chiaramente i comportamenti di questa nuova classe di vermi potenzialmente pericolosi chiamati vermi dello sciame. I vermi sciame si comportano in modo molto simile agli sciami biologici e mostrano un alto grado di apprendimento, comunicazione e intelligenza distribuita. Tali vermi dello sciame sono potenzialmente pi\u00f9 dannosi delle loro controparti generiche simili. Nello specifico, \u00e8 stato creato il primo esempio, a nostra conoscenza, di un simile worm di apprendimento, chiamato ZachiK. ZachiK \u00e8 un semplice worm sciame che cracka password e incorpora diverse strategie di apprendimento e condivisione delle informazioni. Uno sciame di worm di questo tipo \u00e8 stato implementato sia in una rete locale di trenta host -LRB- 30 -RRB-, sia simulato in una topologia da 10.000 nodi. I risultati preliminari hanno mostrato che tali worm sono in grado di compromettere gli ospiti a velocit\u00e0 fino a due ordini di grandezza pi\u00f9 veloci rispetto alla loro controparte generica. Il resto di questo manoscritto \u00e8 strutturato come segue. Nella sezione 2 viene presentato un modello astratto sia dei vermi generici che dei vermi sciame. Questo modello viene utilizzato nella sezione 2.6 per descrivere la prima istanza di un verme sciame, ZachiK. Nella sezione 4 vengono presentati i risultati preliminari ottenuti sia tramite misurazioni empiriche che tramite simulazione. Infine, nella sezione 5 vengono presentate le nostre conclusioni e approfondimenti sul lavoro futuro. 5. SOMMARIO E LAVORO FUTURO In questo manoscritto abbiamo presentato un modello astratto, simile per alcuni aspetti a quello di Weaver -LSB- 5 -RSB-, che aiuta a spiegare la natura generica dei vermi.Il modello presentato nella sezione 2 \u00e8 stato esteso per incorporare una nuova classe di vermi potenzialmente pericolosi chiamati Swarm Worms. I vermi sciame si comportano in modo molto simile agli sciami biologici e mostrano un alto grado di apprendimento, comunicazione e intelligenza distribuita. Tali vermi dello sciame sono potenzialmente pi\u00f9 dannosi delle loro controparti generiche. Inoltre, per quanto ne sappiamo, \u00e8 stato creato il primo esempio di un simile worm di apprendimento, chiamato ZachiK. ZachiK \u00e8 un semplice worm sciame che cracka password e incorpora diverse strategie di apprendimento e condivisione delle informazioni. Uno sciame di worm di questo tipo \u00e8 stato implementato sia in una rete locale di trenta host -LRB- 30 -RRB-, sia simulato in una topologia da 10.000 nodi. I risultati preliminari hanno mostrato che tali worm sono in grado di compromettere gli host a una velocit\u00e0 fino a 2 ordini di grandezza pi\u00f9 veloce rispetto alla loro controparte generica, pur mantenendo capacit\u00e0 invisibili. Questo lavoro apre una nuova area di problemi interessanti. Alcuni dei problemi pi\u00f9 interessanti e urgenti da considerare sono i seguenti: \u2022 \u00c8 possibile applicare alcuni dei concetti di apprendimento sviluppati negli ultimi dieci anni nelle aree dell'intelligenza di sciame, dei sistemi di agenti e del controllo distribuito alla progettazione di sofisticati sistemi di sciame? vermi in modo tale da provocare un vero comportamento emergente? \u2022 Le attuali tecniche in fase di sviluppo nella progettazione di sistemi di rilevamento e contromisura delle intrusioni e di sistemi sopravvissuti sono efficaci contro questa nuova classe di worm? ; e \u2022 Quali tecniche, se ce ne sono, possono essere sviluppate per creare difese contro gli sciami di vermi?si pu\u00f2 sviluppare per creare difese contro gli sciami di vermi?si pu\u00f2 sviluppare per creare difese contro gli sciami di vermi?", "keyphrases": ["malwar", "verme dello sciame", "intelligenza emergente", "verme sbattitore", "meccanico comune locale", "zachik", "metodo prng", "elenco target pre-generazione", "distribuire l'intelligenza", "rilevamento delle intrusioni", "sistema di contromisure"]}
{"file_name": "J-26", "text": "Agenzia combinatoria ABSTRACT Molte ricerche recenti riguardano sistemi, come Internet, i cui componenti sono posseduti e gestiti da parti diverse, ciascuno con il proprio obiettivo \"egoistico\". Il campo dell'Algorithmic Mechanism Design gestisce la questione delle informazioni private detenute dalle diverse parti in tali contesti computazionali. Questo articolo affronta un problema complementare in tali contesti: la gestione delle \"azioni nascoste\" eseguite dalle diverse parti. Il nostro modello \u00e8 una variante combinatoria del classico problema dell\u2019agente principale della teoria economica. Nel nostro context un principale deve motivare una squadra di agenti strategici a compiere sforzi costosi per suo conto, ma le loro azioni gli sono nascoste. La nostra attenzione si concentra sui casi in cui combinazioni complesse degli sforzi degli agenti influenzano il risultato. Il principale motiva gli agenti offrendo loro una serie di contratti, che insieme pongono gli agenti in un punto di equilibrio del gioco indotto. Presentiamo modelli formali per questo context, suggeriamo e intraprendiamo un'analisi di alcune questioni fondamentali, ma lasciamo aperte molte domande. 1. INTRODUZIONE 1.1 Context Una delle caratteristiche pi\u00f9 sorprendenti delle moderne reti di computer - in particolare Internet - \u00e8 che le diverse parti di esse sono possedute e gestite da diversi individui, aziende e organizzazioni. L'analisi e la progettazione di protocolli per questo ambiente devono quindi naturalmente tenere conto dei diversi interessi economici \"egoistici\" dei diversi partecipanti. In particolare, il campo della progettazione di meccanismi algoritmici -LSB- 6 -RSB- utilizza incentivi adeguati per \"estrarre\" le informazioni private dai partecipanti. Questo articolo affronta la mancanza di conoscenza complementare, quella delle azioni nascoste. In molti casi i comportamenti effettivi \u2013 le azioni \u2013 dei diversi partecipanti sono \u201cnascosti\u201d agli altri e influenzano il risultato finale solo indirettamente. Come possiamo garantire che la giusta combinazione di allocazioni venga effettivamente effettuata dai diversi server? Una classe di esempi correlati riguarda le questioni di sicurezza: ciascun \"collegamento\" in un sistema complesso pu\u00f2 esercitare diversi livelli di sforzo per proteggere alcune propriet\u00e0 di sicurezza desiderate del sistema. Come possiamo garantire che il livello desiderato di 5. ASPETTI ALGORITMICI La nostra analisi in tutto il documento fa luce sugli aspetti algoritmici del calcolo del miglior contratto. In questa sezione enunciamo queste implicazioni -LRB- per le dimostrazioni vedi -LSB- 2 -RSB- -RRB-. Consideriamo dapprima il modello generale in cui la funzione tecnologica \u00e8 data da una funzione monotona arbitraria t -LRB- con valori razionali -RRB-, e poi consideriamo il caso di tecnologie strutturate date da una rappresentazione in rete della funzione booleana sottostante. 5.1 Tecnologie ad azione binaria con risultato binario Qui assumiamo che ci venga data una tecnologia e un valore v come input, e il nostro output dovrebbe essere il contratto ottimale, cio\u00e8l'insieme S* degli agenti da contrarre e il contratto pi per ciascun i ES*. Nel caso generale, la funzione di successo t ha dimensione esponenziale in n, il numero di agenti, e dovremo occuparci di questo. Nel caso speciale delle tecnologie anonime, la descrizione di t \u00e8 costituita solo dagli n+1 numeri t0,..., tn, e in questo caso la nostra analisi nella sezione 3 \u00e8 completamente sufficiente per calcolare il contratto ottimo. \u2022 L'orbita della tecnologia sia nei casi di agenzia che in quelli non strategici. \u2022 Un contratto ottimo per ogni dato valore v, sia per l'agenzia che per i casi non strategici. \u2022 Il prezzo dell'irresponsabilit\u00e0 POU -LRB- t, ~ c -RRB-. PROVA. Dimostriamo le affermazioni per il caso non anonimo, la prova per il caso anonimo \u00e8 simile. Mostriamo innanzitutto come costruire l'orbita della tecnologia -LRB-, in entrambi i casi si applica la stessa procedura -RRB-. Per costruire l'orbita troviamo tutti i punti di transizione e gli insiemi che si trovano nell'orbita. Il contratto vuoto \u00e8 sempre ottimo per v = 0. Supponiamo di aver calcolato i contratti ottimi e i punti di transizione fino ad un punto di transizione v per il quale S \u00e8 un contratto ottimo con la pi\u00f9 alta probabilit\u00e0 di successo. Mostriamo come calcolare il successivo punto di transizione e il successivo contratto ottimale. Per il Lemma 3 il prossimo contratto sull'orbita -LRB- per valori pi\u00f9 alti -RRB- ha una probabilit\u00e0 di successo maggiore -LRB- non ci sono due insiemi con la stessa probabilit\u00e0 di successo sull'orbita -RRB-. Calcoliamo il prossimo contratto ottimo mediante la seguente procedura. Esaminiamo tutti gli insiemi T tali che t -LRB- T -RRB- > t -LRB- S -RRB-, e calcoliamo il valore per il quale il principale \u00e8 indifferente tra la contrazione con T e la contrazione con S. Il valore di indifferenza minima \u00e8 il successivo punto di transizione e il contratto che ha il valore minimo di indifferenza \u00e8 il successivo contratto ottimale. La linearit\u00e0 dell'utilit\u00e0 nel valore e la monotonicit\u00e0 della probabilit\u00e0 di successo dei contratti ottimali garantiscono il funzionamento di quanto sopra. Chiaramente il calcolo precedente \u00e8 polinomiale nella dimensione dell'input. Una volta ottenuta l'orbita, \u00e8 chiaro che \u00e8 possibile calcolare un contratto ottimo per ogni dato valore v. Troviamo il punto di transizione pi\u00f9 grande che non \u00e8 maggiore del valore v, e il contratto ottimale in v \u00e8 l'insieme con la maggiore probabilit\u00e0 di successo in questo punto di transizione. Infine, poich\u00e9 possiamo calcolare l\u2019orbita della tecnologia sia nel caso dell\u2019agenzia che in quello non strategico in tempo polinomiale, possiamo trovare il prezzo dell\u2019irresponsabilit\u00e0 in tempo polinomiale. Per il Lemma 1 il prezzo di irresponsabilit\u00e0 POU -LRB- t -RRB- si ottiene in un punto di transizione, quindi dobbiamo solo esaminare tutti i punti di transizione e trovare quello con il massimo rapporto di benessere sociale. Una questione pi\u00f9 interessante \u00e8 se, data la funzione t come una scatola nera, possiamo calcolare il contratto ottimale nel tempo che \u00e8 polinomiale in n. Possiamo dimostrare che, in generale, non \u00e8 cos\u00ec: TEOREMA 5.Data come input una scatola nera per una funzione di successo t -LRB- quando i costi sono identici -RRB-, e un valore v, il numero di query necessarie, nel caso peggiore, per trovare il contratto ottimo \u00e8 esponenziale in n . PROVA. Consideriamo la seguente famiglia di tecnologie. Per alcuni piccoli e > 0 e k = -LSB- n/2 -RSB- definiamo la probabilit\u00e0 di successo per un dato insieme T come segue. Se l'algoritmo interroga al massimo -LRB- n -RRB- -- 2 insiemi fin/2 -RSB- di dimensione k, allora non pu\u00f2 sempre determinare il contratto ottimale -LRB- come uno qualsiasi degli insiemi che non ha interrogato circa potrebbe essere quello ottimale -RRB-. Concludiamo che -LRB- n -RRB- -- 1 query fin/2 -RSB- sono necessarie per determinare il contratto ottimo, e questo \u00e8 esponenziale in n. 5.2 Tecnologie strutturate In questa sezione considereremo la rappresentazione naturale delle reti read-once per la funzione booleana sottostante. Quindi il problema che affronteremo sar\u00e0: Il problema del contratto ottimo per reti Read Once: Input: Una rete read-once G = -LRB- V, E -RRB-, con due vertici specifici s, t ; valori razionali - ye, \u03b4e per ciascun giocatore e \u2208 E -LRB- e ce = 1 -RRB-, e un valore razionale v. Output: un insieme S di agenti che dovrebbero essere contrattati in un contratto ottimo. Sia t -LRB- E -RRB- la probabilit\u00e0 di successo quando ciascun arco riesce con probabilit\u00e0 \u03b4e. Notiamo innanzitutto che anche calcolare il valore t -LRB- E -RRB- \u00e8 un problema difficile: \u00e8 chiamato problema di affidabilit\u00e0 della rete ed \u00e8 noto essere #P \u2212 difficile -LSB- 8 -RSB-. Solo un piccolo sforzo riveler\u00e0 che il nostro problema non \u00e8 pi\u00f9 semplice: TEOREMA 6. Il problema del contratto ottimo per reti Read Once \u00e8 #P - difficile -LRB- per riduzioni di Turing -RRB-. PROVA. Mostreremo che un algoritmo per questo problema pu\u00f2 essere utilizzato per risolvere il problema dell'affidabilit\u00e0 della rete. Data un'istanza di un problema di affidabilit\u00e0 della rete < G, -LCB- -LRB- e -RCB- eEE > -LRB- dove -LRB- e denota la probabilit\u00e0 di successo di e -RRB-, definiamo un'istanza del contratto ottimo problema come segue: definire innanzitutto un nuovo grafo G ' che si ottiene '' And '' ing G con un nuovo giocatore x, con - yx molto vicino a 21 e \u03b4x = 1 \u2212 - yx. Una volta trovato tale valore, scegliamo - yx st c 1 -- 2\u03b3x \u00e8 maggiore di quel valore -RRB-. Indichiamo \u03b2x = 1 \u2212 2-yx. Il valore critico di v dove il giocatore x inserisce il contratto ottimale di G ', pu\u00f2 essere trovato utilizzando la ricerca binaria sull'algoritmo che presumibilmente trova il contratto ottimale per qualsiasi rete e qualsiasi valore. Si noti che a questo valore critico v, il principale \u00e8 indifferente tra l'insieme E ed E \u222a -LCB- x -RCB-. quindi, se riusciamo sempre a trovare il contratto ottimo, siamo anche in grado di calcolare il valore di t -LRB- E -RRB-. In conclusione, calcolare il contratto ottimo in generale \u00e8 difficile. Questi risultati suggeriscono due direzioni naturali di ricerca. La prima strada \u00e8 studiare famiglie di tecnologie i cui contratti ottimi possono essere calcolati in tempo polinomiale.La seconda strada \u00e8 esplorare algoritmi di approssimazione per il problema del contratto ottimo. Un possibile candidato per la prima direzione \u00e8 la famiglia delle reti serie-parallele, per le quali il problema dell'affidabilit\u00e0 della rete -LRB- che calcola il valore di t -RRB- \u00e8 polinomiale.", "keyphrases": ["insieme ottimale di contratto", "classico principale-agente", "qualit\u00e0 del servizio", "agenzia combinatoria", "equilibrio di Nash", "azione contrattuale", "orbita k", "tecnologia anonima", "rete serie-parallela", "prezzo di non conto"]}
{"file_name": "J-11", "text": "Reti commerciali con agenti di fissazione dei prezzi SOMMARIO In un'ampia gamma di mercati, i singoli acquirenti e venditori spesso commerciano attraverso intermediari, che determinano i prezzi sulla base di considerazioni strategiche. In genere, non tutti gli acquirenti e i venditori hanno accesso agli stessi intermediari e commerciano a prezzi corrispondentemente diversi che riflettono la loro relativa quantit\u00e0 di potere sul mercato. Modelliamo questo fenomeno utilizzando un gioco in cui acquirenti, venditori e trader intraprendono scambi commerciali su un grafico che rappresenta l'accesso che ciascun acquirente e venditore ha ai trader. In questo modello, i trader fissano i prezzi in modo strategico, quindi acquirenti e venditori reagiscono ai prezzi che vengono loro offerti. Mostriamo che il gioco risultante ha sempre un equilibrio di Nash perfetto nel sottogioco e che tutti gli equilibri portano a un'allocazione dei beni efficiente -LRB- cio\u00e8 socialmente ottimale -RRB-. Estendiamo questi risultati a un tipo pi\u00f9 generale di mercato di abbinamento, come quello che si trova nell'abbinamento tra candidati e datori di lavoro. Infine, consideriamo come i profitti ottenuti dai trader dipendano dal grafico sottostante: grossomodo, un trader pu\u00f2 ottenere un profitto positivo se e solo se ha una connessione \"essenziale\" nella struttura della rete, fornendo cos\u00ec un grafico- base teorica per quantificare l\u2019entit\u00e0 della concorrenza tra gli operatori. Il nostro lavoro differisce dai recenti studi su come il prezzo \u00e8 influenzato dalla struttura della rete attraverso il nostro modello di fissazione dei prezzi come attivit\u00e0 strategica svolta da un sottoinsieme di agenti nel sistema, piuttosto che studiare i prezzi fissati tramite equilibrio competitivo o da un meccanismo veritiero. 1. INTRODUZIONE In una serie di contesti in cui i mercati mediano le interazioni tra acquirenti e venditori, si osservano diverse propriet\u00e0 ricorrenti: i singoli acquirenti e venditori spesso commerciano attraverso intermediari, non tutti gli acquirenti e i venditori hanno accesso agli stessi intermediari, e non tutti gli acquirenti e i venditori i venditori commerciano allo stesso prezzo. Un esempio di questo context \u00e8 il commercio di prodotti agricoli nei paesi in via di sviluppo. Date le reti di trasporto inadeguate e l\u2019accesso limitato al capitale da parte degli agricoltori poveri, molti agricoltori non hanno alternative al commercio con intermediari in mercati locali inefficienti. Un paese in via di sviluppo pu\u00f2 avere molti di questi mercati parzialmente sovrapposti esistenti accanto a mercati moderni ed efficienti -LSB- 2 -RSB-. I mercati finanziari forniscono un esempio diverso di un context con queste caratteristiche generali. In questi mercati gran parte del commercio tra acquirenti e venditori \u00e8 intermediato da una variet\u00e0 di agenti che vanno dai broker ai market maker fino ai sistemi di commercio elettronico. Per molti asset non esiste un mercato unico; lo scambio di un singolo asset pu\u00f2 avvenire simultaneamente sul pavimento di una borsa, su reti incrociate, su scambi elettronici e nei mercati di altri paesi. Alcuni acquirenti e venditori hanno accesso a molte o a tutte queste sedi di negoziazione; altri hanno accesso solo a uno o alcuni di essi. Il prezzo al quale l'asset viene scambiato pu\u00f2 variare a seconda delle sedi di negoziazione.In realt\u00e0, non esiste un \u201cprezzo\u201d poich\u00e9 diversi commercianti pagano o ricevono prezzi diversi. In molti contesti esiste anche un divario tra il prezzo che un acquirente paga per un asset, il prezzo ask, e il prezzo che un venditore riceve per l\u2019asset, il prezzo bid. Gli spread, definiti come la differenza tra i prezzi bid e ask, differiscono significativamente tra questi mercati, anche se lo stesso asset viene scambiato nei due mercati. In questo articolo, sviluppiamo un quadro in cui tali fenomeni emergono da un modello di commercio basato sulla teoria dei giochi, con acquirenti, venditori e trader che interagiscono su una rete. I confini della rete collegano i trader agli acquirenti e ai venditori e rappresentano quindi l\u2019accesso che i diversi partecipanti al mercato hanno tra loro. I trader fungono da intermediari in un gioco di negoziazione in due fasi: scelgono strategicamente le offerte e chiedono i prezzi da offrire ai venditori e agli acquirenti a cui sono collegati; i venditori e gli acquirenti reagiscono quindi ai prezzi che devono affrontare. Pertanto, la rete codifica il potere relativo nelle posizioni strutturali dei partecipanti al mercato, compresi i livelli impliciti di concorrenza tra i trader. Mostriamo che questo gioco ha sempre un equilibrio di Nash perfetto nei sottogiochi e che tutti gli equilibri portano a un'allocazione dei beni efficiente -LRB- cio\u00e8 socialmente ottimale -RRB-. Analizzeremo anche come i profitti dei trader dipendano dalla struttura della rete, essenzialmente caratterizzando in termini di teoria dei grafi come il profitto di un trader sia determinato dalla quantit\u00e0 di concorrenza che sperimenta con altri trader. Sviluppando un modello di rete che include esplicitamente i trader come agenti di fissazione dei prezzi, in un sistema insieme ad acquirenti e venditori, siamo in grado di catturare la formazione dei prezzi in un context di rete come un processo strategico portato avanti dagli intermediari, piuttosto che come il risultato di un meccanismo controllato centralmente o esogeno. Il modello base: beni indistinguibili. Il nostro obiettivo nel formulare il modello \u00e8 esprimere il processo di fissazione dei prezzi in mercati come quelli discussi sopra, dove i partecipanti non hanno tutti un accesso uniforme gli uni agli altri. Ci viene dato un insieme B di acquirenti, un insieme S di venditori e un insieme T di commercianti. C'\u00e8 un grafico G non orientato che indica chi \u00e8 in grado di commerciare con chi. Ci\u00f2 riflette i vincoli che tutte le transazioni acquirente-venditore passano attraverso i trader come intermediari. Nella versione pi\u00f9 elementare del modello, consideriamo beni identici, di cui inizialmente ciascun venditore ne detiene una copia. Acquirenti e venditori hanno ciascuno un valore per una copia del bene e presumiamo che questi valori siano di conoscenza comune. Successivamente generalizzeremo questo concetto a un context in cui i beni sono distinguibili, gli acquirenti possono valutare beni diversi in modo diverso e potenzialmente anche i venditori possono valutare diversamente le transazioni con acquirenti diversi. Avere valutazioni diverse degli acquirenti cattura impostazioni come gli acquisti di case; l'aggiunta di diverse valutazioni del venditore e l'acquisizione di mercati corrispondenti, ad esempio,venditori come candidati a un posto di lavoro e acquirenti come datori di lavoro, entrambi preoccupati di chi finisce con quale \"buono\" -LRB- e con i commercianti che agiscono come servizi che mediano la ricerca di lavoro -RRB-. Quindi, partendo dal modello base, esiste un'unica tipologia di bene; il bene si presenta in unit\u00e0 indivisibili; e ciascun venditore detiene inizialmente una unit\u00e0 del bene. Tutti e tre i tipi di agenti valutano il denaro allo stesso tasso; e ciascun i EBUS valuta inoltre una copia del bene pari a \u03b8i unit\u00e0 di moneta. Nessun agente desidera pi\u00f9 di una copia del bene, quindi le copie aggiuntive vengono valutate pari a 0. Ciascun agente ha una dotazione iniziale di moneta che \u00e8 maggiore di qualsiasi valutazione individuale \u03b8i ; l'effetto di ci\u00f2 \u00e8 quello di garantire che qualsiasi acquirente che si ritrova senza una copia del bene venga escluso dal mercato a causa della sua valutazione e della posizione nella rete, non per mancanza di fondi. Immaginiamo che ogni bene venduto scorra lungo una sequenza di due bordi: da un venditore a un commerciante, e poi dal commerciante a un acquirente. Il modo particolare in cui fluiscono le merci \u00e8 determinato dal gioco seguente. Innanzitutto, ciascun trader offre un prezzo bid a ciascun venditore a cui \u00e8 connesso e un prezzo ask a ciascun acquirente a cui \u00e8 connesso. Venditori e acquirenti scelgono quindi tra le offerte presentate loro dai commercianti. Se pi\u00f9 trader propongono lo stesso prezzo a un venditore o a un acquirente, non esiste una risposta migliore e rigorosa per il venditore o l\u2019acquirente. Infine, ciascun commerciante acquista una copia del bene da ciascun venditore che accetta la sua offerta e vende una copia del bene a ciascun acquirente che accetta la sua offerta. Se un particolare commerciante scopre che pi\u00f9 acquirenti che venditori accettano le sue offerte, allora si \u00e8 impegnato a fornire pi\u00f9 copie del bene di quelle che ha ricevuto, e diremo che ci\u00f2 si traduce in una grossa penalit\u00e0 per il commerciante per inadempienza; l'effetto di ci\u00f2 \u00e8 che in equilibrio nessun trader sceglier\u00e0 prezzi bid e ask che si tradurranno in un default. Pi\u00f9 precisamente, una strategia per ciascun trader t \u00e8 la specificazione di un prezzo bid 3ti per ciascun venditore i a cui t \u00e8 connesso, e di un prezzo ask \u03b1tj per ciascun acquirente j a cui t \u00e8 connesso. -LRB- Possiamo anche gestire un modello in cui un trader pu\u00f2 scegliere di non fare un'offerta ad alcuni dei suoi venditori o acquirenti adiacenti. -RRB- Ogni venditore o acquirente sceglie poi al massimo un margine incidente, indicando il commerciante con il quale effettuer\u00e0 la transazione, al prezzo indicato. -LRB- La scelta di un singolo vantaggio riflette il fatto che i venditori -LRB- a -RRB- inizialmente hanno ciascuno solo una copia del bene, e gli acquirenti -LRB- b -RRB- vogliono ciascuno solo una copia del bene. -RRB- I guadagni sono i seguenti: Per ogni venditore i, il profitto derivante dalla selezione del trader t \u00e8 3ti, mentre il profitto derivante dalla selezione di nessun trader \u00e8 \u03b8i. -LRB- Nel primo caso il venditore riceve 3ti unit\u00e0 di moneta, mentre nel secondo conserva la sua copia del bene, che valuta \u03b8i. -RRB- Per ciascun acquirente j, il profitto derivante dalla selezione del trader t \u00e8 \u03b8j -- \u03b1tj, mentre il profitto derivante dalla selezione di nessun trader \u00e8 0.-LRB- Nel primo caso, l'acquirente riceve il bene ma cede \u03b1tj unit\u00e0 di moneta. -RRB- Per ogni trader t, con le offerte accettate dai venditori i1,..., is e dagli acquirenti j1,..., jb, il profitto \u00e8 Pr \u03b1tjr -- Pr 3tir, meno una penalit\u00e0 \u03c0 se b > s. La penalit\u00e0 viene scelta in modo tale che un trader non la incorrer\u00e0 mai in equilibrio, e quindi generalmente non ci preoccuperemo della sanzione. Questo definisce gli elementi base del gioco. Il concetto di equilibrio che usiamo \u00e8 l\u2019equilibrio di Nash perfetto del sottogioco. Qualche esempio. Per aiutare a riflettere sul modello, descriviamo ora tre esempi illustrativi, rappresentati nella Figura 1. Tutti i venditori negli esempi avranno valutazioni per il bene pari a 0; all'interno del suo cerchio viene disegnata la valutazione di ciascun acquirente; e il prezzo bid o ask su ciascun bordo viene disegnato sopra il bordo. Nella Figura 1 -LRB- a -RRB-, mostriamo come un'asta standard al secondo prezzo nasca naturalmente dal nostro modello. Supponiamo che le valutazioni dell'acquirente dall'alto verso il basso siano w > x > y > z. I prezzi bid e ask mostrati sono coerenti con un equilibrio in cui i1 e j1 accettano le offerte del trader t1, e nessun altro acquirente accetta l'offerta del trader adiacente: quindi, il trader t1 riceve il bene con un prezzo bid pari a x, e produce w -- x vendendo il bene all'acquirente j1 in cambio di w. In questo modo, possiamo considerare questo caso particolare come un'asta per un singolo bene in cui i commercianti agiscono come \"delegati\" per i loro acquirenti adiacenti. L'acquirente con la valutazione pi\u00f9 alta del bene finisce con esso e il surplus viene diviso tra il venditore e il commerciante associato. Si noti che \u00e8 possibile costruire un'asta di k unit\u00e0 con f > k acquirenti altrettanto facilmente, costruendo un grafico bipartito completo su k venditori e f commercianti, e quindi collegando ciascun commerciante a un singolo acquirente distinto. Nella Figura 1 -LRB- b -RRB-, mostriamo come i nodi con diverse posizioni nella topologia di rete possono ottenere profitti diversi, anche quando tutti Figura 1: -LRB- a -RRB- Un'asta, mediata dai trader, in cui il se lo aggiudica l'acquirente con la valutazione pi\u00f9 alta del bene. -LRB- b -RRB- Una rete in cui il venditore e l'acquirente intermedio beneficiano della concorrenza perfetta tra i trader, mentre gli altri venditori e acquirenti non hanno alcun potere a causa della loro posizione nella rete. -LRB- c -RRB- Una forma di concorrenza perfetta implicita: tutti gli spread bid/ask saranno pari a zero in equilibrio, anche se nessun trader \u201ccompete\u201d direttamente con nessun altro trader per la stessa coppia acquirente-venditore. le valutazioni degli acquirenti sono le stesse numericamente. Nello specifico, il venditore i2 e l'acquirente j2 occupano posizioni di potere, perch\u00e9 i due trader sono in competizione per i loro affari; d\u2019altro canto, gli altri venditori e acquirenti sono in posizioni deboli, perch\u00e9 ciascuno di loro ha una sola opzione. E in effetti, in ogni equilibrio, esiste un numero reale x E -LSB- 0, 1 -RSB- tale che entrambi i trader offrono prezzi bid e ask rispettivamente da x a i2 e j2,mentre offrono offerte pari a 0 e chiedono 1 agli altri venditori e acquirenti. Pertanto, questo esempio illustra alcuni ingredienti cruciali che identificheremo tra breve a un livello pi\u00f9 generale. Nello specifico, i2 e j2 sperimentano i vantaggi della concorrenza perfetta, in quanto i due trader portano gli spread bid-ask a 0 nel competere per la propria attivit\u00e0. D'altra parte, gli altri venditori e acquirenti sperimentano gli svantaggi del monopolio: ricevono un profitto pari a 0 poich\u00e9 hanno una sola opzione per lo scambio e il trader corrispondente ottiene tutto il profitto. Si noti inoltre come questo comportamento naturale emerga dal fatto che i trader sono in grado di offrire prezzi diversi a diversi agenti, cogliendo il fatto che non esiste un \"prezzo\" fisso nei tipi di mercati che motivano il modello, ma piuttosto diversi prezzi. prezzi che riflettono il potere relativo dei diversi agenti coinvolti. L'esempio precedente mostra forse il modo pi\u00f9 naturale in cui il profitto di un trader su una particolare transazione pu\u00f2 scendere a 0: quando c'\u00e8 un altro trader che pu\u00f2 replicare esattamente la sua funzione. -LRB- In questo esempio, due trader avevano ciascuno la possibilit\u00e0 di spostare una copia del bene da i2 a j2. -RRB- Ma come mostreranno i nostri risultati successivi, i trader realizzano profitti pari a zero pi\u00f9 in generale per ragioni globali e legate alla teoria dei grafici. L'esempio in Figura 1 -LRB- c -RRB- d\u00e0 una prima indicazione di ci\u00f2: si pu\u00f2 mostrare che per ogni equilibrio, c'\u00e8 ay E -LSB- 0, 1 -RSB- tale che ogni prezzo bid e ask \u00e8 uguale a y. In altre parole, tutti i trader realizzano profitti pari a zero, indipendentemente dal fatto che una copia del bene li attraversi o meno, eppure non esistono due trader che abbiano in comune un percorso venditore-acquirente. Gli spread di prezzo sono stati portati a zero da un vincolo globale imposto dal ciclo lungo a tutti gli agenti; questo \u00e8 un esempio di concorrenza perfetta implicita determinata dalla topologia della rete. Estendere il modello ai beni distinguibili. Estendiamo il modello base ad un context con beni distinguibili, come segue. Una strategia per un trader ora consiste nell'offrire un'offerta a ciascun venditore che specifica sia un prezzo che un acquirente, e nell'offrire una domanda a ciascun acquirente che specifica sia un prezzo che un venditore. -LRB- Possiamo anche gestire un modello in cui un trader offre rispettivamente offerte -LRB-, chiede -RRB- sotto forma di vettori, specificando essenzialmente un `` menu '' con un prezzo allegato a ciascun acquirente -LRB- risp. venditore -RRB-. -RRB- Ciascun acquirente e venditore seleziona un'offerta da un commerciante adiacente e i profitti per tutti gli agenti vengono determinati come prima. Qui i venditori sono candidati al lavoro, gli acquirenti sono datori di lavoro e i commercianti sono gli agenti che mediano il mercato del lavoro. Naturalmente, se si specificano valutazioni a coppie per gli acquirenti ma solo valutazioni singole per i venditori, modelliamo un ambiente in cui gli acquirenti possono distinguere tra i beni, ma ai venditori non interessa a chi vendono - questo -LRB- pi\u00f9 o meno -RRB- cattura contesti come i mercati immobiliari. I nostri risultati. Per precisarli,introduciamo la seguente notazione. -LRB- I venditori che non compaiono in nessuna tripla conservano la loro copia del bene. -RRB- Diciamo che il valore dell'allocazione \u00e8 pari a Pe \u2208 M \u03b8jeie -- \u03b8ieje. Sia \u03b8 \u2217 il valore massimo di qualsiasi allocazione M fattibile data la rete. Mostriamo che ogni istanza del nostro gioco ha un equilibrio e che in ognuno di questi equilibri l'allocazione ha valore \u03b8 \u2217 -- in altre parole, raggiunge il miglior valore possibile. Pertanto, gli equilibri in questo modello sono sempre efficienti, nel senso che il mercato consente all\u2019insieme \u201cgiusto\u201d di persone di ottenere il bene, soggetto ai vincoli della rete. Stabiliamo l'esistenza e l'efficienza degli equilibri costruendo un programma lineare per catturare il flusso di beni attraverso la rete; il duale di questo programma lineare contiene informazioni sufficienti per estrarre i prezzi di equilibrio. Per definizione del gioco, il valore dell\u2019allocazione di equilibrio \u00e8 suddiviso come profitto per gli agenti, ed \u00e8 interessante chiedersi come viene distribuito questo valore, in particolare quanto profitto un trader \u00e8 in grado di realizzare in base alla sua posizione. nella rete. Troviamo che, sebbene tutti gli equilibri abbiano lo stesso valore, il profitto di un dato trader pu\u00f2 variare a seconda degli equilibri. Otteniamo anche risultati per la somma di tutti i profitti del trader. Lavoro correlato. L\u2019approccio di base standard per analizzare l\u2019interazione tra acquirenti e venditori \u00e8 il modello Walrasiano in cui acquirenti e venditori anonimi scambiano un bene a un prezzo di equilibrio del mercato unico. Questa forma ridotta di commercio, costruita sull\u2019idealizzazione di un prezzo di mercato, \u00e8 un modello potente che ha portato a molte intuizioni. Ma non \u00e8 un buon modello da utilizzare per esaminare da dove provengono i prezzi o esattamente come acquirenti e venditori commerciano tra loro. La difficolt\u00e0 \u00e8 che nel modello Walrasiano non esiste un agente che fissa il prezzo e gli agenti non commerciano tra loro. Non esiste infatti un mercato, nel senso quotidiano del termine, nel modello walrasiano. Cio\u00e8, non esiste un luogo fisico o virtuale in cui acquirenti e venditori interagiscono per commerciare e fissare i prezzi. Pertanto, in questo modello semplice, tutti gli acquirenti e i venditori sono uniformi e commerciano allo stesso prezzo, e non vi \u00e8 alcun ruolo per gli intermediari. Esistono diverse letterature in economia e finanza che esaminano il modo in cui vengono fissati i prezzi piuttosto che limitarsi a determinare i prezzi di equilibrio. La letteratura sulla concorrenza imperfetta \u00e8 forse la pi\u00f9 antica di queste. Qui un monopolista, o un gruppo di oligopolisti, sceglie i prezzi per massimizzare i propri profitti -LRB- vedere -LSB- 14 -RSB- per la trattazione standard da manuale di questi mercati -RRB-. Un monopolista utilizza la sua conoscenza della domanda di mercato per scegliere un prezzo o un insieme di prezzi se discrimina. Gli oliogopolisti giocano a un gioco in cui i loro guadagni dipendono dalla domanda del mercato e dalle azioni dei loro concorrenti. In questa letteratura ci sono agenti che fissano i prezzi, ma viene mantenuta la finzione di un mercato unico. Nella letteratura sulla ricerca dell\u2019equilibrio,le imprese stabiliscono i prezzi e i consumatori li cercano -LRB- vedi -LSB- 3 -RSB- -RRB-. I consumatori finiscono per pagare prezzi diversi, ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.ma tutti i consumatori hanno accesso a tutte le aziende e non ci sono intermediari. Nella letteratura sull\u2019equilibrio generale ci sono stati vari tentativi di introdurre la determinazione del prezzo. Una tecnica di prova standard dell\u2019esistenza di un equilibrio competitivo prevede un meccanismo di aggiustamento dei prezzi in cui i prezzi rispondono all\u2019eccesso di domanda. Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Sono stati introdotti processi pi\u00f9 sofisticati per studiare la stabilit\u00e0 dei prezzi di equilibrio o le informazioni necessarie per calcolarli. Ma anche qui non ci sono agenti che fissano i prezzi. Nella letteratura finanziaria il lavoro sulla microstruttura del mercato ha agenti di fissazione dei prezzi -LRB- specialisti -RRB-, parti di esso determinano prezzi bid e ask separati, e diversi agenti ricevono prezzi diversi per lo stesso asset -LRB- vedi -LSB - 12 -RSB- per una trattazione della teoria della microstruttura -RRB-. Il lavoro nell'economia dell'informazione ha identificato fenomeni simili -LRB- vedi ad esempio -LSB- 7 -RSB- -RRB-. Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.Ma c\u2019\u00e8 poca ricerca in queste letterature che esamini l\u2019effetto delle restrizioni su chi pu\u00f2 commerciare con chi. Sono stati adottati diversi approcci per studiare il modo in cui la struttura della rete determina i prezzi. Questi hanno postulato la determinazione dei prezzi attraverso definizioni basate sull\u2019equilibrio competitivo o sul core, o attraverso l\u2019uso di meccanismi veritieri. Nel rivedere brevemente questo lavoro, noteremo il contrasto con il nostro approccio, in quanto modelliamo i prezzi come derivanti dal comportamento strategico degli agenti nel sistema. In un lavoro recente, Kakade et al. -LSB- 8 -RSB- hanno studiato la distribuzione dei prezzi in equilibrio competitivo in un grafico bipartito su acquirenti e venditori, generato utilizzando un modello probabilistico in grado di produrre distribuzioni di grado a coda pesante -LSB- 11 -RSB-. Even-Dar et al. -LSB- 6 -RSB- si basa su questo per considerare gli aspetti strategici della formazione della rete quando i prezzi emergono dall'equilibrio competitivo. Leonard studia i prezzi del VCG in questo context; Babaioff et al. e Chu e Shen forniscono inoltre un meccanismo di pareggio del bilancio. Al contrario, il nostro modello conosce valutazioni e prezzi derivanti dal comportamento strategico dei trader. Demange, Gale e Sotomayor -LSB- 5 -RSB-, e Kranton e Minehart -LSB- 9 -RSB-, analizzano i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.analizzare i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.analizzare i prezzi ai quali avviene il commercio in una rete, lavorando nel quadro della progettazione del meccanismo. Kranton e Minehart utilizzano un grafico bipartito con collegamenti diretti tra acquirenti e venditori, quindi utilizzano un meccanismo di asta ascendente, anzich\u00e9 intermediari strategici, per determinare i prezzi. La loro asta ha propriet\u00e0 di equilibrio desiderabili ma, come notano Kranton e Minehart, \u00e8 un'astrazione del modo in cui i beni vengono allocati e vengono determinati i prezzi, simile nello spirito all'astrazione del banditore walrasiano.", "keyphrases": ["Teoria dei giochi algoritmici", "mercato", "rete commerciale", "interagiscono tra acquirente e venditore", "initi dotare di soldi", "prezzo di offerta", "concorrenza perfetta", "beneficio", "importo massimo e minimo", "economia e finanza", "comportamento strategico del trader", "gioco complementare", "monopolio"]}
{"file_name": "I-15", "text": "Ricerca e condivisione di informazioni in reti dinamiche su larga scala ABSTRACT Trovare gli agenti giusti in una rete vasta e dinamica per fornire le risorse necessarie in modo tempestivo \u00e8 un problema di vecchia data. Questo articolo presenta un metodo per la ricerca e la condivisione di informazioni che combina indici di routing con metodi basati su token. Il metodo proposto consente agli agenti di effettuare ricerche in modo efficace acquisendo gli interessi dei loro vicini, pubblicizzando le loro capacit\u00e0 di fornitura di informazioni e mantenendo indici per l'instradamento delle query, in modo integrato. Nello specifico, l'articolo dimostra attraverso esperimenti sulle prestazioni come reti statiche e dinamiche di agenti possono essere \"messe a punto\" per rispondere alle domande in modo efficace mentre raccolgono prove degli interessi e delle capacit\u00e0 di fornitura di informazioni di altri, senza alterare la topologia o imporre una struttura di sovrapposizione alla rete. di conoscenti. 1. INTRODUZIONE reti di agenti associati. D'altra parte, c'\u00e8 molta ricerca sulle reti di ricerca semantica peer to peer e sui social network -LSB- 1,5,6,8,9,10,16,18,19 -RSB- molti dei quali si occupano di ottimizzazione una rete di pari per un\u2019efficace ricerca e condivisione delle informazioni. Lo fanno principalmente imponendo strutture di sovrapposizione logiche e semantiche. Tuttavia, per quanto ne sappiamo, non esiste alcun lavoro che dimostri l'efficacia di un processo di ottimizzazione graduale in reti dinamiche su larga scala che studi l'impatto delle informazioni raccolte dagli agenti man mano che sempre pi\u00f9 query vengono emesse e servite in sessioni simultanee nel sistema. rete. La questione principale in questo articolo riguarda la \"messa a punto\" di una rete di agenti, ciascuno con una competenza specifica, per la ricerca e la condivisione di informazioni efficiente ed efficace, senza alterare la topologia o imporre una struttura di sovrapposizione tramite clustering, introduzione di indici di scelta rapida o ri- cablaggio. Il `tuning' \u00e8 il compito di condividere e raccogliere la conoscenza necessaria affinch\u00e9 gli agenti possano propagare le richieste ai giusti conoscenti, minimizzando lo sforzo di ricerca, aumentando l'efficienza e il beneficio del sistema. Nello specifico, questo articolo propone un metodo per la ricerca e la condivisione di informazioni in reti dinamiche e su larga scala, che combina indici di instradamento con metodi basati su token per la condivisione di informazioni in sistemi multi-agente su larga scala. Questo documento \u00e8 strutturato come segue: la sezione 2 presenta il lavoro correlato e motiva il metodo proposto. La sezione 3 espone il problema e la sezione 4 presenta in dettaglio le singole tecniche e il metodo complessivamente proposto. La sezione 5 presenta l'impostazione sperimentale e i risultati, mentre la sezione 6 conclude il documento, delineando il lavoro futuro. 2. LAVORI CORRELATI La fornitura e la condivisione di informazioni possono essere considerate un processo decisionale markoviano decentralizzato e parzialmente osservabile -LSB- 3,4,11,14 -RSB-. Nel caso generale, il controllo decentralizzato di sistemi dinamici su larga scala di agenti cooperativi \u00e8 un problema difficile. Le soluzioni ottime possono essere approssimate solo mediante euristiche,attraverso allentamenti del problema originario o soluzioni centralizzate. Tuttavia, in un sistema dinamico su larga scala con controllo decentralizzato \u00e8 molto difficile per gli agenti possedere accurate visioni parziali dell\u2019ambiente, ed \u00e8 ancora pi\u00f9 difficile per gli agenti possedere una visione globale dell\u2019ambiente. Inoltre, le osservazioni degli agenti non possono essere considerate indipendenti, poich\u00e9 le azioni di un agente possono influenzare le osservazioni degli altri: ad esempio, quando un agente si unisce/lascia il sistema, ci\u00f2 pu\u00f2 influenzare la valutazione da parte di altri agenti delle capacit\u00e0 di fornitura di informazioni dei vicini. . Considerando attivit\u00e0 e osservazioni indipendenti, gli autori di -LSB- 4 -RSB- propongono una soluzione teorica della decisione che tratta l'azione standard e lo scambio di informazioni come scelte esplicite che il decisore deve fare. Approssimano la soluzione utilizzando un algoritmo miope. Il loro lavoro differisce da quello riportato qui nei seguenti aspetti: in primo luogo, mira a ottimizzare la comunicazione, mentre l'obiettivo qui \u00e8 sintonizzare la rete per un'efficace condivisione delle informazioni, riducendo la comunicazione e aumentando i benefici del sistema. In terzo luogo, ritengono che le transizioni e le osservazioni fatte dagli agenti siano indipendenti, il che, come gi\u00e0 discusso, non \u00e8 vero nel caso generale. Infine, in contrasto con il loro approccio in cui gli agenti trasmettono messaggi, qui gli agenti decidono non solo quando comunicare, ma anche a chi inviare un messaggio. Gli approcci basati su token sono promettenti per ampliare efficacemente il coordinamento e quindi la fornitura e la condivisione di informazioni su sistemi su larga scala. In -LSB- 11 -RSB- gli autori forniscono un quadro matematico per l'instradamento dei token, fornendo anche un'approssimazione per risolvere il problema originale in caso di attivit\u00e0 di agenti indipendenti. Il metodo proposto richiede un elevato volume di calcoli che gli autori mirano a ridurre restringendo la sua applicazione a gruppi logici statici di agenti associati. In accordo con questo approccio, in -LSB- 12,13,14 -RSB-, la condivisione delle informazioni \u00e8 considerata solo per le reti statiche e l'autotuning delle reti non \u00e8 dimostrato. Come verr\u00e0 mostrato nella sezione 5, i nostri esperimenti mostrano che sebbene questi approcci possano gestire la condivisione delle informazioni in reti dinamiche, richiedono una maggiore quantit\u00e0 di messaggi rispetto all'approccio qui proposto e non possono sintonizzare la rete per una condivisione efficiente delle informazioni. La comunicazione proattiva \u00e8 stata proposta in -LSB- 17 -RSB- come risultato di una determinazione teorica delle decisioni dinamiche delle strategie di comunicazione. Questo approccio si basa sulla specificazione degli agenti come \"fornitori\" e \"necessari\": ci\u00f2 avviene mediante un precalcolo basato su un piano delle esigenze di informazione e delle capacit\u00e0 di fornitura degli agenti. Tuttavia, questo approccio non pu\u00f2 adattarsi a reti grandi e dinamiche, poich\u00e9 sarebbe altamente inefficiente per ciascun agente calcolare e determinare le sue potenziali esigenze e capacit\u00e0 di fornitura di informazioni data la sua potenziale interazione con centinaia di altri agenti.Considerando il recupero delle informazioni nei sistemi peer-to-peer dalla prospettiva di un sistema multi-agente, l'approccio proposto in -LSB- 18 -RSB- si basa su un modello linguistico di raccolta dei documenti degli agenti. Sfruttando i modelli di altri agenti nella rete, gli agenti costruiscono la loro visione della rete che viene utilizzata per prendere decisioni di routing. Inizialmente, gli agenti costruiscono le loro opinioni utilizzando i modelli dei loro vicini. Quindi, il sistema si riorganizza formando cluster di agenti con contenuti simili. I cluster vengono sfruttati durante il recupero delle informazioni utilizzando un approccio kNN e uno schema di ricerca a gradiente. Sebbene questo lavoro miri a mettere a punto una rete per un'efficiente fornitura di informazioni -LRB- attraverso la riorganizzazione -RRB-, non dimostra l'efficacia dell'approccio rispetto a questo problema. Inoltre, sebbene durante la riorganizzazione e il recupero misurino la somiglianza del contenuto tra gli agenti, \u00e8 necessario un approccio pi\u00f9 dettagliato che consenta agli agenti di misurare le somiglianze degli elementi informativi o delle sottoraccolte di elementi informativi. Basandosi sul loro lavoro sui sistemi peer-to-peer, H.Zhand e V.Lesser in -LSB- 19 -RSB- studiano sessioni di ricerca simultanee. Considerando la ricerca sui sistemi semantici peer-to-peer1, la maggior parte degli approcci sfrutta quello che pu\u00f2 essere genericamente definito un \"indice di instradamento\". Una questione importante riguardante la ricerca di informazioni \u00e8 \"quali informazioni devono essere condivise tra pari, quando e quali aggiustamenti devono essere apportati affinch\u00e9 le domande vengano indirizzate a fonti di informazione affidabili nel modo pi\u00f9 efficace ed efficiente\". RICORDARE I peer -LSB- 10 -RSB- raccolgono informazioni riguardanti le domande a cui \u00e8 stato risposto con successo da altri peer, in modo da selezionare successivamente i peer a cui inoltrare le richieste: questo \u00e8 un approccio di apprendimento lento che non comporta la pubblicit\u00e0 della fornitura di informazioni tra pari abilit\u00e0. Ci\u00f2 si traduce in un processo di ottimizzazione in cui il richiamo complessivo aumenta nel tempo, mentre il numero di messaggi per query rimane pi\u00f9 o meno lo stesso. Qui, gli agenti pubblicizzano attivamente le loro capacit\u00e0 di fornire informazioni in base agli interessi valutati dei loro pari: ci\u00f2 si traduce in un numero molto inferiore di messaggi per query rispetto a quelli riportati in REMINDIN'. In -LSB- 5,6 -RSB- i peer, utilizzando un'ontologia comune, pubblicizzano la propria esperienza, che viene sfruttata per la formazione di una rete semantica sovrapposta: le query vengono propagate in questa rete a seconda della loro somiglianza con l'esperienza dei peer. Secondo il nostro approccio, gli agenti pubblicizzano selettivamente le loro capacit\u00e0 di fornire informazioni su argomenti specifici ai loro vicini con interessi informativi simili -LRB- e solo a questi -RRB-. Tuttavia, ci\u00f2 avviene con il passare del tempo e mentre gli agenti ricevono richieste dai loro peer. Generano un sovraccarico sostanziale in ambienti altamente dinamici, dove i nodi si uniscono/escono dal sistema. 248 La Sesta Intl.. Conf. congiunta.gli agenti pubblicizzano le loro capacit\u00e0 di fornire informazioni tenendo conto degli interessi dei loro vicini. Dato il successo di questo metodo, studieremo come l'aggiunta di percorsi logici e la graduale evoluzione della topologia della rete possano aumentare ulteriormente l'efficacia del metodo proposto. 6. CONCLUSIONI Questo articolo presenta un metodo per l'elaborazione di query semantiche in grandi reti di agenti che combina indici di routing con metodi di condivisione delle informazioni. Il metodo presentato consente agli agenti di tenere traccia degli interessi dei conoscenti, di pubblicizzare le proprie capacit\u00e0 di fornitura di informazioni a coloro che hanno un alto interesse nei loro confronti e di mantenere indici per instradare le query a quegli agenti che hanno le capacit\u00e0 di fornitura di informazioni richieste. Nello specifico, l'articolo dimostra attraverso estesi esperimenti sulle prestazioni: -LRB- a -RRB- Come le reti di agenti possono essere \"sintonizzate\" in modo da fornire le informazioni richieste in modo efficace, aumentando i benefici e l'efficienza del sistema. -LRB- b -RRB- Come diversi tipi di conoscenza locale -LRB- numero, archivi di informazioni locali, percentuale, interessi e capacit\u00e0 di fornitura di informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.gli interessi e la capacit\u00e0 di fornire informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.gli interessi e la capacit\u00e0 di fornire informazioni dei conoscenti -RRB- possono guidare gli agenti a rispondere efficacemente alle domande, bilanciando efficienza ed efficacia. -LRB- c -RRB- Che il compito di ``tuning'' proposto riesce ad aumentare l'efficienza della ricerca e della condivisione delle informazioni in reti altamente dinamiche e di grandi dimensioni. -LRB- d -RRB- Che le informazioni raccolte e mantenute dagli agenti supportino una ricerca e una condivisione di informazioni efficiente ed efficace: informazioni iniziali sui conoscenti, capacit\u00e0 di fornitura di informazioni non sono necessarie ed \u00e8 sufficiente una piccola percentuale di conoscenti. Ulteriore lavoro riguarda la sperimentazione con dati reali e ontologie, differenze nelle ontologie tra agenti, cambiamenti di competenze e la costruzione parallela di strutture sovrapposte.", "keyphrases": ["informare, cercare e condividere", "rete sociale", "agente Cooper", "rete di ricerca peer to peer", "sistema peer-to-peer", "rete dinamica e su larga scala", "processo decentralizzato con osservazione parziale markov decis", "controllo decentrato", "algoritmo miope", "approccio knn", "schema di ricerca del gradiente"]}
{"file_name": "J-1", "text": "Meccanismi generalizzati di riduzione del commercio ABSTRACT Quando si progetta un meccanismo ci sono diverse propriet\u00e0 desiderabili da mantenere come la compatibilit\u00e0 degli incentivi -LRB- IC -RRB-, la razionalit\u00e0 individuale -LRB- IR -RRB- e l'equilibrio di bilancio -LRB- BB -RRB-. \u00c8 noto -LSB- 15 -RSB- che \u00e8 impossibile che un meccanismo massimizzi il benessere sociale pur essendo IR, IC e BB. Ci sono stati diversi tentativi di aggirare -LSB- 15 -RSB- scambiando welfare con BB, ad esempio in ambiti quali aste a doppia faccia -LSB- 13 -RSB-, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura problemi -LSB- 2, 4 -RSB-. In questo articolo forniamo una procedura chiamata Riduzione Commerciale Generalizzata -LRB- GTR -RRB- per giocatori a valore singolo, che dato un meccanismo IR e IC, produce un meccanismo che \u00e8 IR, IC e BB con una perdita di welfare. Abbiamo limitato il benessere raggiunto dalla nostra procedura per un'ampia gamma di domini. In particolare, i nostri risultati migliorano le soluzioni esistenti per problemi quali mercati a doppia faccia con beni omogenei, mercati distribuiti e diversi tipi di catene di approvvigionamento. Inoltre, la nostra soluzione fornisce meccanismi di bilanciamento del bilancio per diversi problemi aperti come aste combinatorie a doppia faccia e mercati distribuiti con vantaggi di trasporto strategici. 1. INTRODUZIONE Quando si progetta un meccanismo ci sono diverse propriet\u00e0 chiave che \u00e8 opportuno mantenere. In molti meccanismi la funzione obiettivo che il progettista di un meccanismo tenta di massimizzare \u00e8 il benessere sociale, ovvero il beneficio totale per la societ\u00e0. Tuttavia, \u00e8 ben noto da -LSB- 15 -RSB- che qualsiasi meccanismo che massimizza il benessere sociale pur mantenendo la razionalit\u00e0 individuale e la compatibilit\u00e0 degli incentivi va necessariamente in deficit, cio\u00e8 non \u00e8 in pareggio di bilancio. Per mantenere la propriet\u00e0 BB in un meccanismo IR e IC \u00e8 necessario scendere a compromessi sull\u2019ottimalit\u00e0 del benessere sociale. 1.1 Lavori correlati e soluzioni specifiche Ci sono stati diversi tentativi di progettare meccanismi di pareggio di bilancio per domini particolari2. Nel problema dei mercati distribuiti -LRB- e nei problemi strettamente correlati -RRB- le merci vengono trasportate tra localit\u00e0 geografiche sostenendo costi costanti per il trasporto. -LSB- 16, 9, 3 -RSB- presentano meccanismi che si avvicinano al benessere sociale ottenendo un meccanismo IR, IC e BB. Per i problemi della catena di fornitura -LSB- 2, 4 -RSB- limita la perdita di benessere sociale che \u00e8 necessario infliggere al meccanismo per ottenere la combinazione desiderata di IR, IC e BB. Nonostante i lavori discussi sopra, la questione di come progettare un meccanismo generale che raggiunga IR, IC e BB indipendentemente dal dominio del problema rimane aperta. Inoltre, ci sono diversi ambiti in cui la questione di come progettare un meccanismo IR, IC e BB che approssimi il benessere sociale rimane un problema aperto. Per esempio,nell\u2019importante ambito delle aste combinatorie a doppia faccia non esiste alcun risultato noto che limiti la perdita di benessere sociale necessaria per raggiungere il pareggio di bilancio. Un altro esempio interessante \u00e8 la domanda aperta lasciata da -LSB- 3 -RSB-: come si pu\u00f2 limitare la perdita di benessere sociale necessaria per raggiungere l\u2019equilibrio di bilancio in un mercato distribuito IR e IC in cui i margini dei trasporti sono strategici. Naturalmente una risposta al mercato distribuito dei BB con margini strategici ha vaste implicazioni pratiche, ad esempio per le reti di trasporto. 1.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.2 Il nostro contributo In questo articolo unifichiamo tutti i problemi discussi sopra -LRB- sia quelli risolti che quelli aperti -RRB- in un'unica procedura di concetto di soluzione. La procedura risolutiva denominata Riduzione Commerciale Generalizzata -LRB- GTR -RRB-. GTR accetta un meccanismo IR e IC per giocatori a valore singolo e produce un meccanismo IR, IC e BB. Il meccanismo di output potrebbe subire una certa perdita di benessere come compromesso per raggiungere il BB. Ci sono casi problematici in cui non \u00e8 necessaria alcuna perdita di welfare ma per -LSB- 15 -RSB- ci sono casi problematici in cui c'\u00e8 perdita di welfare. Tuttavia per un'ampia classe di problemi siamo in grado di delimitare la perdita di benessere. Un caso particolarmente interessante \u00e8 quello in cui il meccanismo di input \u00e8 un\u2019allocazione efficiente. Oltre ad unificare molti dei problemi BB sotto un unico concetto di soluzione, la procedura GTR migliora i risultati esistenti e risolve diversi problemi aperti in letteratura. Le soluzioni esistenti migliorate dalla nostra procedura GTR sono aste bilaterali omogenee, mercati distribuiti -LSB- 3 -RSB- e catena di fornitura -LSB- 2, 4 -RSB-. Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato concorrenza interna ed esterna.Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.Per le aste omogenee a doppia faccia la procedura della soluzione GTR migliora la ben nota soluzione -LSB- 13 -RSB- consentendo alcuni casi di nessuna riduzione degli scambi. Per i mercati distribuiti -LSB- 3 -RSB- e la catena di fornitura -LSB- 2, 4 -RSB- la procedura di soluzione GTR migliora il limite delle perdite di welfare, ovvero consente di realizzare un meccanismo IR, IC e BB con minori perdite sul welfare sociale. Recentemente abbiamo anche appreso che la procedura GTR consente di trasformare il modello appena presentato -LSB- 6 -RSB- in un meccanismo BB. Oltre al contributo principale sopra descritto, questo documento definisce anche un'importante classificazione dei domini problematici. Definiamo domini basati su classi e domini basati su classi di approvvigionamento. Le definizioni di cui sopra si basano sui diversi \u201cpoteri\u201d competitivi degli attori in un meccanismo chiamato competizione interna ed esterna.", "keyphrases": ["riduzione commerciale", "saldo di bilancio", "concorso stagisti", "concorrenza esterna", "effici", "potere del giocatore", "riduzione del commercio generale", "gtr", "ottimo", "ineguale nel benessere", "giocatore multi-mente", "meccanismo del pareggio di bilancio", "buono omogeneo", "mercato della distribuzione spaziale"]}
{"file_name": "H-24", "text": "Investigare il comportamento di interrogazione e navigazione degli utenti avanzati dei motori di ricerca ABSTRACT Un modo per aiutare tutti gli utenti dei motori di ricerca Web commerciali ad avere pi\u00f9 successo nelle loro ricerche \u00e8 capire meglio cosa stanno facendo gli utenti con maggiore esperienza di ricerca e utilizzare questa conoscenza a vantaggio di tutti . In questo articolo studiamo i log di interazione degli utenti avanzati dei motori di ricerca -LRB- e di quelli non cos\u00ec avanzati -RRB- per comprendere meglio come questi gruppi di utenti effettuano le ricerche. I risultati mostrano che ci sono differenze marcate nelle query, nei clic sui risultati, nella navigazione post-query e nel successo della ricerca degli utenti che classifichiamo come avanzati -LRB- in base al loro utilizzo degli operatori di query -RRB-, rispetto a quelli classificati come non- Avanzate. I nostri risultati hanno implicazioni su come gli utenti avanzati dovrebbero essere supportati durante le loro ricerche e su come le loro interazioni potrebbero essere utilizzate per aiutare gli utenti di tutti i livelli di esperienza a trovare informazioni pi\u00f9 pertinenti e ad apprendere strategie di ricerca migliorate. 1. INTRODUZIONE La formulazione di affermazioni di query che catturino sia gli aspetti salienti dei bisogni informativi sia che siano significativi per i sistemi di Information Retrieval -LRB- IR -RRB- pone una sfida per molti ricercatori -LSB- 3 -RSB-. Queste tecniche possono essere utili per migliorare la precisione dei risultati, tuttavia, oltre alle analisi logaritmiche -LRB- ad esempio, -LSB- 15 -RSB- -LSB- 27 -RSB- -RRB-, sono state generalmente trascurate dalla comunit\u00e0 di ricerca nei tentativi per migliorare la qualit\u00e0 dei risultati di ricerca. La ricerca IR si \u00e8 generalmente concentrata su modi alternativi con cui gli utenti possono specificare le proprie esigenze piuttosto che aumentare l'adozione della sintassi avanzata. La ricerca sulle tecniche pratiche per integrare la tecnologia di ricerca esistente e supportare gli utenti si \u00e8 intensificata negli ultimi anni -LRB- ad es. -LSB- 18 -RSB- -LSB- 34 -RSB- -RRB-. Tuttavia, \u00e8 difficile implementare tali tecniche su larga scala con latenze tollerabili. Le query tipiche inviate ai motori di ricerca Web assumono la forma di una serie di token separati da spazi. Generalmente \u00e8 presente un operatore booleano AND implicito tra i token che limita i risultati della ricerca ai documenti contenenti tutti i termini di query. De Lima e Pedersen -LSB- 7 -RSB- hanno studiato l'effetto dell'analisi, del riconoscimento delle frasi e dell'espansione sulle query di ricerca sul Web. Hanno dimostrato che il riconoscimento automatico delle frasi nelle query pu\u00f2 migliorare la precisione dei risultati nella ricerca sul Web. Tuttavia, il valore della sintassi avanzata per i ricercatori tipici \u00e8 stato generalmente limitato, poich\u00e9 la maggior parte degli utenti non conosce la sintassi avanzata o non capisce come usarla -LSB- 15 -RSB-. In questo articolo esploriamo l'uso degli operatori di query in maggiore dettaglio e proponiamo applicazioni alternative che non richiedono a tutti gli utenti di utilizzare esplicitamente la sintassi avanzata. Ipotizziamo che gli utenti che utilizzano la sintassi avanzata delle query dimostrino un grado di esperienza nella ricerca che la maggior parte della popolazione degli utenti non possiede; un'affermazione supportata dalla ricerca precedente -LSB- 13 -RSB-.Lo studio del comportamento di questi utenti avanzati dei motori di ricerca pu\u00f2 fornire importanti informazioni sulla ricerca e sulla navigazione dei risultati da cui altri potrebbero trarre vantaggio. Utilizzando i log raccolti da un gran numero di utenti consenzienti, esaminiamo le differenze tra il comportamento di ricerca di coloro che utilizzano la sintassi avanzata e quelli che non lo fanno, e le differenze nelle informazioni a cui vengono indirizzati gli utenti. Siamo interessati a rispondere a tre domande di ricerca: -LRB- i -RRB- Esiste una relazione tra l'uso della sintassi avanzata e altre caratteristiche di una ricerca? -LRB- ii -RRB- Esiste una relazione tra l'uso della sintassi avanzata e i comportamenti di navigazione post-query? -LRB- iii -RRB- Esiste una relazione tra l'uso della sintassi avanzata e le misure del successo della ricerca? Attraverso uno studio e un\u2019analisi sperimentale, offriamo potenziali risposte per ciascuna di queste domande. Una relazione tra l'uso della sintassi avanzata e una qualsiasi di queste funzionalit\u00e0 potrebbe supportare la progettazione di sistemi su misura per gli utenti avanzati dei motori di ricerca o utilizzare le interazioni degli utenti avanzati per aiutare gli utenti non avanzati ad avere pi\u00f9 successo nelle loro ricerche. Descriviamo il lavoro correlato nella Sezione 2, i dati che abbiamo utilizzato in questo studio basato sui log nella Sezione 3, le caratteristiche di ricerca su cui focalizziamo la nostra analisi nella Sezione 4 e i risultati di questa analisi nella Sezione 5. 2. LAVORO CORRELATO Fattori come la mancanza di conoscenza del dominio, la scarsa comprensione della raccolta di documenti oggetto della ricerca e un bisogno di informazioni poco sviluppato possono influenzare la qualit\u00e0 delle query che gli utenti inviano ai sistemi IR -LRB- -LSB- 24 -RSB-, -LSB- 28 -RSB- -RRB-. Sono state condotte numerose ricerche sui diversi modi per aiutare gli utenti a specificare le proprie esigenze di informazione in modo pi\u00f9 efficace. Belkin et al. -LSB- 4 -RSB- ha sperimentato fornendo spazio aggiuntivo affinch\u00e9 gli utenti potessero digitare una descrizione pi\u00f9 dettagliata delle loro esigenze di informazione. Un approccio simile \u00e8 stato tentato da Kelly et al. -LSB- 18 -RSB-, che ha utilizzato moduli di chiarimento per ottenere dagli utenti informazioni aggiuntive sul context di ricerca. Questi approcci si sono dimostrati efficaci nei sistemi di recupero della corrispondenza migliore in cui query pi\u00f9 lunghe generalmente portano a risultati di ricerca pi\u00f9 pertinenti -LSB- 4 -RSB-. Tuttavia, nella ricerca sul Web, dove molti sistemi sono basati su un modello di recupero booleano esteso, query pi\u00f9 lunghe possono effettivamente compromettere le prestazioni di recupero, portando al recupero di un numero limitato di risultati potenzialmente irrilevanti. Non \u00e8 semplicemente sufficiente richiedere maggiori informazioni agli utenti; queste informazioni devono essere di migliore qualit\u00e0. Il feedback sulla pertinenza -LRB- RF -RRB- -LSB- 22 -RSB- e l'espansione interattiva delle query -LSB- 9 -RSB- sono tecniche popolari che sono state utilizzate per migliorare la qualit\u00e0 delle informazioni che gli utenti forniscono ai sistemi IR in merito alle loro esigenze informative . Nel caso di RF, l'utente presenta al sistema esempi di informazioni rilevanti che vengono poi utilizzati per formulare una query migliorata o recuperare una nuova serie di documenti.Si \u00e8 dimostrato difficile convincere gli utenti a utilizzare RF nel dominio Web a causa della difficolt\u00e0 nel trasmettere il significato e i vantaggi della RF agli utenti tipici -LSB- 17 -RSB-. I suggerimenti sulle query offerti in base ai log delle query hanno il potenziale per migliorare le prestazioni di recupero con un carico utente limitato. Questo approccio si limita alla riesecuzione delle query pi\u00f9 frequenti e gli utenti spesso ignorano i suggerimenti presentati loro -LSB- 1 -RSB-. Inoltre, entrambe queste tecniche non aiutano gli utenti a imparare a produrre query pi\u00f9 efficaci. La maggior parte dei motori di ricerca commerciali fornisce una sintassi di query avanzata che consente agli utenti di specificare le proprie esigenze di informazioni in modo pi\u00f9 dettagliato. Gli operatori booleani -LRB- AND, OR e NOT -RRB- possono unire termini e frasi e modificatori come ``site:'' e ``link:'' possono essere utilizzati per limitare lo spazio di ricerca. Le query create con queste tecniche possono essere potenti. L'analisi basata sui log delle interazioni degli utenti con i motori di ricerca Excite e AltaVista ha dimostrato che solo il 10-20% delle query conteneva una sintassi avanzata -LSB- 14 -RSB- -LSB- 25 -RSB-. Questa analisi pu\u00f2 essere un modo utile per acquisire caratteristiche degli utenti che interagiscono con i sistemi IR. La ricerca sulla modellazione degli utenti -LSB- 6 -RSB- e sulla personalizzazione -LSB- 30 -RSB- ha dimostrato che la raccolta di pi\u00f9 informazioni sugli utenti pu\u00f2 migliorare l'efficacia delle ricerche, ma richiede pi\u00f9 informazioni sugli utenti di quelle generalmente disponibili solo dai log di interazione. A meno che non sia abbinato a una tecnica qualitativa, come un questionario post-sessione -LSB- 23 -RSB-, pu\u00f2 essere difficile associare le interazioni alle caratteristiche dell'utente. Nel nostro studio ipotizziamo che, data la difficolt\u00e0 nell'individuare funzionalit\u00e0 di ricerca avanzata all'interno della tipica interfaccia di ricerca e i potenziali problemi nella comprensione della sintassi, quegli utenti che utilizzano regolarmente la sintassi avanzata rappresentano una classe distinta di ricercatori che mostreranno altri comportamenti di ricerca comuni . Altri studi sui comportamenti di ricerca dei ricercatori avanzati hanno tentato di comprendere meglio la conoscenza strategica che hanno acquisito. Tuttavia, possono fornire informazioni preziose sui comportamenti degli utenti con competenze di dominio, sistema o ricerca che superano quelle dell'utente medio. In particolare, il comportamento delle query \u00e8 stato ampiamente studiato per comprendere meglio gli utenti -LSB- 31 -RSB- e supportare altri utenti -LSB- 16 -RSB-. In questo articolo studiamo altre caratteristiche di ricerca degli utenti della sintassi avanzata nel tentativo di determinare se c'\u00e8 qualcosa di diverso nel modo in cui questi utenti dei motori di ricerca effettuano le ricerche e se le loro ricerche possono essere utilizzate a vantaggio di coloro che non utilizzano le funzionalit\u00e0 avanzate dei motori di ricerca. Per fare ci\u00f2 utilizziamo i log di interazione raccolti da un ampio insieme di utenti consenzienti per un periodo prolungato. Nella sezione successiva descriviamo i dati che utilizziamo per studiare il comportamento degli utenti che utilizzano la sintassi avanzata, rispetto a quelli che non utilizzano questa sintassi. Atti SIGIR 2007 Sessione 11:Comportamento dell'interazione come query, clic sui risultati, navigazione post-query e successo della ricerca. La classificazione approssimativa degli utenti basata su una sola caratteristica facilmente estraibile dal flusso di query produce risultati notevoli sul comportamento di interazione degli utenti che non utilizzano la sintassi e di quelli che la usano. Come abbiamo suggerito, i sistemi di ricerca possono sfruttare le interazioni di questi utenti per migliorare il posizionamento dei documenti, consigliare le pagine o persino formare gli utenti.", "keyphrases": ["motore di ricerca", "queri", "informazioni rilevanti", "strategie di ricerca", "latente tollerante", "sintassi avanzata", "comportamento di navigazione", "comportamento di ricerca", "successo della ricerca", "feedback rilevante", "rilev"]}
{"file_name": "J-8", "text": "Forte equilibrio nei giochi di connessione con condivisione dei costi * ABSTRACT In questo lavoro studiamo giochi di connessione con condivisione dei costi, in cui ogni giocatore ha una sorgente e un pozzo che vorrebbe connettere, e il costo dei bordi \u00e8 condiviso equamente -LRB- giochi di connessione equi - RRB- o in modo arbitrario -LRB- giochi di connessione generali -RRB-. Studiamo le topologie dei grafi che garantiscono l'esistenza di un equilibrio forte -LRB- dove nessuna coalizione pu\u00f2 migliorare il costo di ciascuno dei suoi membri -RRB- indipendentemente dai costi specifici sui bordi. I nostri principali risultati di esistenza sono i seguenti: -LRB- 1 -RRB- Per una singola sorgente e pozzo mostriamo che esiste sempre un equilibrio forte -LRB- sia per giochi di connessione equi che generali -RRB-. -LRB- 2 -RRB- Per una singola sorgente con pi\u00f9 sink mostriamo che per un grafo parallelo in serie esiste sempre un equilibrio forte -LRB- sia per giochi di connessione equi che generali -RRB-. -LRB- 3 -RRB- Per multi source e sink mostriamo che un grafo parallelo di estensione ammette sempre un equilibrio forte nei giochi di connessione equi. Per quanto riguarda la qualit\u00e0 dell'equilibrio forte mostriamo che in ogni gioco di connessione equa il costo di un equilibrio forte \u00e8 \u0398 -LRB- log n -RRB- dalla soluzione ottima, dove n \u00e8 il numero di giocatori. -LRB- Questo dovrebbe essere contrapposto al prezzo \u03a9 -LRB- n -RRB- dell'anarchia per la stessa impostazione. -RRB- Per i giochi a connessione generale a sorgente singola e i giochi a connessione equa a singolo sink a sorgente singola, mostriamo che un equilibrio forte \u00e8 sempre una soluzione ottima. * Ricerca finanziata in parte da un finanziamento della Israel Science Foundation, della Binational Science Foundation -LRB- BSF -RRB-, della GermanIsraeli Foundation -LRB- GIF -RRB-, della Lady Davis Fellowship, di un premio della facolt\u00e0 IBM e del programma IST della Comunit\u00e0 Europea, nell'ambito della Rete di Eccellenza PASCAL, IST-2002-506778. Questa pubblicazione riflette solo le opinioni degli autori. 1. INTRODUZIONE La teoria dei giochi computazionali ha introdotto la questione degli incentivi in \u200b\u200bmolti dei classici problemi di ottimizzazione combinatoria. Consideriamo i classici problemi di routing e trasporto come i problemi multicast o multi-commodity, che molte volte vengono visti come segue. Ci viene fornito un grafico con i costi edge e le richieste di connettivit\u00e0 tra i nodi e il nostro obiettivo \u00e8 trovare una soluzione a costo minimo. Il punto di vista della teoria dei giochi presupporrebbe che ogni domanda individuale sia controllata da un giocatore che ottimizza la propria utilit\u00e0, e il risultato risultante potrebbe essere lontano dalla soluzione ottimale. Quando si considerano gli incentivi individuali \u00e8 necessario discutere il concetto di soluzione appropriata. Gran parte della ricerca sulla teoria computazionale dei giochi si \u00e8 concentrata sul classico equilibrio di Nash come concetto di soluzione primaria. In effetti l\u2019equilibrio di Nash presenta molti vantaggi e, cosa pi\u00f9 importante, esiste sempre -LRB- nelle strategie miste -RRB-. Tuttavia, il concetto di soluzione dell\u2019equilibrio di Nash \u00e8 resistente solo a deviazioni unilaterali, mentre in realt\u00e0 i giocatori potrebbero essere in grado di coordinare le proprie azioni.Un equilibrio forte -LSB- 4 -RSB- \u00e8 uno stato dal quale nessuna coalizione -LRB- di qualsiasi dimensione -RRB- pu\u00f2 discostarsi e migliorare l\u2019utilit\u00e0 di ogni membro della coalizione -LRB- riducendo al contempo l\u2019utilit\u00e0 dei giocatori esterni all\u2019equilibrio. coalizione -RRB-. Questa resilienza alle deviazioni delle coalizioni degli attori \u00e8 molto attraente, e si pu\u00f2 sperare che, una volta raggiunto un equilibrio forte, esso abbia molte probabilit\u00e0 di mantenersi. Da un punto di vista della teoria dei giochi computazionale, un ulteriore vantaggio di un equilibrio forte \u00e8 che ha il potenziale di ridurre la distanza tra la soluzione ottimale e la soluzione ottenuta come risultato di un comportamento egoistico. Il prezzo forte dell\u2019anarchia -LRB- SPoA -RRB-, introdotto in -LSB- 1 -RSB-, \u00e8 il rapporto tra il costo del peggior equilibrio forte e il costo di una soluzione ottima. Ovviamente, SPoA ha significato solo nei casi in cui esiste un equilibrio forte. Uno dei principali svantaggi dell\u2019equilibrio forte \u00e8 che la maggior parte dei giochi non ammette alcun equilibrio forte. Anche semplici giochi classici come il dilemma del prigioniero non possiedono alcun equilibrio forte -LRB- che \u00e8 anche un esempio di gioco di congestione che non possiede equilibri forti -RRB-. Questo fatto sfortunato ha ridotto la concentrazione nell'equilibrio forte, nonostante le sue propriet\u00e0 altamente attrattive. In questo lavoro ci concentreremo sui giochi di connessione a ripartizione dei costi, introdotti da -LSB- 3, 2 -RSB-. In un gioco del genere, esiste un grafico diretto sottostante con costi marginali e i singoli utenti hanno richieste di connettivit\u00e0 -LRB- tra una sorgente e un sink -RRB-. Consideriamo due modelli. Il modello di connessione a costo equo -LSB- 2 -RSB- consente a ciascun giocatore di selezionare un percorso dalla sorgente al sink2. In questo gioco il costo di un bordo \u00e8 diviso equamente tra tutti i giocatori che hanno selezionato il bordo, e il costo del giocatore \u00e8 la somma dei suoi costi sui bordi che ha selezionato. Il gioco di connessione generale -LSB- 3 -RSB- consente a ciascun giocatore di offrire prezzi per i bordi. In questo gioco un vantaggio viene acquistato se la somma delle offerte copre almeno il suo costo, e il costo del giocatore \u00e8 la somma delle sue offerte sui margini acquistati -LRB- in entrambi i giochi assumiamo che il giocatore debba garantire il connettivit\u00e0 tra la sua sorgente e il suo sink -RRB-. In questo lavoro ci concentriamo su due questioni importanti. Il primo \u00e8 identificare in quali condizioni \u00e8 garantita l'esistenza di un equilibrio forte, e il secondo \u00e8 la qualit\u00e0 degli equilibri forti. Per quanto riguarda l'esistenza, identifichiamo famiglie di topologie di grafi che possiedono un forte equilibrio per qualsiasi assegnazione di costi degli archi. Si pu\u00f2 vedere questa separazione tra la topologia del grafo e i costi dei margini, come una separazione tra l\u2019infrastruttura sottostante e i costi che i giocatori osservano per acquistare i margini. Mentre ci si aspetta che l\u2019infrastruttura sia stabile per lunghi periodi di tempo, i costi osservati dagli attori possono essere facilmente modificati in brevi periodi di tempo. I nostri risultati sono i seguenti.Per il caso della singola merce -LRB- tutti i giocatori hanno la stessa sorgente e lo stesso pozzo -RRB-, esiste un forte equilibrio in qualsiasi grafico -LRB- sia per i giochi equi che per i giochi di connessione generale -RRB-. Inoltre, l'equilibrio forte \u00e8 anche s mentre \u00e8 noto che qualsiasi gioco di congestione ammette almeno un equilibrio di Nash nelle strategie pure -LSB- 16 -RSB-. 2Lo schema di equa condivisione dei costi \u00e8 interessante anche dal punto di vista della progettazione del meccanismo, in quanto \u00e8 un meccanismo di condivisione dei costi a prova di strategia -LSB- 14 -RSB-. la soluzione ottimale -LRB- vale a dire, i giocatori condividono un percorso pi\u00f9 breve dalla sorgente comune al pozzo comune -RRB-. Per il caso di una singola sorgente e pi\u00f9 pozzi -LRB-, ad esempio, in un albero multicast -RRB-, mostriamo che in un gioco di connessioni eque c'\u00e8 un forte equilibrio se il grafo sottostante \u00e8 un grafo parallelo in serie, e mostriamo un esempio di grafo parallelo non serie che non ha un equilibrio forte. Per il caso di multi-commodity -LRB- multi sorgenti e pozzi -RRB-, mostriamo che in un gioco di connessione equa se il grafico \u00e8 un grafo parallelo di estensione allora c'\u00e8 sempre un equilibrio forte, e mostriamo un esempio di una serie grafico parallelo che non ha un equilibrio forte. Per quanto ne sappiamo, siamo i primi a fornire una caratterizzazione topologica dell\u2019esistenza dell\u2019equilibrio nei giochi di rete multi-commodity e single-source. Per ogni gioco di connessione equa mostriamo che se esiste un equilibrio forte \u00e8 al massimo un fattore di \u0398 -LRB- log n -RRB- dalla soluzione ottima, dove n \u00e8 il numero di giocatori. Questo dovrebbe essere contrapposto al limite \u0398 -LRB- n -RRB- che esiste per il prezzo dell\u2019anarchia -LSB- 2 -RSB-. Per i giochi di connessione generale a sorgente singola, mostriamo che qualsiasi grafo parallelo in serie possiede un equilibrio forte e mostriamo un esempio di un grafico che non ha un equilibrio forte. In questo caso dimostriamo anche che qualsiasi equilibrio forte \u00e8 ottimo. Lavoro correlato Le caratterizzazioni topologiche per i giochi di rete di un singolo bene sono state recentemente fornite per varie propriet\u00e0 di equilibrio, tra cui l'esistenza dell'equilibrio -LSB- 12, 7, 8 -RSB-, l'unicit\u00e0 dell'equilibrio -LSB- 10 -RSB- e l'efficienza dell'equilibrio -LSB- 17 , 11 -RSB-. L'esistenza di un equilibrio di Nash puro nei giochi di congestione della rete di un singolo bene con costi o pesi specifici del giocatore \u00e8 stata studiata in -LSB- 12 -RSB-. L'esistenza di un equilibrio forte \u00e8 stata studiata sia in -LRB- con utilit\u00e0 decrescente, ad esempio nei giochi di congestione -RRB- di routing, sia in quelli con utilit\u00e0 crescente -LRB-, ad esempio nei giochi di congestione con equa condivisione dei costi. -LSB-7, 8 -RSB- hanno fornito una caratterizzazione topologica completa dell'esistenza di un SE nei giochi di congestione che riducono l'utilit\u00e0 di un singolo bene, e hanno dimostrato che un SE esiste sempre se e solo se il grafico sottostante \u00e8 parallelo all'estensione. -LSB- 19 -RSB- hanno dimostrato che nei giochi di congestione che aumentano l'utilit\u00e0 di un singolo bene, la caratterizzazione topologica \u00e8 essenzialmente equivalente ai collegamenti paralleli. Inoltre,hanno dimostrato che questi risultati valgono anche per gli equilibri forti correlati -LRB- in contrasto con l'impostazione decrescente, dove gli equilibri forti correlati potrebbero non esistere affatto -RRB-. Sebbene i giochi a condivisione dei costi equi che studiamo siano giochi di congestione della rete che aumentano l'utilit\u00e0, ne ricaviamo una caratterizzazione diversa rispetto a -LSB- 19 -RSB- a causa delle diverse ipotesi riguardanti le azioni dei giocatori.3 4. GIOCHI DI CONNESSIONE GENERALE In questa sezione, ricavare i nostri risultati per i giochi di connessione generali. 4.1 Esistenza di un equilibrio forte Iniziamo con una caratterizzazione dell'esistenza di un equilibrio forte nei giochi di connessione generale simmetrici. Similmente al Teorema 3.1 -LRB- utilizzando una dimostrazione simile -RRB- stabiliamo, TEOREMA 4.1. In ogni gioco simmetrico di connessione equa esiste un equilibrio forte. Anche se ogni gioco di connessione generale a sorgente singola possiede un equilibrio di Nash puro -LSB- 3 -RSB-, non ammette necessariamente un equilibrio forte.11 Il gioco di connessione equa ha ispirato questo esempio. TEOREMA 4.2. Esiste un gioco di connessione generale a sorgente unica che non ammette alcun equilibrio forte. PROVA. Consideriamo un gioco di connessione generale a fonte singola con 3 giocatori sul grafico illustrato nella Figura 4. Abbiamo dimostrato che nessuno dei NE \u00e8 SE, e quindi il gioco non possiede alcun SE. Successivamente mostriamo che per la classe dei grafi paralleli in serie, esiste sempre un equilibrio forte nel caso di una singola sorgente. PROVA. Sia \u039b un gioco di connessione generale a sorgente singola su un SPG G = -LRB- V, E -RRB- con sorgente se sink t. Consideriamo innanzitutto il seguente ordine parziale tra i giocatori. Per i giocatori i e j, abbiamo che i \u2192 j se esiste un percorso diretto da ti a tj. L'algoritmo COMPUTE-SE, considera i giocatori in ordine crescente, a partire dal giocatore 1. Ogni giocatore i acquister\u00e0 interamente un sottoinsieme degli archi, e ogni giocatore j > i considerer\u00e0 il costo di quelli -LRB- acquistati -RRB- bordi come zero. Quando COMPUTE-SE considera il giocatore j, il costo dei bordi che i giocatori da 1 a j \u2212 1 hanno acquistato \u00e8 fissato a zero, e il giocatore j acquista interamente un percorso minimo Qj da s a tj. Vale a dire, per ogni arco e G Qj \\ Ui < jQi abbiamo pj -LRB- e -RRB- = ce altrimenti pj -LRB- e -RRB- = 0. Mostriamo poi che l'algoritmo COMPUTESE calcola un SE. Supponiamo per assurdo che il profilo p non sia un SE. Allora esiste una coalizione che pu\u00f2 migliorare i costi di tutti i suoi attori attraverso una deviazione. Sia \u0393 una tale coalizione di dimensione minima e sia il giocatore i = max -LCB- j G \u0393 -RCB-. Per un giocatore j G \u0393 siano \u00af Qj e \u00af pj rispettivamente il percorso e il pagamento del giocatore j dopo la deviazione. Sia Q ' un percorso dal pozzo del giocatore i, cio\u00e8 ti, al pozzo di G, iet Allora Q = \u00af Qi UQ ' \u00e8 un percorso dalla sorgente s al pozzo t. Per ogni giocatore j < i, sia yj il vertice di intersezione di Q e tj -LRB- per il Lemma 2.1 \u00e8 garantito che esista -RRB-. Sia y il vertice pi\u00f9 lontano sul cammino Q tale che y = yj per qualche j < i.Il percorso dalla sorgente s al nodo y \u00e8 stato interamente pagato dai giocatori j < i in p -LRB- prima della deviazione -RRB-. I casi che consideriamo sono due. caso a : Dopo la deviazione il giocatore i non paga per i bordi in U j \u2208 \u0393 \\ -LCB- i -RCB- \u00af Qj. Prima della deviazione della coalizione \u0393, il percorso da s a y veniva interamente pagato dai giocatori j < i. Successivamente mostriamo che nessun giocatore k > i paga per alcun vantaggio su qualsiasi percorso da s a ti. Consideriamo un giocatore k > i e sia Q0k = Qk U Q00k, dove Q00k \u00e8 un percorso che collega tk a t. Sia yk il vertice di intersezione di Q0k e ti. Poich\u00e9 esiste un percorso da s a yk che \u00e8 stato interamente pagato dai giocatori j < k prima della deviazione, in particolare il percorso Qis, yk, il giocatore k non pagher\u00e0 alcun vantaggio su qualsiasi percorso che collega s e yk. Pertanto il giocatore i paga interamente per tutti gli archi sul percorso \u00af Qiy, ti, cio\u00e8, \u00af pi -LRB- e -RRB- = ce per tutti gli archi e E \u00af Qiy, ti. Consideriamo ora l'algoritmo COMPUTESE nel passo in cui il giocatore i seleziona il percorso pi\u00f9 breve dalla sorgente s al suo sink ti e determina il suo pagamento pi. A questo punto il giocatore i potrebbe acquistare il percorso \u00af Qiy, ti, poich\u00e9 il percorso da s a y \u00e8 gi\u00e0 stato pagato dai giocatori j < i. Quindi, ci -LRB- \u00af p -RRB- > ci -LRB- p -RRB-. Ci\u00f2 contraddice il fatto che il giocatore i ha migliorato il proprio costo e quindi non tutti i giocatori in \u0393 riducono il proprio costo. Ci\u00f2 implica che p \u00e8 un equilibrio forte. 4.2 Forte prezzo dell'anarchia Mentre per ogni gioco con connessione generale a sorgente singola, vale che PoS = 1 -LSB- 3 -RSB-, il prezzo dell'anarchia pu\u00f2 essere grande quanto n, anche per due archi paralleli. Qui mostriamo che qualsiasi equilibrio forte nei giochi di connessione generale a fonte singola produce il costo ottimale. PROVA. Sia p = -LRB- p1,..., pn -RRB- un equilibrio forte, e sia T \u2217 l'albero di Steiner di costo minimo su tutti i giocatori, radicato nella sorgente -LRB- singola -RRB- s. Sia Te \u2217 il sottoalbero di T \u2217 disconnesso da s quando viene rimosso l'arco e. Sia \u0393 -LRB- Te -RRB- l'insieme dei giocatori che hanno pozzi in Te. Per un insieme di archi E, sia c -LRB- E -RRB- = Ee \u2208 E ce. Assumiamo per assurdo che c -LRB- p -RRB- > c -LRB- T \u2217 -RRB-. Mostreremo che esiste un sottoalbero T0 di T \u2217, che connette un sottoinsieme di giocatori \u0393 C _ N, e un nuovo insieme di pagamenti \u00af p, tale che per ogni i E \u0393, ci -LRB- \u00af p - RRB- < ci -LRB- p -RRB-. Ci\u00f2 contraddir\u00e0 l\u2019ipotesi che p sia un equilibrio forte. Per prima cosa mostriamo come trovare un sottoalbero T0 di T \u2217, tale che per ogni arco e, i pagamenti dei giocatori con sink in Te \u2217 siano maggiori del costo di Te \u2217 U -LCB- e -RCB-. Per costruire T0, definire un arco e come cattivo se il costo di Te \u2217 U -LCB- e -RCB- \u00e8 almeno il pagamento dei giocatori con pozzi in Te \u2217, cio\u00e8 c -LRB- Te \u2217 U -LCB - e -RCB- -RRB- > P -LRB- Te \u2217 -RRB-. Sia B l\u2019insieme degli archi difettosi. Pertanto, in T0 per ogni arco e, abbiamo che c -LRB- Te0 U -LCB- e -RCB- -RRB- < P -LRB- T0e -RRB-.Ci\u00f2 che resta \u00e8 trovare i pagamenti p \u00af per i giocatori in \u0393 -LRB- T0 -RRB- tali che compreranno l'albero T0 e ogni giocatore in \u0393 -LRB- T0 -RRB- abbasser\u00e0 il suo costo, cio\u00e8 ci -LRB- p -RRB- > ci -LRB- \u00af p -RRB- per i E \u0393 -LRB- T0 -RRB-. -LRB- Ricordiamo che i pagamenti hanno la restrizione che il giocatore i pu\u00f2 pagare solo per i bordi sul percorso da s a ti. -RRB- Definiremo ora i pagamenti della coalizione \u00af p. Siano ci -LRB- \u00af p, T0 e \u2208 Te \u00af pi -LRB- e -RRB- i pagamenti del giocatore i per il sottoalbero T0e. Consideriamo il seguente processo bottom up che definisce \u00af p. Assegniamo i pagamenti del bordo e in T0, dopo aver assegnato i pagamenti a tutti i bordi in T0e. Possiamo quindi aggiornare i pagamenti p \u00af dei giocatori i E \u0393 -LRB- T0e -RRB-, impostando dove abbiamo utilizzato il fatto che E e -RRB-.", "keyphrases": ["gioco di connessione a condivisione dei costi", "numero di giocatori", "sorgente e lavandino unici", "lavello multiplo a sorgente singola", "multifonte e lavandino", "costo del bordo", "gioco di connessione equa", "gioco di connessione gener", "topologia del grafico", "forte equilibrio", "coalizione", "costo specifico", "estende il grafico parallelo", "soluzione ottim"]}
{"file_name": "C-3", "text": "Applicazioni auto-adattative sulla griglia Abstract Le griglie sono intrinsecamente eterogenee e dinamiche. Un problema importante nel grid computing \u00e8 la selezione delle risorse, ovvero la ricerca di un insieme di risorse appropriato per l'applicazione. Un altro problema \u00e8 l\u2019adattamento alle mutevoli caratteristiche dell\u2019ambiente della rete. Le soluzioni esistenti a questi due problemi richiedono che sia noto un modello di prestazioni per un'applicazione. Tuttavia, costruire tali modelli \u00e8 un compito complesso. In questo articolo analizziamo un approccio che non richiede modelli prestazionali. Avviamo un'applicazione su qualsiasi insieme di risorse. Durante l'esecuzione dell'applicazione, raccogliamo periodicamente le statistiche sull'esecuzione dell'applicazione e deduciamo i requisiti dell'applicazione da queste statistiche. Quindi, adattiamo il set di risorse per adattarlo meglio alle esigenze dell'applicazione. Questo approccio ci consente di evitare colli di bottiglia nelle prestazioni, come collegamenti WAN sovraccarichi o processori molto lenti, e quindi pu\u00f2 produrre miglioramenti significativi delle prestazioni. Valutiamo il nostro approccio in una serie di scenari tipici della Grid. 1. Introduzione Negli ultimi anni il grid computing \u00e8 diventato una reale alternativa al tradizionale calcolo parallelo. Una griglia fornisce molta potenza computazionale e quindi offre la possibilit\u00e0 di risolvere problemi molto grandi, soprattutto se le applicazioni possono essere eseguite su pi\u00f9 siti contemporaneamente -LRB-7; 15; 20 -RRB-. Tuttavia, anche la complessit\u00e0 degli ambienti Grid \u00e8 molte volte maggiore di quella delle tradizionali macchine parallele come cluster e supercomputer. Un problema importante \u00e8 la selezione delle risorse, ovvero la selezione di un insieme di nodi di calcolo in modo tale che l'applicazione raggiunga buone prestazioni. In un ambiente di griglia questo problema \u00e8 ancora pi\u00f9 difficile, a causa dell'eterogeneit\u00e0 delle risorse: i nodi di calcolo hanno vari Un altro problema importante \u00e8 che le prestazioni e la disponibilit\u00e0 delle risorse di griglia variano nel tempo: i collegamenti di rete o i nodi di calcolo potrebbero sovraccaricarsi, o i nodi di calcolo potrebbero non essere pi\u00f9 disponibili a causa di arresti anomali o perch\u00e9 sono stati richiesti da un'applicazione con priorit\u00e0 pi\u00f9 elevata. Inoltre, potrebbero diventare disponibili risorse nuove e migliori. Per mantenere un livello di prestazioni ragionevole, l'applicazione deve quindi adattarsi alle mutevoli condizioni. Il problema dell'adattamento pu\u00f2 essere ridotto al problema della selezione delle risorse: la fase di selezione delle risorse pu\u00f2 essere ripetuta durante l'esecuzione dell'applicazione, sia a intervalli regolari, sia quando viene rilevato un problema di prestazioni, o quando diventano disponibili nuove risorse. Questo approccio \u00e8 stato adottato da numerosi sistemi -LRB-5; 14; 18 -RRB-. Per la selezione delle risorse, il tempo di esecuzione dell'applicazione viene stimato per alcuni set di risorse e viene selezionato per l'esecuzione il set che fornisce il tempo di esecuzione pi\u00f9 breve. Per prevedere il tempo di esecuzione dell'applicazione su un determinato insieme di risorse, tuttavia, \u00e8 necessaria la conoscenza dell'applicazione. Tipicamente, viene utilizzato un modello di prestazione analitica,ma costruire un modello del genere \u00e8 intrinsecamente difficile e richiede competenze che i programmatori di applicazioni potrebbero non avere. In questo articolo introduciamo e valutiamo un approccio alternativo all'adattamento delle applicazioni e alla selezione delle risorse che non necessita di un modello di prestazioni. Avviamo un'applicazione su qualsiasi insieme di risorse. Durante l'esecuzione dell'applicazione, raccogliamo periodicamente informazioni sui tempi di comunicazione e sui tempi di inattivit\u00e0 dei processori. Utilizziamo queste statistiche per stimare automaticamente i requisiti di risorse dell'applicazione. Successivamente, adattiamo il set di risorse su cui \u00e8 in esecuzione l'applicazione aggiungendo o rimuovendo nodi di calcolo o anche interi cluster. I processori vengono aggiunti o eliminati per rimanere entro le soglie, adattandosi cos\u00ec automaticamente all'ambiente in evoluzione. Uno dei principali vantaggi del nostro approccio \u00e8 che migliora le prestazioni delle applicazioni in molte situazioni diverse tipiche del grid computing. Gestisce tutti i seguenti casi: Il nostro lavoro presuppone che l'applicazione sia malleabile e possa essere eseguita -LRB- in modo efficiente -RRB- su pi\u00f9 siti di una griglia -LRB- ovvero utilizzando la co-allocazione -LRB- 15 -RRB- -RRB- . latenze di zona. Abbiamo applicato le nostre idee alle applicazioni \u201cdivide et impera\u201d che soddisfano questi requisiti. Il divide et impera ha dimostrato di essere un paradigma attraente per la programmazione delle applicazioni grid -LRB-4; 20 -RRB-. Riteniamo che il nostro approccio possa essere esteso ad altre classi di applicazioni con i presupposti dati. Abbiamo implementato la nostra strategia in Satin, che \u00e8 un framework incentrato su Java per la scrittura di applicazioni divide et impera abilitate alla griglia -LRB- 20 -RRB-. Il resto di questo documento \u00e8 strutturato come segue. Nella Sezione 2 spieghiamo quali ipotesi stiamo facendo riguardo alle applicazioni e alle risorse di rete. Nella Sezione 3 presentiamo la nostra strategia di selezione e adattamento delle risorse. Nella Sezione 4 descriviamo la sua implementazione nel framework Satin. Nella Sezione 5, valutiamo il nostro approccio in una serie di scenari di rete. Nella Sezione 6 confrontiamo il nostro approccio con il lavoro correlato. Infine, nella Sezione 7, concludiamo e descriviamo il lavoro futuro. 2. Context e ipotesi In questa sezione descriviamo le nostre ipotesi sulle applicazioni e sulle loro risorse. Assumiamo il seguente modello di risorsa. Le applicazioni vengono eseguite su pi\u00f9 siti contemporaneamente, dove i siti sono cluster o supercomputer. I processori appartenenti a un sito sono collegati tramite una LAN veloce con bassa latenza e larghezza di banda elevata. I diversi siti sono collegati da una WAN. La comunicazione tra i siti soffre di latenze elevate. Abbiamo studiato il problema dell'adattamento nel context delle applicazioni divide et impera. Riteniamo tuttavia che la nostra metodologia possa essere utilizzata anche per altri tipi di applicazioni. In questa sezione riassumiamo le ipotesi sulle applicazioni che sono importanti per il nostro approccio. La prima ipotesi che facciamo \u00e8 che l'applicazione sia malleabile, cio\u00e8\u00e8 in grado di gestire i processori che si uniscono e lasciano il calcolo in corso. In -LRB- 23 -RRB-, abbiamo mostrato come le applicazioni divide et impera possono essere rese tolleranti ai guasti e malleabili. I processori possono essere aggiunti o rimossi in qualsiasi momento del calcolo con un sovraccarico minimo. Il secondo presupposto \u00e8 che l'applicazione possa essere eseguita in modo efficiente su processori con velocit\u00e0 diverse. Ci\u00f2 pu\u00f2 essere ottenuto utilizzando una strategia di bilanciamento del carico dinamico, come il furto di lavoro utilizzato dalle applicazioni divide et impera -LRB- 19 -RRB-. Inoltre, le applicazioni master-worker utilizzano tipicamente strategie di bilanciamento del carico dinamico -LRB- ad esempio, MW - un framework per scrivere applicazioni master-worker abilitate alla griglia -LRB- 12 -RRB- -RRB-. Riteniamo che sia un presupposto ragionevole per un'applicazione di rete, poich\u00e9 le applicazioni per le quali il processore pi\u00f9 lento diventa un collo di bottiglia non saranno in grado di utilizzare in modo efficiente le risorse di rete. Infine, l'applicazione dovrebbe essere insensibile alle latenze su vasta area, in modo che possa funzionare in modo efficiente su una griglia su vasta area -LRB-16; 17 -RRB-. 6. Lavori correlati Numerosi progetti Grid affrontano la questione della selezione e dell'adattamento delle risorse. In GrADS -LRB- 18 -RRB- e ASSIST -LRB- 1 -RRB-, la selezione e l'adattamento delle risorse richiedono un modello di prestazioni che consenta di prevedere i tempi di esecuzione dell'applicazione. Nella fase di selezione delle risorse, viene esaminato un numero di possibili set di risorse e viene selezionato il set di risorse con il tempo di esecuzione previsto pi\u00f9 breve. Se durante il calcolo viene rilevato un degrado delle prestazioni, la fase di selezione delle risorse viene ripetuta. GrADS utilizza il rapporto tra i tempi di esecuzione previsti -LRB- di determinate fasi dell'applicazione -RRB- e i tempi di esecuzione reali come indicatore delle prestazioni dell'applicazione. ASSIST utilizza il numero di iterazioni per unit\u00e0 di tempo -LRB- per applicazioni iterative -RRB- o il numero di attivit\u00e0 per unit\u00e0 di tempo -LRB- per normali applicazioni master-worker -RRB- come indicatore di prestazione. La principale differenza tra questi approcci e il nostro approccio \u00e8 l\u2019uso di modelli di performance. Il vantaggio principale \u00e8 che una volta noto il modello di prestazione, il sistema \u00e8 in grado di prendere decisioni di migrazione pi\u00f9 accurate rispetto al nostro approccio. Tuttavia, anche se le prestazioni non presentano alcun adattamento con adattamento, \u00e8 noto il problema di trovare un set di risorse ottimale -LRB- ovvero il set di risorse con il tempo di esecuzione minimo - RRB- \u00e8 NP-completo. All\u2019aumentare del numero di risorse di rete disponibili, l\u2019accuratezza di questo approccio diminuisce, poich\u00e9 il sottoinsieme dei possibili insiemi di risorse che possono essere esaminati in un tempo ragionevole diventa pi\u00f9 piccolo. Un altro svantaggio di questi sistemi \u00e8 che il rilevamento del degrado delle prestazioni \u00e8 adatto solo per applicazioni iterative o regolari. Cactus -LRB- 2 -RRB- e GridWay -LRB- 14 -RRB- non utilizzano modelli prestazionali. Tuttavia, questi framework sono adatti solo per applicazioni sequenziali -LRB- GridWay -RRB- o per sito singolo -LRB- Cactus -RRB-.In tal caso, il problema della selezione delle risorse si riduce alla selezione della macchina o del cluster pi\u00f9 veloce. La velocit\u00e0 di clock del processore, il carico medio e il numero di processori in un cluster -LRB- Cactus -RRB- vengono utilizzati per classificare le risorse e viene selezionata la risorsa con il rango pi\u00f9 alto. L'applicazione viene migrata se viene rilevato un peggioramento delle prestazioni o se vengono scoperte risorse migliori. Sia Cactus che GridWay utilizzano il numero di iterazioni per unit\u00e0 di tempo come indicatore di prestazione. Il limite principale di questa metodologia \u00e8 che \u00e8 adatta solo per applicazioni sequenziali o su un singolo sito. Inoltre, la selezione delle risorse in base alla velocit\u00e0 di clock non \u00e8 sempre accurata. Infine, il rilevamento del degrado delle prestazioni \u00e8 adatto solo per applicazioni iterative e non pu\u00f2 essere utilizzato per calcoli irregolari come problemi di ricerca e ottimizzazione. Il problema della selezione delle risorse \u00e8 stato studiato anche dal progetto AppLeS -LRB- 5 -RRB-. Nell'ambito di questo progetto sono state studiate una serie di applicazioni e sono stati creati modelli prestazionali per tali applicazioni. Sulla base di tale modello viene costruito un agente di pianificazione che utilizza il modello di prestazioni per selezionare il miglior set di risorse e la migliore pianificazione delle applicazioni su questo set. Gli agenti di pianificazione AppLeS vengono scritti caso per caso e non possono essere riutilizzati per un'altra applicazione. Sono stati inoltre sviluppati due modelli riutilizzabili per classi specifiche di applicazioni, ovvero le applicazioni master-worker -LRB- modello AMWAT -RRB- e le applicazioni di parametrizzazione -LRB- modello APST -RRB-. \u00c8 iniziato il crash di 2 cluster su 9 aggiungendo nodi 96 nodi raggiunti In -LRB- 13 -RRB- viene studiato il problema della schedulazione delle applicazioni master-worker. Pertanto il problema si riduce a trovare il giusto numero di lavoratori. L'approccio qui \u00e8 simile al nostro in quanto non viene utilizzato alcun modello di prestazione. Invece, il sistema tenta di dedurre i requisiti dell'applicazione in fase di esecuzione e regola il numero di lavoratori per avvicinarsi al numero ideale. 7. Conclusioni e lavoro futuro In questo articolo, abbiamo studiato il problema della selezione e dell'adattamento delle risorse negli ambienti grid. Gli approcci esistenti a questi problemi presuppongono in genere l'esistenza di un modello di prestazioni che consenta di prevedere i tempi di esecuzione delle applicazioni su vari insiemi di risorse. Tuttavia, la creazione di modelli prestazionali \u00e8 intrinsecamente difficile e richiede la conoscenza dell'applicazione. Proponiamo un approccio che non richiede una conoscenza approfondita dell'applicazione. Avviamo l'applicazione su un insieme arbitrario di risorse e ne monitoriamo le prestazioni. Il monitoraggio delle prestazioni ci consente di conoscere determinati requisiti dell'applicazione come il numero di processori necessari all'applicazione o i requisiti di larghezza di banda dell'applicazione. Utilizziamo questa conoscenza per affinare gradualmente il set di risorse rimuovendo i nodi inadeguati o aggiungendo nuovi nodi se necessario. Questo approccio non si traduce nell'insieme di risorse ottimale, ma in un insieme di risorse ragionevole, ad esun set esente da vari colli di bottiglia prestazionali come connessioni di rete lente o processori sovraccarichi. Il nostro approccio consente inoltre all'applicazione di adattarsi alle mutevoli condizioni della rete. Se l\u2019efficienza media ponderata scende al di sotto di un certo livello, il coordinatore dell\u2019adattamento inizia a rimuovere i nodi \u201cpeggiori\u201d. Se l'efficienza media ponderata supera un certo livello, vengono aggiunti nuovi nodi. L'applicazione si adatta in modo completamente automatico alle mutevoli condizioni. Il lavoro futuro comporter\u00e0 l\u2019estensione della nostra strategia di adattamento per sostenere la migrazione opportunistica. Ci\u00f2, tuttavia, richiede pianificatori di rete con funzionalit\u00e0 pi\u00f9 sofisticate di quelle attualmente esistenti. Sono inoltre necessarie ulteriori ricerche per ridurre il sovraccarico del benchmarking. Un'altra linea di ricerca che desideriamo indagare \u00e8 l'utilizzo del controllo del feedback per affinare la strategia di adattamento durante l'esecuzione dell'applicazione. Infine, l'implementazione centralizzata del coordinatore dell'adattamento potrebbe diventare un collo di bottiglia per le applicazioni che funzionano su un numero molto elevato di nodi -LRB- centinaia o migliaia -RRB-.", "keyphrases": ["calcolo della griglia", "selezione delle risorse", "ambiente della griglia", "calcolo parallelo", "ambiente parallelo omogeneo", "eterogeneo di risorse", "rete locale a larghezza di banda elevata", "rete WAN con larghezza di banda inferiore", "collegamento di rete", "tempo comune", "tempo di inattivit\u00e0 del processore", "grado di parallelo", "sovraccarico delle risorse", "divide et impera"]}
{"file_name": "I-19", "text": "Fare offerte ottimali in aste simultanee al secondo prezzo di beni perfettamente sostituibili ABSTRACT Deriviamo strategie di offerta ottimali per un agente offerente globale che partecipa a pi\u00f9 aste simultanee al secondo prezzo con sostituti perfetti. Consideriamo innanzitutto un modello in cui tutti gli altri offerenti sono locali e partecipano a un'unica asta. In questo caso, dimostriamo che, assumendo la libera disposizione, l'offerente globale dovrebbe sempre fare offerte diverse da zero in tutte le aste disponibili, indipendentemente dalla distribuzione della valutazione degli offerenti locali. Inoltre, per distribuzioni di valutazione non decrescenti, dimostriamo che il problema di trovare le offerte ottimali si riduce a due dimensioni. Questi risultati valgono sia nel caso in cui il numero di offerenti locali sia noto sia quando questo numero \u00e8 determinato da una distribuzione di Poisson. Questa analisi si estende ai mercati online dove, in genere, le aste si svolgono sia contemporaneamente che in sequenza. Inoltre, combinando risultati analitici e di simulazione, dimostriamo che risultati simili valgono nel caso di diversi offerenti globali, a condizione che il mercato sia composto sia da offerenti globali che locali. Infine, affronteremo l\u2019efficienza del mercato complessivo e dimostreremo che le informazioni sul numero di offerenti locali sono un fattore determinante per il modo in cui un offerente globale influisce sull\u2019efficienza. 1. INTRODUZIONE Il recente aumento di interesse per le aste online ha portato a un numero crescente di aste che offrono prodotti molto simili o simili. Solo su eBay, ad esempio, ci sono spesso centinaia o talvolta addirittura migliaia di aste simultanee in tutto il mondo che vendono tali articoli sostituibili1. In questo context, \u00e8 essenziale sviluppare strategie di offerta che gli agenti autonomi possano utilizzare per operare in modo efficace in un ampio numero di aste. Come mostreremo, tuttavia, questa analisi \u00e8 rilevante anche per un context pi\u00f9 ampio in cui le aste vengono condotte in sequenza, oltre che contemporaneamente. Al contrario, qui consideriamo strategie di offerta per mercati con pi\u00f9 aste simultanee e sostituti perfetti. In particolare, la nostra attenzione \u00e8 rivolta alle aste Vickrey o alle aste con offerta sigillata di secondo prezzo. Tuttavia, i nostri risultati si generalizzano alle impostazioni con aste inglesi poich\u00e9 queste sono strategicamente equivalenti alle aste di secondo prezzo. In questo context, siamo in grado di caratterizzare, per la prima volta, la strategia di massimizzazione dell'utilit\u00e0 di un offerente per fare offerte simultaneamente in un numero qualsiasi di aste di questo tipo e per qualsiasi tipo di distribuzione della valutazione degli offerenti. Pi\u00f9 in dettaglio, consideriamo innanzitutto un mercato in cui un singolo offerente, chiamato offerente globale, pu\u00f2 fare offerte in un numero qualsiasi di aste, mentre si presuppone che gli altri offerenti, chiamati offerenti locali, facciano offerte solo in una singola asta. In questo caso, otteniamo i seguenti risultati: \u2022 Mentre nel caso di un'asta singola al secondo prezzo, la migliore strategia di un offerente \u00e8 offrire il suo valore reale, la migliore strategia per un offerente globale \u00e8 fare un'offerta inferiore a tale valore. \u2022 Siamo in grado di dimostrare che,anche se un offerente globale richiede un solo articolo, l'utilit\u00e0 attesa viene massimizzata partecipando a tutte le aste in cui viene venduto l'articolo desiderato. \u2022 Trovare l'offerta ottimale per ciascuna asta pu\u00f2 essere un compito arduo se si considerano tutte le possibili combinazioni. 2. LAVORI CORRELATI La ricerca nel campo delle aste simultanee pu\u00f2 essere segmentata lungo due grandi linee. Tali analisi vengono tipicamente utilizzate quando il formato d'asta utilizzato nelle aste concorrenti \u00e8 lo stesso -LRB-, ad esempio ci sono aste M Vickrey o aste M primo prezzo -RRB-. Questo articolo adotta il primo approccio nello studio di un mercato di M aste Vickrey simultanee poich\u00e9 questo approccio produce strategie di offerta dimostrabilmente ottimali. Il loro lavoro analizza un mercato composto da coppie di pari valutazione che vogliono fare un'offerta per un com\u00f2. Pertanto, lo spazio di offerta della coppia pu\u00f2 contenere al massimo due offerte poich\u00e9 marito e moglie possono partecipare al massimo a due aste distribuite geograficamente contemporaneamente. Derivano un equilibrio di Nash a strategia mista per il caso speciale in cui il numero di acquirenti \u00e8 elevato. La nostra analisi differisce dalla loro in quanto studiamo aste concorrenti in cui gli offerenti hanno valutazioni diverse e l'offerente globale pu\u00f2 fare offerte in tutte le aste contemporaneamente -LRB-, cosa del tutto possibile con agenti autonomi -RRB-. Successivamente -LSB- 7 -RSB- ha studiato il caso di aste simultanee con beni complementari. Analizzano il caso degli offerenti sia locali che globali e caratterizzano le offerte degli acquirenti e la conseguente efficienza del mercato. L'impostazione prevista in -LSB- 7 -RSB- viene ulteriormente estesa al caso di valori comuni in -LSB- 9 -RSB-. Tuttavia, nessuno di questi lavori si estende facilmente al caso dei beni sostituibili che consideriamo. Per questo caso speciale viene derivato lo spazio delle strategie di equilibrio misto simmetriche, ma ancora una volta il nostro risultato \u00e8 pi\u00f9 generale. Infine, -LSB- 11 -RSB- considera il caso delle aste inglesi concorrenti, in cui sviluppa algoritmi di offerta per acquirenti con diverse attitudini al rischio. Tuttavia, impone che le offerte siano le stesse in tutte le aste, cosa che come mostriamo in questo documento non sempre \u00e8 ottimale. 7. CONCLUSIONI In questo articolo deriviamo strategie di massimizzazione dell'utilit\u00e0 per fare offerte in aste multiple e simultanee al secondo prezzo. Analizziamo innanzitutto il caso in cui un singolo offerente globale fa un'offerta in tutte le aste, mentre tutti gli altri offerenti sono locali e fanno un'offerta in un'unica asta. Per questa impostazione, troviamo il risultato controintuitivo secondo cui \u00e8 ottimale piazzare offerte diverse da zero in tutte le aste che vendono l'oggetto desiderato, anche quando un offerente richiede solo un singolo oggetto e non trae alcun vantaggio aggiuntivo dall'averne di pi\u00f9. Pertanto, un potenziale acquirente pu\u00f2 ottenere notevoli vantaggi partecipando a pi\u00f9 aste e impiegando una strategia di offerta ottimale. Per un certo numero di distribuzioni di valutazione comuni, mostriamo analiticamente che il problema di trovare offerte ottimali si riduce a due dimensioni.Ci\u00f2 semplifica notevolmente il problema di ottimizzazione originale e pu\u00f2 quindi essere utilizzato nella pratica per calcolare le offerte ottimali per qualsiasi numero di aste. Inoltre, indaghiamo un context con pi\u00f9 offerenti globali combinando soluzioni analitiche con un approccio di simulazione. Troviamo che la strategia di un offerente globale non si stabilizza quando sul mercato sono presenti solo offerenti globali, ma converge solo quando sono presenti anche offerenti locali. Sosteniamo, tuttavia, che \u00e8 probabile che i mercati del mondo reale contengano offerenti sia locali che globali. I risultati convergenti sono quindi molto simili all'impostazione con un singolo offerente globale e scopriamo che un offerente trae vantaggio presentando offerte ottimali in pi\u00f9 aste. Per i contesti pi\u00f9 complessi con pi\u00f9 offerenti globali, la simulazione pu\u00f2 quindi essere utilizzata per trovare queste offerte per casi specifici. Infine, confrontiamo l'efficienza di un mercato con pi\u00f9 aste simultanee con e senza un offerente globale. Mostriamo che, se l'offerente pu\u00f2 prevedere con precisione il numero di offerenti locali in ciascuna asta, l'efficienza aumenta leggermente. Al contrario, se c\u2019\u00e8 molta incertezza, l\u2019efficienza diminuisce significativamente all\u2019aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale si aggiudichi pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.l'efficienza diminuisce significativamente all'aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale vinca pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.l'efficienza diminuisce significativamente all'aumentare del numero di aste a causa della maggiore probabilit\u00e0 che un offerente globale vinca pi\u00f9 di due oggetti. Questi risultati mostrano che il modo in cui l\u2019efficienza, e quindi il benessere sociale, viene influenzato da un offerente globale dipende dalle informazioni a sua disposizione. Nel lavoro futuro, intendiamo estendere i risultati a sostituti imperfetti -LRB-, ovvero quando un offerente globale guadagna vincendo oggetti aggiuntivi -RRB-, e ad ambienti in cui le aste non sono pi\u00f9 identiche. Quest'ultimo si verifica, ad esempio, quando il numero di offerenti locali medi -RRB- -RRB- differisce per asta o le aste hanno impostazioni diverse per parametri come il prezzo di riserva.", "keyphrases": ["strategie di offerta ottimali", "agente di offerta globale", "asta simultanea al secondo prezzo", "distribuzione del valore non decrescente", "mercato online", "sistema multiag", "efficienza del mercato", "sostituto perfetto", "asta vickrei", "scienze sociali e del comportamento", "strategia di massima utilit\u00e0"]}
{"file_name": "J-2", "text": "Ridistribuzione ottimale dei pagamenti VCG nel caso peggiore nelle aste di articoli eterogenei con domanda unitaria. ABSTRACT Molti problemi importanti nei sistemi multiagente implicano l'allocazione di pi\u00f9 risorse tra gli agenti. Per i problemi di allocazione delle risorse, il noto meccanismo VCG soddisfa un elenco di propriet\u00e0 desiderate, tra cui l\u2019efficienza, la resistenza alla strategia, la razionalit\u00e0 individuale e la propriet\u00e0 di non deficit. Tuttavia, VCG generalmente non ha un budget equilibrato. Con VCG, gli agenti pagano i pagamenti VCG, il che riduce il benessere sociale. Per compensare la perdita di benessere sociale dovuta ai pagamenti di VCG, sono stati introdotti meccanismi di ridistribuzione di VCG. Questi meccanismi mirano a ridistribuire quanto pi\u00f9 possibile i pagamenti VCG agli agenti, pur mantenendo le propriet\u00e0 desiderate del meccanismo VCG sopra menzionate. Continuiamo la ricerca di meccanismi di ridistribuzione VCG ottimali nel caso peggiore: meccanismi che massimizzano la frazione del pagamento totale di VCG ridistribuito nel caso peggiore. In precedenza, un meccanismo di ridistribuzione VCG ottimale nel caso peggiore -LRB- indicato da WCO -RRB- era caratterizzato per aste multi-unit\u00e0 con valori marginali non crescenti -LSB- 7 -RSB-. Successivamente, il WCO \u00e8 stato generalizzato a contesti che coinvolgono elementi eterogenei -LSB-4 -RSB-, dando vita al meccanismo HETERO. -LSB- 4 -RSB- ha ipotizzato che HETERO sia fattibile e ottimale nel caso peggiore per aste di articoli eterogenei con domanda unitaria. In questo articolo proponiamo un modo pi\u00f9 naturale per generalizzare il meccanismo del WCO. Dimostriamo che il nostro meccanismo generalizzato, sebbene rappresentato diversamente, in realt\u00e0 coincide con HETERO. Sulla base di questa nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore nelle aste di articoli eterogenei con domanda unitaria. Infine, congetturiamo che HETERO rimanga fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 1. INTRODUZIONE 1.1 Meccanismi di ridistribuzione VCG Molti problemi importanti nei sistemi multiagente coinvolgono l'allocazione di pi\u00f9 risorse tra gli agenti. Per i problemi di allocazione delle risorse, il noto meccanismo VCG soddisfa il seguente elenco di propriet\u00e0 desiderate: \u2022 Efficienza: l'allocazione massimizza la valutazione totale degli agenti -LRB- senza considerare i pagamenti -RRB-. \u2022 A prova di strategia: per qualsiasi agente, riferire in modo veritiero \u00e8 una strategia dominante, indipendentemente dalla tipologia degli altri agenti. \u2022 -LRB- Razionalit\u00e0 individuale ex post -RRB- : l'utilit\u00e0 finale di ogni agente -LRB- dopo aver dedotto il suo pagamento -RRB- \u00e8 sempre non negativa. \u2022 Non-deficit: il pagamento totale da parte degli agenti non \u00e8 negativo. Tuttavia, VCG generalmente non ha un budget equilibrato. Con VCG, gli agenti pagano i pagamenti VCG, il che riduce il benessere sociale. Per compensare la perdita di benessere sociale dovuta ai pagamenti di VCG, sono stati introdotti meccanismi di ridistribuzione di VCG. Questi meccanismi continuano ad allocare le risorse utilizzando VCG. Oltre al VCG,questi meccanismi cercano di ridistribuire quanto pi\u00f9 possibile i pagamenti VCG agli agenti. Richiediamo che la redistribuzione di un agente sia indipendente dal suo tipo. Ci\u00f2 \u00e8 sufficiente per mantenere la resistenza alla strategia e l'efficienza -LRB- un agente non ha alcun controllo sulla propria redistribuzione -RRB-. Per i domini collegati in modo uniforme -LRB- comprese le aste multiunit\u00e0 con valori marginali non crescenti e le aste di articoli eterogenei con domanda unitaria -RRB-, il requisito di cui sopra \u00e8 necessario anche per mantenere la resistenza e l'efficienza della strategia -LSB- 8 -RSB-. Un meccanismo di ridistribuzione del VCG \u00e8 fattibile se mantiene tutte le propriet\u00e0 desiderate del meccanismo VCG. Chiediamo cio\u00e8 anche che il processo di redistribuzione mantenga la razionalit\u00e0 individuale e la propriet\u00e0 di non deficit. Sia n il numero di agenti. Poich\u00e9 tutti i meccanismi di ridistribuzione VCG iniziano con l'allocazione secondo il meccanismo VCG, un meccanismo di ridistribuzione VCG \u00e8 caratterizzato dal suo schema di ridistribuzione r ~ = -LRB- r1, r2,..., rn -RRB-. Nel meccanismo di ridistribuzione VCG ~ r, la ridistribuzione dell'agente i \u00e8 uguale a ri -LRB- 01,..., 0i \u2212 1, 0i +1,..., 0n -RRB-, dove 0j \u00e8 il tipo dell'agente j. -LRB- Non dobbiamo distinguere tra il vero tipo di un agente e il tipo riportato, poich\u00e9 tutti i meccanismi di ridistribuzione VCG sono a prova di strategia. -RRB- Un meccanismo di ridistribuzione VCG anonimo \u00e8 caratterizzato da un'unica funzione r. Sotto il meccanismo di ridistribuzione -LRB- anonimo -RRB- VCG r, la ridistribuzione dell'agente i \u00e8 uguale a r -LRB- 0 \u2212 i -RRB-, dove 0 \u2212 i \u00e8 il multiset dei tipi di agenti diversi da i. Usiamo \u03b8 ~ per denotare il profilo del tipo. Sia V CG -LRB- ~ \u03b8 -RRB- il totale. Organizziamo i risultati esistenti in base alle loro impostazioni. Pagamento VCG per questo tipo di profilo. Un meccanismo di ridistribuzione delle VCG soddisfa la propriet\u00e0 di non-deficit se la redistribuzione totale non supera mai il pagamento totale delle VCG. Un meccanismo di ridistribuzione VCG r \u00e8 -LRB- ex post -RRB- individualmente razionale se l'utilit\u00e0 finale di ogni agente \u00e8 sempre non negativa. Dopo la ridistribuzione, l'utilit\u00e0 dell'agente i \u00e8 esattamente la sua ridistribuzione r -LRB- \u03b8 \u2212 i -RRB-. Vogliamo trovare meccanismi di ridistribuzione dei VCG che massimizzino la frazione del pagamento totale dei VCG ridistribuiti nel caso peggiore. Questo problema di progettazione del meccanismo \u00e8 equivalente al seguente modello di ottimizzazione funzionale: In questo articolo, caratterizzeremo analiticamente un meccanismo di ridistribuzione VCG ottimale nel caso peggiore per aste di articoli eterogenei con domanda unitaria.1 Concludiamo questa sottosezione con un esempio di meccanismo di ridistribuzione VCG in l'impostazione pi\u00f9 semplice delle aste per singolo articolo. In un'asta per un singolo oggetto, il tipo di agente \u00e8 un numero reale non negativo che rappresenta la sua utilit\u00e0 per aggiudicarsi l'oggetto. Nelle aste a singolo oggetto, il meccanismo di ridistribuzione Bailey-Cavallo VCG -LSB- 2, 3 -RSB- funziona come segue: \u2022 Assegna l'oggetto secondo VCG: L'Agente 1 si aggiudica l'oggetto e paga \u03b82. Gli altri agenti non vincono nulla e non pagano.\u2022 Ogni agente riceve una ridistribuzione pari a n1 volte il secondo altro tipo pi\u00f9 alto: l'agente 1 e 2 ricevono ciascuno n1 \u03b83. Gli altri agenti ricevono ciascuno n1\u03b82. Il meccanismo di cui sopra ovviamente mantiene la resistenza alla strategia e l'efficienza -LRB- la redistribuzione di un agente non dipende dal suo tipo -RRB-. Mantiene anche la razionalit\u00e0 individuale perch\u00e9 tutte le ridistribuzioni sono non negative. La ridistribuzione totale \u00e8 pari a 2n \u03b83 + il meccanismo di cui sopra mantiene la propriet\u00e0 di non deficit. Infine, la ridistribuzione totale 2 n aste di articoli, la frazione di ridistribuzione nel caso peggiore di questo meccanismo di esempio \u00e8 n \u2212 2 n 1.2 Ricerca precedente sui meccanismi di ridistribuzione VCG ottimale nel caso peggiore In questa sottosezione, esaminiamo i risultati esistenti sul VCG ottimale nel caso peggiore meccanismi di redistribuzione. Ridistribuzione ottimale nel caso peggiore nelle aste a pi\u00f9 unit\u00e0 con domanda unitaria -LSB- 7, 12 -RSB-: Nelle aste a pi\u00f9 unit\u00e0 con domanda unitaria, gli articoli in vendita sono identici. Ogni agente desidera al massimo una copia dell'articolo. -LRB- Le aste a articolo singolo sono casi speciali di aste multi-unit\u00e0 con domanda unitaria. -RRB- Sia m il numero di elementi. In questo articolo considereremo solo i casi in cui m \u2264 n \u2212 2.2. Qui il tipo di un agente \u00e8 un numero reale non negativo che rappresenta la sua valutazione per aver vinto una copia dell'oggetto. -LSB- 7 -RSB- ha inoltre caratterizzato un meccanismo di ridistribuzione VCG per aste multiunit\u00e0 con domanda unitaria, chiamato meccanismo WCO.3 La frazione di ridistribuzione nel caso peggiore di WCO \u00e8 esattamente \u03b1 \u2217. Cio\u00e8, \u00e8 ottimale nel caso peggiore. Il WCO \u00e8 stato ottenuto ottimizzando all'interno della famiglia dei meccanismi di ridistribuzione lineare del VCG. Un meccanismo di ridistribuzione lineare del VCG r assume la forma seguente: Qui, ci sono costanti. -LRB- Consideriamo solo i ci che corrispondono a possibili meccanismi di ridistribuzione del VCG. -RRB- -LSB- \u03b8 \u2212 i -RSB- j \u00e8 il j-esimo tipo pi\u00f9 alto tra \u03b8 \u2212 i. Il meccanismo lineare r \u00e8 caratterizzato dai valori di ci. I valori ottimali ci sono i seguenti: La caratterizzazione di WCO segue quindi: Ridistribuzione ottimale nel caso peggiore nelle aste multi-unit\u00e0 con valori marginali non crescenti -LSB- 7 -RSB-: Aste multi-unit\u00e0 con non2 -LSB- 7 -RSB - ha dimostrato che per aste multi-unit\u00e0 con domanda unitaria, quando m = n \u2212 1, la frazione di ridistribuzione nel caso peggiore -LRB- di qualsiasi meccanismo di ridistribuzione VCG fattibile -RRB- \u00e8 al massimo 0. Poich\u00e9 l'impostazione studiata in questo articolo \u00e8 pi\u00f9 generali -LRB- aste per articoli eterogenei con domanda unitaria -RRB-, abbiamo anche che la frazione di ridistribuzione nel caso peggiore \u00e8 al massimo 0 quando m = n \u2212 1. Poich\u00e9 le aste per articoli eterogenei con x unit\u00e0 sono casi speciali di aste eterogenee -item con x + 1 unit\u00e0, abbiamo che per la nostra impostazione la frazione di ridistribuzione nel caso peggiore \u00e8 al massimo 0 quando m \u2265 n \u2212 1. Cio\u00e8, non ridistribuire nulla \u00e8 ottimale nel caso peggiore quando m \u2265 n \u2212 1. Inoltre, per l'obiettivo di -LSB- 12 -RSB- , il meccanismo ottimo coincide con il WCO solo quando viene applicato il vincolo di razionalit\u00e0 individuale.i valori marginali crescenti sono pi\u00f9 generali delle aste multi-unit\u00e0 con domanda unitaria. In questo context pi\u00f9 generale, gli articoli sono ancora identici, ma un agente pu\u00f2 richiedere pi\u00f9 di una copia dell'articolo. La valutazione di un agente per aver vinto la prima copia dell'oggetto \u00e8 chiamata valore iniziale/primo marginale. Allo stesso modo, la valutazione aggiuntiva di un agente per aver vinto l'i-esima copia dell'oggetto \u00e8 chiamata il suo i-esimo valore marginale. Il tipo di un agente contiene m numeri reali non negativi -LRB- i-esimo valore marginale per i = 1,..., m -RRB-. In questo context, si presuppone inoltre che i valori marginali non siano crescenti. Come discusso in precedenza, in questo context pi\u00f9 generale, la frazione di ridistribuzione nel caso peggiore di qualsiasi meccanismo di ridistribuzione VCG \u00e8 ancora delimitata sopra da \u03b1 *. -LSB- 7 -RSB- ha generalizzato la WCO a questo context e ha dimostrato che la sua frazione di ridistribuzione nel caso peggiore rimane la stessa. Pertanto, WCO -LRB- dopo la generalizzazione -RRB- \u00e8 anche ottimale nel caso peggiore per aste multi-unit\u00e0 con valori marginali non crescenti. La definizione originale di WCO non si generalizza direttamente alle aste multi-unit\u00e0 con valori marginali non crescenti. Quando si tratta di aste multi-unit\u00e0 con valori marginali non crescenti, il tipo di un agente non \u00e8 pi\u00f9 un valore singolo, il che significa che non esiste qualcosa come \"il j-esimo tipo pi\u00f9 alto tra \u03b8_i\". Abusiamo della notazione non differenziando gli agenti e i loro tipi. Ad esempio, \u03b8_i \u00e8 equivalente all'insieme di agenti diversi da i. Sia S un insieme di agenti. io - 1 -RRB-. Qui, U -LRB- S, j -RRB- \u00e8 il nuovo insieme di agenti, dopo aver rimosso da S l'agente con il j-esimo valore marginale iniziale pi\u00f9 alto in S. La forma generale di WCO \u00e8 la seguente: Caso peggiore Ottimale Ridistribuzione nelle aste di articoli eterogenei con domanda unitaria -LSB- 4 -RSB-: Nelle aste di articoli eterogenei con domanda unitaria, gli articoli in vendita sono diversi. Ogni agente richiede al massimo un articolo. Qui, il tipo di un agente \u00e8 costituito da m numeri reali non negativi -LRB- la sua valutazione per l'elemento vincente i per i = 1,...,m -RRB-. Le aste di articoli eterogenei con domanda unitaria sono l'obiettivo principale di questo documento. Poich\u00e9 le aste per articoli eterogenei con domanda unitaria sono pi\u00f9 generali delle aste multi-unit\u00e0 con domanda unitaria, \u03b1 * \u00e8 ancora un limite superiore alla frazione di ridistribuzione nel caso peggiore. -LSB- 4 -RSB- ha proposto il meccanismo HETERO, generalizzando il WCO. Gli autori hanno ipotizzato che HETERO sia fattibile e abbia una frazione di ridistribuzione nel caso peggiore pari ad \u03b1 *. Cio\u00e8, gli autori hanno ipotizzato che HETERO sia il caso peggiore ottimale in questo context. Il contributo principale di questo articolo \u00e8 una dimostrazione di questa congettura. Ridistribuzione nelle aste combinatorie con sostituti lordi -LSB- 6 -RSB-: La condizione dei sostituti lordi \u00e8 stata proposta per la prima volta in -LSB- 9 -RSB-. Come la domanda unitaria, la condizione dei sostituti lordi \u00e8 una condizione sul tipo di agente -LRB- e non dipende dal meccanismo in discussione -RRB-. In parole,Il tipo di agente soddisfa la condizione di sostituzione lorda se la sua domanda per un bene non diminuisce quando i prezzi degli altri beni aumentano. Sia le aste multi-unit\u00e0 con valori marginali non crescenti che le aste per articoli eterogenei con domanda unitaria sono casi particolari di aste combinatorie con sostituti lordi -LSB- 5, 9 -RSB-. Gli autori non hanno trovato un meccanismo ottimale nel caso peggiore per questa impostazione. Alla fine di questo articolo, congettureremo che HETERO sia ottimale per le aste combinatorie con sostituti lordi. Infine, Naroditskiy et al. -LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.-LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.-LSB- 13 -RSB- ha proposto una tecnica numerica per progettare meccanismi di ridistribuzione ottimale nel caso peggiore. La tecnica proposta funziona solo per domini a parametro singolo. Non si applica alla nostra impostazione -LRB- dominio multiparametrico -RRB-. 1.3 Il nostro contributo Generalizziamo il WCO ad aste di articoli eterogenei con domanda unitaria. Dimostriamo che il meccanismo generalizzato, sebbene rappresentato diversamente, coincide con il meccanismo HETERO proposto in -LSB- 4 -RSB-. Cio\u00e8, quello che abbiamo proposto non \u00e8 un nuovo meccanismo, ma una nuova rappresentazione di un meccanismo esistente. Sulla base della nostra nuova rappresentazione di HETERO, dimostriamo che HETERO \u00e8 effettivamente fattibile e ottimale nel caso peggiore quando applicato ad aste di articoli eterogenei con domanda unitaria, confermando cos\u00ec la congettura sollevata in -LSB- 4 -RSB-. Concludiamo con una nuova congettura secondo cui HETERO rimane fattibile e ottimale nel caso peggiore nel context ancora pi\u00f9 generale delle aste combinatorie con sostituti lordi. 4. CONCLUSIONE Concludiamo il nostro articolo con la seguente congettura: CONGETTURA 1. I sostituti lordi implicano monotonicit\u00e0 della redistribuzione. Cio\u00e8, HETERO rimane fattibile e ottimale nel caso peggiore nelle aste combinatorie con sostituti lordi. L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.L\u2019idea \u00e8 che sia le aste multi-unit\u00e0 con valori marginali non crescenti sia le aste per articoli eterogenei con domanda unitaria soddisfano la monotonicit\u00e0 della redistribuzione. Una congettura naturale \u00e8 che il \u201ccomune pi\u00f9 restrittivo\u201d di questi due assetti soddisfi anche la monotonia redistributiva. Esistono molte impostazioni d'asta ben studiate che contengono sia aste multi-unit\u00e0 con valori marginali non crescenti sia aste per articoli eterogenei con domanda unitaria -LRB- un elenco delle quali pu\u00f2 essere trovato in -LSB- 10 -RSB- -RRB-. Tra questi contesti ben studiati, le aste combinatorie con sostituti lordi sono le pi\u00f9 restrittive.", "keyphrases": ["progettazione meccanica", "vickrei-clark-grove", "ridistribuire il pagamento", "meccanica efficiente", "a prova di strategia", "meccanismo di razione individuale", "meccan", "meccanismo di ridistribuzione lineare vcg", "trasformarsi in un programma lineare", "carattere analista", "meccanico ottimale nel caso peggiore"]}
{"file_name": "H-4", "text": "Verso valutazioni di gestione delle informazioni personali basate su attivit\u00e0 ABSTRACT La gestione delle informazioni personali -LRB- PIM -RRB- \u00e8 un'area di ricerca in rapida crescita che riguarda il modo in cui le persone archiviano, gestiscono e ritrovano le informazioni. Una caratteristica della ricerca PIM \u00e8 che molti sistemi sono stati progettati per aiutare gli utenti a gestire e ritrovare le informazioni, ma pochissimi sono stati valutati. Ci\u00f2 \u00e8 stato notato da diversi studiosi e spiegato con le difficolt\u00e0 legate all\u2019esecuzione delle valutazioni PIM. Le difficolt\u00e0 includono che le persone ritrovano le informazioni all'interno di raccolte personali uniche; i ricercatori sanno poco sui compiti che inducono le persone a ritrovare le informazioni; e numerose questioni relative alla privacy riguardanti le informazioni personali. In questo documento miriamo a facilitare le valutazioni PIM affrontando ciascuna di queste difficolt\u00e0. Nella prima parte presentiamo uno studio diario sui compiti di ricerca delle informazioni. Lo studio esamina il tipo di attivit\u00e0 che richiedono agli utenti di ritrovare informazioni e produce una tassonomia delle attivit\u00e0 di ricerca per messaggi di posta elettronica e pagine web. Nella seconda parte, proponiamo una metodologia di valutazione basata sui compiti basata sui nostri risultati ed esaminiamo la fattibilit\u00e0 dell'approccio utilizzando due diversi metodi di creazione dei compiti. 1. INTRODUZIONE La gestione delle informazioni personali -LRB- PIM -RRB- \u00e8 un'area di ricerca in rapida crescita che riguarda il modo in cui le persone archiviano, gestiscono e ritrovano le informazioni. I sistemi PIM - i metodi e le procedure con cui le persone gestiscono, classificano e recuperano le informazioni quotidianamente -LSB- 18 -RSB- - stanno diventando sempre pi\u00f9 popolari. Tuttavia la valutazione di questi sistemi PIM \u00e8 problematica. Una delle principali difficolt\u00e0 \u00e8 causata dalla natura personale del PIM. Le persone raccolgono informazioni come conseguenza naturale del completamento di altre attivit\u00e0. Ci\u00f2 significa che le raccolte che le persone generano sono uniche solo per loro e le informazioni all'interno di una raccolta sono intrinsecamente legate alle esperienze personali del proprietario. Poich\u00e9 le raccolte personali sono uniche, non \u00e8 possibile creare attivit\u00e0 di valutazione applicabili a tutti i partecipanti a una valutazione. In secondo luogo, le raccolte personali possono contenere informazioni che i partecipanti si sentono a disagio nel condividere nell\u2019ambito di una valutazione. La natura precisa di queste informazioni (quali informazioni gli individui preferiscono mantenere private) varia da individuo a individuo, rendendo difficile basare le attivit\u00e0 di ricerca sui contenuti delle singole raccolte. Pertanto, gli sperimentatori devono affrontare una serie di sfide per condurre valutazioni PIM realistiche ma controllate. Recentemente, tuttavia, i ricercatori hanno iniziato a concentrarsi sui modi per affrontare il problema della valutazione PIM. Capra -LSB- 6 -RSB- identifica anche la necessit\u00e0 di valutazioni di laboratorio PIM controllate per integrare altre tecniche di valutazione, ponendo un'enfasi specifica sulla necessit\u00e0 di comprendere il comportamento del PIM a livello di attivit\u00e0. In questo articolo, tentiamo di affrontare le difficolt\u00e0 coinvolte nel facilitare le valutazioni PIM controllate di laboratorio.Nella prima parte di questo articolo presentiamo uno studio diario sui compiti di ricerca delle informazioni. Lo studio esamina il tipo di attivit\u00e0 che richiedono agli utenti di ritrovare informazioni e produce una tassonomia delle attivit\u00e0 di ricerca per messaggi di posta elettronica e pagine web. Esaminiamo anche le caratteristiche dei compiti che rendono difficile il ritrovamento. Nella seconda parte, proponiamo una metodologia di valutazione basata sui compiti basata sui nostri risultati ed esaminiamo la fattibilit\u00e0 dell'approccio utilizzando diversi metodi di creazione dei compiti. Pertanto, questo articolo offre due contributi al campo: una maggiore comprensione del comportamento del PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini. 2. LAVORI CORRELATI Sono disponibili diversi approcci per studiare il PIM. Gli approcci naturalistici studiano i partecipanti che si comportano in modo naturale, completando i propri compiti man mano che si verificano, all'interno di ambienti familiari. Questi approcci consentono ai ricercatori di superare molte delle difficolt\u00e0 causate dalla natura personale del PIM. Poich\u00e9 i compiti svolti sono \"reali\" e non simulati, i partecipanti possono utilizzare le proprie esperienze, conoscenze precedenti e raccolte di informazioni per completare i compiti. Sia i metodi etnografici che quelli sul campo richiedono la presenza di uno sperimentatore per valutare come viene eseguita la PIM, il che solleva una serie di questioni. In primo luogo, la valutazione in questo modo \u00e8 costosa; impiegare lunghi periodi di tempo per studiare un piccolo numero di partecipanti e questi piccoli campioni potrebbero non essere rappresentativi del comportamento di popolazioni pi\u00f9 grandi. In secondo luogo, poich\u00e9 i partecipanti non possono essere osservati continuamente, gli sperimentatori devono scegliere quando osservare e ci\u00f2 pu\u00f2 influenzare i risultati. Una strategia alternativa alla conduzione di valutazioni naturalistiche consiste nell'utilizzare l'analisi dei file di registro. Questo approccio fa uso di software di registrazione che cattura un ampio campione di attivit\u00e0 degli utenti nel context dell'uso naturale di un sistema. Ci\u00f2 rivela la necessit\u00e0 di integrare gli studi naturalistici con esperimenti controllati in cui lo sperimentatore pu\u00f2 mettere in relazione il comportamento dei partecipanti allo studio con obiettivi associati a compiti di ricerca noti. Una difficolt\u00e0 nell'esecuzione di questo tipo di valutazione \u00e8 l'approvvigionamento delle collezioni da valutare. Kelly -LSB- 16 -RSB- propone l'introduzione di una raccolta di test condivisa che fornirebbe set di dati, attivit\u00e0 e metriche condivisibili e riutilizzabili per coloro che sono interessati a condurre ricerche PIM. Tuttavia, una raccolta condivisa non sarebbe adatta per gli studi sugli utenti perch\u00e9 non sarebbe possibile incorporare gli aspetti personali del PIM utilizzando una raccolta comune e non familiare. Un approccio alternativo consiste nel chiedere agli utenti di fornire le proprie raccolte di informazioni per simulare ambienti familiari all'interno del laboratorio. Questo approccio \u00e8 stato applicato per studiare il ritrovamento di fotografie personali -LSB- 11 -RSB-, messaggi di posta elettronica -LSB- 20 -RSB- e segnalibri web -LSB- 21 -RSB-. L'utilit\u00e0 di questo approccio dipende dalla facilit\u00e0 con cui \u00e8 possibile trasferire la raccolta o ottenere l'accesso remoto.Un'altra soluzione \u00e8 utilizzare l'intero Web come raccolta quando si studia la ricerca di pagine Web -LSB- 4 -RSB-. Questo potrebbe essere appropriato per studiare la ricerca di pagine web perch\u00e9 studi precedenti hanno dimostrato che le persone spesso utilizzano i motori di ricerca web per questo scopo -LSB- 5 -RSB-. Una seconda difficolt\u00e0 nello svolgimento degli studi di laboratorio PIM \u00e8 la creazione di compiti da far eseguire ai partecipanti che possono essere risolti effettuando una ricerca in una raccolta condivisa o personale. I compiti si riferiscono all'attivit\u00e0 che determina la necessit\u00e0 di informazioni -LSB- 14 -RSB- e sono riconosciuti come importanti nel determinare il comportamento dell'utente -LSB- 26 -RSB-. \u00c8 stato svolto un ampio lavoro per comprendere la natura dei compiti e come il tipo di compito influenza il comportamento degli utenti nella ricerca di informazioni. Ad esempio, i compiti sono stati classificati in termini di complessit\u00e0 crescente -LSB- 3 -RSB- e si ritiene che la complessit\u00e0 dei compiti influenzi il modo in cui gli utenti percepiscono le loro esigenze di informazione -LSB- 25 -RSB- e il modo in cui cercano di trovare informazioni -LSB- 3 -RSB-. Altri lavori precedenti hanno fornito metodologie che consentono la simulazione di compiti durante lo studio del comportamento di ricerca di informazioni -LSB- 2 -RSB-. Tuttavia, si sa poco sui tipi di attivit\u00e0 che inducono le persone a cercare nei propri archivi personali o a ritrovare informazioni gi\u00e0 viste in precedenza. Di conseguenza, \u00e8 difficile ideare situazioni di attivit\u00e0 lavorative simulate per il PIM. L'eccezione \u00e8 lo studio sulla gestione delle fotografie personali, dove il lavoro di Rodden sulla categorizzazione dei compiti di ricerca di fotografie personali ha facilitato la creazione di situazioni di compiti di lavoro simulati -LSB- 22 -RSB-. Sono stati forniti altri suggerimenti su come classificare le attivit\u00e0 PIM. Sebbene queste siano propriet\u00e0 interessanti che possono influenzare il modo in cui un compito verr\u00e0 eseguito, non danno agli sperimentatori spazio sufficiente per ideare i compiti. Le raccolte personali sono uno dei motivi per cui la creazione di attivit\u00e0 \u00e8 cos\u00ec difficile. La tassonomia delle attivit\u00e0 fotografiche di Rodden fornisce una soluzione in questo caso perch\u00e9 consente di classificare le attivit\u00e0 su misura per le collezioni private. I sistemi possono quindi essere confrontati tra tipi di attivit\u00e0 per diversi utenti -LSB- 11 -RSB-. Sfortunatamente non esiste una tassonomia equivalente per altri tipi di oggetti informativi. Inoltre, altri tipi di oggetti sono pi\u00f9 sensibili alla privacy rispetto alle fotografie; \u00e8 improbabile che i partecipanti siano cos\u00ec contenti di consentire ai ricercatori di sfogliare le loro raccolte di posta elettronica per creare attivit\u00e0 come facevano con le fotografie in -LSB- 11 -RSB-. Ci\u00f2 presenta un problema serio: come possono i ricercatori ideare compiti che corrispondano a collezioni private senza comprendere il tipo di compiti che le persone svolgono o senza mettere a repentaglio la privacy dei partecipanti allo studio? Sono stati proposti alcuni metodi. Ad esempio, -LSB- 20 -RSB- ha studiato la ricerca e-mail chiedendo ai partecipanti di ritrovare le e-mail che erano state inviate a ogni membro di un dipartimento; consentendo di utilizzare gli stessi compiti per tutti i partecipanti allo studio.Questo approccio ha garantito che i problemi di privacy fossero evitati e che i partecipanti potessero utilizzare le cose che ricordavano per completare le attivit\u00e0. Tuttavia, i sistemi sono stati testati utilizzando solo un tipo di attivit\u00e0: ai partecipanti \u00e8 stato chiesto di trovare singole e-mail, ciascuna delle quali condivideva propriet\u00e0 comuni. Nella sezione 4 mostriamo che le persone eseguono una gamma pi\u00f9 ampia di attivit\u00e0 di ritrovamento della posta elettronica rispetto a questa. In -LSB- 4 -RSB-, attivit\u00e0 di ricerca generiche venivano create artificialmente eseguendo valutazioni su due sessioni. Nella prima sessione, ai partecipanti \u00e8 stato chiesto di completare attivit\u00e0 lavorative che prevedevano la ricerca di alcune informazioni sconosciute. Nella seconda sessione, i partecipanti hanno completato nuovamente gli stessi compiti, il che naturalmente ha comportato qualche comportamento di ricerca. I limiti di questa tecnica sono che non consente ai partecipanti di sfruttare alcuna connessione personale con le informazioni perch\u00e9 le informazioni che stanno cercando potrebbero non corrispondere a nessun altro aspetto della loro vita. La nostra revisione degli approcci di valutazione motiva la necessit\u00e0 di esperimenti di laboratorio controllati che consentano di testare aspetti strettamente definiti di sistemi o interfacce. Sfortunatamente, \u00e8 stato anche dimostrato che ci sono difficolt\u00e0 nell'esecuzione di questo tipo di valutazione: \u00e8 difficile reperire collezioni e ideare compiti che corrispondano a collezioni private, proteggendo allo stesso tempo la privacy dei partecipanti allo studio. Nella sezione seguente presentiamo uno studio diario sulle attivit\u00e0 di ricerca per e-mail e pagine web. Il risultato \u00e8 una classificazione dei compiti simile a quella ideata da Rodden per le fotografie personali -LSB- 22 -RSB-. Nella sezione 5 ci baseremo su questo lavoro esaminando i metodi per creare attivit\u00e0 che non compromettano la privacy dei partecipanti e discuteremo come il nostro lavoro pu\u00f2 facilitare le valutazioni degli utenti PIM basate sulle attivit\u00e0. Mostriamo che raccogliendo attivit\u00e0 utilizzando diari elettronici, non solo possiamo conoscere le attivit\u00e0 che inducono le persone a ritrovare informazioni personali, ma possiamo conoscere il contenuto delle raccolte private senza compromettere la privacy dei partecipanti. Questa conoscenza pu\u00f2 quindi essere utilizzata per costruire attivit\u00e0 da utilizzare nelle valutazioni PIM. 6. CONCLUSIONI Il presente documento si \u00e8 concentrato sul superamento delle difficolt\u00e0 legate all'esecuzione delle valutazioni PIM. La natura personale del PIM significa che \u00e8 difficile costruire esperimenti equilibrati perch\u00e9 ciascuno dei partecipanti ha le proprie collezioni uniche che si autogenerano completando altre attivit\u00e0. Abbiamo suggerito che per incorporare gli aspetti personali del PIM nelle valutazioni, le prestazioni dei sistemi o degli utenti dovrebbero essere esaminate quando gli utenti completano le attivit\u00e0 sulle proprie raccolte. Questo approccio presenta di per s\u00e9 dei problemi perch\u00e9 la creazione di attivit\u00e0 per le raccolte personali \u00e8 difficile: i ricercatori non sanno molto sui tipi di attivit\u00e0 di ricerca che le persone eseguono e non sanno quali informazioni si trovano all'interno delle singole raccolte personali.In questo documento abbiamo descritto le modalit\u00e0 per superare queste sfide per facilitare le valutazioni degli utenti PIM basate sulle attivit\u00e0. Nella prima parte dell'articolo abbiamo eseguito uno studio diario che ha esaminato le attivit\u00e0 che inducono le persone a ritrovare messaggi di posta elettronica e pagine web. I dati raccolti includevano un'ampia gamma di attivit\u00e0 lavorative e non lavorative e, sulla base dei dati, abbiamo creato una tassonomia delle attivit\u00e0 di ricerca sul web e tramite posta elettronica. Abbiamo scoperto che le persone eseguono tre tipi principali di attivit\u00e0 di ricerca: attivit\u00e0 che richiedono informazioni specifiche all'interno di una singola risorsa, attivit\u00e0 che richiedono una singola risorsa completa e attivit\u00e0 che richiedono il recupero di informazioni da pi\u00f9 risorse. Nella seconda parte del lavoro abbiamo discusso il significato della tassonomia rispetto alla valutazione PIM. Abbiamo dimostrato che \u00e8 possibile condurre esperimenti equilibrati confrontando le prestazioni del sistema o dell'utente nelle categorie di attivit\u00e0 all'interno della tassonomia. Abbiamo anche suggerito due metodi per creare attivit\u00e0 che possono essere completate sulle collezioni personali. Questi metodi non compromettono la privacy dei partecipanti allo studio. Abbiamo esaminato le tecniche suggerite, in primo luogo simulando una situazione sperimentale: ai partecipanti \u00e8 stato chiesto di ripetere i propri compiti mentre li registravano, e in secondo luogo, nel context di una valutazione completa. L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.L'esecuzione di valutazioni in questo modo consentir\u00e0 di testare i sistemi proposti per migliorare la capacit\u00e0 degli utenti di gestire e ritrovare le proprie informazioni, in modo da poter conoscere le esigenze e i desideri degli utenti. Pertanto, questo articolo ha offerto due contributi al campo: una maggiore comprensione del comportamento dei PIM a livello di compito e un metodo di valutazione che faciliter\u00e0 ulteriori indagini.", "keyphrases": ["persona informa il manag", "misura", "sperimentare", "fattore umano", "ritrovare le informazioni", "problema privato", "tassonomi", "raccogliere individualmente", "messaggio di posta elettronica", "approccio naturalista", "studi di laboratorio"]}
{"file_name": "I-12", "text": "Condividere esperienze per apprendere le caratteristiche degli utenti in ambienti dinamici con dati sparsi ABSTRACT Questo articolo indaga il problema della stima del valore dei parametri probabilistici necessari per il processo decisionale in ambienti in cui un agente, operante all'interno di un sistema multi-agente, non ha informazioni a priori su la struttura della distribuzione dei valori dei parametri. L'agente deve essere in grado di produrre stime anche quando ha effettuato solo un numero limitato di osservazioni dirette, e quindi deve essere in grado di operare con dati sparsi. L'articolo descrive un meccanismo che consente all'agente di migliorare significativamente la propria stima integrando le sue osservazioni dirette con quelle ottenute da altri agenti con cui si sta coordinando. Per evitare distorsioni indesiderate in ambienti relativamente eterogenei e allo stesso tempo utilizzare in modo efficace i dati rilevanti per migliorare le proprie stime, il meccanismo valuta i contributi delle osservazioni di altri agenti sulla base di una stima in tempo reale del livello di somiglianza tra ciascuno di questi agenti e se stesso. Il modulo di \u201cautonomia di coordinamento\u201d di un sistema di gestione del coordinamento ha fornito un context empirico per la valutazione. Le valutazioni basate sulla simulazione hanno dimostrato che il meccanismo proposto supera le stime basate esclusivamente sulle osservazioni di un agente, nonch\u00e9 le stime basate su un aggregato non ponderato delle osservazioni di tutti gli altri agenti. 1. INTRODUZIONE Per molti scenari del mondo reale, gli agenti autonomi devono operare in ambienti dinamici e incerti in cui hanno solo informazioni incomplete sui risultati delle loro azioni e sulle caratteristiche di altri agenti o persone con cui devono cooperare o collaborare. In tali ambienti, gli agenti possono trarre vantaggio dalla condivisione delle informazioni raccolte, mettendo in comune le loro esperienze individuali per migliorare le loro stime dei parametri sconosciuti richiesti per ragionare sulle azioni in condizioni di incertezza. Questo articolo affronta il problema dell'apprendimento della distribuzione dei valori di un parametro probabilistico che rappresenta una caratteristica di una persona che sta interagendo con un agente informatico. La caratteristica da apprendere \u00e8 -LRB- o \u00e8 chiaramente correlata a -RRB-, un fattore importante nel processo decisionale dell'agente.1 L'impostazione di base che consideriamo \u00e8 quella in cui un agente accumula osservazioni su una specifica caratteristica dell'utente e le utilizza produrre una stima tempestiva di una misura che dipende dalla distribuzione di quella caratteristica. In genere, gli agenti devono prendere decisioni in tempo reale, contemporaneamente all\u2019esecuzione delle attivit\u00e0 e nel mezzo di una grande incertezza. Nel resto di questo articolo utilizzeremo il termine \u201cveloce\u201d per riferirci a tali ambienti. In ambienti frenetici, la raccolta di informazioni pu\u00f2 essere limitata e non \u00e8 possibile apprendere offline o attendere la raccolta di grandi quantit\u00e0 di dati prima di prendere decisioni. Cos\u00ec,L'obiettivo dei metodi di stima presentati in questo documento \u00e8 quello di minimizzare l'errore medio nel tempo, piuttosto che determinare un valore accurato al termine di un lungo periodo di interazione. Cio\u00e8, si prevede che l'agente collabori con l'utente per un tempo limitato e tenti di ridurre al minimo l'errore complessivo nelle sue stime. In tali ambienti, i dati acquisiti individualmente da un agente -LRB- e le sue osservazioni -RRB- sono troppo scarsi perch\u00e9 possa ottenere buone stime nell'intervallo di tempo richiesto. Data l\u2019assenza di vincoli strutturali dell\u2019ambiente, gli approcci che dipendono da distribuzioni strutturate possono comportare un bias di stima significativamente elevato. Consideriamo questo problema nel context di un sistema distribuito multi-agente in cui gli agenti informatici supportano persone che svolgono compiti complessi in un ambiente dinamico. Il fatto che gli agenti facciano parte di un ambiente multi-agente, in cui anche altri agenti possono raccogliere dati per stimare una caratteristica simile dei loro utenti, offre la possibilit\u00e0 ad un agente di aumentare le proprie osservazioni con quelle di altri agenti, migliorando cos\u00ec l\u2019accuratezza del suo processo di apprendimento. Inoltre, negli ambienti che consideriamo, gli agenti solitamente accumulano dati a una velocit\u00e0 relativamente simile. Tuttavia, la misura in cui le osservazioni di altri agenti saranno utili a un dato agente dipende dalla misura in cui le distribuzioni delle caratteristiche dei loro utenti sono correlate con quella dell'utente di questo agente. Non vi \u00e8 alcuna garanzia che la distribuzione di due agenti diversi sia altamente correlata positivamente, per non parlare che siano gli stessi. Pertanto, per utilizzare un approccio di condivisione dei dati, un meccanismo di apprendimento deve essere in grado di identificare efficacemente il livello di correlazione tra i dati raccolti da diversi agenti e di pesare i dati condivisi in base al livello di correlazione. La progettazione di un modulo di autonomia di coordinamento -LRB- CA -RRB- all'interno di un sistema di gestione del coordinamento -LRB- come parte del progetto DARPA Coordinators -LSB- 18 -RSB- -RRB-, in cui gli agenti supportano un'attivit\u00e0 di pianificazione distribuita, ha fornito la motivazione iniziale e un context concettuale per questo lavoro. Tuttavia, i meccanismi stessi sono generali e possono essere applicati non solo ad altri ambiti dal ritmo frenetico, ma anche in altri contesti multi-agente in cui gli agenti raccolgono dati che si sovrappongono in una certa misura, a velocit\u00e0 approssimativamente simili, e in cui l\u2019ambiente impone i vincoli di assenza di struttura, limitazione e utilizzo anticipato definiti sopra -LRB-, ad esempio l'esplorazione di pianeti remoti -RRB-. In particolare, le nostre tecniche sarebbero utili in qualsiasi context in cui un gruppo di agenti intraprende un compito in un nuovo ambiente, in cui ciascun agente ottiene osservazioni a una velocit\u00e0 simile dei parametri individuali di cui ha bisogno per il proprio processo decisionale. In questo articolo presentiamo un meccanismo utilizzato per apprendere le caratteristiche chiave dell'utente in ambienti frenetici.Il meccanismo fornisce stime relativamente accurate in tempi brevi, integrando le osservazioni dirette di un singolo agente con osservazioni ottenute da altri agenti con i quali si sta coordinando. In particolare, ci concentreremo sui problemi connessi alla stima del costo di interruzione di una persona e alla stima della probabilit\u00e0 che quella persona disponga delle informazioni richieste dal sistema. Il meccanismo \u00e8 stato testato con successo utilizzando un sistema che simula l'ambiente di un coordinatore. La sezione successiva dell'articolo descrive il problema della stima dei parametri relativi all'utente in domini a ritmo serrato. La sezione 3 fornisce una panoramica dei metodi che abbiamo sviluppato. L'implementazione, il context empirico e i risultati sono forniti nelle Sezioni 4 e 5. Un confronto con i metodi correlati \u00e8 fornito nella Sezione 6 e le conclusioni nella sezione 7. 6. LAVORI CORRELATI Oltre alla letteratura sulla gestione delle interruzioni esaminata nella Sezione 2, molti altri le aree di lavoro precedente sono rilevanti per il meccanismo di condivisione selettiva descritto in questo documento. Il filtraggio collaborativo, che fa previsioni -LRB- filtraggio -RRB- sugli interessi di un utente -LSB- 7 -RSB-, funziona in modo simile alla condivisione selettiva. Tuttavia, i sistemi di filtraggio collaborativo mostrano scarse prestazioni quando non ci sono informazioni sufficienti sugli utenti e quando non ci sono informazioni sufficienti su un nuovo utente il cui gusto il sistema tenta di prevedere -LSB- 7 -RSB-. La condivisione selettiva si basa sulla capacit\u00e0 di trovare somiglianze tra parti specifiche della funzione di distribuzione di probabilit\u00e0 associata a una caratteristica di utenti diversi. Questa capacit\u00e0 \u00e8 strettamente correlata al clustering e alla classificazione, un'area ampiamente studiata nell'apprendimento automatico. Date considerazioni di spazio, la nostra revisione di quest\u2019area \u00e8 limitata ad alcuni approcci rappresentativi per il clustering. Di particolare importanza \u00e8 che l'AC deve trovare somiglianze tra funzioni, definite su un intervallo continuo, senza attributi predefiniti distinti. Un'ulteriore difficolt\u00e0 \u00e8 definire la misura della distanza. Molte tecniche di clustering sono state utilizzate nel data mining -LSB- 2 -RSB-, con particolare attenzione agli aggiornamenti incrementali del clustering, a causa delle dimensioni molto grandi dei database -LSB- 3 -RSB-. Tuttavia, l\u2019applicabilit\u00e0 di questi a domini dal ritmo frenetico \u00e8 piuttosto limitata perch\u00e9 si basano su un ampio insieme di dati esistenti. Il metodo pi\u00f9 rilevante per i nostri scopi \u00e8 l'indice di entropia relativa di Kullback-Leibler utilizzato nella teoria della probabilit\u00e0 e nella teoria dell'informazione -LSB- 12 -RSB-. Tuttavia, il metodo funzioner\u00e0 male negli scenari in cui le funzioni si alternano tra diversi livelli mantenendo la struttura e i momenti \"generali\". 208 La Sesta Intl.. Conf. congiunta. su agenti autonomi e sistemi multi-agente -LRB- AAMAS 07 -RRB-, mentre il nostro approccio basato su Wilcoxon dar\u00e0 loro il punteggio pi\u00f9 alto in termini di somiglianza. Sebbene il test di Wilcoxon sia una procedura statistica ampiamente utilizzata -LSB- 22,14 -RSB-, viene solitamente utilizzato per confrontare due insiemi di dati a variabile singola. A nostra conoscenza, non \u00e8 stato ancora fatto alcun tentativo per estendere le sue propriet\u00e0 come infrastruttura per determinare con chi e in che misura le informazioni dovrebbero essere condivise, come presentato in questo documento. In queste applicazioni viene utilizzato principalmente come strumento di identificazione e criterio di classificazione.", "keyphrases": ["parametro probabilistico", "agente", "informare condividere", "decidere", "ambiente frenetico", "sistema di distribuzione multi-agente", "impara la meccanica", "seleziona-condividi", "stima dei parametri"]}
{"file_name": "I-1", "text": "Aborting Tasks in BDI Agents ABSTRACT Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent 's own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system -LRB- such as JACK, Jadex, Jason, or SPARK -RRB-. A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence. 1. INTRODUCTION Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan can not be guaranteed -LSB- 13 -RSB-. Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence -LSB- 21 -RSB-. In agent architectures inspired by the Belief-Desire-Intention -LRB- BDI -RRB- model -LSB- 16 -RSB-, these properties are often characterized by the interactions between beliefs, goals, and plans -LSB- 2 -RSB-.1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success -LSB- 12, 14 -RSB-. Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent 's execution cycle. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. Aborting a task or plan is distinct from its failure. In contrast, aborting says nothing about the ability to perform ; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up. There has been a considerable amount of work on plan failures -LRB- such as detecting and resolving resource conflicts -LSB- 20, 10 -RSB- -RRB- and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct 1One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of `` the task having been performed '', and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use `` task '' to also refer to goals. fails -LRB- common approaches include either letting the other branch run to completion unhindered or dropping it completely -RRB-. In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN -LSB- 23, 17 -RSB-. Our focus is on a single agent, complementary to related work that considers exception handling for single - and multiagent systems -LRB- e.g., -LSB- 22, 5, 6 -RSB- -RRB-. This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work. 5. RELATED WORK Plan failure is handled in the extended version of AgentSpeak found in the Jason system -LSB- 6 -RSB-. Failure `` clean-up '' plans are triggered from goal deletion events --! g. In a goal deletion plan, the programmer can specify any `` undo '' actions and whether to attempt the goal again. If no goal deletion plan is provided, Jason 's default behaviour is to not reattempt the goal. Failure handling is applied only to plans triggered by addition of an achievement or test goal ; in particular, goal deletion events are not posted for failure of a goal deletion plan. The implementation of H \u00a8 ubner et al. -LSB- 6 -RSB- requires Jason 's internal actions. A requirement for implementing our approach is a reflective capability in the BDI agent implementation. All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states. Such meta level facilities are also required by the approach of Unruh et al. -LSB- 21 -RSB-, who define goal-based semantic compensation for an agent. Failure-handling goals are invoked according to failurehandling strategy rules, by a dedicated agent Failure Handling Component -LRB- FHC -RRB- that tracks task execution. These goals are specified by the agent programmer and attached to tasks, much like our FAb -LRB- P, PF, PA -RRB- construct associates failure and abort methods with a plan P. Note, however, that in contrast to both -LSB- 6 -RSB- and our semantics, -LSB- 21 -RSB- attach the failure-handling knowledge at the goal, not plan, level. Their failure-handling goals may consist of stabilization goals that perform localized, immediate `` clean-up '' to restore the agent 's state to a known, stable state, and compensation goals that perform `` undo '' actions. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure -LRB- i.e., if the FHC directs the agent to retry the failed goal and the retry is successful -RRB-. This contrasts with simplistic plan-level failure handling in which the what and how are intermingled in domain task knowledge. While our approach is defined at the plan level, our extended BDI semantics provides for the separation of execution and failure handling. Further, the FHC explicitly maintains data structures to track agent execution. We leverage the existing execution structures and self-reflective ability of a BDI agent to accomplish both aborting and failure handling without additional overhead. FHC 's failure-handling strategy rules -LRB- e.g., whether to retry a failed goal -RRB- are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent 's nature -LRB- e.g., blindly committed -RRB-. The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism -LRB- although not tied to any one agent system -RRB-. 14 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- a state-based protocol. This approach, together with state checkpointing, is used for multi-agent systems in -LSB- 22 -RSB-. The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery. Other work on multi-agent exception handling includes AOEX 's distributed exception handling agents -LSB- 5 -RSB-, and the similar sentinels of -LSB- 8 -RSB-. In both cases, failure-handling logic and knowledge are decoupled from the agents ; by contrast, while separating exception handling from domain-specific knowledge, Unruh et al. 's FHC and our approach both retain failure-handling logic within an agent. 6. CONCLUSION AND FUTURE WORK The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them -LRB- perhaps at the request of another agent to do so -RRB-, or by unbidden factors that lead to failure. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN. We are planning to implement an instance of our approach in the SPARK agent system -LSB- 9 -RSB- ; in particular, the work of this paper will be the basis for SPARK 's abort handling mechanism. An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action. We have assumed the default behaviour of a BDI-style agent, according to its nature : for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain -LRB- in which case to fail the task -RRB-. Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish -LSB- 19, 12 -RSB-. Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process. This would in particular influence the commitment the agent has towards a particular task : the higher the cost, the greater the commitment. A further item of interest is extending our approach to failure and abort to maintenance goals -LSB- 1 -RSB-. For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves.", "keyphrases": ["intellig agent", "failur", "deal", "cleanup method", "abort-method", "oper semant", "task", "goal", "goal construct"]}
{"file_name": "I-9", "text": "Temporal Linear Logic as a Basis for Flexible Agent Interactions ABSTRACT Interactions between agents in an open system such as the Internet require a significant degree of flexibility. A crucial aspect of the development of such methods is the notion of commitments, which provides a mechanism for coordinating interactive behaviors among agents. In this paper, we investigate an approach to model commitments with tight integration with protocol actions. This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments. We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic, which incorporates both temporal and resource-sensitive reasoning. We also discuss the application of this framework to scenarios such as online commerce. 1. INTRODUCTION AND MOTIVATION The agent paradigm has become well suited as a design metaphor to deal with complex systems comprising many components each having their own thread of control and purposes and involved in dynamic and complex interactions. In multi-agent environments, agents often need to interact with each other to fulfill their goals. Protocols are used to regulate interactions. In traditional approaches to protocol specification, like those using Finite State Machines or Petri Nets, protocols are often predetermined legal sequences of interactive behaviors. Therefore, agents are required to adapt their interactive behaviors to succeed and interactions among agents should not be constructed rigidly. To achieve flexibility, as characterized by Yolum and Singh in -LSB- 11 -RSB-, interaction protocols should ensure that agents have autonomy over their interactive behaviors, and be free from any unnecessary constraints. Also, agents should be allowed to adjust their interactive actions to take advantages of opportunities or handle exceptions that arise during interaction. For example, consider the scenario below for online sales. Cus has a goal of obtaining from Mer a cricket bat at some time. There are two options for Cus to pay. If Cus uses credit payment, Mer needs a bank Ebank to check Cus 's credit. If Cus 's credit is approved, Ebank will arrange the credit payment. Otherwise, Cus may then take the option to pay via PayPal. The interaction ends when goods are delivered and payment is arranged. A flexible approach to this example should include several features. Secondly, there should be no unnecessary constraint on the order in which actions are performed, such as which of making payments and sending the cricket bat should come first. Thirdly, choosing a sequence of interactive actions should be based on reasoning about the intrinsic meanings of protocol actions, which are based on the notion of commitment, i.e. which refers to a strong promise to other agent -LRB- s -RRB- to undertake some courses of action. Current approaches -LSB- 11, 12, 10, 1 -RSB- to achieve flexibilities using the notion of commitment make use of an abstract layer of commitments. However, in these approaches, a mapping from protocol actions onto operations on commitments as well as handling and enforcement mechanisms of commitments must be externally provided. Execution of protocol actions also requires concurrent execution of operations on related commitments. As a result, the overhead of processing the commitment layer makes specification and execution of protocols more complicated and error prone. There is also a lack of a logic to naturally express aspects of resources, internal and external choices as well as time of protocols. Rather than creating another layer of commitment outside protocol actions, we try to achieve a modeling of commitments that is integrated with protocol actions. Both commitments and protocol actions can then be reasoned about in one consistent system. In order to achieve that, we specify protocols in a declarative manner, i.e. what is to be achieved rather then how agents should interact. A key to this is using logic. Temporal logic, in particular, is suitable for describing and reasoning about temporal constraints while linear logic -LSB- 3 -RSB- is quite suitable for modeling resources. We suggest using a combination of linear logic and temporal logic to construct a commitment based interaction framework which allows both temporal and resource-related reasoning for interaction protocols. This provides a natural manipulation and reasoning mechanism as well as internal enforcement mechanisms for commitments based on proof search. Section 2 discusses the background material of linear logic, temporal linear logic and commitments. Section 3 introduces our modeling framework and specification of protocols. Section 4 discusses how our framework can be used for an example of online sale interactions between a merchant, a bank and a customer. We then discuss the advantages and limitations of using our framework to model interaction protocols and achieve flexibility in Section 5. Section 6 presents our conclusions and items of further work. 2. BACKGROUND In order to increase the agents ' autonomy over their interactive behaviors, protocols should be specified in terms of what is to be achieved rather than how the agents should act. In other words, protocols should be specified in a declarative manner. Using logic is central to this specification process. 2.1 Linear Logic Logic has been used as formalism to model and reason about agent systems. Linear logic -LSB- 3 -RSB- is well-known for modeling resources as well as updating processes. It has been considered in agent systems to support agent negotiation and planning by means of proof search -LSB- 5, 8 -RSB-. In real life, resources are consumed and new resources are created. In such logic as classical or temporal logic, however, a direct mapping of resources onto formulas is troublesome. If we model resources like A as `` one dollar '' and B as `` a chocolate bar '', then A = * B in classical logic is read as `` from one dollar we can get a chocolate bar ''. In order to resolve such resource - formula mapping issues, Girard proposed the constraints on which formulas will be used exactly once and can no longer be freely added or removed in derivations and hence treating linear logic formulas as resources. In linear logic, a linear implication A -- B, however, allows A to be removed after deriving B, which means the dollar is gone after using one dollar to buy a chocolate bar. Classical conjunction -LRB- and -RRB- and disjunction -LRB- or -RRB- are recast over different uses of contexts - multiplicative as combining and additive as sharing to come up with four connectives. The ability to specify choices via the additive connectives is a particularly useful feature of linear logic. A & -LRB- additive conjunction -RRB- B, stands for one own choice, either of A or B but not both. In agent systems, this duality between inner and outer choices is manifested by one agent having the power to choose between alternatives and the other having to react to whatever choice is made. Moreover, during interaction, the ability to match consumption and supply of resources among agents can simplify the specification of resource allocations. Linear logic is a natural mechanism to provide this ability -LSB- 5 -RSB-. In addition, it is emphasized in -LSB- 8 -RSB- that linear logic is used to model agent states as sets of consumable resources and particularly, linear implication is used to model transitions among states and capabilities of agents. 2.2 Temporal Linear Logic While linear logic provides advantages to modeling and reasoning about resources, it does not deal naturally with time constraints. Temporal logic, on the other hand, is a formal system which addresses the description and reasoning about the changes of truth values of logic expressions over time -LSB- 2 -RSB-. Temporal logic can be used for specification and verification of concurrent and reactive programs -LSB- 2 -RSB-. Temporal Linear Logic -LRB- TLL -RRB- -LSB- 6 -RSB- is the result of introducing temporal logic into linear logic and hence is resourceconscious as well as deals with time. The temporal operators used are Q -LRB- next -RRB-, \u2751 -LRB- anytime -RRB-, and O -LRB- sometime -RRB- -LSB- 6 -RSB-. Formulas with no temporal operators can be considered as being available only at present. Adding Q to a formula A, i.e. QA, means that A can be used only at the next time and exactly once. Similarly, \u2751 A means that A can be used at any time and exactly once. OA means that A can be used once at some time. Though both \u2751 and O refer to a point in time, the choice of which time is different. Regarding \u2751, the choice is an internal choice, as appropriate to one 's own capability. With O, the choice is externally decided by others. 2.3 Commitment The concept of social commitment has been recognized as fundamental to agent interaction. Indeed, social commitment provides intrinsic meanings of protocol actions and states -LSB- 11 -RSB-. In particular, persistence in commitments introduces into agents ' consideration a certain level of predictability of other agents ' actions, which is important when agents deal with issues of inter-dependencies, global constraints or The Sixth Intl.. Joint Conf. resources sharing -LSB- 7 -RSB-. Commitment based approaches associate protocols actions with operations on commitments and protocol states with the set of effective commitments -LSB- 11 -RSB-. Completing the protocol is done via means-end reasoning on commitment operations to bring the current state to final states where all commitments are resolved. From then, the corresponding legal sequences of interactive actions are determined. Hence, the approaches systematically enhance a variety of legal computations -LSB- 11 -RSB-. Commitments can be reduced to a more fundamental form known as pre-commitments. A pre-commitment here refers to a potential commitment that specifies what the owner agent is willing to commit -LSB- 4 -RSB-, like performing some actions or achieving a particular state. Agents can negotiate about pre-commitments by sending proposals of them to others. Once a precommitment is agreed, it then becomes a commitment and the process moves from negotiation phase to commitment phase, in which the agents act to fulfill their commitments.", "keyphrases": ["multi-agent environ", "interact behavior", "tempor constraint", "interact protocol", "linear logic", "multipl conjunct", "classic conjunct", "level of predict", "pre-commit", "linear implic", "emerg protocol", "condit commit", "request messag", "causal relationship"]}
{"file_name": "J-3", "text": "Budget Optimization in Search-Based Advertising Auctions ABSTRACT Internet search companies sell advertisement slots based on users ' search queries via an auction. While there has been previous work on the auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return -LRB- the number of user clicks on their ads -RRB- for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least a 1 \u2212 1/e fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem. 1. INTRODUCTION Online search is now ubiquitous and Internet search companies such as Google, Yahoo! and MSN let companies and * Work done while visiting Google, Inc., New York, NY. individuals advertise based on search queries posed by users. Conventional media outlets, such as TV stations or newspapers, price their ad slots individually, and the advertisers buy the ones they can afford. In contrast, Internet search companies find it difficult to set a price explicitly for the advertisements they place in response to user queries. Thus, they rely on the market to determine suitable prices by using auctions amongst the advertisers. It is a challenging problem to set up the auction in order to effect a stable market in which all the parties -LRB- the advertisers, users as well as the Internet search company -RRB- are adequately satisfied. The perspective in this paper is not of the Internet search company that displays the advertisements, but rather of the advertisers. The challenge from an advertiser 's point of view is to understand and interact with the auction mechanism. The advertiser determines a set of keywords of their interest and then must create ads, set the bids for each keyword, and provide a total -LRB- often daily -RRB- budget. When a user poses a search query, the Internet search company determines the advertisers whose keywords match the query and who still have budget left over, runs an auction amongst them, and presents the set of ads corresponding to the advertisers who `` win '' the auction. The advertiser whose ad appears pays the Internet search company if the user clicks on the ad. The focus in this paper is on how the advertisers bid. For the particular choice of keywords of their interest1, an advertiser wants to optimize the overall effect of the advertising campaign. The Internet search companies are supportive to1The choice of keywords is related to the domain-knowledge of the advertiser, user behavior and strategic considerations. Internet search companies provide the advertisers with summaries of the query traffic which is useful for them to optimize their keyword choices interactively. We do not directly address the choice of keywords in this paper, which is addressed elsewhere -LSB- 13 -RSB-. wards advertisers and provide statistics about the history of click volumes and prediction about the future performance of various keywords. 9 There are complex interactions between keywords because a user query may match two or more keywords, since the advertiser is trying to cover all the possible keywords in some domain. In effect the advertiser ends up competing with herself. As a result, the advertisers face a challenging optimization problem. The focus of this paper is to solve this optimization problem. 1.1 The Budget Optimization Problem We present a short discussion and formulation of the optimization problem faced by advertisers ; a more detailed description is in Section 2. A given advertiser sees the state of the auctions for searchbased advertising as follows. There is a set K of keywords of interest ; in practice, even small advertisers typically have a large set K. There is a set Q of queries posed by the users. For each query q G Q, there are functions giving the clicksq -LRB- b -RRB- and costq -LRB- b -RRB- that result from bidding a particular amount b in the auction for that query, which we model more formally in the next section. There is a bipartite graph G on the two vertex sets representing K and Q. For any query q G Q, the neighbors of q in K are the keywords that are said to `` match '' the query q. 2 The budget optimization problem is as follows. Given graph G together with the functions clicksq -LRB-. -RRB- and costq -LRB-. -RRB- on the queries, as well as a budget U, determine the bids bk for each keyword k G K such that Pq clicksq -LRB- bq -RRB- is maximized subject to Pq costq -LRB- bq -RRB- < U, where the `` effective bid '' bq on a query is some function of the keyword bids in the neighborhood of q. While we can cast this problem as a traditional optimization problem, there are different challenges in practice depending on the advertiser 's access to the query and graph information, and indeed the reliability of this information -LRB- e.g., it could be based on unstable historical data -RRB-. Thus it is important to find solutions to this problem that not only get many clicks, but are also simple, robust and less reliant on the information. In this paper we define the notion of a `` uniform '' strategy which is essentially a strategy that bids uniformly on all keywords. Since this type of strategy obviates the need to know anything about the particulars of the graph, and effectively aggregates the click and cost functions on the queries, it is quite robust, and thus desirable in practice. What is surprising is that uniform strategy actually performs well, which we will prove. 1.2 Our Main Results and Technical Overview We present positive and negative results for the budget optimization problem. In particular, we show : 9 Nearly all formulations of the problem are NP-Hard. In cases slightly more general than the formulation above, where the clicks have weights, the problem is inapproximable better than a factor of 1 -- 1e, unless P = NP. 9 We give a -LRB- 1 -- 1/e -RRB- - approximation algorithm for the budget optimization problem. The strategy found by the algorithm is a two-bid uniform strategy, which means that it randomizes between bidding some value b1 on all keywords, and bidding some other value b2 on all keywords until the budget is exhausted3. We show that this approximation ratio is tight for uniform strategies. We also give a -LRB- 1/2 -RRB- - approximation algorithm that offers a single-bid uniform strategy, only using one value b1. -LRB- This is tight for single-bid uniform strategies. -RRB- These strategies can be computed in time nearly linear in JQJ + JKJ, the input size. Uniform strategies may appear to be naive in first consideration because the keywords vary significantly in their click and cost functions, and there may be complex interaction between them when multiple keywords are relevant to a query. After all, the optimum can configure arbitrary bids on each of the keywords. Even for the simple case when the graph is a matching, the optimal algorithm involves placing different bids on different keywords via a knapsack-like packing -LRB- Section 2 -RRB-. So, it might be surprising that a simple two-bid uniform strategy is 63 % or more effective compared to the optimum. Our proof of the 1 -- 1/e approximation ratio relies on an adversarial analysis. We define a factor-revealing LP -LRB- Section 4 -RRB- where primal solutions correspond to possible instances, and dual solutions correspond to distributions over bidding strategies. We have conducted simulations using real auction data from Google. The results of these simulations, which are highlighted at the end of Section 4, suggest that uniform bidding strategies could be useful in practice. 8. CONCLUDING REMARKS Another interesting generalization is to consider weights on the clicks, which is a way to model conversions. -LRB- A conversion corresponds to an action on the part of the user who clicked through to the advertiser site ; e.g., a sale or an account sign-up. -RRB- Finally, we have looked at this system as a black box returning clicks as a function of bid, whereas in reality it is a complex repeated game involving multiple advertisers. In -LSB- 3 -RSB-, it was shown that when a set of advertisers use a strategy similar to the one we suggest here, under a slightly modified first-price auction, the prices approach a well-understood market equilibrium.", "keyphrases": ["budget optim", "search-base advertis auction", "internet", "advertis", "game theori", "intrigu heurist", "keyword", "uniform bid strategi", "vickrei clark grove", "lp", "gener second price"]}
{"file_name": "I-18", "text": "Collaboration Among a Satellite Swarm ABSTRACT The paper deals with on-board planning for a satellite swarm via communication and negotiation. We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements. We will present the formalization of the problem, a communication protocol, a solving method based on reactive decision rules, and first results. 1. INTRODUCTION Multi-agent architectures have been developed for satellite swarms -LSB- 36, 38, 42 -RSB- but strong assumptions on deliberation and communication capabilities are made in order to build a collective plan. In a multi-agent context, agents that build a collective plan must be able to change their goals, reallocate resources and react to environment changes and to the others ' choices. However, this step needs high communication and computation capabilities. In order to relax communication constraints, coordination based on norms and conventions -LSB- 16 -RSB- or strategies -LSB- 17 -RSB- are considered. Norms constraint agents in their decisions in such a way that the possibilities of conflicts are reduced. Strategies are private decision rules that allow an agent to draw benefit from the knowledgeable world without communication. However, communication is still needed in order to share information and build collective conjectures and plans. Communication can be achieved through a stigmergic approach -LRB- via the environment -RRB- or through message exchange and a protocol. A protocol defines interactions between agents and can not be uncoupled from its goal, e.g. exchanging information, finding a trade-off, allocating tasks and so on. Protocols can be viewed as an abstraction of an interaction -LSB- 9 -RSB-. However, an agent can not always communicate with another agent or the communication possibilites are restricted to short time intervals. At the individual level, agents are deliberative in order to create a local plan but at the collective level, they use normative decision rules in order to coordinate with one another. We will present the features of our problem, a communication protocol, a method for request allocation and finally, collaboration strategies. 7. EXPERIMENTS Satellite swarm simulations have been implemented in JAVA with the JADE platform -LSB- 3 -RSB-. The on-board planner is implemented with linear programming using ILOG CPLEX -LSB- 1 -RSB-. The simulation scenario implements 3 satellites on 6hour orbits. Two scenarios have been considered : the first one with a set of 40 requests with low mutual exclusion and conflict rate and the second one with a set of 74 requests with high mutual exclusion and conflict rate. In the case of low mutual exclusion and conflict rate -LRB- Table 1 -RRB-, centralized and isolated simulations lead to the same number of observations, with the same average priorities. Isolation leading to a lower cost is due to the high number of redundancies : many agents carry out the same request at different costs. The informed simulation reduces the number of redundancies but sligthly increases the average cost for the same reason. We can notice that the use of 5For instance, the rank-1-expert agent withdraws due to the altruist strategy and the cost increases by a in the worst case, then rank-2-expert agent withdraws due to the altruist strategy and the cost increases by e in the worst case. So the cost has increased by 2e in the worst case. 292 The Sixth Intl.. Joint Conf. Table 1 : Scenario 1 - the 40-request simulation results Table 2 : Scenario 2 - the 74-request simulation results collaboration strategies allows the number of redundancies to be much more reduced but the number of observations decreases owing to the constraint created by commitments. Furthermore, the average cost is increased too. Nevertheless each avoided redundancy corresponds to saved resources to realize on-board generated requests during the simulation. In the case of high mutual exclusion and conflict rate -LRB- Table 2 -RRB-, noteworthy differences exist between the centralized and isolated simulations. We can notice that all informed simulations -LRB- with or without strategies -RRB- allow to perform more observations than isolated agents do with less redundancies. Likewise, we can notice that all politics reduce the average cost contrary to the first scenario. The drastic politics is interesting because not only does it allow to perform more observations than isolated agents do but it allows to highly reduce the average cost with the lowest number of redundancies. As far as the number of exchanged messages is concerned, there are 12 meetings between 2 agents during the simulations. In the worst case, at each meeting each agent sends N pieces of information on the requests plus 3N pieces of information on the agents ' intentions plus 1 message for the end of communication, where N is the total number of requests. Consequently, 3864 messages are exchanged in the worst case for the 40-request simulations and 7128 messages for the 74-request simulations. These numbers are much higher than the number of messages that are actually exchanged. We can notice that the informed simulations, that communicate only requests, allow a higher reduction. In the general case, using communication and strategies allows to reduce redundancies and saves resources but increases the average cost : if a request is realized, agents that know it do not plan it even if its cost can be reduce afterwards. It is not the case with isolated agents. Using strategies on little constrained problems such as scenario 1 constrains the agents too much and causes an additional cost increase. Strategies are more useful on highly constrained problems such as scenario 2. Although agents constrain themselves on the number of observations, the average cost is widely reduce. 8. CONCLUSION AND FUTURE WORK An observation satellite swarm is a cooperative multiagent system with strong constraints in terms of communication and computation capabilities. In order to increase the global mission outcome, we propose an hybrid approach : deliberative for individual planning and reactive for collaboration. Agents reason both on requests to carry out and on the other agents ' intentions -LRB- candidacies -RRB-. An epidemic communication protocol uses all communication opportunities to update this information. Reactive decision rules -LRB- strategies -RRB- are proposed to solve conflicts that may arise between agents. Through the tuning of the strategies -LRB- \u03b1, e and \u03bb -RRB- and their plastic interlacing within the protocol, it is possible to coordinate agents without additional communication : the number of exchanged messages remains nearly the same between informed simulations and simulations implementing strategies. Some simulations have been made to experimentally validate these protocols and the first results are promising but raise many questions. What is the trade-off between the constraint rate of the problem and the need of strategies? To what extent are the number of redundancies and the average cost affected by the tuning of the strategies? Future works will focus on new strategies to solve new conflicts, specially those arising when relaxing the independence assumption between the requests. A second point is to take into account the complexity of the initial planning problem. Indeed, the chosen planning approach results in a combinatory explosion with big sets of requests : an anytime or a fully reactive approach has to be considered for more complex problems.", "keyphrases": ["on-board plan", "satellit swarm", "commun and negoti", "reactiv decis rule", "inform system applic", "multiag system", "task and resourc alloc", "objectag architectur", "teamag", "dip", "prospect ant"]}
{"file_name": "H-16", "text": "The Impact of Caching on Search Engines ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. 1. INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial. Caching can be applied at different levels with increasing response latencies or processing requirements. The decision of what to cache is either off-line -LRB- static -RRB- or online -LRB- dynamic -RRB-. A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss. Such online decisions are based on a cache policy, and several different policies have been studied in the past. For a search engine, there are two possible ways to use a cache memory : Caching answers : As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries. Caching terms : As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing. Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers. Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges : when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient. Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time. Figure 1 : One caching level in a distributed search architecture. In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms. More concretely, our main conclusions are that : \u2022 Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists ; \u2022 Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the KNAPSACK problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90 % ; \u2022 Changes of the query distribution over time have little impact on static caching. Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2. RELATED WORK There is a large body of work devoted to query optimization. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists -LSB- 1, 4, 15 -RSB-. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. Markatos -LSB- 10 -RSB- shows the existence of temporal locality in queries, and compares the performance of different caching policies. Fagni et al. follow Markatos ' work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio -LSB- 7 -RSB-. Different from our work, they consider caching and prefetching of pages of results. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system -LSB- 13 -RSB-. Their goal for such systems has been to improve response time for hierarchical engines. In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput. Long and Suel propose a caching system structured according to three different levels -LSB- 9 -RSB-. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy. Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching -LSB- 17 -RSB-, Web caching -LSB- 5 -RSB-, and even caching of posting lists -LSB- 9 -RSB-, but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists. 8. CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching. Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50 % using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12 %. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10 % higher compared these strategies. We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. Figure 14 : Impact of distribution changes on the static caching of posting lists.", "keyphrases": ["effici cach system", "web search engin", "static cach", "dynam cach", "cach queri result", "cach post list", "static cach", "answer and post list", "queri log", "effect of static cach", "distribut of the queri", "data-access hierarchi", "disk layer", "remot server layer"]}
{"file_name": "J-32", "text": "Nash Equilibria in Graphical Games on Trees Revisited * Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents. When the number of players is large, and the underlying graph has low degree, they provide a concise way to represent the players ' payoffs. It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD, indicating that it is unlikely that there is any polynomial-time algorithm for this problem. In this paper, we study the complexity of graphical games with two actions per player on bounded-degree trees. This setting was first considered by Kearns, Littman and Singh, who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games. The running time of their algorithm is exponential, though approximate equilibria can be computed efficiently. Later, Littman, Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time. We show that this modified algorithm is incorrect -- the output is not always a Nash equilibrium. We then propose a new algorithm that is based on the ideas of Kearns et al. and computes all Nash equilibria in quadratic time if the input graph is a path, and in polynomial time if it is an arbitrary graph of maximum degree 2. Moreover, our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees, but the running time can be exponential, even when the tree has bounded degree. We show that this is inevitable -- any algorithm of this type will take exponential time, even on bounded-degree trees with pathwidth 2. It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1, but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete -LRB- so is unlikely to be tractable -RRB-. * This research is supported by the EPSRC research grants `` Algorithmics of Network-sharing Games '' and `` Discontinuous Behaviour in the Complexity of randomized Algorithms ''. 1. INTRODUCTION Graphical games were introduced in the papers of Kearns et al. -LSB- 8 -RSB- and Littman et al. -LSB- 9 -RSB- as a succinct representation of games with a large number of players. The classical normal form -LRB- or matrix form -RRB- representation has a size that is exponential in the number of players, making it unsuitable for large-scale distributed games. A graphical game associates each player with a vertex of an underlying graph G, and the payoff to that player is a function of the actions chosen by himself and his neighbours in G ; if G has low degree, this is a concise way to represent a game with many players. The papers -LSB- 8, 9 -RSB- give a dynamic-programming algorithm for finding Nash equilibria in graphical games where there are two actions per player and G is a tree. The first of these papers describes a generic algorithm for this problem that can be specialized in two ways : as an algorithm that computes approximations to all Nash equilibria in time polynomial in the input size and the approximation quality, or as an exponential-time algorithm that allows the exact computation of all Nash equilibria in G. In -LSB- 9 -RSB-, the authors propose a modification to the latter algorithm that aims to find a single Nash equilibrium in polynomial time. This does not quite work, as we show in Section 3, though it introduces a useful idea. 1.1 Background The generic algorithm of -LSB- 8 -RSB- consists of two phases which we will refer to as the upstream pass and the downstream pass ; 1 the former starts at the leaves of the tree and ends at the root, while the latter starts at the root and ends at the leaves. there is a Nash equilibrium in the graphical game downstream of V -LRB- inclusive -RRB- given that W plays w -LRB- for a more technical definition, the reader is referred to Section 2 -RRB-. The generic algorithm does not address the problem of representing the best response policy ; in fact, the most important difference between the two instantiations of the generic algorithm described in -LSB- 8 -RSB- is in their approach to this issue. The computation is performed inductively : the best response policy for V is computed based on the best response policies of V 's children U1,..., Uk. By the end of the upstream pass, all children of the root have computed their best response policies. In the beginning of the downstream pass, the root selects its strategy and informs its children about its choice. It also selects a strategy for each child. A necessary and sufficient condition for the algorithm to proceed is that the strategy of the root is a best response to the strategies of its children and, for each child, the chosen strategy is one of the pre-computed potential best responses to the chosen strategy of the root. The equilibrium then propagates downstream, with each vertex selecting its children 's actions. The action of the child is chosen to be any strategy from the pre-computed potential best responses to the chosen strategy of the parent. To bound the running time of this algorithm, the paper -LSB- 8 -RSB- shows that any best response policy can be represented as a union of an exponential number of rectangles ; the polynomial time approximation algorithm is obtained by combining this representation with a polynomial-sized grid. 1.2 Our Results One of the main contributions of our paper is to show that the algorithm proposed by -LSB- 9 -RSB- is incorrect. In Section 3 we describe a simple example for which the algorithm of -LSB- 9 -RSB- outputs a vector of strategies that does not constitute a Nash equilibrium of the underlying game. In Sections 4, 5 and 6 we show how to fix the algorithm of -LSB- 9 -RSB- so that it always produces correct output. Section 4 considers the case in which the underlying graph is a path of length n. For this case, we show that the number of rectangles in each of the best response policies is O -LRB- n2 -RRB-. This gives us an O -LRB- n3 -RRB- algorithm for finding a Nash equilibrium, and for computing a representation of all Nash equilibria. -LRB- This algorithm is a special case of the generic algorithm of -LSB- 8 -RSB- -- we show that it runs in polynomial time when the underlying graph is a path. -RRB- We can improve the running time of the generic algorithm using the ideas of -LSB- 9 -RSB-. In particular, we give an O -LRB- n2 -RRB- algorithm for finding a Nash equilibrium of a graphical game on a path of length n. Instead of storing best response policies, this algorithm stores appropriately-defined subsets, which, following -LSB- 9 -RSB-, we call breakpoint policies -LRB- modifying the definition as necessary -RRB-. We obtain the following theorem THEOREM 1. There is an O -LRB- n2 -RRB- algorithm that finds a Nash equilibrium of a graphical game with two actions per player on an n-vertex path. There is an O -LRB- n3 -RRB- algorithm that computes a representation of all Nash equilibria of such a game. In Section 5 we extend the results of Section 4 to general degree2 graphs, obtaining the following theorem. THEOREM 2. There is a polynomial-time algorithm thatfinds a Nash equilibrium of a graphical game with two actions per player on a graph with maximum degree 2. In Section 6 we extend our algorithm so that it can be used to find a Nash equilibrium of a graphical game on an arbitrary tree. Even when the tree has bounded degree, the running time can be exponential. We show that this is inevitable by constructing a family of graphical games on bounded-degree trees for which best response policies of some of the vertices have exponential size, and any twopass algorithm -LRB- i.e., an algorithm that is similar in spirit to that of -LSB- 8 -RSB- -RRB- has to store almost all points of the best response policies. In particular, we show the following. THEOREM 3. There is an infinite family ofgraphical games on bounded-degree trees with pathwidth 2 such that any two-pass algorithm for finding Nash equilibria on these trees requires exponential time and space. It is interesting to note that the trees used in the proof of Theorem 3 have pathwidth 2, that is, they are very close to being paths. It is an open question whether our algorithm runs in polynomial time for graphs of pathwidth 1. This question can be viewed as a generalization of a very natural computational geometry problem -- we describe it in more detail in Section 8. In Section 7, we give a complexity-theoretic intractability result for the problem of finding a Nash equilibrium of a graphical game on a graph with small pathwidth. We prove the following theorem. THEOREM 4. Consider the problem offinding a Nash equilibrium for a graphical game in which the underlying graph has maximum degree 3 and pathwidth k. There is a constant k such that this problem is PPAD-complete. Theorem 4 limits the extent to which we can exploit `` path-like '' properties of the underlying graph, in order to find Nash equilibria. To prove Theorem 4, we use recent PPAD-completeness results for games, in particular the papers -LSB- 7, 4 -RSB- which show that the problem of finding Nash equilibria in graphical games of degree d -LRB- for d > 3 -RRB- is computationally equivalent to the problem of solving r-player normal-form games -LRB- for r > 4 -RRB-, both of which are PPAD-complete. 8. OPEN PROBLEMS The most important problem left open by this paper is whether it is possible to find a Nash equilibrium of a graphical game on a bounded-degree tree in polynomial time. Our construction shows that any two-pass algorithm that explicitly stores breakpoint policies needs exponential time and space. However, it does not preclude the existence of an algorithm that is based on a similar idea, but, instead of computing the entire breakpoint policy for each vertex, uses a small number of additional passes through the graph to decide which -LRB- polynomial-sized -RRB- parts of each breakpoint policy should be computed. In particular, such an algorithm may be based on the approximation algorithm of -LSB- 8 -RSB-, where the value of e is chosen adaptively. Another intriguing question is related to the fact that the graph for which we constructed an exponential-sized breakpoint policy has pathwidth 2, while our positive results are for a path, i.e., a graph of pathwidth 1. It is not clear if for any bounded-degree graph of pathwidth 1 the running time of -LRB- the breakpoint policybased version of -RRB- our algorithm will be polynomial. In particular, it is instructive to consider a `` caterpillar '' graph, i.e., the graph that can be obtained from Tn by deleting the vertices S1,..., Sn. This implies that the problem of bounding the size of the best response policy -LRB- or, alternatively, the breakpoint policy -RRB-, can be viewed as a generalization of the following computational geometry problem, which we believe may be of independent interest : PROBLEM 1. Ifyes, can it be the case that in this set, there is no path with a polynomial number of turns that connects the endpoints of the original segment? This implies that even for a caterpillar, the best response policy can be exponentially large. However, in our example -LRB- which is omitted from this version of the paper due to space constraints -RRB-, there exists a polynomial-size path through the best response policy, i.e., it does not prove that the breakpoint policy is necessarily exponential in size. If one can prove that this is always the case, it may be possible to adapt this proof to show that there can be an exponential gap between the sizes of best response policies and breakpoint policies.", "keyphrases": ["graphic game", "larg-scale distribut network", "nash equilibrium", "degre", "dynam program-base algorithm", "ppad-complet", "bound-degre tree", "gener algorithm", "respons polici", "downstream pass", "breakpoint polici"]}
{"file_name": "I-16", "text": "An Advanced Bidding Agent for Advertisement Selection on Public Displays ABSTRACT In this paper we present an advanced bidding agent that participates in first-price sealed bid auctions to allocate advertising space on BluScreen -- an experimental public advertisement system that detects users through the presence of their Bluetooth enabled devices. Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts, and the auctions that it participates within. It then uses these models to maximise the exposure that its adverts receive. We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy, random allocation, and a centralised optimal allocation with perfect foresight. Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation, and in a mixed population of agents it is able to expose its adverts to 25 % more users than the simple bidding strategy. Moreover, its performance is within 7.5 % of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate. 1. INTRODUCTION Electronic displays are increasingly being used within public environments, such as airports, city centres and retail stores, in order to advertise commercial products, or to entertain and inform passersby. of interactive public displays have been proposed. As such, these systems assume prior knowledge about the target audience, and require either that a single user has exclusive access to the display, or that users carry specific tracking devices so that their presence can be identified -LSB- 6, 11 -RSB-. However, these approaches fail to work in public spaces, where no prior knowledge regarding the users who may view the display exists, and where such displays need to react to the presence of several users simultaneously. By contrast, Payne et al. have developed an intelligent public display system, named BluScreen, that detects and tracks users through the Bluetooth enabled devices that they carry with them everyday -LSB- 8 -RSB-. Within this system, a decentralised multi-agent auction mechanism is used to efficiently allocate advertising time on each public display. Each advert is represented by an individual advertising agent that maintains a history of users who have already been exposed to the advert. This agent then seeks to acquire advertising cycles -LRB- during which it can display its advert on the public displays -RRB- by submitting bids to a marketplace agent who implements a sealed bid auction. The value of these bids is based upon the number of users who are currently present in front of the screen, the history of these users, and an externally derived estimate of the value of exposing an advert to a user. In this paper, we present an advanced bidding agent that significantly extends the sophistication of this approach. In particular, we consider the more general setting in which it is impossible to determine an a priori valuation for exposing an advert to a user. In addition, it is also likely to be the case within new commercial installations where limited market experience makes estimating a valuation impossible. The advertising agent is then simply tasked with using this budget to maximum effect -LRB- i.e. to achieve the maximum possible advert exposure within this time period -RRB-. Now, in order to achieve this goal, the advertising agent must be capable of modelling the behaviour of the users in order to predict the number who will be present in any future advertising cycle. In addition, it must also understand the auction environment in which it competes, in order that it may make best use of its limited budget. Thus, in developing an advanced bidding agent that achieves this, we advance the state of the art in four key ways : 1. We enable the advertising agents to model the arrival and departure of users as independent Poisson processes, and to make maximum likelihood estimates of the rates of these processes based on their observations. We show how these agents can then calculate the expected number of users who will be present during any future advertising cycle. 2. Using a decision theoretic approach we enable the advertising agents to model the probability of winning any given auction when a specific amount is bid. The cumulative form of the gamma distribution is used to represent this probability, and its parameters are fitted using observations of both the closing price of previous auctions, and the bids that that advertising agent itself submits. 3. We show that our explicit assumption that the advertising agent derives no additional benefit by showing an advert to a single user more than once, causes the expected utility of each future advertising cycle to be dependent on the expected outcome of all the auctions that precede it. We thus present a stochastic optimisation algorithm based upon simulated annealing that enables the advertising agent to calculate the optimal sequence of bids that maximises its expected utility. 4. The remainder of this paper is organised as follows : Section 2 discusses related work where agents and auction-based marketplaces are used to allocated advertising space. Section 3 describes the prototype BluScreen system that motivates our work. In section 4 we present a detailed description of the auction allocation mechanism, and in section 5 we describe our advanced bidding strategy for the advertising agents. In section 6 we present an empirical validation of our approach, and finally, we conclude in section 7. 2. RELATED WORK The commercial attractiveness of targeted advertising has been amply demonstrated on the internet, where recommendation systems and contextual banner adverts are the norm -LSB- 1 -RSB-. Attempts to apply these approaches within the real world have been much more limited. Gerding et al. present a simulated system -LRB- CASy -RRB- whereby a Vickrey auction mechanism is used to sell advertising space within a modelled electronic shopping mall -LSB- 2 -RSB-. The auction is used to rank a set of possible advertisements provided by different retail outlets, and the top ranking advertisements are selected for presentation on public displays. Feedback is provided through subsequent sales information, allowing the model to build up a profile of a user 's preferences. However, unlike the BluScreen Figure 1 : A deployed BluScreen prototype. system that we consider here, it is not suitable for advertising to many individuals simultaneously, as it requires explicit interaction with a single user to acquire the user 's preferences. User identification is based on infrared badges and embedded sensors within an office environment. When several users pass by the display, a centralised system compares the user 's profiles to identify common areas of interest, and content that matches this common interest is shown. Thus, whilst CASy is a simulated system that allows advertisers to compete for the attention of single user, GroupCast is a prototype system that detects the presence of groups of users and selects content to match their profiles. Despite their similarities, neither system addresses the settings that interests us here : how to allocate advertising space between competing advertisers who face an audience of multiple individuals about whom there is no a priori profile information. Thus, in the next section we describe the prototype BluScreen system that motivates our work. 7. CONCLUSIONS In this paper, we presented an advanced bidding strategy for use by advertising agents within the BluScreen advertising system. This bidding strategy enabled advertising agents to model and predict the arrival and departure of users, and also to model their success within a first-price sealed bid auction by observing both the bids that they themselves submitted and the winning bid. The ex The Sixth Intl.. Joint Conf. Figure 8 : Comparison of an evenly mixed population of advertising agents using simple and advanced bidding strategies over a range of parameter settings. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. Figure 9 : Comparison of an unevenly mixed population of advertising agents using simple and advanced bidding strategies. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. pected utility, measured as the number of users who the advertising agent exposes its advert to, was shown to depend on these factors, and resulted in a complex expression where the expected utility of each auction depended on the success or otherwise of earlier auctions. We presented an algorithm based upon simulated annealing to solve for the optimal bidding strategy, and in simulation, this bidding strategy was shown to significantly outperform a simple bidding strategy that had none of these features. Its performance closely approached that of a central optimal allocation, with perfect knowledge of the arrival and departure of users, despite the uncertain environment in which the strategy must operate. This work will continue to be done in conjunction with the deployment of more BluScreen prototypes in order to gain further real world experience.", "keyphrases": ["advanc bid agent", "bluscreen", "experiment public advertis system", "bluetooth", "probabilist model", "centralis optim alloc", "distribut artifici intellig", "decentralis multi-agent auction mechan", "independ poisson process", "decis theoret approach", "stochast optimis algorithm"]}
{"file_name": "J-18", "text": "Mediators in Position Auctions ABSTRACT A mediator is a reliable entity, which can play on behalf of agents in a given game. A mediator however can not enforce the use of its services, and each agent is free to participate in the game directly. In this paper we introduce a study of mediators for games with incomplete information, and apply it to the context of position auctions, a central topic in electronic commerce. VCG position auctions, which are currently not used in practice, possess some nice theoretical properties, such as the optimization of social surplus and having dominant strategies. These properties may not be satisfied by current position auctions and their variants. We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions. We require that accepting the mediator services, and reporting honestly to the mediator, will form an ex post equilibrium, which satisfies the following rationality condition : an agent 's payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator 's services, or by the agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price -LRB- Google-like -RRB- position auctions, as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k = 1, the self-price position auction, we show that the existence of such mediator depends on the tie breaking rule used in the auction. 1. INTRODUCTION Consider an interaction in a multi-agent system, in which every player holds some private information, which is called the player 's type. For example, in an auction interaction the type of a player is its valuation, or, in more complex auctions, its valuation function. This interaction is modeled as a game with incomplete information. This game is called a Bayesian game, when a commonly known probability measure on the profiles of types is added to the system. Otherwise it is called a pre-Bayesian game. In this paper we deal only with pre-Bayesian games. Consider the following simple example of a pre-Bayesian game, which possesses an ex post equilibrium. The game is denoted by H.", "keyphrases": ["auction", "mediat", "ex post equilibrium", "agent", "posit auction", "electron commerc", "richer class of posit auction", "next-price posit auction", "multi-agent system", "t-strategi", "vcg outcom function", "self-price posit auction"]}
{"file_name": "H-29", "text": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension : the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given query 's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness -LRB- worst-case performance -RRB- by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method. 1. INTRODUCTION Uncertainty is an inherent feature of information retrieval. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback -LRB- PRF -RRB- tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio 's formula -LSB- 16 -RSB-, or more recent language modeling approaches such as Relevance Models -LSB- 10 -RSB-. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model '. The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model 's weight combines two complementary factors : the model 's probability of generating the query, and the variance of the model, with high-variance models getting lower weight. ` For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings -LRB- see -LSB- 10 -RSB-, p. 62 -RRB-. Figure 1 : Estimating the uncertainty of the feedback model for a single query. 4. RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision. The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. On the document side, recent work by Zhou & Croft -LSB- 21 -RSB- explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. -LSB- 17 -RSB- proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. Greiff, Morgan and Ponte -LSB- 8 -RSB- explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance -- high noise -- increases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models. Estimates of output variance have recently been used for improved text classification. Lee et al. -LSB- 11 -RSB- used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Ando and Zhang proposed a method that they call structural feedback -LSB- 3 -RSB- and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector \u02c6wi of the documents is calculated. Principal component analysis -LRB- PCA -RRB- is then applied to the \u02c6wi to obtain the matrix 4 -RRB- of H left singular vectors \u03c6h that are used to obtain the new, expanded query The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai -LSB- 18 -RSB- describe a method for more robust feedback that allows each document to have a different feedback \u03b1. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter \u03b7 based on a function of some quality measure of feedback documents. 5. CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.", "keyphrases": ["feedback method", "posterior distribut", "enhanc feedback model", "inform retriev", "queri expans", "probabl distribut", "pseudo-relev feedback", "vector space-base algorithm", "risk", "feedback model", "estim uncertainti", "languag model", "feedback distribut"]}
{"file_name": "H-20", "text": "New Event Detection Based on Indexing-tree and Named Entity ABSTRACT New Event Detection -LRB- NED -RRB- aims at detecting from one or multiple streams of news stories that which one is reported on a new event -LRB- i.e. not reported previously -RRB-. With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium -LRB- LDC -RRB- datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems. 1. INTRODUCTION New Event Detection -LRB- NED -RRB- is one of the five tasks in TDT. A Topic is defined as `` a seminal event or activity, along with directly related events and activities '' -LSB- 2 -RSB-. An Event is defined as `` something -LRB- non-trivial -RRB- happening in a certain place at a certain time '' -LSB- 3 -RSB-. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering. In most of state-of-the-art -LRB- currently -RRB- NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. The core problem of NED is to identify whether two stories are on the same topic. Obviously, these systems can not take advantage of topic information. Other systems organize previous stories into clusters -LRB- each cluster corresponds to a topic -RRB-, and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic. On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities -LSB- 10, 11, 12, 13 -RSB-. However, none of the systems have considered that terms of different types -LRB- e.g. Noun, Verb or Person name -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. For example, the names of election candidates -LRB- Person name -RRB- are very important for stories of election class ; the locations -LRB- Location name -RRB- where accidents happened are important for stories of accidents class. -LRB- 2 -RRB- How to make good use of cluster -LRB- topic -RRB- information to improve accuracy? -LRB- 3 -RRB- How to obtain better news story representation by better understanding of named entities. Driven by these problems, we have proposed three approaches in this paper. -LRB- 1 -RRB- To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity. Comparisons between current story and previous clusters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. -LRB- 2 -RRB- We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster -LRB- topic -RRB- information is used properly, so the problem of theme decentralization is avoided. -LRB- 3 -RRB- Based on observations on the statistics obtained from training data, we found that terms of different types -LRB- e.g. Noun and Verb -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2. Section 3 presents the basic model for NED that most current systems use. Section 4 describes our new detection procedure based on news indexing-tree. In section 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evaluation metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2. RELATED WORK Papka et al. proposed Single-Pass clustering on NED -LSB- 6 -RSB-. When a new story was encountered, it was processed immediately to extract term features and a query representation of the story 's content is built up. Then it was compared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic -LSB- 7 -RSB-. In this manner comparisons happen between stories and clusters. Recent years, most work focus on proposing better methods on comparison of stories and document representation. Good improvements on TDT bench-marks were shown. Stokes et al. -LSB- 9 -RSB- utilized a combination of evidence from two distinct representations of a document 's content. One of the representations was the usual free text vector, the other made use of lexical chains -LRB- created using WordNet -RRB- to build another term vector. Then the two representations are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used. Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities -LSB- 10 -RSB-. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity -LSB- 11 -RSB- -LSB- 12 -RSB-. UMass -LSB- 13 -RSB- research group split document representation into two parts : named entities and non-named entities. And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both -LSB- 10 -RSB- and -LSB- 13 -RSB- used text categorization technique to classify news stories in advance. In -LSB- 13 -RSB- news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In -LSB- 10 -RSB- frequent terms for each class are removed from document representation. In their work, effectiveness of different kinds of names -LRB- or terms with different POS -RRB- for NED in different news classes are not investigated. 8. CONCLUSION We have proposed a news indexing-tree based detection procedure in our model. It reduces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensions to the basic TF-IDF model. The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set. And the second extension to basic TF-IDF model is better use of term types -LRB- named entities types and part-of-speed -RRB- according to news categories. Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy. For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.", "keyphrases": ["new event detect", "stream of new stori", "volum of new", "new index-tree", "term reweight approach", "ned accuraci", "term weight", "statist", "train data", "name entiti reweight mode", "class of stori", "linguist data consortium", "baselin system", "exist system"]}
{"file_name": "H-7", "text": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user 's interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens. 1. INTRODUCTION For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a user 's history. One major personalization topic studied in the information retrieval community is content-based personal recommendation systems '. These systems learn user-specific pro'Content - based recommendation is also called adaptive fil files from user feedback so that they can recommend information tailored to each individual user 's interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems. A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the `` cold start '' problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile. There has been much research on improving classification accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal -LSB- 26 -RSB-. Another approach is using domain knowledge. The third approach is borrowing training data from other resources -LSB- 5 -RSB- -LSB- 7 -RSB-. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data. One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user -LSB- 27 -RSB- -LSB- 25 -RSB-. In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data. A mature recommendation system usually works for millions of users. It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario -LRB- Section 4 -RRB-, we find that the EM tering, or item-based collaborative filtering. In this paper, the words `` filtering '' and `` recommendation '' are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O -LRB- MK -RRB-, where M is the number of users and K is the number of dimensions. This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the `` Modified EM algorithm. '' This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results. The organization of the remaining parts of this paper is as follows : Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6. 2. RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970 's. The approaches that have been used to solve this problem can be roughly classified into two major categories : content based filtering versus collaborative filtering. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user. Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past. Memorybased heuristics and model based approaches have been used in collaborative filtering task -LSB- 15 -RSB- -LSB- 8 -RSB- -LSB- 2 -RSB- -LSB- 14 -RSB- -LSB- 12 -RSB- -LSB- 11 -RSB-. This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks -LSB- 27 -RSB- -LSB- 25 -RSB-. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback. Similar to some other researchers -LSB- 18 -RSB- -LSB- 1 -RSB- -LSB- 21 -RSB-, we found that a recommendation system will be more effective when both techniques are combined. 7. CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors. This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a scenario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users. The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model parameters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems.", "keyphrases": ["model", "content-base", "recommend system", "linear regress", "collabor filter", "paramet", "learn techniqu", "ir", "em algorithm", "classif", "rate"]}
{"file_name": "I-32", "text": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions ABSTRACT Multiagent environments are often not cooperative nor collaborative ; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties -LRB- e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model -RRB-. We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms ' appropriateness. 1. INTRODUCTION Early research in multiagent systems -LRB- MAS -RRB- considered cooperative groups of agents ; because individual agents had MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries. Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states -LRB- e.g., -LSB- 8, 4, 5 -RSB- -RRB-. However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments. In addition, traditional search methods -LRB- like Min-Max -RRB- do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning -LSB- 9, 3, 11 -RSB-. In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans -LSB- 4 -RSB- model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms ; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings. We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players ' behavior using a large set of completed match log 4. RELATED WORK However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents ' behavior in it. The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. Additional Adversarial planning work was done by Willmott et al. -LSB- 13 -RSB-, which provided an adversarial planning approach to the game of GO. The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agent 's advantage. However, the basic limitations of those search methods still apply ; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5. CONCLUSIONS We presented an Adversarial Environment model for a 2These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines. The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment. The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research.", "keyphrases": ["multiag environ", "adversari interact", "adversari environ", "behavior axiom", "bilater and multilater instanti", "evalu function", "benefici action", "connect-four game", "empir studi", "axiomat model", "zero-sum encount", "treatment group", "eval valu", "interact"]}
{"file_name": "I-29", "text": "Distributed Management of Flexible Times Schedules ABSTRACT We consider the problem of managing schedules in an uncertain, distributed environment. We assume a team of collaborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unexpected events will force changes to some prescribed activities and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms : -LRB- 1 -RRB- a `` flexible times '' representation of the agent 's schedule -LRB- using a Simple Temporal Network -RRB- and -LRB- 2 -RRB- an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent 's schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportunities for solution improvement through joint change. Using a simulator to model the environment, we compare the performance of our multi-agent system with that of an expected optimal -LRB- but non-scalable -RRB- centralized MDP solver. 1. INTRODUCTION The practical constraints of many application environments require distributed management of executing plans and schedules. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency -LRB- fall-back -RRB- options. Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. In a flexible-times representation of an agent 's schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. However their use in distributed problem solving settings has been quite sparse -LRB- -LSB- 7 -RSB- is one exception -RRB-, and prior approaches to multi-agent scheduling -LRB- e.g., -LSB- 6, 13, 5 -RSB- -RRB- have generally operated with fixed-times representations of agent schedules. We define an agent architecture centered around incremental management of a flexible times schedule. Local change is ac Figure 1 : A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process -LRB- similar in some respects to -LSB- 5 -RSB- -RRB-, aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans. 8. STATUS AND DIRECTIONS Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods. This scale places much higher computational demands on all of the agent 's components. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems. To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms. To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling -LSB- 7 -RSB- to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules.", "keyphrases": ["manag schedul", "distribut environ", "agent architectur", "schedul", "inter-depend activ", "geograph separ", "flexibl time", "central plan", "manag", "schedul-execut", "slack", "shortest path algorithm", "activ alloc", "conflict-driven approach", "optimist synchron", "inter-agent coordin", "perform"]}
{"file_name": "C-30", "text": "Bullet : High Bandwidth Data Dissemination Using an Overlay Mesh ABSTRACT In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include : i -RRB- an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii -RRB- a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii -RRB- a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node 's parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate. 1. INTRODUCTION In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable -LRB- and hence extensible -RRB- end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree -LSB- 2, 6 -RSB-. However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node 's single parent in the tree. Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically transmit disjoint data sets to various points in the network. Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the `` last object '', which may only be available at a few nodes. To illustrate Bullet 's behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available -LRB- TCP-friendly -RRB- bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data. Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly -LSB- 15 -RSB-. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to `` peer '' with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants ; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure `` high up '' in a multicast overlay tree. In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques -LSB- 1, 18, 21, 24, 34 -RSB-. Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Finally, we use TFRC -LSB- 15 -RSB- to transfer data both down the overlay tree and among peers. One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source -LRB- its parent -RRB-. Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research -LSB- 20, 35 -RSB-, accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system. Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. We have completed a prototype of Bullet running on top of a number of overlay trees. Our evaluation of a 1000-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree -LRB- using an offline algorithm and global network topology information -RRB-, all while remaining TCP friendly. For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 2 presents Bullet 's system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet 's performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions. 5. RELATED WORK Snoeren et al. -LSB- 36 -RSB- use an overlay mesh to achieve reliable and timely delivery of mission-critical data. In this system, every node chooses n `` parents '' from which to receive duplicate packet streams. Since its foremost emphasis is reliability, the system does not attempt to improve the bandwidth delivered to the overlay participants by sending disjoint data at each level. Further, during recovery from parent failure, it limits an overlay router 's choice of parents to nodes with a level number that is less than its own level number. Kazaa nodes are organized into a scalable, hierarchical structure. Individual users search for desired content in the structure and proceed to simultaneously download potentially disjoint pieces from nodes that already have it. Since Kazaa does not address the multicast communication model, a large fraction of users downloading the same file would consume more bandwidth than nodes organized into the Bullet overlay structure. BitTorrent -LSB- 3 -RSB- is another example of a file distribution system currently deployed on the Internet. The tracker poses a scalability limit, as it continuously updates the systemwide distribution of the file. Similar to Bullet, BitTorrent incorporates the notion of `` choking '' at each node with the goal of identifying receivers that benefit the most by downloading from that particular source. FastReplica -LSB- 11 -RSB- addresses the problem of reliable and efficient file distribution in content distribution networks -LRB- CDNs -RRB-. In the basic algorithm, nodes are organized into groups of fixed size -LRB- n -RRB-, with full group membership information at each node. To distribute the file, a node splits it into n equal-sized portions, sends the portions to other group members, and instructs them to download the missing pieces in parallel from other group members. Since only a fixed portion of the file is transmitted along each of the overlay links, the impact of congestion is smaller than in the case of tree distribution. However, since it treats all paths equally, FastReplica does not take full advantage of highbandwidth overlay links in the system. There are numerous protocols that aim to add reliability to IP multicast. In Scalable Reliable Multicast -LRB- SRM -RRB- -LSB- 16 -RSB-, nodes multicast retransmission requests for missed packets. Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. In pbcast -LSB- 2 -RSB-, a node has global group membership, and periodically chooses a random subset of peers to send a digest of its received packets. A node that receives the digest responds to the sender with the missing packets in a last-in, first-out fashion. Since lbpcast does not require an underlying tree for data distribution and relies on the push-gossiping model, its network overhead can be quite high. Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not `` blindly '' request retransmissions from their peers. Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Further, a Bullet node splits the retransmission load between all of its peers. We note that pbcast nodes contain a mechanism to rate-limit retransmitted packets and to send different packets in response to the same digest. However, this does not guarantee that packets received in parallel from multiple peers will not be duplicates. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree. Narada -LSB- 19 -RSB- builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. It then runs a standard routing protocol on top of the overlay mesh to construct forwarding trees using each node as a possible source. Narada nodes maintain global knowledge about all group participants, limiting system scalability to several tens of nodes. Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Overcast -LSB- 21 -RSB- is an example of a bandwidth-efficient overlay tree construction algorithm. In this system, all nodes join at the root and migrate down to the point in the tree where they are still able to maintain some minimum level of bandwidth. Bullet is expected to be more resilient to node departures than any tree, including Overcast. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers. Overcast convergence time is limited by probes to immediate siblings and ancestors. Bullet is able to provide approximately a target bandwidth without having a fully converged tree. In parallel to our own work, SplitStream -LSB- 9 -RSB- also has the goal of achieving high bandwidth data dissemination. It operates by splitting the multicast stream into k stripes, transmitting each stripe along a separate multicast tree built using Scribe -LSB- 34 -RSB-. Perhaps more importantly, SplitStream assumes that there is enough available bandwidth to carry each stripe on every link of the tree, including the links between the data source and the roots of individual stripe trees independently chosen by Scribe. To some extent, Bullet and SplitStream are complementary. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet -LSB- 29 -RSB- considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. In the case of on-demand streaming, CoopNet -LSB- 30 -RSB- addresses the flash-crowd problem at the central server by redirecting incoming clients to a fixed number of nodes that have previously retrieved portions of the same content. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file. 6. CONCLUSIONS Typically, high bandwidth overlay data streaming takes place over a distribution tree. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures. Specifically, this paper makes the following contributions : 9 We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques. 9 We provide a technique for recovering missing data from peers in a scalable and efficient manner. RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants. 9 We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes. 9 A large-scale evaluation of 1000 overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree.", "keyphrases": ["overlai mesh", "data dissemin", "overlai network", "ip multicast", "multipoint commun", "high-bandwidth data distribut", "larg-file transfer", "real-time multimedia stream", "bullet", "bandwidth probe", "peer-to-peer", "ransub", "content deliveri", "tfrc"]}
{"file_name": "I-30", "text": "Distributed Task Allocation in Social Networks ABSTRACT This paper proposes a new variant of the task allocation problem, where the agents are connected in a social network and tasks arrive at the agents distributed over the network. We show that the complexity of this problem remains NPhard. Moreover, it is not approximable within some factor. We develop an algorithm based on the contract-net protocol. Our algorithm is completely distributed, and it assumes that agents have only local knowledge about tasks and resources. We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time. Three different types of networks, namely small-world, random and scale-free networks, are used to represent various social relationships among agents in realistic applications. The results demonstrate that our algorithm works well and that it scales well to large-scale applications. 1. INTRODUCTION Recent years have seen a lot of work on task and resource allocation methods, which can potentially be applied to many real-world applications. However, some interesting applications where relations between agents play a role require a slightly more general model. range of task allocation methods. The question is how VOs are to be dynamically composed and re-composed from individual agents, when different tasks and subtasks need to be performed. This would be done by allocating them to different agents who may each be capable of performing different subsets of those tasks. In this paper, we study the problem of task allocation from the perspective of such a complex interrelated structure. Specifically, therefore, we consider agents to be connected to each other in a social network. Other than modeling the interrelated structure between business partners, the social network introduced in this paper can also be used to represent other types of connections or constraints among autonomous entities that arise from other application domains. The next section gives a formal description of the task allocation problem on social networks. In Section 3, we prove that the complexity of this problem remains NP-hard. We then proceed to develop a distributed algorithm in Section 4, and perform a series of experiments with this algorithm, as described in Section 5. Section 6 discusses related work, and Section 7 concludes. 6. RELATED WORK Task allocation in multiagent systems has been investigated by many researchers in recent years with different assumptions and emphases. However, most of the research to date on task allocation does not consider social connections among agents, and studies the problem in a centralized The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 505 Figure 6 : The quality of the GDAP algorithm for a uniform and a skewed task benefit distribution related to the resource ratio -LRB- the first graph -RRB-, and the network degree -LRB- the second graph -RRB-. setting. For example, Kraus et al. -LSB- 12 -RSB- develop an auction protocol that enables agents to form coalitions with time constraints. It assumes each agent knows the capabilities of all others. The proposed protocol is centralized, where one manager is responsible for allocating the tasks to all coalitions. Manisterski at al. -LSB- 14 -RSB- discuss the possibilities of achieving efficient allocations in both cooperative and noncooperative settings. They propose a centralized algorithm to find the optimal solution. In contrast to this work, we introduce also an efficient completely distributed protocol that takes the social network into account. Task allocation has also been studied in distributed settings by for example Shehory and Kraus -LSB- 18 -RSB- and by Lerman and Shehory -LSB- 13 -RSB-. They propose distributed algorithms with low communication complexity for forming coalitions in large-scale multiagent systems. However, they do not assume the existence of any agent network. The work of Sander et al. -LSB- 16 -RSB- introduces computational geometry-based algorithms for distributed task allocation in geographical domains. Agents are then allowed to move and actively search for tasks, and the capability of agents to perform tasks is homogeneous. In order to apply their approach, agents need to have some knowledge about the geographical positions of tasks and some other agents. Other work -LSB- 17 -RSB- proposes a location mechanism for open multiagent systems to allocate tasks to unknown agents. In this approach each agent caches a list of agents they know. The analysis of the communication complexity of this method is based on lattice-like graphs, while we investigate how to efficiently solve task allocation in a social network, whose topology can be arbitrary. Networks have been employed in the context of task allocation in some other works as well, for example to limit the Figure 8 : The quality of the GDAP algorithm compared to the upper bound. interactions between agents and mediators -LSB- 1 -RSB-. Mediators in this context are agents who receive the task and have connections to other agents. They break up the task into subtasks, and negotiate with other agents to obtain commitments to execute these subtasks. Their focus is on modeling the decision process of just a single mediator. Another approach is to partition the network into cliques of nodes, representing coalitions which the agents involved may use as a coordination mechanism -LSB- 20 -RSB-. The focus of that work is distributed coalition formation among agents, but in our approach, we do not need agents to form groups before allocating tasks. Easwaran and Pitt -LSB- 6 -RSB- study ` complex tasks ' that require ` services ' for their accomplishment. The problem concerns the allocation of subtasks to service providers in a supply chain. Another study of task allocation in supply chains is -LSB- 21 -RSB-, where it is argued that the defining characteristic of Supply Chain Formation is hierarchical subtask decomposition -LRB- HSD -RRB-. HSD is implemented using task dependency networks -LRB- TDN -RRB-, with agents and goods as nodes, and I/O relations between them as edges. Here, the network is given, and the problem is to select a subgraph, for which the authors propose a market-based algorithm, in particular, a series of auctions. Compared to these works, our approach is more general in the sense that we are able to model different types of connections or constraints among agents for different problem domains in addition to supply chain formation. Finally, social networks have been used in the context of team formation. Previous work has shown how to learn which relations are more beneficial in the long run -LSB- 8 -RSB-, and adapt the social network accordingly. We believe these results can be transferred to the domain of task allocation as well, leaving this as a topic for further study. Figure 7 : The run time of the GDAP algorithm. 506 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 7. CONCLUSIONS In this paper we studied the task allocation problem in a social network -LRB- STAP -RRB-, which can be seen as a new, more general, variant of the TAP. We believe it has a great amount of potential for realistic problems. We provided complexity results on computing the efficient solution for the STAP, as well as a bound on possible approximation algorithms. Next, we presented a distributed protocol, related to the contractnet protocol. We also introduced an exponential algorithm to compute the optimal solution, as well as a fast upperbound algorithm. The results presented in this paper show that the distributed algorithm performs well in small-world, scale-free, and random networks, and for many different settings. Also other experiments were done -LRB- e.g. on grid networks -RRB- and these results held up over a wider range of scenarios. Furthermore, we showed that it scales well to large networks, both in terms of quality and of required computation time. The results also suggest that small-world networks are slightly better suited for local task allocation, because there are no nodes with very few neighbors. There are many interesting extensions to our current work. In this paper, we focus on the computational aspect in the design of the distributed algorithm. In our future work, we would also like to address some of the related issues in game theory, such as strategic agents, and show desirable properties of a distributed protocol in such a context. In the current algorithm we assume that agents can only contact their neighbors to request resources, which may explain why our algorithm performs not as good in the scalefree networks as in the small-world networks. Our future work may allow agents to reallocate -LRB- sub -RRB- tasks. We are interested in seeing how such interactions will affect the performance of task allocation in different social networks. A third interesting topic for further work is the addition of reputation information among the agents. This may help to model changing business relations and incentivize agents to follow the protocol. Finally, it would be interesting to study real-life instances of the social task allocation problem, and see how they relate to the randomly generated networks of different types studied in this paper. Acknowledgments.", "keyphrases": ["social network", "social relationship", "task alloc", "util", "alloc", "algorithm", "commun messag", "behavior", "multiag system", "strateg agent", "interact"]}
{"file_name": "J-21", "text": "A Strategic Model for Information Markets ABSTRACT Information markets, which are designed specifically to aggregate traders ' information, are becoming increasingly popular as a means for predicting future events. Recent research in information markets has resulted in two new designs, market scoring rules and dynamic parimutuel markets. We develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection game, that serves as a useful model for information markets. We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets, and also captures the essence of the strategies in market scoring rules. The projection game is tractable to analyze, and has an attractive geometric visualization that makes the strategic moves and interactions more transparent. We use it to prove several strategic properties about the dynamic parimutuel market. We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule, and it is strategically similar to other scoring rules. Finally, we illustrate two applications of the model to analysis of complex strategic scenarios : we analyze the precision of a market in which traders have inertia, and a market in which a trader can profit by manipulating another trader 's beliefs. 1. INTRODUCTION Markets have long been used as a medium for trade. As a side effect of trade, the participants in a market reveal something about their preferences and beliefs. For example, in a financial market, agents would buy shares which they think are undervalued, and sell shares which they think are overvalued. It has long been observed that, because the market price is influenced by all the trades taking place, it aggregates the private information of all the traders. Thus, in a situation in which future events are uncertain, and each trader might have a little information, the aggregated information contained in the market prices can be used to predict future events. This has motivated the creation of information markets, which are mechanisms for aggregating the traders ' information about an uncertain event. Information markets can be modeled as a game in which the participants bet on a number of possible outcomes, such as the results of a presidential election, by buying shares of the outcomes and receiving payoffs when the outcome is realized. As in financial markets, the participants aim to maximize their profit by buying low and selling high. The benefit of well-designed information markets goes beyond information aggregation ; they can also be used as a hedging instrument, to allow traders to insure against risk. Recently, researchers have turned to the problem of designing market structures specifically to achieve better information aggregation properties than traditional markets. Two designs for information markets have been proposed : the Dynamic Parimutuel Market -LRB- DPM -RRB- by Pennock -LSB- 10 -RSB- and the Market Scoring Rules -LRB- MSR -RRB- by Hanson -LSB- 6 -RSB-. Both the DPM and the MSR were designed with the goal of giving informed traders an incentive to trade, and to reveal their information as soon as possible, while also controlling the subsidy that the market designer needs to pump into the market. One version of the DPM was implemented in the Yahoo! Buzz market -LSB- 8 -RSB- to experimentally test the market 's prediction properties. The innovation in the MSR is to use these scoring rules as instruments that can be traded, thus providing traders who have new information an incentive to trade. The MSR was to be used in a policy analysis market in the Middle East -LSB- 15 -RSB-, which was subsequently withdrawn. Information markets rely on informed traders trading for their own profit, so it is critical to understand the strategic properties of these markets. This is not an easy task, because markets are complex, and traders can influence each other 's beliefs through their trades, and hence, can potentially achieve long term gains by manipulating the market. For the DPM, we are not aware of any prior strategic analysis of this nature ; in fact, a strategic hole was discovered while testing the DPM in the Yahoo! Buzz market -LSB- 8 -RSB-. 1.1 Our Results In this paper, we seek to develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection 1 game, that serves as a useful model for information markets. The projection game is conceptually simpler than the MSR and DPM, and thus it is easier to analyze. In addition it has an attractive geometric visualization, which makes the strategic moves and interactions more transparent. We present an analysis of the optimal strategies and profits in this game. We then undertake an analysis of traders ' costs and profits in the dynamic parimutuel market. Remarkably, we find that the cost of a sequence of trades in the DPM is identical to the cost of the corresponding moves in the projection game. Further, if we assume that the traders beliefs at the end of trading match the true probability of the event being predicted, the traders ' payoffs and profits in the DPM are identical to their payoffs and profits in a corresponding projection game. We use the equivalence between the DPM and the projection game to prove that the DPM is arbitrage-free, deduce profitable strategies in the DPM, and demonstrate that constraints on the agents ' trades are necessary to prevent a strategic breakdown. We also prove an equivalence between the projection game and the MSR : We show that play in the MSR is strategically equivalent to play in a restricted projection game, at least for myopic strategies and small trades. This allows us to use the projection game as a conceptual model for market scoring rules. Further, because the restricted projection game corresponds to a DPM with a natural trading constraint, this sheds light on an intriguing connection between the MSR and the DPM. Lastly, we illustrate how the projection game model can be used to analyze the potential for manipulation of information markets for long-term gain.2 We present an example scenario in which such manipulation can occur, and suggest additional rules that might mitigate the possibility of manipulation. We also illustrate another application to analyzing how a market maker can improve the prediction accuracy of a market in which traders will not trade unless their expected profit is above a threshold. 1.2 Related Work Numerous studies have demonstrated empirically that market prices are good predictors of future events, and seem to aggregate the collected wisdom of all the traders -LSB- 2, 3, 12, 1, 5, 16 -RSB-. A number of recent studies have addressed the design of the market structure and trading rules for information markets, as well as the incentive to participate and other strategic issues. However, strategic issues in information markets have also been studied by Mangold et al. -LSB- 8 -RSB- and by Hanson, Oprea and Porter -LSB- 7 -RSB-. An upcoming survey paper -LSB- 11 -RSB- discusses costfunction formulations of automated market makers. Organization of the paper The rest of this paper is organized as follows : In Section 2, we describe the projection game, and analyze the players ' costs, profits, and optimal strategies in this game. In Section 3, we study the dynamic parimutuel market, and show that trade in a DPM is equivalent to a projection game. We establish a connection between the projection game and the MSR in Section 4. In Section 5, we illustrate how the projection game can be used to analyze non-myopic, and potentially manipulative, actions. We present our conclusions, and suggestions for future work, in Section 6. 6. CONCLUSIONS AND FUTURE WORK We have presented a simple geometric game, the projection game, that can serve as a model for strategic behavior in information markets, as well as a tool to guide the design of new information markets. We have used this model to analyze the cost, profit, and strategies of a trader in a dynamic parimutuel market, and shown that both the dynamic parimutuel market and the spherical market scoring rule are strategically equivalent to the restricted projection game under slight distortion of the prior probabilities. The general analysis was based on the assumption that traders do not actively try to mislead other traders for future profit. In section 5, however, we analyze a small example market without this assumption. We demonstrate that the projection game can be used to analyze traders ' strategies in this scenario, and potentially to help design markets with better strategic properties. Our results raise several very interesting open questions. Firstly, the payoffs of the projection game can not be directly implemented in situations in which the true probability is not ultimately revealed. Finally, the existence of long-range manipulative strategies in information markets is of great interest. The example we studied in section 5 merely scratches the surface of this area. A general study of this class of manipulations, together with a characterization of markets in which it can or can not arise, would be very useful for the design of information markets.", "keyphrases": ["inform market", "dynam parimutuel market", "project game model", "predict market", "market score rule", "spheric score rule", "long-rang manipul strategi", "social and behavior scienc-econom", "liquid time"]}
{"file_name": "H-18", "text": "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents ABSTRACT Topic detection and tracking -LSB- 26 -RSB- and topic segmentation -LSB- 15 -RSB- play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information -LRB- MI -RRB- and weighted mutual information -LRB- WMI -RRB- that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI -LRB- or WMI -RRB-. Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation. 1. INTRODUCTION Many researchers have worked on topic detection and tracking -LRB- TDT -RRB- -LSB- 26 -RSB- and topic segmentation during the past decade. Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure. Topic segmentation tasks usually fall into two categories -LSB- 15 -RSB- : text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics. Traditional approaches perform topic segmentation on documents one at a time -LSB- 15, 25, 6 -RSB-. Most of them perform badly in subtle tasks like coherent document segmentation -LSB- 15 -RSB-. Often, end-users seek documents that have the similar content. At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest. Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized. Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment -LSB- 15, 25, 6 -RSB-. However, they usually suffer the issue of identifying stop words. For example, additional document-dependent stop words are removed together with the generic stop words in -LSB- 15 -RSB-. There are two reasons that we do not remove stop words directly. First, identifying stop words is another issue -LSB- 12 -RSB- that requires estimation in each domain. Removing common stop words may result in the loss of useful information in a specific domain. We employ a soft classification using term weights. Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster. Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem. Usually, human readers can identify topic transition based on cue words, and can ignore stop words. Inspired by this, we give each term -LRB- or term cluster -RRB- a weight based on entropy among different documents and different segments of documents. Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words. These words are common in a document. Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed -LSB- 15 -RSB-. Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment. The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria. Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents. Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly. Obviously, our approach can handle single documents as a special case when multiple documents are unavailable. It can detect shared topics among documents to judge if they are multiple documents on the same topic. We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further. We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice. Some of our prior work is in -LSB- 24 -RSB-. The rest of this paper is organized as follows : In Section 2, we review related work. Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI. In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by us Figure 1 : Illustration of multi-document segmentation and alignment. ing dynamic programming. In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm. Conclusions and some future directions of the research work are discussed in Section 6. 2. PREVIOUS WORK Supervised learning usually has good performance, since it learns functions from labelled training sets. However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired. Some approaches also focus on cue words as hints of topic transitions -LSB- 11 -RSB-. While some existing methods only consider information in single documents -LSB- 6, 15 -RSB-, others utilize multiple documents -LSB- 16, 14 -RSB-. There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents. Previous research studied methods to find shared topics -LSB- 16 -RSB- and topic segmentation and summarization between just a pair of documents -LSB- 14 -RSB-. Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods. Criteria of these approaches can be utilized in the issue of topic segmentation. Some of those methods have been extended into the area of topic segmentation, such as PLSA -LSB- 5 -RSB- and maximum entropy -LSB- 7 -RSB-, but to our best knowledge, using MI for topic segmentation has not been studied. 6. CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases. We used dynamic programming to optimize our algorithm. Our approach outperforms all the previous methods on singledocument cases. Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously. Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance. We only tested our method on limited data sets. More data sets especially complicated ones should be tested. More previous methods should be compared with. Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries. Supervised learning also can be considered.", "keyphrases": ["topic detect", "track", "topic segment", "local and sequenti inform of document", "singl document", "multipl document", "wmu", "share topic", "optim boundari", "singl-document segment", "multi-document segment", "cue term", "stop word", "term weight", "perform of topic segment"]}
{"file_name": "I-6", "text": "Dynamic Semantics for Agent Communication Languages ABSTRACT This paper proposes dynamic semantics for agent communication languages -LRB- ACLs -RRB- as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems. Based on the idea of providing alternative semantic `` variants '' for speech acts and transition rules between them that are contingent on previous agent behaviour, our framework provides an improved notion of grounding semantics in ongoing interaction, a simple mechanism for distinguishing between compliant and expected behaviour, and a way to specify sanction and reward mechanisms as part of the ACL itself. We extend a common framework for commitment-based ACL semantics to obtain these properties, discuss desiderata for the design of concrete dynamic semantics together with examples, and analyse their properties. 1. INTRODUCTION The field of agent communication language -LRB- ACL -RRB- research has long been plagued by problems of verifiability and grounding -LSB- 10, 13, 17 -RSB-. Unable to safeguard themselves against abuse by malicious, deceptive or malfunctioning agents, mentalistic semantics are inherently unreliable and inappropriate for use in open MAS in which agents with potentially conflicting objectives might deliberately exploit their adversaries ' conceptions of message semantics to provoke a certain behaviour. Commitment-based semantics -LSB- 6, 8, 14 -RSB-, on the other hand, define the meaning of messages exchanged among agents in terms of publicly observable commitments, i.e. pledges to bring about a state of affairs or to perform certain actions. Such semantics solve the verifiability problem as they allow for tracing the status of existing commitments at any point in time given observed messages and actions so that any observer can, for example, establish whether an agent has performed a promised action. Further, this implies that the semantics specification does not provide an interface to agents ' deliberation and planning mechanisms and hence it is unclear how rational agents would be able to decide whether to subscribe to a suggested ACL semantics when it is deployed. Finally, none of the existing approaches allows the ACL to specify how to respond to a violation of its semantics by individual agents. Secondly, existing approaches fail to exploit the possibilities of sanctioning and rewarding certain behaviours in a communication-inherent way by modifying the future meaning of messages uttered or received by compliant/deviant agents. In this paper, we propose dynamic semantics -LRB- DSs -RRB- for ACLs as a solution to these problems. Our notion of DS is based on the very simple idea of defining different alternatives for the meaning of individual speech acts -LRB- so-called semantic variants -RRB- in an ACL semantics specification, and transition rules between semantic states -LRB- i.e. collections of variants for different speech acts -RRB- that describe the current meaning of the ACL. These elements taken together result in a FSM-like view of ACL specifications where each individual state provides a complete ACL semantics and state transitions are triggered by observed agent behaviour in order to -LRB- 1 -RRB- reflect future expectations based on previous interaction experience and -LRB- 2 -RRB- sanction or reward certain kinds of behaviour. In defining a DS framework for commitment-based ACLs, this paper makes three contributions : 1. An extension of commitment-based ACL semantics to provide an improved notion of grounding commitments in agent interaction and to allow ACL specifications to be directly used for planning-based rational decision making. 2. A simple way of distinguishing between compliant and expected behaviour with respect to an ACL specification that enables reasoning about the potential behaviour of agents purely from an ACL semantics perspective. 3. A mechanism for specifying how meaning evolves with agent behaviour and how this can be used to describe communication-inherent sanctioning and rewarding mechanisms essential to the design of open MASs.. Furthermore, we discuss desiderata for DS design that can be derived from our framework, present examples and analyse their properties. The remainder of this paper is structured as follows : Section 2 introduces a formal framework for dynamic ACL semantics. In section 3 we present an analysis and discussion of this framework and discuss desiderata for the design of ACLs with dynamic semantics. Section 4 reviews related approaches, and section 5 concludes. 4. RELATED WORK Expectation-based reasoning about interaction was first proposed in -LSB- 2 -RSB-, considering the evolution of expectations described as probabilistic expectations of communication and action sequences. The same authors suggested a more general framework for expectation-based communication semantics -LSB- 9 -RSB-, and argue for a `` consequentialist '' view of semantics that is based on defining the meaning of utterances in terms of their expected consequences and updating these expectations with new observations -LSB- 11 -RSB-. However, their approach does not use an explicit notion of commitments which in our framework mediates between communication and behaviour-based grounding, and provides a clear distinction between a normative notion of compliance and a more empirical notion of expectation. Grounding for -LRB- mentalistic -RRB- ACL semantics has been investigated in -LSB- 7 -RSB- where grounded information is viewed as `` information that is publicly expressed and accepted as being true by all the agents participating in a conversation ''. Like -LSB- 1 -RSB- -LRB- which bases the notion of `` publicly expressed '' on roles rather than internal states of agents -RRB- these authors ' main concern is to provide a verifiable basis for determining the semantics of expressed mental states and commitments. 11In a non-trivial sense, i.e. when some initial transitions are possible in principle 106 The Sixth Intl.. Joint Conf. Our framework is also related to deontic methods for the specification of obligations, norms and sanctions. In this area, -LSB- 16 -RSB- is the only framework that we are aware of which considers dynamic obligations, norms and sanctions. However, as we have described above we solely utilise semantic evolution as a sanctioning and rewarding mechanism, i.e. unlike this work we do not assume that agents can be directly punished or rewarded. 5. CONCLUSION This paper introduces dynamic semantics for ACLs as a method for dealing with some fundamental problems of agent communication in open systems, the simple underlying idea being that different courses of agent behaviour can give rise to different interpretations of meaning of the messages exchanged among agents. Based on a common framework of commitment-based semantics, we presented a notion of grounding for commitments based on notions of compliant and expected behaviour. We then defined dynamic semantics as state transition systems over different semantic states that can be viewed as different `` versions '' of ACL semantics in the traditional sense, and can be easily associated with a planning-based view of reasoning about communication. Thereby, our focus was on simplicity and on providing mechanisms for tracking semantic evolution in a `` down-toearth '', algorithmic fashion to ensure applicability to many different agent designs. We discussed the properties of our framework showing how it can be used as a powerful communication-inherent mechanism for rewarding and sanctioning agent behaviour in open systems without compromising agent autonomy, discussed its integration with agents ' planning processes, complexity issues, and presented a list of desiderata for the design of ACLs with such semantics.", "keyphrases": ["agent commun languag", "dynam semant", "social reason", "commit-base semant", "state transit system", "reput-base adapt", "mutual of expect", "recoveri mechan", "non-redund"]}
{"file_name": "J-13", "text": "On The Complexity of Combinatorial Auctions : Structured Item Graphs and Hypertree Decompositions ABSTRACT The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth -LRB- called structured item graphs -RRB-. Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists -LRB- and if so, computing one -RRB- was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with -LRB- dual -RRB- hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph. 1. INTRODUCTION Combinatorial auctions. Combinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items. This is desirable when a bidder 's valuation of a bundle of items is not equal to the sum of her valuations of the individual items. An outcome for -LRB- Z, B -RRB- is a subset b of B such that item -LRB- Bi -RRB- n item -LRB- Bj -RRB- = 0, for each pair Bi and Bj of bids in b with i = ~ j. The winner determination problem. A crucial problem for combinatorial auctions is to determine the outcome b \u2217 that maximizes the sum of the accepted bid prices -LRB- i.e., Bi \u2208 b \u2217 pay -LRB- Bi -RRB- -RRB- over all the possible outcomes. This problem, called winner determination problem -LRB- e.g., -LSB- 11 -RSB- -RRB-, is known to be intractable, actually NP-hard -LSB- 17 -RSB-, and even not approximable in polynomial time unless NP = ZPP -LSB- 19 -RSB-. Hence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions -LRB- e.g., -LSB- 20, 5, 2, 8, 23 -RSB- -RRB- and to identify classes of instances where solving the winner determination problem is feasible in polynomial time -LRB- e.g., -LSB- 15, 22, 12, 21 -RSB- -RRB-. In fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions. Item graphs. Currently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any Figure 1 : Example MaxWSP problem : -LRB- a -RRB- Hypergraph H -LRB- To, go -RRB-, and a packing h for it ; -LRB- b -RRB- Primal graph for H -LRB- To, go -RRB- ; and, -LRB- c, d -RRB- Two item graphs for H -LRB- To, go -RRB-. bid, the items occurring in it induce a connected subgraph. Indeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure -LSB- 3 -RSB- -- formally bounded treewidth -LSB- 16 -RSB-. To have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H -LRB- T, g -RRB- such that its set of nodes N -LRB- H -LRB- T, g -RRB- -RRB- coincides with set of items I, and where its edges E -LRB- H -LRB- T, g -RRB- -RRB- are precisely the bids of the buyers -LCB- item -LRB- Bi -RRB- | Bi \u2208 B -RCB-. A special item graph for ~ I, B ~ is the primal graph of H -LRB- T, g -RRB-, denoted by G -LRB- H -LRB- T, g -RRB- -RRB-, which contains an edge between any pair of nodes in some hyperedge of H -LRB- T, g -RRB-. Then, any item graph for H -LRB- T, g -RRB- can be viewed as a simplification of G -LRB- H -LRB- T, g -RRB- -RRB- obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge. EXAMPLE 1. The hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB- is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = -LCB- I1,..., I5 -RCB-, and item -LRB- Bi -RRB- = hi, for each 1 \u2264 i \u2264 3. The primal graph for H -LRB- To, go -RRB- is reported in Figure 1. -LRB- b -RRB-, while two example item graphs are reported in Figure 1. -LRB- c -RRB- and -LRB- d -RRB-, where edges required for maintaining the connectivity for h1 are depicted in bold. < Open Problem : Computing structured item graphs efficiently. The above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined. However, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain -LRB- constant -RRB- treewidth exists, and if so, how to compute such a structured item graph efficiently. Weighted Set Packing. Let us note that the hypergraph representation H -LRB- T, g -RRB- of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear -LRB- e.g., -LSB- 17 -RSB- -RRB-. Formally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h ' \u2208 h with h = ~ h ', it holds that h \u2229 h ' = \u2205. Then, the set of the solutions for the weighted set packing problem for H -LRB- T, g -RRB- w.r.t. w -LRB- T, g -RRB- coincides with the set of the solutions for the winner determination problem on ~ I, B ~. EXAMPLE 2. Consider again the hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB-. An example packing for H -LRB- To, go -RRB- is h = -LCB- h1 -RCB-, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1. Indeed, the packing Contributions The primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable. Towards this aim, we first study structured item graphs and solve the open problem in -LSB- 3 -RSB-. The result is very bad news : \u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3. More formally, letting C -LRB- ig, k -RRB- denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph -LRB- associated with a combinatorial auction problem -RRB- belongs to C -LRB- ig, 3 -RRB- is NP-complete. In the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem. E -LRB- H -RRB- -RCB- is in E. We show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width -LSB- 7 -RSB- bounded by k -LRB- short : class C -LRB- hw, k -RRB- of hypergraphs -RRB-. Note that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \u00af instead of the auction hypergraph H. In fact, we can show that MaxWSP remains NP-hard even when H is acyclic -LRB- i.e., when it has hypertree width 1 -RRB-, even when each node is contained in 3 hyperedges at most. \u25ba For some relevant special classes of hypergraphs in C -LRB- hw, k -RRB-, we design a higly-parallelizeable algorithm for MaxWSP. Recall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P -LRB- see, e.g., -LSB- 9 -RSB- -RRB-. \u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs. To the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs. In fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach. Intuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition. \u25ba The proof of the above results give us some interesting insight into the notion of structured item graph. Indeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions. The rest of the paper is organized as follows. Section 2 discusses the intractability of structured item graphs. Section 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable. The comparison between the classes C -LRB- ig, k -RRB- and C -LRB- hw, k -RRB- is discussed in Section 4. Finally, in Section 5 we draw our conclusions by also outlining directions for further research. 5. CONCLUSIONS We have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario. The result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3. Motivated by this result, we investigated the use of hypertree decomposition -LRB- on the dual hypergraph associated with the scenario -RRB- and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width. For some special, yet relevant cases, a highly parallelizable algorithm is also discussed. Interestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width -LRB- hence, the reason of their intractability is not their generality -RRB-. In particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions -LRB- on the dual hypergraph -RRB-, called query decompositions -LRB- see, e.g., -LSB- 7 -RSB- -RRB-. In the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.", "keyphrases": ["hypergraph", "combinatori auction", "hypertre decomposit", "well-known mechan for resourc and task alloc", "hypertre-base decomposit method", "hypergraph hg", "complex of structur item graph", "simplif of the primal graph", "structur item graph", "fix treewidth", "accept bid price", "polynomi time"]}
{"file_name": "C-33", "text": "Rewards-Based Negotiation for Providing Context Information ABSTRACT How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly. 1. INTRODUCTION Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates -LSB- 1 -RSB-. Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers -LRB- sensors or other data sources of context information -RRB- with different quality levels. For example, Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized `` arbitrator '' provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers ' interests. Moreover, when such `` arbitrator '' is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide `` good '' context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages : 1. We do not need an `` arbitrator '' provided by the middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware. 2. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized `` arbitrator ''. 3. It can guarantee providers ' interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. 4. This approach can choose an appropriate provider au tomatically. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process -LRB- i.e. rewards -RRB-. It will cost more time to reach agreement. It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. Section 2 presents some assumptions. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers ' strategies. Section 4 evaluates our approach. In section 5 we introduce some related work and conclude in section 6. 5. RELATED WORK In -LSB- 4 -RSB-, Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. Their adaptive middleware uses utility functions to choose the best context provider -LRB- given the QoC requirements of applications and the QoC of alternative means of context acquisition -RRB-. In our negotiation model, the calculation of utility function Uc was inspired by this approach. Henricksen and Indulska propose an approach to modelling and using imperfect information in -LSB- 3 -RSB-. They characterize various types and sources of imperfect context information and present a set of novel context modelling constructs. They also outline a software infrastructure that supports the management and use of imperfect context information. -LSB- 10 -RSB- presents a framework for realizing dynamic context consistency management. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. Most approaches to provide appropriate context utilize a centralized `` arbitrator ''. In our approach, we let distributed context providers themselves decide who can provide appropriate context information. Our approach can reduce the burden of the middleware, because we do not need the middleware to provide a context selection mechanism. Also, it can guarantee context providers ' interests. 6. CONCLUSION AND FUTURE WORK How to provide the appropriate context information is a challenging problem in pervasive computing. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers ' interests by a relatively fair proceeds allocation. In this paper, we only consider how to choose an appropriate context provider from two providers. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem. We assume that the context provider will fulfill its promise of reward in the next negotiation process. In fact, the context provider might deceive its opponent and provide illusive promise. We should solve this problem in the future.", "keyphrases": ["context-awar", "context provid", "negoti", "context-awar comput", "concret negoti model", "distribut applic", "pervas comput", "reput", "qualiti of context", "persuas argument"]}
{"file_name": "H-12", "text": "Fast Generation of Result Snippets in Web Search ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58 % over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache. 1. INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type. In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents. Snippets are short fragments of text extracted from the document content -LRB- or its metadata -RRB-. A query-biased snippet is one selectively extracted on the basis of its relation to the searcher 's query. The addition of informative snippets to search results may substantially increase their value to searchers. Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored. In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searcher 's real information need, such as the contact details of a person or an organization. Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load -LRB- remembering that each search typically generates ten snippets -RRB-. The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required. The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load. Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems. Note that the utility of snippets is by no means restricted to whole-of-Web search applications. Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov -LRB- c. 25 million pages -RRB- and govsearch.australia.gov.au -LRB- c. 5 million pages -RRB- and within large enterprises such as IBM -LSB- 2 -RSB- -LRB- c. 50 million pages -RRB-. Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present. We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets. We report the proportion of time spent for disk seeks, disk reads and cpu processing ; demonstrating that the time for locating each document -LRB- seek time -RRB- dominates, as expected. As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required. However, this is only true if there is no caching of documents in RAM. Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation. In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided : the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists -LSB- 24 -RSB-. As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document. This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2. RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query. Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document. Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms. To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page -LSB- 1 -RSB- only under the heading of future work. Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary -LSB- 20, 21 -RSB-, rather than efficient generation of summaries. Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature. White et al -LSB- 21 -RSB- report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. N/M documents. The total amount of RAM required by a single machine, therefore, would be N/M -LRB- 8.192 + 10.24 + 8 -RRB- bytes. Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine. These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache. In this work we have deliberately avoided committing to one particular scoring method for sentences in documents. Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets -LSB- 20 -RSB-. The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here can not be used. As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper. We will explore alternate compression schemes in future work.", "keyphrases": ["search engin", "snippet gener", "document cach", "link graph measur", "perform", "web summari", "special-purpos filesystem", "ram", "document compact", "text fragment", "precomput final result page", "vbyte code scheme", "semi-static compress"]}
{"file_name": "H-10", "text": "Regularized Clustering for Documents * ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization -LRB- CLGR -RRB-. We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods. 1. INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies. In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful. Generally, document clustering methods can be mainly categorized into two classes : hierarchical methods and partitioning methods. The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches. For example, hierarchical agglomerative clustering -LRB- HAC -RRB- -LSB- 13 -RSB- is a typical bottom-up hierarchical clustering method. It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster. On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions. For instance, K-means -LSB- 13 -RSB- is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers. In this paper, we will focus on the partitioning methods. In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods -LSB- 19 -RSB- -LSB- 28 -RSB-. Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community. The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points. Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph. After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal. In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph. In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. So we call our method Clustering with Local and Global Regularization -LRB- CLGR -RRB-. The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning -LSB- 31 -RSB-, and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods. The rest of this paper is organized as follows : in section 2 we will introduce our CLGR algorithm in detail. The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 4. CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization. Our method preserves the merit of local learning algorithms and spectral clustering. Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm.", "keyphrases": ["document cluster", "regular", "global regular", "cluster hierarchi", "spectrum", "specifi search", "hierarch method", "partit method", "label predict", "function estim", "manifold"]}
{"file_name": "C-32", "text": "BuddyCache : High-Performance Object Storage for Collaborative Strong-Consistency Applications in a WAN * ABSTRACT Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area networks because of high network latency. BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers. We have implemented a BuddyCache prototype and evaluated its performance. Analytical results, confirmed by measurements of the BuddyCache prototype using the multiuser 007 benchmark indicate that for typical Internet latencies, e.g. ranging from 40 to 80 milliseconds round trip time to the storage server, peers using BuddyCache can reduce by up to 50 % the latency of access to shared objects compared to accessing the remote servers directly. 1. INTRODUCTION Nevertheless, distributed applications may perform poorly in wide-area network environments. Network bandwidth problems will improve in the foreseeable future, but improvement in network latency is fundamentally limited. BuddyCache is a new object caching technique that addresses the network latency problem for collaborative applications in wide-area network environment. Collaborative applications provide a shared work environment for groups of networked users collaborating on a common task, for example a team of engineers jointly overseeing a construction project. Strong-consistency collaborative applications, for example CAD systems, use client/server transactional object storage systems to ensure consistent access to shared persistent data. Up to now however, users have rarely considered running consistent network storage systems over wide-area networks as performance would be unacceptable -LSB- 24 -RSB-. For transactional storage systems, the high cost of wide-area network interactions to maintain data consistency is the main cost limiting the performance and therefore, in wide-area network environments, collaborative applications have been adapted to use weaker consistency storage systems -LSB- 22 -RSB-. Adapting an application to use weak consistency storage system requires significant effort since the application needs to be rewritten to deal with a different storage system semantics. If shared persistent objects could be accessed with low-latency, a new field of distributed strong-consistency applications could be opened. Cooperative web caching -LSB- 10, 11, 15 -RSB- is a well-known approach to reducing client interaction with a server by allowing one client to obtain missing objects from a another client instead of the server. However, cooperative web caching techniques do not provide two important properties needed by collaborative applications, strong consistency and efficient access to fine-grained objects. Cooperative object caching systems -LSB- 2 -RSB- provide these properties. However, they rely on interaction with the server to provide fine-grain cache coherence that avoids the problem of false sharing when accesses to unrelated objects appear to conflict because they occur on the same physical page. Interaction with the server increases latency. The contribution of this work is extending cooperative caching techniques to provide strong consistency and efficient access to fine-grain objects in wide-area environments. The engineers use a collaborative CAD application to revise and update complex project design documents. The shared documents are stored in transactional repository servers at the company home site. The engineers use workstations running repository clients. The workstations are interconnected by a fast local Ethernet but the network connection to the home repository servers is slow. To improve access latency, clients fetch objects from repository servers and cache and access them locally. A coherence protocol ensures that client caches remain consistent when objects are modified. The performance problem facing the collaborative application is coordinating with the servers consistent access to shared objects. With BuddyCache, a group of close-by collaborating clients, connected to storage repository via a high-latency link, can avoid interactions with the server if needed objects, updates or coherency information are available in some client in the group. BuddyCache presents two main technical challenges. One challenge is how to provide efficient access to shared finegrained objects in the collaborative group without imposing performance overhead on the entire caching system. The other challenge is to support fine-grain cache coherence in the presence of slow and failed nodes. BuddyCache uses a '' redirection '' approach similar to one used in cooperative web caching systems -LSB- 11 -RSB-. A redirector server, interposed between the clients and the remote servers, runs on the same network as the collaborating group and, when possible, replaces the function of the remote servers. If the client request can not be served locally, the redirector forwards it to a remote server. When one of the clients in the group fetches a shared object from the repository, the object is likely to be needed by other clients. BuddyCache redirects subsequent requests for this object to the caching client. Similarly, when a client creates or modifies a shared object, the new data is likely to be of potential interest to all group members. BuddyCache uses redirection to support peer update, a lightweight '' application-level multicast '' technique that provides group members with consistent access to the new data committed within the collaborating group without imposing extra overhead outside the group. Nevertheless, in a transactional system, redirection interferes with shared object availability. Solo commit, is a validation technique used by BuddyCache to avoid the undesirable client dependencies that reduce object availability when some client nodes in the group are slow, or clients fail independently. A salient feature of solo commit is supporting fine-grained validation using inexpensive coarse-grained coherence information. We designed and implemented a BuddyCache prototype and studied its performance benefits and costs using analytical modeling and system measurements. We compared the storage system performance with and without BuddyCache and considered how the cost-benefit balance is affected by network latency. These strong performance gains could make transactional object storage systems more attractive for collaborative applications in wide-area environments. 2. RELATED WORK Cooperative caching techniques -LSB- 20, 16, 13, 2, 28 -RSB- provide access to client caches to avoid high disk access latency in an environment where servers and clients run on a fast local area network. These techniques use the server to provide redirection and do not consider issues of high network latency. Cooperative Web caching techniques, -LRB- e.g. -LSB- 11, 15 -RSB- -RRB- investigate issues of maintaining a directory of objects cached in nearby proxy caches in wide-area environment, using distributed directory protocols for tracking cache changes. This work does not consider issues of consistent concurrent updates to shared fine-grained objects. This multicast transport level solution is geared to the single writer semantics of web objects. In contrast, BuddyCache uses '' application level '' multicast and a sender-reliable coherence protocol to provide similar access latency improvements for transactional objects. Application level multicast solution in a middle-ware system was described by Pendarakis, Shi and Verma in -LSB- 27 -RSB-. The schema supports small multi-sender groups appropriate for collaborative applications and considers coherence issues in the presence of failures but does not support strong consistency or fine-grained sharing. The protocol uses leases to provide fault-tolerant call-backs and takes advantage of nearby caches to reduce the cost of lease extensions. The study uses simulation to investigate latency and fault tolerance issues in hierarchical avoidance-based coherence scheme. In contrast, our work uses implementation and analysis to evaluate the costs and benefits of redirection and fine grained updates in an optimistic system. Anderson, Eastham and Vahdat in WebFS -LSB- 29 -RSB- present a global file system coherence protocol that allows clients to choose on per file basis between receiving updates or invalidations. Updates and invalidations are multicast on separate channels and clients subscribe to one of the channels. The protocol exploits application specific methods e.g. last-writer-wins policy for broadcast applications, to deal with concurrent updates but is limited to file systems. BuddyCache provides similar bandwidth improvements when objects are available in the group cache. 7. CONCLUSION Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area network because of high network latency. This paper described BuddyCache, a new transactional cooperative caching -LSB- 20, 16, 13, 2, 28 -RSB- technique that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The technique improves performance yet provides strong correctness and availability properties in the presence of node failures and slow clients. Redirection, however, can interfere with object availability. Solo commit, is a new validation technique that allows a client in a group to commit independently of slow or failed peers. It provides fine-grained validation using inexpensive coarse-grain version information. We have designed and implemented BuddyCache prototype in Thor distributed transactional object storage system -LSB- 23 -RSB- and evaluated the benefits and costs of the system over a range of network latencies. fine-grain strong-consistency access in high-latency environments, 2. an implementation of the system prototype that yields strong performance gains over the base system, 3. analytical and measurement based performance evaluation of the costs and benefits of the new techniques capturing the dominant performance cost, high network latency.", "keyphrases": ["object storag system", "collabor strong-consist applic", "wide-area network", "cooper web cach", "fine-grain share", "transact", "fault-toler properti", "buddycach", "domin perform cost", "optimist system", "peer fetch", "multi-user oo7 benchmark"]}
{"file_name": "J-17", "text": "Truthful Mechanism Design for Multi-Dimensional Scheduling via Cycle Monotonicity ABSTRACT We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design, where the machines are the strategic players. This is a multidimensional scheduling domain, and the only known positive results for makespan minimization in such a domain are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We study a well-motivated special case of this problem, where the processing time of a job on each machine may either be `` low '' or `` high '', and the low and high values are public and job-dependent. This preserves the multidimensionality of the domain, and generalizes the restricted-machines -LRB- i.e., -LCB- pj, \u221e -RCB- -RRB- setting in scheduling. We give a general technique to convert any c-approximation algorithm to a 3capproximation truthful-in-expectation mechanism. This is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion. When the low and high values are the same for all jobs, we devise a deterministic 2-approximation truthful mechanism. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Our constructions are novel in two respects. First, we do not utilize or rely on explicit price definitions to prove truthfulness ; instead we design algorithms that satisfy cycle monotonicity. Cycle monotonicity -LSB- 23 -RSB- is a necessary and sufficient condition for truthfulness, is a generalization of value monotonicity for multidimensional domains. However, whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains, ours is the first work that leverages cycle monotonicity in the multidimensional setting. Second, our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem, and then converting it into a truthfulin-expectation mechanism. This builds upon a technique of -LSB- 16 -RSB-, and shows the usefulness of fractional mechanisms in truthful mechanism design. 1. INTRODUCTION Mechanism design studies algorithmic constructions under the presence of strategic players who hold the inputs to the algorithm. Algorithmic mechanism design has focused mainly on settings were the social planner or designer wishes to maximize the social welfare -LRB- or equivalently, minimize social cost -RRB-, or on auction settings where revenuemaximization is the main goal. In this paper, we consider such an alternative goal in the context of machine scheduling, namely, makespan minimization. There are n jobs or tasks that need to be assigned to m machines, where each job has to be assigned to exactly one machine. Hence, we approach the problem via mechanism design : the social designer, who holds the set of jobs to be assigned, needs to specify, in addition to a schedule, suitable payments to the players in order to incentivize them to reveal their true processing times. Such a mechanism is called a truthful mechanism. Instead, it corresponds to maximizing the minimum welfare and the notion of max-min fairness, and appears to be a much harder problem from the viewpoint of mechanism design. In particular, the celebrated VCG -LSB- 26, 9, 10 -RSB- family of mechanisms does not apply here, and we need to devise new techniques. The possibility of constructing a truthful mechanism for makespan minimization is strongly related to assumptions on the players ' processing times, in particular, the `` dimensionality '' of the domain. Nisan and Ronen considered the setting of unrelated machines where the pij values may be arbitrary. This is a multidimensional domain, since a player 's private value is its entire vector of processing times -LRB- pij -RRB- j. Very few positive results are known for multidimensional domains in general, and the only positive results known for multidimensional scheduling are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We emphasize that regardless of computational considerations, even the existence of a truthful mechanism with a significantly better -LRB- than m -RRB- approximation ratio is not known for any such scheduling domain. On the negative side, -LSB- 22 -RSB- showed that no truthful deterministic mechanism can achieve approximation ratio better than 2, and strengthened this lower bound to m for two specific classes of deterministic mechanisms. Recently, -LSB- 20 -RSB- extended this lower bound to randomized mechanisms, and -LSB- 8 -RSB- improved the deterministic lower bound. In stark contrast with the above state of affairs, much stronger -LRB- and many more -RRB- positive results are known for a special case of the unrelated machines problem, namely, the setting of related machines. Here, we have pij = pj/si for every i, j, where pj is public knowledge, and the speed si is the only private parameter of machine i. This assumption makes the domain of players ' types single-dimensional. Truthfulness in such domains is equivalent to a convenient value-monotonicity condition -LSB- 21, 3 -RSB-, which appears to make it significantly easier to design truthful mechanisms in such domains. Archer and Tardos -LSB- 3 -RSB- first considered the related machines setting and gave a randomized 3-approximation truthful-in-expectation mechanism. The gap between the single-dimensional and multidimensional domains is perhaps best exemplified by the fact that -LSB- 3 -RSB- showed that there exists a truthful mechanism that always outputs an optimal schedule. -LRB- Recall that in the multidimensional unrelated machines setting, it is impossible to obtain a truthful mechanism with approximation ratio better than 2. -RRB- Various follow-up results -LSB- 2, 4, 1, 13 -RSB- have strengthened the notion of truthfulness and/or improved the approximation ratio. Such difficulties in moving from the single-dimensional to the multidimensional setting also arise in other mechanism design settings -LRB- e.g., combinatorial auctions -RRB-. Thus, in addition to the specific importance of scheduling in strategic environments, ideas from multidimensional scheduling may also have a bearing in the more general context of truthful mechanism design for multidimensional domains. In this paper, we consider the makespan-minimization problem for a special case of unrelated machines, where the processing time of a job is either `` low '' or `` high '' on each machine. We call this model the `` jobdependent two-values '' case. This model generalizes the classic `` restricted machines '' setting, where pij \u2208 -LCB- Lj, \u221e -RCB- which has been well-studied algorithmically. A special case of our model is when Lj = L and Hj = H for all jobs j, which we denote simply as the `` two-values '' scheduling model. Both of our domains are multidimensional, since the machines are unrelated : one job may be low on one machine and high on the other, while another job may follow the opposite pattern. Thus, the private information of each machine is a vector specifying which jobs are low and high on it. Thus, they retain the core property underlying the hardness of truthful mechanism design for unrelated machines, and by studying these special settings we hope to gain some insights that will be useful for tackling the general problem. Our Results and Techniques We present various positive results for our multidimensional scheduling domains. Our first result is a general method to convert any capproximation algorithm for the job-dependent two values setting into a 3c-approximation truthful-in-expectation mechanism. This is one of the very few known results that use an approximation algorithm in a black-box fashion to obtain a truthful mechanism for a multidimensional problem. Our result implies that there exists a 3-approximation truthfulin-expectation mechanism for the Lj-Hj setting. Our second result applies to the twovalues setting -LRB- Lj = L, Hj = H -RRB-, for which we improve both the approximation ratio and strengthen the notion of truthfulness. We obtain a deterministic 2-approximation truthful mechanism -LRB- along with prices -RRB- for this problem. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Complementing this, we observe that even this seemingly simple setting does not admit truthful mechanisms that return an optimal schedule -LRB- unlike in the case of related machines -RRB-. By exploiting the multidimensionality of the domain, we prove that no truthful deterministic mechanism can obtain an approximation ratio better than 1.14 to the makespan -LRB- irrespective of computational considerations -RRB-. The main technique, and one of the novelties, underlying our constructions and proofs, is that we do not rely on explicit price specifications in order to prove the truthfulness of our mechanisms. Instead we exploit certain algorithmic monotonicity conditions that characterize truthfulness to first design an implementable algorithm, i.e., an algorithm for which prices ensuring truthfulness exist, and then find these prices -LRB- by further delving into the proof of implementability -RRB-. This kind of analysis has been the method of choice in the design of truthful mechanisms for singledimensional domains, where value-monotonicity yields a convenient characterization enabling one to concentrate on the algorithmic side of the problem -LRB- see, e.g., -LSB- 3, 7, 4, 1, 13 -RSB- -RRB-. Our work is the first to leverage monotonicity conditions for truthful mechanism design in arbitrary domains. The monotonicity condition we use, which is sometimes called cycle monotonicity, was first proposed by Rochet -LSB- 23 -RSB- -LRB- see also -LSB- 11 -RSB- -RRB-. It is a generalization of value-monotonicity and completely characterizes truthfulness in every domain. Our methods and analyses demonstrate the potential benefits of this characterization, and show that cycle monotonicity can be effectively utilized to devise truthful mechanisms for multidimensional domains. Consider, for example, our first result showing that any c-approximation algorithm can be `` exported '' to a 3c-approximation truthful-in-expectation mechanism. At the level of generality of an arbitrary approximation algorithm, it seems unlikely that one would be able to come up with prices to prove truthfulness of the constructed mechanism. But, cycle monotonicity does allow us to prove such a statement. In fact, some such condition based only on the underlying algorithm -LRB- and not on the prices -RRB- seems necessary to prove such a general statement. The method for converting approximation algorithms into truthful mechanisms involves another novel idea. Our randomized mechanism is obtained by first constructing a truthful mechanism that returns a fractional schedule. Moving to a fractional domain allows us to `` plug-in '' truthfulness into the approximation algorithm in a rather simple fashion, while losing a factor of 2 in the approximation ratio. We then use a suitable randomized rounding procedure to convert the fractional assignment into a random integral assignment. This preserves truthfulness, but we lose another additive factor equal to the approximation ratio. Our construction uses and extends some observations of Lavi and Swamy -LSB- 16 -RSB-, and further demonstrates the benefits of fractional mechanisms in truthful mechanism design. Related Work Nisan and Ronen -LSB- 22 -RSB- first considered the makespan-minimization problem for unrelated machines. They gave an m-approximation positive result and proved various lower bounds. This been improved in -LSB- 2, 4, 1, 13 -RSB- to : a 2-approximation randomized mechanism -LSB- 2 -RSB- ; an FPTAS for any fixed number of machines given by Andelman, Azar and Sorani -LSB- 1 -RSB-, and a 3-approximation deterministic mechanism by Kov \u00b4 acs -LSB- 13 -RSB-. The algorithmic problem -LRB- i.e., without requiring truthfulness -RRB- of makespan-minimization on unrelated machines is well understood and various 2-approximation algorithms are known. Lenstra, Shmoys and Tardos -LSB- 18 -RSB- gave the first such algorithm. Shmoys and Tardos -LSB- 25 -RSB- later gave a 2approximation algorithm for the generalized assignment problem, a generalization where there is a cost cij for assigning a job j to a machine i, and the goal is to minimize the cost subject to a bound on the makespan. Recently, Kumar, Marathe, Parthasarathy, and Srinivasan -LSB- 14 -RSB- gave a randomized rounding algorithm that yields the same bounds. We use their procedure in our randomized mechanism. The characterization of truthfulness for arbitrary domains in terms of cycle monotonicity seems to have been first observed by Rochet -LSB- 23 -RSB- -LRB- see also Gui et al. -LSB- 11 -RSB- -RRB-. This generalizes the value-monotonicity condition for single-dimensional domains which was given by Myerson -LSB- 21 -RSB- and rediscovered by -LSB- 3 -RSB-. As mentioned earlier, this condition has been exploited numerous times to obtain truthful mechanisms for single-dimensional domains -LSB- 3, 7, 4, 1, 13 -RSB-. For convex domains -LRB- i.e., each players ' set of private values is convex -RRB-, it is known that cycle monotonicity is implied by a simpler condition, called weak monotonicity -LSB- 15, 6, 24 -RSB-. But even this simpler condition has not found much application in truthful mechanism design for multidimensional problems. Objectives other than social-welfare maximization and revenue maximization have received very little attention in mechanism design. In the context of combinatorial auctions, the problems of maximizing the minimum value received by a player, and computing an envy-minimizing allocation have been studied briefly. Lavi, Mu'alem, and Nisan -LSB- 15 -RSB- showed that the former objective can not be implemented truthfully ; Bezakova and Dani -LSB- 5 -RSB- gave a 0.5-approximation mechanism for two players with additive valuations. These lower bounds were strengthened in -LSB- 20 -RSB-. 2. PRELIMINARIES 2.1 The scheduling domain In our scheduling problem, we are given n jobs and m machines, and each job must be assigned to exactly one machine. In the unrelated-machines setting, each machine i is characterized by a vector of processing times -LRB- pij -RRB- j, where pij E R \u2265 0 U -LCB- oo -RCB- denotes i 's processing time for job j with the value oo specifying that i can not process j. We consider two special cases of this problem : 1. The job-dependent two-values case, where pij E -LCB- Lj, Hj -RCB- for every i, j, with Lj < Hj, and the values Lj, Hj are known. This generalizes the classic scheduling model of restricted machines, where Hj = oo. 2. We say that a job j is low on machine i if pij = Lj, and high if pij = Hj. We will use the terms schedule and assignment interchangeably. We will also consider randomized algorithms and algorithms that return a fractional assignment. We denote the load of machine i -LRB- under a given assignj xijpij, and the makespan of a schedule is defined as the maximum load on any machine, i.e., maxi li. The goal in the makespan-minimization problem is to assign the jobs to the machines so as to minimize the makespan of the schedule. 2.2 Mechanism design We consider the makespan-minimization problem in the above scheduling domains in the context of mechanism design. Mechanism design studies strategic settings where the social designer needs to ensure the cooperation of the different entities involved in the algorithmic procedure. Following the work of Nisan and Ronen -LSB- 22 -RSB-, we consider the machines to be the strategic players or agents. The social designer holds the set of jobs that need to be assigned, but does not know the -LRB- true -RRB- processing times of these jobs on the different machines. Each machine is a selfish entity, that privately knows its own processing time for each job. We consider direct-revelation mechanisms : each machine reports its -LRB- possibly false -RRB- vector of processing times, the mechanism then computes a schedule and hands out payments to the players -LRB- i.e., machines -RRB- to compensate them for the cost they incur in processing their assigned jobs. A -LRB- direct-revelation -RRB- mechanism thus consists of a tuple -LRB- x, P -RRB- : x specifies the schedule, and P = -LCB- Pi -RCB- specifies the payments handed out to the machines, where both x and the Pis are functions of the reported processing times p = -LRB- pij -RRB- i, j. The mechanism must therefore incentivize the machines/players to truthfully reveal their processing times via the payments. This is made precise using the notion of dominant-strategy truthfulness. To put it in words, in a truthful mechanism, no machine can improve its utility by declaring a false processing time, no matter what the other machines declare. We will also consider fractional mechanisms that return a fractional assignment, and randomized mechanisms that are allowed to toss coins and where the assignment and the payments may be random variables. The notion of truthfulness for a fractional mechanism is the same as in Definition 2.1, where x1, x2 are now fractional assignments. For a randomized mechanism, we will consider the notion of truthfulness in expectation -LSB- 3 -RSB-, which means that a machine -LRB- player -RRB- maximizes her expected utility by declaring her true processing-time vector. For our two scheduling domains, the informational assumption is that the values Lj, Hj are publicly known. The private information of a machine is which jobs have value Lj -LRB- or L -RRB- and which ones have value Hj -LRB- or H -RRB- on it. We emphasize that both of our domains are multidimensional, since each machine i needs to specify a vector saying which jobs are low and high on it.", "keyphrases": ["mechan design", "approxim algorithm", "schedul", "multi-dimension schedul", "cycl monoton", "makespan minim", "algorithm", "random mechan", "us of fraction mechan", "truth mechan design", "fraction domain"]}
{"file_name": "I-26", "text": "Sequential Decision Making in Parallel Two-Sided Economic Search ABSTRACT This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships. In each search stage, each of the agents is randomly matched with several other agents in parallel, and makes a decision whether to accept a potential partnership with one of them. The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized -LRB- instantaneous -RRB- decision protocol and can sequentially accept and reject partnerships within the same search stage. We analyze the dynamics which drive the agents ' strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model. By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments. 1. INTRODUCTION A two-sided economic search is a distributed mechanism for forming agents ' pairwise partnerships -LSB- 5 -RSB-.1 On every stage of the process, each of the agents is randomly matched with another agent 1Notice that the concept of '' search '' here is very different from the classical definition of '' search '' in AI. While AI search is an active process in which an agent finds a sequence of actions that will bring it from the initial state to a goal state, economic search refers to the identification of the best agent to commit to a partnership with. and the two interact bilaterally in order to learn the benefit encapsulated in a partnership between them. The interaction does not involve bargaining thus each agent merely needs to choose between accepting or rejecting the partnership with the other agent. A typical market where this kind of two-sided search takes place is the marriage market -LSB- 22 -RSB-. Recent literature suggests various software agent-based applications where a two-sided distributed -LRB- i.e., with no centralized matching mechanisms -RRB- search takes place. An important class of such applications includes secondary markets for exchanging unexploited resources. For example, through a twosided search, agents, representing different service providers, can exchange unused bandwidth -LSB- 21 -RSB- and communication satellites can transfer communication with a greater geographical coverage. Twosided agents-based search can also be found in applications of buyers and sellers in eMarkets and peer-to-peer applications. The twosided nature of the search suggests that a partnership between a pair of agents is formed only if it is mutually accepted. By forming a partnership the agents gain an immediate utility and terminate their search. When resuming the search, on the other hand, a more suitable partner might be found however some resources will need to be consumed for maintaining the search process. In this paper we focus on a specific class of two-sided search matching problems, in which the performance of the partnership applies to both parties, i.e., both gain an equal utility -LSB- 13 -RSB-. The equal utility scenario is usually applicable in domains where the partners gain from the synergy between them. In all these applications, any two agents can form a partnership and the performance of any given partnership depends on the skills or the characteristics of its members. Furthermore, the equal utility scenario can also hold whenever there is an option for side-payments and the partnership 's overall utility is equally split among the two agents forming it -LSB- 22 -RSB-. While the two-sided search literature offers comprehensive equilibrium analysis for various models, it assumes that the agents ' search is conducted in a purely sequential manner : each agent locates and interacts with one other agent in its environment at a time -LSB- 5, 22 -RSB-. Nevertheless, when the search is assigned to autonomous software agents a better search strategy can be used. Here an agent can take advantage of its unique inherent filtering and information processing capabilities and its ability to efficiently -LRB- in comparison to people -RRB- maintain concurrent interactions with several other agents at each stage of its search. Such use of parallel interactions in search is favorable whenever the average cost2 per interaction with another agent, when interacting in parallel with a batch of other agents, is smaller than the cost of maintaining one interaction at a time -LRB- i.e., advantage to size -RRB-. For example, the analysis of the costs associated with evaluating potential partnerships between service providers reveals both fixed and variable components when using the parallel search, thus the average cost per interaction decreases as the number of parallel interactions increases -LSB- 21 -RSB-. Despite the advantages identified for parallel interactions in adjacent domains -LRB- e.g., in one-sided economic search -LSB- 7, 16 -RSB- -RRB-, a first attempt for modeling a repeated pairwise matching process in which agents are capable of maintaining interaction with several other agents at a time was introduced only recently -LSB- 21 -RSB-. However, the agents in that seminal model are required to synchronize their decision making process. Thus each agent, upon reviewing the opportunities available in a specific search stage, has to notify all other agents of its decision whether to commit to a partnership -LRB- at most with one of them -RRB- or reject the partnership -LRB- with the rest of them -RRB-. This inherent restriction imposes a significant limitation on the agents ' strategic behavior. In our model, the agents are free to notify the other agents of their decisions in an asynchronous manner. The asynchronous approach allows the agents to re-evaluate their strategy, based on each new response they receive from the agents they interact with. The new model is a much more realistic pairwise model and, as we show in the analysis section, is always preferred by any single agents participating in the process. In the absence of other economic two-sided parallel search models, we use the model that relies on an instantaneous -LRB- synchronous -RRB- decision making process -LSB- 21 -RSB- -LRB- denoted I-DM throughout the rest of the paper -RRB- as a benchmark for evaluating the usefulness of our proposed sequential -LRB- asynchronous -RRB- decision making strategy -LRB- denoted S-DM -RRB-. The main contributions of this paper are threefold : First, we formally model and analyze a two-sided search process in which the agents have no temporal decision making constraints concerning the rejection of or commitment to potential partnerships they encounter in parallel -LRB- the S-DM model -RRB-. This model is a general search model which can be applied in various -LRB- not necessarily software agents-based -RRB- domains. Second, we prove that the agents ' SDM strategy weakly dominates the I-DM strategy, thus every agent has an incentive to deviate to the S-DM strategy when all other agents are using the I-DM strategy. Finally, by using an innovative recursive presentation of the acceptance probabilities of different potential partnerships, we identify unique characteristics of the equilibrium strategies in the new model. These are used for supplying an appropriate computational means that facilitates the calculation of the agents ' equilibrium strategy. This latter contribution is We manage to extract the agents ' new equilibrium strategies without increasing the computational complexity in comparison to the I-DM model. Throughout the paper we demonstrate the different properties of the new model and compare it with the I-DM model using an artificial synthetic environment. In the following section we formally present the S-DM model. An equilibrium analysis and computational means for finding the equilibrium strategy are provided in Section 3. In Section 4 we review related MAS and economic search theory literature. 4. RELATED WORK The two-sided economic search for partnerships in AI literature is a sub-domain of coalition formation8. As in the general 8The use of the term '' partnership '' in this context refers to the agreement between two individual agents to cooperate in a pre-defined manner. For example, in the buyer-seller application a partnership is defined as an agreed transaction between the two-parties -LSB- 9 -RSB-. coalition formation case, agents have the incentive to form partnerships when they are incapable of executing a task by their own or when the partnership can improve their individual utilities -LSB- 14 -RSB-. Various centralized matching mechanisms can be found in the literature -LSB- 6, 2, 8 -RSB-. However, in many MAS environments, in the absence of any reliable central matching mechanism, the matching process is completely distributed. While the search in agent-based environments is well recognized to be costly -LSB- 11, 21, 1 -RSB-, most of the proposed coalition formation mechanisms assume that an agent can scan as many partnership opportunities in its environment as needed or have access to central matchers or middle agents -LSB- 6 -RSB-. The incorporation of costly search in this context is quite rare -LSB- 21 -RSB- and to the best of our knowledge, a distributed two-sided search for partners model similar to the S-DM model has not been studied to date. Classical economic search theory -LRB- -LSB- 15, 17 -RSB-, and references therein -RRB- widely addresses the problem of a searcher operating in a costly environment, seeking to maximize his long term utility. In these models, classified as one-sided search, the focus is on establishing the optimal strategies for the searcher, assuming no mutual search activities -LRB- i.e., no influence on the environment -RRB-. Here the sequential search procedure is often applied, allowing the searcher to investigate a single -LSB- 15 -RSB- or multiple -LSB- 7, 19 -RSB- opportunities at a time. While the latter method is proven to be beneficial for the searcher, it was never used in the '' two-sided '' search models that followed -LRB- where dual search activities are modeled -RRB- -LSB- 22, 5, 18 -RSB-. Therefore, in these models, the equilibrium strategies are always developed based on the assumption that the agents interact with others sequentially -LRB- i.e., with one agent at a time -RRB-. A first attempt to integrate the parallel search into a two-sided search model is given in -LSB- 21 -RSB-, as detailed in the introduction section. The models presented in this area do not associate the coalition formation process with search costs, which is the essence of the analysis that economic search theory aims to supply. Furthermore, even in repeated pairwise bargaining -LSB- 10 -RSB- models the agents are always limited to initiating a single bargaining interaction at a time.", "keyphrases": ["pairwis partnership", "decis", "peer-to-peer applic", "inform process", "util", "search cost", "multi-equilibrium scenario", "equilibrium strategi", "parallel interact", "bound methodolog", "coalit format", "partnership format", "partnership", "costli environ", "search perform", "instantan decis make", "sequenti decis make", "two-side search"]}
{"file_name": "H-8", "text": "Robust Test Collections for Retrieval Evaluation ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable : the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability : with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. 1. INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments? Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem : for most retrieval tasks, it is impossible to judge the relevance of every document ; there are simply too many of them. The solution used by NIST at TREC -LRB- Text REtrieval Conference -RRB- is the pooling method -LSB- 19, 20 -RSB- : all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool -LSB- 21 -RSB-. This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of -LRB- paid -RRB- annotator time. As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems. Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be `` reusable ''. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. We need a more careful definition of reusability. Specifically, the question of reusability is not how accurately we can evaluate new systems. A `` malicious adversary '' can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree. Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done `` in-house '' for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time. 6. CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of `` reusability '' of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments : focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judg Table 5 : Accuracy, W, mean \u03c4, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100 % confidence in every evaluation -- and there is a full test collection for the task. It could be applied to evaluation on a dynamic test collection as defined by Soboroff -LSB- 18 -RSB-. The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration. We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust : with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.", "keyphrases": ["inform retriev", "evalu", "relev judgement", "reusabl", "lowerest-confid comparison", "mtc", "rtc", "expect", "varianc", "distribut of relev"]}
{"file_name": "H-21", "text": "Robust Classification of Rare Queries Using Web Knowledge ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique : given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. 1. INTRODUCTION One thing, however, has remained constant : people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not -LRB- yet! -RRB- omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in today 's search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. However, the `` tail '' queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries -LSB- 22 -RSB-. More recent studies -LSB- 18, 21 -RSB- also attempted to gather some additional knowledge from the Web. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising -LSB- 11 -RSB-. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure ; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge -LRB- e.g., using search summaries vs. entire crawled pages -RRB-. We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification -LSB- 20 -RSB-, but is supported by research in mainstream text classification -LSB- 5 -RSB-. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri -LSB- 22 -RSB-, or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results -LSB- 16, 15 -RSB-. More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. -LSB- 10 -RSB- found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. -LSB- 1 -RSB- used semi-supervised learning as well as unlabeled data -LSB- 2 -RSB-. Gravano et al. -LSB- 6 -RSB- classified queries with respect to geographic locality in order to determine whether their intent is local or global. The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories -LSB- 11, 18, 20, 9, 21 -RSB-. The KDD task specification provided a small taxonomy -LRB- 67 nodes -RRB- along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are -LRB- 1 -RRB- how to build a document classifier, -LRB- 2 -RRB- how to translate its classifications into the target taxonomy, and -LRB- 3 -RRB- how to determine the query class based on document classifications. The winning solution of the KDD Cup -LSB- 18 -RSB- proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue -LRB- 1 -RRB- above, their solution used the Open Directory Project -LRB- ODP -RRB- to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper -LSB- 19 -RSB-, Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set -LRB- ODP -RRB- is used. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies -LRB- we used a much larger taxonomy -RRB-, our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. When using search results, one can either use only summaries of the results provided by 3Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages -LRB- Table 1 -RRB-. These results are consistent with prior studies -LSB- 5 -RSB-, which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. -LSB- 19 -RSB-, who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the -LRB- fixed -RRB- taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. We plan to further investigate this direction in our future work. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.", "keyphrases": ["queri classif", "search engin", "search advertis", "machin learn", "relev feedback", "vote scheme", "crawl", "topic taxonomi", "affin score", "condit probabl", "adapt", "inform retriev"]}
{"file_name": "C-4", "text": "Intra-flow Loss Recovery and Control for ABSTRACT `` Best effort '' packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold : on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal out is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss lengths with which we can describe the loss distribution within a These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs -LRB- PCM -RRB- with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs -LRB- G. 729 -RRB- on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may `` break '' at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data to minimize the impact of packet loss. We designate these methods as loss recovery and control. At the end-to-end level, identification of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow to -LRB- statistically -RRB- trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both ets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term. 1. INTRODUCTION Considering that a real-time may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity : Temporal sensitivity : Loss of which is correlated in time may lead to disruptions in the service. For voice, as a single packet contains typically several -LRB- voice frames -RRB- this effect is thus more significant than e.g. for video. It translates basically to isolated packet losses versus losses that occur in bursts. Figure 1 : Schematic utility functions dependent on the loss of more and less -LRB- -1 -RRB- important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance -LRB- we same size, frequency and no interdependence between the frames -RRB-. Under the loss of 50 % of the packets, the perceptual quality varies hugely between the where the 50 % of the frames with high perceptual importance are received and the where the 50 % less important frames received. Network support for real-time multimedia flows can on one hand aim at offering a service, which, however, to be implemented within pa & et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. Let us now consider the case that 50 % of packets of flow identified more important -LRB- designated by or less important due to any of the above sensitivity constraints. Figure 1 a -RRB- shows a generic utility function describing the level Quality of Service dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the and -1 packets are equal. Due to the over-proportional sensitivity of the packets to loss as well as the dependence of the end loss recovery performance on the packets, the utility function is decreasing significantly in a non-linear way -LRB- approximated in the figure by piece-wise linear functions -RRB- with an increasing loss rate. Figure 1 b -RRB- presents the where all packets are protected at the expense of -1 The decay of the utility function -LRB- for loss rates < 50 % -RRB- is reduced, because the packets are protected and the endto-end loss recovery can thus operate properly a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application 's utility. Note that the higher the non-linearity of the utility contribution of the packets is -LRB- deviation from the dotted curve in Fig. 1 a -RRB-, the higher is the potential gain in utility when the protection for is enabled. Results for actual perceived quality utility for multimedia applications exhibit such non-linear behavior *. As mechanisms have to be implemented within the network -LRB- hopby-hop -RRB- and/or in the end systems -LRB- end-to-end -RRB-, we have another axis of classification. The adaptation of the sender 's to the current network congestion state an scheme -LRB- loss avoidance, is difficult to apply to voice. Considering that voice flows have very low the relative cost of transmitting the feedback information is -LRB- when compared e.g. to a video flow -RRB-. The major however, the lack of a codec is truly scalable in terms of its output and corresponding perceptual quality. when the availability of computing power is assumed, the lowest codec can be chosen permanently without actually decreasing the perceptual quality. For loss on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction -LRB- FEC -RRB- have been proposed While attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. Using redundancy has also implications to the delay adaptation -LRB- -LSB- lo -RSB- -RRB- employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to we have obtained results which confirm the shape of the `` overall utility '' curve shown in Fig. 1, clearly the utility functions of the `` sub ''. flows and their relationship are more complex and only approximately additive. Table 1 : State and transition probabilities computed for an end-to-end Internet trace using a general Markov model -LRB- third order -RRB- by Yajnik et. al.. which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but can not come close to eliminate them. Therefore in this work we assume that the lowest possible trate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual and measure their utility in the presence of packet loss using objective speech quality measurement. The paper is structured as follows : Section 2 introduces packet - and user-level metrics. We employ these metrics to describe the sensitivity of traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and by-hop level. Section 6 concludes the paper.", "keyphrases": ["end-to-end model", "sampl-base codec", "loss recoveri and control", "loss sensit", "network support for real-time multimedia", "qualiti of servic", "end-to-end loss recoveri", "voip traffic", "intra-flow loss control", "packet-level metric", "gener markov model", "sensit of voip traffic", "queue manag algorithm", "frame-base codec"]}
{"file_name": "C-22", "text": "Runtime Metrics Collection for Middleware Supported Adaptation of Mobile Applications ABSTRACT This paper proposes, implements, and evaluates in terms of worst case performance, an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host managers, runtimes and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead. 1. INTRODUCTION Effective adaptation requires detailed and up to date information about both the system and the software itself. Metrics related to system wide information -LRB- e.g. processor, memory and network load -RRB- are referred to as environmental metrics -LSB- 5 -RSB-, while metrics representing application behaviour are referred as software metrics -LSB- 8 -RSB-. Furthermore, the type of metrics required for performing adaptation is dependent upon the type of adaptation required. For example, service-based adaptation, in which service quality or service behaviour is modified in response to changes in the runtime environment, generally requires detailed environmental metrics but only simple software metrics -LSB- 4 -RSB-. On the other hand, adaptation via object mobility -LSB- 6 -RSB-, also requires detailed software metrics -LSB- 9 -RSB- since object placement is dependent on the execution characteristics of the mobile objects themselves. With the exception of MobJeX -LSB- 6 -RSB-, existing mobile object systems such as Voyager -LSB- 10 -RSB-, FarGo -LSB- 11, 12 -RSB-, and JavaParty -LSB- 13 -RSB- do not provide automated adaptation, and therefore lack the metrics collection process required to support this process. In the case of MobJeX, although an adaptation engine has been implemented -LSB- 5 -RSB-, preliminary testing was done using synthetic pre-scripted metrics since there is little prior work on the dynamic collection of software metrics in mobile object frameworks, and no existing means of automatically collecting them. Consequently, the main contribution of this paper is a solution for dynamic metrics collection to support adaptation via object mobility for mobile applications. This problem is non-trivial since typical mobile object frameworks consist of multiple application and middleware components, and thus metrics collection must be performed at different locations and the results efficiently propagated to the adaptation engine. The rest of this paper is organised as follows : Section 2 describes the general structure and implementation of mobile object frameworks in order to understand the challenges related to the collection, propagation and delivery of metrics as described in section 3. Section 4 describes some initial testing and results and section 5 closes with a summary, conclusions and discussion of future work. 2. BACKGROUND In general, an object-oriented application consists of objects collaborating to provide the functionality required by a given problem domain. Mobile object frameworks allow some of these objects to be tagged as mobile objects, providing middleware support for such objects to be moved at runtime to other hosts. At a minimum, a mobile object framework with at least one running mobile application consists of the following components : runtimes, mobile objects, and proxies -LSB- 14 -RSB-, although the terminology used by individual frameworks can differ -LSB- 6, 10-13 -RSB-. A runtime is a container process for the management of mobile objects. For example, in FarGo -LSB- 15 -RSB- this component is known as a core and in most systems separate runtimes are required to allow different applications to run independently, although this is not the case with MobJeX, which can run multiple applications in a single runtime using threads. The applications themselves comprise mobile objects, which interact with each other through proxies -LSB- 14 -RSB-. Upon migration, proxy objects move with the source object. The Java based system MobJeX, which is used as the implementation platform for the metrics collection solution described in this paper, adds a number of additional middleware components. Firstly, a host manager -LRB- known as a service in MobJeX -RRB- provides a central point of communication by running on a known port on a per host basis, thus facilitating the enumeration or lookup of components such as runtimes or mobile objects. Secondly, MobJeX has a per-application mobile object container called a transport manager -LRB- TM -RRB-. As such the host and transport managers are considered in the solution provided in the next section but could be omitted in the general case. Finally, depending on adaptation mode, MobJeX can have a centralised system controller incorporating a global adaptation engine for performing system wide optimisation. 5. SUMMARY AND CONCLUSIONS Given the challenges of developing mobile applications that run in dynamic/heterogeneous environments, and the subsequent interest in application adaptation, this paper has proposed and implemented an online metrics collection strategy to assist such adaptation using a mobile object framework and supporting middleware. Controlled lab studies were conducted to determine worst case performance, as well as show the reduction in collection overhead when applying simple collection criteria. In addition, further testing provided an initial indication of the characteristics of application objects -LRB- based on method execution time -RRB- that would be good candidates for adaptation using the worst case implementation of the proposed metrics collection strategy. A key feature of the solution was the specification of multiple configurable criteria to control the propagation of metrics through the system, thereby reducing collection overhead. Furthermore, such a temporal history could also facilitate intelligent decisions regarding the collection of metrics since for example a metric that is known to be largely constant need not be frequently measured. Future work will also involve the evaluation of a broad range of adaptation scenarios on the MobJeX framework to quantity the gains that can be made via adaptation through object mobility and thus demonstrate in practise, the efficacy of the solution described in this paper. Finally, the authors wish to explore applying the metrics collection concepts described in this paper to a more general and reusable context management system -LSB- 20 -RSB-.", "keyphrases": ["data", "object-orient applic", "mobil object framework", "mobjex", "java", "metricscontain", "metric collect", "proxi", "perform and scalabl", "measur", "propag and deliveri", "framework"]}
{"file_name": "H-26", "text": "A Support Vector Method for Optimizing Average Precision ABSTRACT Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision -LRB- MAP -RRB-, despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora -LRB- WT10g -RRB-, comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores. 1. INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision -LRB- MAP -RRB-. Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy -LSB- 17, 15 -RSB-, ROCArea -LSB- 1, 5, 10, 11, 13, 21 -RSB- or modifications of ROCArea -LSB- 4 -RSB-, and NDCG -LSB- 2, 3 -RSB-. Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance -LSB- 7 -RSB-. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft -LSB- 16 -RSB- and Caruana et al. -LSB- 6 -RSB-, our technique is computationally efficient while finding a globally optimal solution. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We have also developed a software package implementing our algorithm that is available for public user. 6. CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance. The learning framework used by our method is fairly general.", "keyphrases": ["machin learn", "rank retriev system", "learn techniqu", "mean averag precis", "optim solut", "relax of map", "inform retriev system", "probabl", "surrog measur", "loss function", "supervis learn"]}
{"file_name": "H-25", "text": "Term Feedback for Information Retrieval with Language Models ABSTRACT I n t hi s paper w e s t udy t er m - based f eedback f or i nf or mat i on r etrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top. 1. INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents -LSB- 25, 13 -RSB-. This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user 's assistance for query model construction, in the sense that the refined query model -LRB- based on terms -RRB- is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query `` Hubble telescope achievements '', when a relevant document talks more about the telescope 's repair than its discoveries, irrelevant terms such as `` spacewalk '' can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a -LRB- reasonable -RRB- number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms : he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. This is especially helpful for interactive adhoc search. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user 's information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6. Section 7 concludes this paper. 2. RELATED WORK Relevance feedback -LSB- 17, 19 -RSB- has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the user 's information need better than the original one, which is often just a short keyword query. A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback -LSB- 5, 16 -RSB- and usually still brings performance improvement. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance -LSB- 1, 23 -RSB-. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as -LSB- 12 -RSB-, there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. In many cases term relevance feedback has been found to effectively improve retrieval performance -LSB- 6, 22, 12, 4, 10 -RSB-. For example, the study in -LSB- 12 -RSB- shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit -LSB- 3, 14 -RSB-, even if the user likes interacting with expansion terms. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. Our work differs from the previous ones in two important aspects. The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 7. CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C 's performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top. We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment. Third, we have plans to incorporate term feedback into our UCAIR toolbar -LSB- 20 -RSB-, an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents.", "keyphrases": ["term-base feedback", "inform retriev", "languag model", "queri expans process", "queri model", "interact adhoc search", "retriev perform", "probabl", "kl-diverg", "present term"]}
{"file_name": "H-9", "text": "Learn from Web Search Logs to Organize Search Results ABSTRACT Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well : -LRB- 1 -RRB- the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user 's perspective ; and -LRB- 2 -RRB- the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by -LRB- 1 -RRB- learning `` interesting aspects '' of a topic from Web search logs and organizing search results accordingly ; and -LRB- 2 -RRB- generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels. 1. INTRODUCTION The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly. Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization. The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results ; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document. As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively -LSB- 9, 15, 26, 27, 28 -RSB-. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. However, this clustering strategy has two deficiencies which make it not always work well : First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the user 's perspective. But the clusters discovered by the current methods may partition the results into `` local codes '' and `` international codes. '' Such clusters would not be very useful for users ; even the best cluster would still have a low precision. Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem : -LRB- 1 -RRB- The clusters are not corresponding to a user 's interests, so their labels would not be very meaningful or useful. For example, the ambiguous query `` jaguar '' may mean an animal or a car. A cluster may be labeled as `` panthera onca. '' In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following : First, we will learn `` interesting aspects '' of similar topics from search logs and organize search results based on these `` interesting aspects ''. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as `` jaguar '' we can expect to see some clear clusters corresponding different senses of `` jaguar ''. Such aspects can be very useful for organizing future search results about `` car ''. Second, we will generate more meaningful cluster labels using past query words entered by users. Thus they can be better labels than those extracted from the ordinary contents of search results. To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm -LSB- 2 -RSB- to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster. We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches. The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7. 2. RELATED WORK Our work is closely related to the study of clustering search results. In -LSB- 9, 15 -RSB-, the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis -LSB- 20 -RSB- that relevant documents tend to form clusters. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm -LRB- STC -RRB- was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in -LSB- 28 -RSB-, supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In -LSB- 13 -RSB-, the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo -LSB- 22 -RSB-. However, in all these works, the clusters are generated solely based on the search results. Thus the obtained clusters do not necessarily reflect users ' preferences and the generated labels may not be informative from a user 's viewpoint. Methods of organizing search results based on text categorization are studied in -LSB- 6, 8 -RSB-. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query. Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions -LRB- FAQ -RRB- is studied in -LSB- 24, 4 -RSB-. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm -LSB- 2 -RSB-, which is a graph partition based approach, to learn interesting aspects from search logs given a new query. 7. CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users ' perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse. Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results. There are several interesting directions for further extending our work : First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user -LRB- e.g., the aspect chosen by a user to view -RRB-. It would thus be interesting to study how to further improve the organization of the results based on such feedback information. Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user.", "keyphrases": ["retriev model", "rank function", "ambigu", "cluster view", "meaning cluster label", "histori collect", "past queri", "clickthrough", "star cluster algorithm", "suffix tree cluster algorithm", "search result snippet", "monothet cluster algorithm", "pseudo-document", "pairwis similar graph", "similar threshold paramet", "centroid-base method", "cosin similar", "centroid prototyp", "reciproc rank", "log-base method", "mean averag precis"]}
{"file_name": "H-5", "text": "Utility-based Information Distillation Over Temporally Sequenced Documents ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs -LRB- ` tasks ' with multiple queries -RRB-. Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys -LRB- nuggets -RRB- were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components. 1. INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering -LRB- AF -RRB- is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples -LRB- if available -RRB-, an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications : adaptive filtering setup -- he or she reacts to the system only when the system makes a ` yes ' decision on a document, by confirming or rejecting that decision. A more ` active ' alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents -LRB- or passages -RRB- per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists. The latter form of user interaction has been highly effective in standard retrieval for ad hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3. System-selected documents are often highly redundant. A conventional AF system would select all these redundant news stories for user feedback, wasting the user 's time while offering little gain. Clearly, techniques for novelty detection can help in principle -LSB- 25, 2, 22 -RSB- for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to user 's -LRB- fine-grained -RRB- feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user. We call the new process utility-based information distillation. Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus -- the TDT4 collection of news stories and TV broadcasts -- with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1. To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG -LRB- Normalized Discounted Cumulated Gain -RRB- -LSB- 9 -RSB- for assessing the utility of ranked passages as a function of both relevance and novelty. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAF \u00b4 E -- CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives. 7. CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAF \u00b4 E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off.", "keyphrases": ["util-base inform distil", "tempor order document", "passag rank", "adapt filter", "ad-hoc retriev", "novelti detect", "new evalu methodolog", "answer kei", "nugget-match rule", "unifi framework", "ndcg metric"]}
{"file_name": "C-8", "text": "Operation Context and Context-based Operational Transformation ABSTRACT Operational Transformation -LRB- OT -RRB- is a technique for consistency maintenance and group undo, and is being applied to an increasing number of collaborative applications. The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems, as well as the quality of those solutions. The theory of causality has been the foundation of all prior OT systems, but it is inadequate to capture essential correctness requirements. Past research had invented various patches to work around this problem, resulting in increasingly intricate and complicated OT algorithms. After having designed, implemented, and experimented with a series of OT algorithms, we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems, reducing its complexity, and supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The COT algorithm is capable of supporting both do and undo of any operations at anytime, without requiring transformation functions to preserve Reversibility Property, Convergence Property 2, Inverse Properties 2 and 3. The COT algorithm is not only simpler and more efficient than prior OT control algorithms, but also simplifies the design of transformation functions. We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications. 1. INTRODUCTION Operational Transformation -LRB- OT -RRB- was originally invented for consistency maintenance in plain-text group editors -LSB- 4 -RSB-. To effectively and efficiently support existing and new applications, we must continue to improve the capability and quality of OT in solving both old and new problems. The soundness of the theoretical foundation for OT is crucial in this process. However, the theory of causality is inadequate to capture essential OT conditions for correct transformation. The limitation of the causality theory had caused correctness problems from the very beginning of OT. The dOPT algorithm was the first OT algorithm and was based solely on the concurrency relationships among operations -LSB- 4 -RSB- : a pair of operations are transformable as long as they are concurrent. However, later research discovered that the concurrency condition alone is not sufficient to ensure the correctness of transformation. Another condition is that the two concurrent operations must be defined on the same document state. This puzzle was solved in various ways, but the theory of causality as well as its limitation were inherited by all follow-up OT algorithms. The causality theory limitation became even more prominent when OT was applied to solve the undo problem in group editors. The concept of causality is unsuitable to capture the relationships between an inverse operation -LRB- as an interpretation of a meta-level undo command -RRB- and other normal editing operations. In fact, the causality relation is not defined for inverse operations -LRB- see Section 2 -RRB-. Various patches were invented to work around this problem, resulting in more intricate complicated OT algorithms -LSB- 18, 21 -RSB-. supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. First, we define causal-dependency / - independency and briefly describe their limitations in Section 2. Then, we present the key elements of the operation context theory, including the definition of operation context, context-dependency / - independency relations, context-based conditions, and context vectors in Section 3. In Section 4, we present the basic COT algorithm for supporting consistency maintenance -LRB- do -RRB- and group undo under the assumption that underlying transformation functions are able to preserve some important transformation properties. Then, these transformation properties and their pre-conditions are discussed in Section 5. The COT solutions to these transformation properties are presented in Section 6. Comparison of the COT work to prior OT work, OT correctness issues, and future work are discussed in Section 7. Finally, major contributions of this work are summarized in Section 8. 8. CONCLUSIONS We have contributed the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The theory of operation context is capable of capturing essential relationships and conditions for all types of operation in an OT system ; it provides a new foundation for better understanding and resolving OT problems. The COT algorithm provides uniformed solutions to both consistency maintenance and undo problems ; it is simpler and more efficient than prior OT control algorithms with similar capabilities ; and it significantly simplifies the design of transformation functions. The COT algorithm has been implemented in a generic collaboration engine and used for supporting a range of novel collaborative applications -LSB- 24 -RSB-. Real-world applications provide exciting opportunities and challenges to future OT research. The theory of operation context and the COT algorithm shall serve as new foundations for addressing the technical challenges in existing and emerging OT applications.", "keyphrases": ["oper transform", "cot", "context-base ot", "causal-depend", "concurr condit", "concurr relat", "invers oper", "document state", "origin oper", "transform oper", "invers cluster", "vector represent of oper context", "histori buffer", "exclus transform"]}
{"file_name": "J-4", "text": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions ABSTRACT Keyword auctions lie at the core of the business models of today 's leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google 's auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders ' values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword. 1. INTRODUCTION Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for * This work was done while the author was at Yahoo! Research. `` iPod '', the highest paying advertisers -LRB- for example, Apple or Best Buy -RRB- for that keyword may appear in a separate `` sponsored '' section of the page above or to the right of the algorithmic results. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions. Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture 's success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft 's MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. The search engine evaluates the advertisers ' bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine can not directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. Until recently, Yahoo! ranked bidders in decreasing order of advertisers ' stated values per click, while Google ranks in decreasing order of advertisers ' stated values per impression. We refer to these rules as `` rank-by-bid '' and `` rank-by-revenue '', respectively. ' We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank ` These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. ing rules where bidders are ranked in decreasing order of score eqb, where e denotes an advertiser 's click-through rate -LRB- normalized for position -RRB- and b his bid. Notice that q = 0 corresponds to Yahoo! 's rank-by-bid rule and q = 1 corresponds to Google 's rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz -LSB- 3 -RSB- and Varian -LSB- 11 -RSB-. We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! keyword auctions. We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders ' values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results. Related work. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement `` locally envy-free equilibrium '' and `` symmetric equilibrium '', respectively. Varian also provides some empirical analysis. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani -LSB- 1 -RSB-. That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar -LSB- 5 -RSB- study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting. They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always -LRB- weakly -RRB- dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations. Lahaie -LSB- 8 -RSB- gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture. 8. CONCLUSIONS In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo! and Google models as special cases. In practice, it should be very simple to move between rules within the family : this simply involves changing the exponent q applied to advertiser effects. We also showed that, in principle, the same effect could be obtained by using bidding credits. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices. On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders. In principle the minimum revenue in Nash equilibrium can be found by linear programming. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept. Larger classes of ranking rules could be relevant. For instance, it is possible to introduce discounts ds and rank according to wsbs \u2212 ds ; the equilibrium analysis generalizes to this case as well. With this larger class the virtual score can equal the score, e.g. in the case of a uniform marginal distribution over values. Figure 4 : Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1.", "keyphrases": ["revenu", "keyword auction", "revenu-optim rank", "rank rule", "search engin", "advertis", "sponsor search", "rank-by-bid", "rank-by-revenu", "profit", "advertis revenu", "price search keyword", "optim auction design problem"]}
{"file_name": "C-31", "text": "Apocrita : A Distributed Peer-to-Peer File Sharing System for Intranets ABSTRACT Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Another problem arises when a document is made available on a user 's machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets. 1. INTRODUCTION The Peer-to-Peer -LRB- P2P -RRB- computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8. 7. RELATED WORK Several decentralized P2P systems -LSB- 1, 2, 3 -RSB- exist today that Apocrita features some of their functionality. However, Apocrita also has unique novel searching and indexing features that make this system unique. For example, Majestic-12 -LSB- 4 -RSB- is a distributed search and indexing project designed for searching the Internet. Each user would install a client, which is responsible for indexing a portion of the web. A central area for querying the index is available on the Majestic-12 web page. The index itself is not distributed, only the act of indexing is distributed. The distributed indexing aspect of this project most closely relates Apocrita goals. YaCy -LSB- 6 -RSB- is a peer-to-peer web search application. YaCy is designed to maintain a distributed index of the Internet. It used a distributed hash table -LRB- DHT -RRB- to maintain the index. The local node is used to query but all results that are returned are accessible on the Internet. YaCy used many peers and DHT to maintain a distributed index. Apocrita will also use a distributed index in future implementations and may benefit from using an implementation of a DHT. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita. 8. CONCLUSIONS AND FUTURE WORK We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet. It can help organizations with no network file server or necessary network infrastructure to share documents. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems.", "keyphrases": ["peer-to-peer", "file share system", "intranet", "author", "document", "apocrita", "jxta", "distribut index", "peer-to-peer distribut model", "idl queri", "index file", "incom file", "p2p search"]}
{"file_name": "C-20", "text": "Live Data Center Migration across WANs : A Robust Cooperative Context Aware Approach ABSTRACT A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocate a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities. We make use of server virtualization technologies to enable the replication and migration of server functions. We propose new network functions to enable server migration and replication across wide area networks -LRB- e.g., the Internet -RRB-, and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives. 1. INTRODUCTION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. A relatively minor outage can disrupt and inconvenience a large number of users. Today these services are almost exclusively hosted in data centers. Recent advances in server virtualization technologies -LSB- 8, 14, 22 -RSB- allow for the live migration of services within a local area network -LRB- LAN -RRB- environment. In the LAN environment, these technologies have proven to be a very effective tool to enable data center management in a non-disruptive fashion. Not only can it support planned maintenance events -LSB- 8 -RSB-, but it can also be used in a more dynamic fashion to automatically balance load between the physical servers in a data center -LSB- 22 -RSB-. When using these technologies in a LAN environment, services execute in a virtual server, and the migration services provided by the underlying virtualization framework allows for a virtual server to be migrated from one physical server to another, without any significant downtime for the service or application. In particular, since the virtual server retains the same network address as before, any ongoing network level interactions are not disrupted. Similarly, in a LAN environment, storage requirements are normally met via either network attached storage -LRB- NAS -RRB- or via a storage area network -LRB- SAN -RRB- which is still reachable from the new physical server location to allow for continued storage access. Unfortunately in a wide area environment -LRB- WAN -RRB-, live server migration is not as easily achievable for two reasons : First, live migration requires the virtual server to maintain the same network address so that from a network connectivity viewpoint the migrated server is indistinguishable from the original. Second, while fairly sophisticated remote replication mechanisms have been developed in the context of disaster recovery -LSB- 20, 7, 11 -RSB-, these mechanisms are ill suited to live data center migration, because in general the available technologies are unaware of application/service level semantics. In this paper we outline a design for live service migration across WANs. Our design makes use of existing server virtualization technologies and propose network and storage mechanisms to facilitate migration across a WAN. The essence of our approach is cooperative, context aware migration, where a migration management system orchestrates the data center migration across all three subsystems involved, namely the server platforms, the wide area network and the disk storage system. While conceptually similar in nature to the LAN based work described above, using migration technologies across a wide area network presents unique challenges and has to our knowledge not been achieved. Our main contribution is the design of a framework that will allow the migration across a WAN of all subsystems involved with enabling data center services. We describe new mechanisms as well as extensions to existing technologies to enable this and outline the cooperative, context aware functionality needed across the different subsystems to enable this. 4. RELATED WORK Prior work on this topic falls into several categories : virtual machine migration, storage replication and network support. At the core of our technique is the ability of encapsulate applications within virtual machines that can be migrated without application downtimes -LSB- 15 -RSB-. As indicated earlier, these techniques assume that migration is being done on a LAN. VM migration has also been studied in the Shirako system -LSB- 10 -RSB- and for grid environments -LSB- 17, 19 -RSB-. Current virtual machine software support a suspend and resume feature that can be used to support WAN migration, but with downtimes -LSB- 18, 12 -RSB-. Recently live WAN migration using IP tunnels was demonstrated in -LSB- 21 -RSB-, where an IP tunnel is set up from the source to destination server to transparently forward packets to and from the application ; we advocate an alternate approach that assumes edge router support. An excellent description of these and others, as well as a detailed taxonomy of the different approaches for replication can be found in -LSB- 11 -RSB-. The Ursa Minor system argues that no single fault model is optimal for all applications and proposed supporting data-type specific selections of fault models and encoding schemes for replication -LSB- 1 -RSB-. In the context of network support, our work is related to the RouterFarm approach -LSB- 2 -RSB-, which makes use of orchestrated network changes to realize near hitless maintenance on provider edge routers. In addition to being in a different application area, our approach differs from the RouterFarm work in two regards. Second, due to the stringent timing requirements of live migration, we expect that our approach would require new router functionality -LRB- as opposed to being realizable via the existing configuration interfaces -RRB-. In a similar spirit to ROC, we advocate using mechanisms from live VM migration to storage replication to support planned and unplanned outages in data centers -LRB- rather than full replication to mask such failures -RRB-. 5. CONCLUSION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocated a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We sought to achieve high availability of data center services in the face of both planned and incidental outages of data center facilities. We advocated using server virtualization technologies to enable the replication and migration of server functions. We proposed new network functions to enable server migration and replication across wide area networks -LRB- such as the Internet or a geographically distributed virtual private network -RRB-, and finally showed the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "keyphrases": ["internet-base servic", "data center migrat", "wan", "lan", "virtual server", "storag replic", "synchron replic", "asynchron replic", "network support", "storag", "voic-over-ip", "voip", "databas"]}
{"file_name": "J-20", "text": "Clearing Algorithms for Barter Exchange Markets : Enabling Nationwide Kidney Exchanges ABSTRACT In barter-exchange markets, agents seek to swap their items with one another, in order to improve their own utilities. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. We focus mainly on the upcoming national kidney-exchange market, where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed. Long cycles are forbidden, since, for incentive reasons, all transplants in a cycle must be performed simultaneously. Also, in barter-exchanges generally, more agents are affected if one drops out of a longer cycle. We prove that the clearing problem with this cycle-length constraint is NP-hard. Solving it exactly is one of the main challenges in establishing a national kidney exchange. We present the first algorithm capable of clearing these markets on a nationwide scale. The key is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop techniques that dramatically improve both runtime and memory usage. We conclude that column generation scales drastically better than constraint generation. Our algorithm also supports several generalizations, as demanded by real-world kidney exchanges. Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges. The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted. 1. INTRODUCTION The role of kidneys is to filter waste from blood. Kidney failure results in accumulation of this waste, which leads to death in months. One treatment option is dialysis, in which the patient goes to a hospital to have his/her blood filtered by an external machine. Several visits are required per week, and each takes several hours. The quality of life on dialysis can be extremely low, and in fact many patients opt to withdraw from dialysis, leading to a natural death. Only 12 % of dialysis patients survive 10 years -LSB- 23 -RSB-. Instead, the preferred treatment is a kidney transplant. Kidney transplants are by far the most common transplant. Unfortunately, the demand for kidneys far outstrips supply. In the United States in 2005, 4,052 people died waiting for a life-saving kidney transplant. During this time, almost 30,000 people were added to the national waiting list, while only 9,913 people left the list after receiving a deceaseddonor kidney. For many patients with kidney disease, the best option is to find a living donor, that is, a healthy person willing to donate one of his/her two kidneys. In 2005, there were 6,563 live donations in the US. and his intended recipient are blood-type or tissue-type incompatible. In the past, the incompatible donor was sent home, leaving the patient to wait for a deceased-donor kidney. However, there are now a few regional kidney exchanges in the United States, in which patients can swap their incompatible donors with each other, in order to each obtain a compatible donor. These markets are examples of barter exchanges. In a barter-exchange market, agents -LRB- patients -RRB- seek to swap their items -LRB- incompatible donors -RRB- with each other. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. Barter exchanges are ubiquitous : examples include Peerflix -LRB- DVDs -RRB- -LSB- 11 -RSB-, Read It Swap It -LRB- books -RRB- -LSB- 12 -RSB-, and Intervac -LRB- holiday houses -RRB- -LSB- 9 -RSB-. For many years, there has even been a large shoe exchange in the United States -LSB- 10 -RSB-. People with different-sized feet use this to avoid having to buy two pairs of shoes. Leg amputees have a separate exchange to share the cost of buying a single pair of shoes. We can encode a barter exchange market as a directed graph G = -LRB- V, E -RRB- in the following way. Construct one vertex for each agent. Add a weighted edge e from one agent vi to another vj, if vi wants the item of vj. The weight we of e represents the utility to vi of obtaining vj 's item. A cycle c in this graph represents a possible swap, with each agent in the cycle obtaining the item of the next agent. The weight wc of a cycle c is the sum of its edge weights. An exchange is a collection of disjoint cycles. The weight of an exchange is the sum of its cycle weights. A social welfare maximizing exchange is one with maximum weight. Figure 1 illustrates an example market with 5 agents, -LCB- v1, v2,..., v5 -RCB-, in which all edges have weight 1. The market has 4 cycles, c1 = -LRB- v1, v2 -RRB-, c2 = -LRB- v2, v3 -RRB-, c3 = -LRB- v3, v4 -RRB- and c4 = -LRB- v1, v2, v3, v4, v5 -RRB-, and two -LRB- inclusion -RRB- maximal exchanges, namely M1 = -LCB- c4 -RCB- and M2 = -LCB- c1, c3 -RCB-. Exchange M1 has both maximum weight and maximum cardinality -LRB- i.e., it includes the most edges/vertices -RRB-. Figure 1 : Example barter exchange market. The clearing problem is to find a maximum-weight exchange consisting of cycles with length at most some small constant L. This cycle-length constraint arises naturally for several reasons. For example, in a kidney exchange, all operations in a cycle have to be performed simultaneously ; otherwise a donor might back out after his incompatible partner has received a kidney. Due to such resource constraints, the upcoming national kidney exchange market will likely allow only cycles of length 2 and 3. Another motivation for short cycles is that if the cycle fails to exchange, fewer agents are affected. For example, last-minute testing in a kidney exchange often reveals new incompatibilities that were not detected in the initial testing -LRB- based on which the compatibility graph was constructed -RRB-. In Section 3, we show that -LRB- the decision version of -RRB- the clearing problem is NP-complete for L > 3. One approach then might be to look for a good heuristic or approximation algorithm. However, for two reasons, we aim for an exact algorithm based on an integer-linear program -LRB- ILP -RRB- formulation, which we solve using specialized tree search. 9 First, any loss of optimality could lead to unnecessary patient deaths. 9 Second, an attractive feature of using an ILP formula tion is that it allows one to easily model a number of variations on the objective, and to add additional constraints to the problem. Or, if for various -LRB- e.g., ethical -RRB- reasons one requires a maximum cardinality exchange, one can at least in a second pass find the solution -LRB- out of all maximum cardinality solutions -RRB- that has the fewest 3-cycles. Other variations one can solve for include finding various forms of `` fault tolerant '' -LRB- non-disjoint -RRB- collections of cycles in the event that certain pairs that were thought to be compatible turn out to be incompatible after all. In this paper, we present the first algorithm capable of clearing these markets on a nationwide scale. Straight-forward ILP encodings are too large to even construct on current hardware -- not to talk about solving them. The key then is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop a host of -LRB- mainly problemspecific -RRB- techniques that dramatically improve both runtime and memory usage. 1.1 Prior Work Several recent papers have used simulations and marketclearing algorithms to explore the impact of a national kidney exchange -LSB- 13, 20, 6, 14, 15, 17 -RSB-. For example, using Edmond 's maximum-matching algorithm -LSB- 4 -RSB-, -LSB- 20 -RSB- shows that a national pairwise-exchange market -LRB- using length-2 cycles only -RRB- would result in more transplants, reduced waiting time, and savings of $ 750 million in heath care costs over 5 years. Those results are conservative in two ways. Firstly, the simulated market contained only 4,000 initial patients, with 250 patients added every 3 months. It has been reported to us that the market could be almost double this size. Secondly, the exchanges were restricted to length-2 cycles -LRB- because that is all that can be modeled as maximum matching, and solved using Edmonds 's algorithm -RRB-. Allowing length-3 cycles leads to additional significant gains. This has been demonstrated on kidney exchange markets with 100 patients by using CPLEX to solve an integer-program encoding of the clearing problem -LSB- 15 -RSB-. In this paper, we present an alternative algorithm for this integer program that can clear markets with over 10,000 patients -LRB- and that same number of willing donors -RRB-. Allowing cycles of length more than 3 often leads to no improvement in the size of the exchange -LSB- 15 -RSB-. -LRB- Furthermore, in a simplified theoretical model, any kidney exchange can be converted into one with cycles of length at most 4 -LSB- 15 -RSB-. -RRB- Whilst this does not hold for general barter exchanges, or even for all kidney exchange markets, in Section 5.2.3 we make use of the observation that short cycles suffice to dramatically increase the speed of our algorithm. At a high-level, the clearing problem for barter exchanges is similar to the clearing problem -LRB- aka winner determination problem -RRB- in combinatorial auctions. In both settings, the idea is to gather all the pertinent information about the agents into a central clearing point and to run a centralized clearing algorithm to determine the allocation. Both problems are NP-hard. Both are best solved using tree search techniques. Since 1999, significant work has been done in computer science and operations research on faster optimal tree search algorithms for clearing combinatorial auctions. However, the kidney exchange clearing problem -LRB- with a limit of 3 or more on cycle size -RRB- is different from the combinatorial auction clearing problem in significant ways. The most important difference is that the natural formulations of the combinatorial auction problem tend to easily fit in memory, so time is the bottleneck in practice. In contrast, the natural formulations of the kidney exchange problem -LRB- with L = 3 -RRB- take at least cubic space in the number of patients to even model, and therefore memory becomes a bottleneck much before time does when using standard tree search, such as branch-andcut in CPLEX, to tackle the problem. Therefore, the approaches that have been developed for combinatorial auctions can not handle the kidney exchange problem. 1.2 Paper Outline The rest of the paper is organized as follows. Section 2 discusses the process by which we generate realistic kidney exchange market data, in order to benchmark the clearing algorithms. Section 3 contains a proof that the market clearing decision problem is NP-complete. Sections 4 and 5 each contain an ILP formulation of the clearing problem. We also detail in those sections our techniques used to solve those programs on large instances. Section 6 presents experiments on the various techniques. Section 7 discusses recent fielding of our algorithm. Finally, we present our conclusions in Section 8, and suggest future research directions. 7. FIELDING THE TECHNOLOGY Our algorithm and implementation replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges, in December 2006. We conduct a match run every two weeks, and the first transplants based on our solutions have already been conducted. While there are -LRB- for political/inter-personal reasons -RRB- at least four kidney exchanges in the US currently, everyone understands that a unified unfragmented national exchange would save more lives. We are in discussions with additional kidney exchanges that are interested in adopting our technology. This way our technology -LRB- and the processes around it -RRB- will hopefully serve as a substrate that will eventually help in unifying the exchanges. At least computational scalability is no longer an obstacle. 8. CONCLUSION AND FUTURE RESEARCH In this work we have developed the most scalable exact algorithms for barter exchanges to date, with special focus on the upcoming national kidney-exchange market in which patients with kidney disease will be matched with compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. Our work presents the first algorithm capable of clearing these markets on a nationwide scale. It optimally solves the kidney exchange clearing problem with 10,000 donordonee pairs. The best prior technology -LRB- vanilla CPLEX -RRB- can not handle instances beyond about 900 donor-donee pairs because it runs out of memory. The key to our improvement is incremental problem formulation. We adapted two paradigms for the task : constraint generation and column generation. For each, we developed a host of techniques that substantially improve both runtime and memory usage. Some of the techniques use domain-specific observations while others are domain independent. We conclude that column generation scales dramatically better than constraint generation. Undoubtedly, further parameter tuning and perhaps additional speed improvement techniques could be used to make the algorithm even faster. Our algorithm also supports several generalizations, as desired by real-world kidney exchanges. Because we use an ILP methodology, we can also support a variety of side constraints, which often play an important role in markets in practice -LSB- 19 -RSB-. We can also support forcing part of the allocation, for example, `` This acutely sick teenager has to get a kidney if possible. '' Our work has treated the kidney exchange as a batch problem with full information -LRB- at least in the short run, kidney exchanges will most likely continue to run in batch mode every so often -RRB-. Two important directions for future work are to explicitly address both online and limited-information aspects of the problem. The online aspect is that donees and donors will be arriving into the system over time, and it may be best to not execute the myopically optimal exchange now, but rather save part of the current market for later matches.", "keyphrases": ["barter-exchang market", "match", "column gener", "kidnei", "transplant", "market characterist", "instanc gener", "solut approach", "edg formul", "cycl formul"]}
{"file_name": "H-2", "text": "Personalized Query Expansion for the Web ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user 's Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach. 1. INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are * Part of this work was performed while the author was visiting Yahoo! Research, Barcelona, Spain. inherently ambiguous. The query `` canon book '' for example covers several different areas of interest : religion, photography, literature, and music. Clearly, one would prefer search output to be aligned with user 's topic -LRB- s -RRB- of interest, rather than displaying a selection of popular URLs from each category. Studies have shown that more than 80 % of the users would prefer to receive such personalized search results -LSB- 33 -RSB- instead of the currently generic ones. Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. It has been shown to perform very well over large data sets, especially with short input queries -LRB- see for example -LSB- 19, 3 -RSB- -RRB-. This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the user 's Personal Information Repository -LRB- PIR -RRB-, i.e., the personal collection of text documents, emails, cached Web pages, etc.. Several advantages arise when moving Web search personalization down to the Desktop level -LRB- note that by `` Desktop '' we refer to PIR, and we use the two terms interchangeably -RRB-. First is of course the quality of personalization : The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user. Our algorithms expand Web queries with keywords extracted from user 's PIR, thus implicitly personalizing the search output. After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1. We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best. In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri. The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG -LSB- 15 -RSB- improvements of up to 51.28 %. In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. This yields an additional improvement of 8.47 % over the previously identified best algorithm. We conclude and discuss further work in Section 5. 2. PREVIOUS WORK This paper brings together two IR areas : Search Personalization and Automatic Query Expansion. There exists a vast amount of algorithms for both domains. In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components : -LRB- 1 -RRB- User profiles, and -LRB- 2 -RRB- The actual search algorithm. This section splits the relevant background according to the focus of each article into either one of these elements. Approaches focused on the User Profile. Sugiyama et al. -LSB- 32 -RSB- analyzed surfing behavior and generated user profiles as features -LRB- terms -RRB- of the visited pages. Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile. Qiu and Cho -LSB- 26 -RSB- used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank -LSB- 13 -RSB-. User profiling based on browsing history has the advantage of being rather easy to obtain and process. This is probably why it is also employed by several industrial search engines -LRB- e.g., Yahoo! MyWeb2 -RRB-. However, it is definitely not sufficient for gathering a thorough insight into user 's interests. Moreover, none of these investigated the adaptive application of personalization. Approaches focused on the Personalization Algorithm. Haveliwala -LSB- 13 -RSB- computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval. It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in put keywords before identifying the matching documents returned as output. In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms : -LRB- 1 -RRB- Relevance feedback, -LRB- 2 -RRB- Collection based co-occurrence statistics, and -LRB- 3 -RRB- Thesaurus information. Some other approaches are also addressed in the end of the section. Relevance Feedback Techniques. The main idea of Relevance Feedback -LRB- RF -RRB- is that useful information can be extracted from the relevant documents returned for the initial query. First approaches were manual -LSB- 28 -RSB- in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents. Efthimiadis -LSB- 11 -RSB- presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.. We used some of these as inspiration for our Desktop specific techniques. Chang and Hsu -LSB- 5 -RSB- asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary. RF has also been shown to be effectively automatized by considering the top ranked documents as relevant -LSB- 37 -RSB- -LRB- this is known as Pseudo RF -RRB-. Lam and Jones -LSB- 21 -RSB- used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query. Finally, Yu et al. -LSB- 38 -RSB- selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein. Co-occurrence Based Techniques. Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query -LSB- 17 -RSB-. We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository. Thesaurus Based Techniques. A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords. Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality -LSB- 36 -RSB-. We also use WordNet based expansion terms. However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords. Other Techniques. There are many other attempts to extract expansion terms. Though orthogonal to our approach, two works are very relevant for the Web environment : Cui et al. -LSB- 8 -RSB- generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs. Kraft and Zien -LSB- 19 -RSB- showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 5. CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the user 's Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to user 's interests, personalizing the search output. In this context, the paper includes the following contributions : \u2022 We proposed five techniques for determining expansion terms from personal documents. Each of them produces additional query keywords by analyzing user 's Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri. Figure 1 : Relative NDCG gain -LRB- in % -RRB- for each algorithm overall, as well as separated per query category. \u2022 We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios. We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28 %. \u2022 We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. \u2022 Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47 % over the previously identified best approach. We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms. We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data. Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms.", "keyphrases": ["short keyword queri", "web retriev", "web queri", "person inform repositori", "search output", "addit queri keyword", "granular level", "term and compound level analysi", "global co-occurr statist", "extern thesauru", "extens empir analysi", "ambigu queri", "qualiti", "output rank", "person search framework", "expans process", "variou featur of each queri", "adapt algorithm", "signific improv", "static expans approach"]}
{"file_name": "J-30", "text": "Implementation with a Bounded Action Space ABSTRACT While traditional mechanism design typically assumes isomorphism between the agents ' type - and action spaces, in many situations the agents face strict restrictions on their action space due to, e.g., technical, behavioral or regulatory reasons. We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces. Our contribution is threefold. First, we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies, and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost. Second, we identify necessary conditions for the optimality of action-bounded mechanisms, and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives. Finally, we prove that for any multilinear social-choice rule, the optimal mechanism with k actions incurs an expected loss of O -LRB- k21 -RRB- compared to the optimal mechanisms with unrestricted action spaces. Our results apply to various economic and computational settings, and we demonstrate their applicability to signaling games, public-good models and routing in networks. 1. INTRODUCTION Mechanism design is a sub-field of game theory that studies how to design rules of games resulting in desirable outcomes, when the players are rational. In a standard setting, players hold some private information -- their `` types '' -- and choose `` actions '' from their action spaces to maximize their utilities. The social planner wishes to implement a social-choice function, which maps each possible state of the world -LRB- i.e., a profile of the players ' types -RRB- to a single alternative. For example, a government that wishes to undertake a public-good project -LRB- e.g., building a bridge -RRB- only if the total benefit for the players exceeds its cost. Much of the literature on mechanism design restricts attention to direct revelation mechanisms, in which a player 's action space is identical to his type space. This focus is owing to the revelation principle that asserts that if some mechanism achieves a certain result in an equilibrium, the same result can be achieved in a truthful one -- an equilibrium where each agent simply reports his private type -LSB- 15 -RSB-. Nonetheless, in many environments, direct-revelation mechanisms are not viable since the actions available for the players have a limited expressive power. Consider, for example, the well-studied `` screening '' model, where an insurance firm wishes to sell different types of policies to different drivers based on their caution levels, which is their private information. There are various reasons for such strict restrictions on the action spaces. The buyers in such environemnts face only two actions -- to buy or not to buy -- although they may have an infinite number of possible values for the item. In many similar settings, players might be also reluctant to reveal their accurate types, but willing to disclose partial information about them. For example, agents will typically be unwilling to reveal their types, even if it is beneficial for them in the short run, since it might harm them in future transactions. Agents may also not trust the mechanism to keep their valuations private -LSB- 16 -RSB-, or not even know their exact type while computing it may be expensive -LSB- 12 -RSB-. Consider for example a public-good model : a social planner needs to decide whether to build a bridge. The two players in the game have some privately known benefits \u03b81, \u03b82 \u2208 -LSB- 0, 1 -RSB- from using this bridge. The social planner aims to build the bridge only if the sum of these benfits exceeds the construction cost of the bridge. The social planner can not access the private data of the players, and can only learn about it from the players ' actions. When direct revelation is allowed, the social planner can run the well-known VCG mechanism, where the players have incentives to report their true data ; hence, the planner can elicit the exact private information of the players and build the bridge only when it should be built. Assume now that the players can not send their entire secret data, but can only choose an action out of two possible actions -LRB- e.g., `` 0 '' or `` 1 '' -RRB-. Now, the social planner will clearly no longer be able to always build the bridge according to her objective function, due to the limited expressivness of the players ' messages. In this work we try to analyze what can be achieved in the presence of such restrictions. Restrictions on the action space, for specific models, were studied in several earlier papers. They studied single-item auctions where bidders are allowed to send messages with severely bounded size. They characterized the optimal mechanisms under this restriction, and showed that nearly optimal results can be achieved even with very strict limitations on the action space. Our work generalizes the main results of Blumrosen et al. to a general mechanism-design framework that can be applied to a multitude of models. A standard mechanism design setting is composed of agents with private information -LRB- their `` types '' -RRB-, and a social planner, who wishes to implement a social choice function, c -- a function that maps any profile of the agents ' types into a chosen alternative. A classic result in this setting says that under some monotonicity assumption on the agents ' preferences -- the `` single-crossing '' assumption -LRB- see definition below -RRB- -- a social-choice function is implementable in dominant strategies if and only if it is monotone in the players ' types. However, in environments with restricted action spaces, the social planner can not typically implement every social-choice function due to inherent informational constraints. That is, for some realizations of the players ' types, the decision of the social planner will be incompatible with the social-choice function c. In order to quantitatively measure how well bounded-action mechanisms can approximate the original social-choice functions, we follow a standard assumption that the social choice function is derived from a social-value function, g, which assigns a real value for every alternative A and realization of the players ' types. The social-choice function c will therefore choose an alternative that maximizes the social value function, given the type \u2192 \u2212 \u03b8 = -LRB- \u03b81,. . Observe that the social-value function is not necessarily the social welfare function -- the social welfare function is a special case of g in which g is defined to be the sum of the players ' valuations for the chosen alternative. Following are several simple examples of social-value functions : \u2022 Public goods. A government wishes to build a bridge only if the sum of the benefits that agents gain from it exceeds its construction cost C. The social value functions in a 2-player game will therefore be : g -LRB- \u03b81, \u03b82, `` build '' -RRB- = \u03b81 + \u03b82-C and g -LRB- \u03b81, \u03b82, `` do not build '' -RRB- = 0. \u2022 Routing in networks. Consider a network that is composed of two links in parallel. Each link has a secret probability pi of transferring a message successfully. A sender wishes to send his message through the network only if the probability of success is greater than, say, 90 percent - the known probability in an alternate network. \u2022 Single-item auctions. Consider a 2-player auction, where the auctioneer wishes to allocate the item to the player who values it the most. The social choice function is given by : g -LRB- \u03b81, \u03b82, `` player 1 wins '' -RRB- = \u03b81 and for the second alternative is g -LRB- \u03b81, \u03b82, `` player 2 wins '' -RRB- = \u03b82. 1.1 Our Contribution In this paper, we present a general framework for the study of mechanism design in environments with a limited number of actions. We assume a Bayesian model where players have one-dimensional private types, independently distributed on some real interval. The main question we ask is : when agents are only allowed to use k different actions, which mechanisms achieve the optimal expected social-value? Note that this question is actually composed of two separate questions. The first question is an information-theoretic question : what is the optimal result achievable when the players can only reveal information using these k actions -LRB- recall that their type space may be continuous -RRB-. The other question involves gametheoretic considerations : what is the best result achievable with k actions, where this result should be achieved in a dominant-strategy equilibrium. These questions raise the question about the `` price of implementation '' : can the optimal information-theoretic result always be implemented in a dominant-strategy equilibrium? And if not, to what extent does the dominant-strategy requirement degrades the optimal result? Our first contribution is the characterization of sufficient conditions for implementing the optimal informationtheoretic social-choice rule in dominant strategies. We show that for the family of multilinear social-value functions -LRB- that Theorem : Given any multilinear single-crossing socialvalue function, and for any number of alternatives and players, the social choice rule that is information-theoretically optimal is implementable in dominant strategies. Multilinear social-value functions capture many important and well-studied models, and include, for instance, the routing example given above, and any social welfare function in which the players ' valuations are linear in their types -LRB- such as public-goods and auctions -RRB-. The implementability of the information-theoretically optimal mechanisms enables us to use a standard routine in Mechanism Design and first determine the optimal socialchoice rule, and then calculate the appropriate payments that ensure incentive compatibility. To show this result, we prove a useful lemma that gives another characterization for social-choice functions whose `` price of implementation '' is zero. We show that for any social-choice function, incentive compatibility in action-bounded mechanisms is equivalent to the property that the optimal expected social value is achieved with non-decreasing strategies -LRB- or threshold strategies -RRB-.1 In other words, this lemma implies that one can always implement, with dominant strategies, the best socialchoice rule that is achievable with non-decreasing strategies. Our second contribution is in characterizing the optimal action-bounded mechanisms. We identify some necessary conditions for the optimality of mechanisms in general, and using these conditions, we fully characterize the optimal mechanisms in environments with two players and two alternatives. We complete the characterization of the optimal mechanisms with the depiction of the optimal strategies -- strategies that are `` mutually maximizers ''. Since the payments in a dominantstrategy implementation are uniquely defined by a monotone allocation and a profile of strategies, this also defines the payments in the mechanism. We give an intuitive proof for the optimality of such strategies, generalizing the concept of optimal `` mutually-centered '' strategies from -LSB- 4 -RSB-. Surprisingly, as opposed to the optimal auctions in -LSB- 4 -RSB-, for some non-trivial social-value functions, the optimal `` diagonal '' mechanism may not utilize all the k available actions. Theorem : For any multilinear single-crossing social-value function over two alternatives, the informationally optimal 2-player k-action mechanism is diagonal, and the optimal dominant strategies are mutually-maximizers. Achieving a full characterization of the optimal actionbounded mechanism for multi-player or multi-alternative environments seems to be harder. To support this claim, we observe that the number of mechanisms that satisfy the necessary conditions above is growing exponentially in the number of players. 1The restriction to non-decreasing strategies is very common in the literature. One remarkable result by Athey -LSB- 1 -RSB- shows that when a non-decreasing strategy is a best response for any other profile of non-decreasing strategies, a pure Bayesian-Nash equilibrium must exist. Our next result compares the expected social-value in k-action mechanisms to the optimal expected social value when the action space is unrestricted. For any number of players or alternatives, and for any profile of independent distribution functions, we construct mechanisms that are nearly optimal -- up to an additive difference of O -LRB- k21 -RRB-. This result is achieved in dominant strategies. Theorem : For any multilinear social-value function, the optimal k-action mechanism incurs an expected social loss of O -LRB- k21 -RRB-. Note that there are social-choice functions that can be implemented with k actions with no loss at all -LRB- for example, the rule `` always choose alternative A '' -RRB-. However, we know that in some settings -LRB- e.g., auctions -LSB- 5 -RSB- -RRB- the optimal loss may be proportional to 1k2, thus a better general upper bound is impossible. Finally, we present our results in the context of several natural applications. First, we give an explicit solution for a public-good game with k-actions. This is a natural application in our context since education levels are often discrete -LRB- e.g., B.A, M.A and PhD -RRB-. The latter example illustrates how our results apply to settings where the goal of the social planner is not welfare maximization -LRB- nor variants of it like `` affine maximizers '' -RRB-. The rest of the paper is organized as follows : our model and notations are described in Section 2. We then describe our general results regarding implementation in multi-player and multi-alternative environments in Section 3, including the asymptotic analysis of the social-value loss. In Section 4, we fully characterize the optimal mechanisms for 2player environments with two alternative. In Section 5, we conclude with applying our general results to several wellstudied models. 5. EXAMPLES Our results apply to a variety of economic, computational and networked settings. In this section, we demonstrate the applicability of our results to public-good models, signaling games and routing applications. 5.1 Application 1 : Public Goods The public-good model deals with a social planner -LRB- e.g., government -RRB- that needs to decide whether to supply a public good, such as building a bridge. Let Yes and No denote the respective alternatives of building and not building the bridge. v = v1,..., vn is the vector of the players ' types -- the values they gain from using the bridge. The decision that maximizes the social welfare is to build the bridge if and only if P is built, the social welfare is P i vi is greater than its cost, denoted by C. The utility of player i under payment pi is ui = vi -- pi if the bridge is built, and 0 otherwise. It is well-known that under no restriction on the action space, it is possible to induce truthful revelation by VCG mechanisms, therefore full efficiency can be achieved. Obviously, when the action set is limited to k actions, we can not achieve full efficiency due to the informational constraints. Hence, the information-theoretically optimal kaction mechanism is implementable in dominant strategies. Moreover, as Theorem 3 suggests, in the k-action 2-player public-good game, we can fully characterize the optimal mechanisms. In the proof of Theorem 3, we saw that when for both players g -LRB- \u03b8i, \u03b8i, A -RRB- = g -LRB- \u03b8i, \u03b8i, B -RRB-, the mechanism is non-degenerate with respect to both players.6 This condition clearly holds here -LRB- 1 + 0 -- C = 0 + 1 -- C -RRB-, therefore the optimal mechanisms will use all k actions. 1. Allocation : Build the bridge if j '' b1 + b2 > k. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1, 2. Allocation : Build the bridge if j '' b1 + b2 > k -- 1. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1 : Recall that we define the optimal mechanisms by their allocation scheme and by the optimal strategies for the players. It is well known, that the allocation scheme in monotone mechanisms uniquely defines the payments that ensure incentive-compatibility. In public-good games, these payments satisfy the rule that a player pays his lowest value for which the bridge is built, when the action of the other player is fixed. Therefore, the payments for the players 1 and 2 reporting the actions b1 and b2 are as follows : in mechanism 1 from Proposition 3, p1 = xb2 and p2 = yb1 ; in mechanism 2 from Proposition 3, p1 = xb2 -- 1 and p2 = yb1 -- 1. We now show a more specific example that assumes uniform distributions. The example shows how the optimal mechanism is determined by the cost C : for low costs, mechanism of type 1 is optimal, and for high costs the optimal mechanism is of type 2. An additional interesting feature of the optimal mechanisms in the example is that they are symmetric with respect to the players. This come as opposed to the optimal mechanisms in the auction model -LSB- 5 -RSB- that are asymmetric -LRB- even when the players ' values are drawn from identical distributions -RRB-. Figure 2 : Optimal mechanisms in a 2-player, 2-alternative, 2-action public-goods game, when the types are uniformly distributed in -LSB- 0, 1 -RSB-. The mechanism on the left is optimal when C < 1 and the other is optimal when C > 1. EXAMPLE 1. Suppose that the types of both players are uniformly distributed on -LSB- 0, 1 -RSB-. Figure 2 illustrates the optimal mechanisms for k = 2, and shows how both the allocation scheme and the payments depend on the construction cost C. Then, the welfare-maximizing mechanisms are : 5.2 Application 2 : Signaling We now study a signaling model in labor markets. In this model, the type of each worker, \u03b8i \u2208 -LSB- \u03b8, \u03b8 -RSB-, describes the worker 's productivity level. The firm wants to make her hiring decisions according to a decision function f -LRB- \u2212 \u2192 \u03b8 -RRB-. For example, the firm may want to hire the most productive worker -LRB- like the auction model -RRB-, or hire a group of workers only if their sum of productivities is greater than some threshold -LRB- similar to the public-good model -RRB-. However, the worker 's productivity is invisible to the firm ; the firm only observes the worker 's education level e that should convey signals about her productivity level. Note that the assumption here is that acquiring education, at any level, does not affect the productivity of the worker, but only signals about the worker 's skills. A main component in this model, is the fact that as the worker is more productive, it is easier for him to acquire high-level education. In addition, the cost of acquiring education increases with the education level. More formally, a continuous function C -LRB- e, \u03b8 -RRB- describes the cost to a worker from acquiring each education level as a function of his productivity. An action for a worker in this game is the education level he chooses to acquire. In standard models, this action space is continuous, and then a `` fully separating equilibrium '' exists -LRB- under the single-crossing conditions on the cost function -RRB-. That is, there exists an equilibrium in which every type is mapped into a different education level ; thus, the firm can induce the exact productivity levels of the workers by this signaling mechanism. However, it is hard to imagine a world with a continuum of education levels. It is usually the case that there are only several discrete education levels -LRB- e.g., BSc, MSc, PhD -RRB-. With k education levels, the firm may not be able to exactly follow the decision function f. For achieving the best result in k actions, the firm may want the workers to play according to specific threshold strategies. It turns out that the standard condition, the single-crossing condition on the cost function, suffices for ensuring that these threshold strategies will be dominant for the players. COROLLARY 4. Consider a multilinear decision function f, and a single-crossing cost function for the players. With k education levels, the firm can implement in dominant strategies a decision function that incurs a loss of O -LRB- k21 -RRB- compared with the decision function f. 5.3 Application 3 : Routing In our last example, we show the applicability of our results to routing in lossy networks. In such systems, a sender needs to decide through which network to transmit his message. In this example, we focus on parallel-path networks. The edges in these networks are controlled by different selfish agents, and each edge appears only in one of the networks. Suppose that the sender, who wishes to send a message from the source to the sink, knows the topology of each network, but the probability of success on each link, pi, is the link 's private information. The problem of the sender is to decide whether to send a message through the network N1 or through an alternate network N2. Obviously, the sender wishes to send the message through N1 only if the total probability of success in N1 is greater than the success probability in N2. Let f N -LRB- \u2212 \u2192 p -RRB- denote the probability of success in network N with a successprobability vector \u2192 \u2212 p. The social choice function in this example is thus : c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figure 3 : An example for a parallel-path network, where each link has a probability pi for transmission success. We show that the overall probability of success in such networks is multilinear in pi, and thus the optimal k-action social-choice function is dominant-strategy implementable. In this example, we assume that every agent has a singlecrossing valuation function over the alternatives. That is, each player wishes that the message will be sent through his network, and his benefit is positively correlated with his secret data -LRB- e.g., the valuation of player i may be exactly pi -RRB-. We would like to emphasize that the social planner in this example -LRB- the sender -RRB- does not aim to maximize the social welfare. That is, the social value is not the sum of the players ' types nor any weighted sum of the types -LRB- `` affine maximizer '' -RRB-. The success probability of sending a message through a parallel-path network is multilinear, since it can be expressed by the following multilinear formula -LRB- where P denotes the set of all paths between the source and the sink -RRB- : Note that for every link i, the partial derivative in pi of the success probability written in Equation 3 is positive. In all the other networks, that do not contain link i, the partial derivative is clearly zero. Therefore, the social-value function is single crossing and our general results can be applied. COROLLARY 5. For any social-choice function that maximizes the success probability over parallel-path networks, the informationally optimal k-action social-choice function is implementable -LRB- for any k -RRB-. Acknowledgment. The work of the second author is also supported by the Lady Davis Trust Fellowship.", "keyphrases": ["bound action space", "implement", "domin strategi", "social-choic function", "decis function", "singl-cross condit", "multilinear function", "optim mechan", "action-bound mechan", "probabl of success"]}
{"file_name": "C-19", "text": "Service Interface : A New Abstraction for Implementing and Composing Protocols * ABSTRACT In this paper we compare two approaches to the design of protocol frameworks -- tools for implementing modular network protocols. The most common approach uses events as the main abstraction for a local interaction between protocol modules. We argue that an alternative approach, that is based on service abstraction, is more suitable for expressing modular protocols. It also facilitates advanced features in the design of protocols, such as dynamic update of distributed protocols. We then describe an experimental implementation of a service-based protocol framework in Java. 1. INTRODUCTION They allow complex protocols to be implemented by decomposing them into several modules cooperating together. This approach facilitates code reuse and customization of distributed protocols in order to fit the needs of different applications. Moreover, protocol modules can be plugged in to the system dynamically. All these features of protocol frameworks make them an interesting enabling technology for implementing adaptable systems -LSB- 14 -RSB- - an important class of applications. Most protocol frameworks are based on events -LRB- all frameworks cited above are based on this abstraction -RRB-. Events are used for asynchronous communication between different modules on the same machine. For instance, the composition of modules may require connectors to route events, which introduces burden for a protocol composer -LSB- 4 -RSB-. Protocol frameworks such as Appia and Eva extend the event-based approach with channels. However, in our opinion, this solution is not satisfactory since composition of complex protocol stacks becomes more difficult. In this paper, we propose a new approach for building modular protocols, that is based on a service abstraction. We compare this new approach with the common, event-based approach. We show that protocol frameworks based on services have several advantages, e.g. allow for a fairly straightforward protocol composition, clear implementation, and better support of dynamic replacement of distributed protocols. To validate our claims, we have implemented SAMOA -- an experimental protocol framework that is purely based on the service-based approach to module composition and implementation. The framework allowed us to compare the service - and event-based implementations of an adaptive group communication middleware. Section 2 defines general notions. Section 3 presents the main characteristics of event-based frameworks, and features that are distinct for each framework. Section 4 describes our new approach, which is based on service abstraction. Section 5 discusses the advantages of a service-based protocol framework compared to an event-based protocol framework. The description of our experimental implementation is presented in Section 6. Finally, we conclude in Section 7. 7. CONCLUSION In the paper, we proposed a new approach to the protocol composition that is based on the notion of Service Interface, instead of events. We believe that the service-based framework has several advantages over event-based frameworks. A prototype implementation allowed us to validate our ideas.", "keyphrases": ["protocol framework", "distribut algorithm", "distribut system", "servic interfac", "network", "commun", "event-base framework", "stack", "modul", "request", "repli"]}
{"file_name": "I-21", "text": "Interactions between Market Barriers and Communication Networks in Marketing Systems ABSTRACT We investigate a framework where agents search for satisfying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents. Local behaviour incorporates a satisficing model of choice, a set of rules governing the interactions between agents, including learning about the trustworthiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products. We use various degree distributions dictating the extent of connectivity, and incorporate both small-world effects and the notion of preferential attachment in our network models. We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effectiveness changes with the imposition of market barriers. 1. INTRODUCTION Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product -LSB- 14 -RSB- and the availability of more attractive alternatives -LSB- 17 -RSB-. However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves -LRB- in the guise of formal or informal contracts -RRB-, or other so-called ` switching costs ' or market barriers -LSB- 12, 5 -RSB-. The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. While the level of satisfaction with a currently-used product will largely be a function of one 's own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 2. BACKGROUND 2.1 Word-of-mouth communication The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. Simulation-based investigations of wordof-mouth -LSB- 6, 13 -RSB- have focused on developing strategies for ensuring that a system reaches an equilibrium level where all agents are satisfied, largely by learning about the effectiveness of others ' referrals or by varying the degree of inertia in individual behaviour. The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in -LSB- 6 -RSB- in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects. 2.2 Market barriers The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. A useful typology of market barriers distinguishes ` transactional ' barriers associated with the monetary cost of changing -LRB- e.g. in financial services -RRB-, ` learning ' barriers associated with deciding to replace well-known existing products, and ` contractual ' barriers imposing legal constraints for the term of the contract -LSB- 12 -RSB-. A different typology -LSB- 5 -RSB- introduces the additional aspect of ` relational ' barriers arising from personal relationships that may be interwoven with the use of a particular product. There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. Burnham et al. -LSB- 5 -RSB- find that perceived market barriers account for nearly twice the variance in intention to stay with a product than that explained by satisfaction with the product -LRB- 30 % and 16 % respectively -RRB-, and that so-called relational barriers are considerably more influential than either transactional or learning barriers. Further, they find that switching costs are perceived by consumers to exist even in markets which are fluid and where barriers would seem to be weak. Simply put, market barriers appear to play a greater role in what people do than satisfaction ; and their presence may be more pervasive than is generally thought. 5. CONCLUSIONS AND RELATED WORK Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components : networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world. It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving. In poorly-connected networks, the effect of barriers on market shares is slight. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share. Products with small market share -LRB- which, in the context of our simulations, is generally due to the product offering poor performance -RRB- are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case. Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two ` modules ' comprising network structure and local behaviour. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions.", "keyphrases": ["referr system", "purchas behaviour", "word-of-mouth commun", "market system", "defect behaviour", "psycholog affin", "switch behaviour", "agent-base model", "social psycholog", "market barrier", "consum choic", "switch cost"]}
{"file_name": "C-34", "text": "Researches on Scheme of Pairwise Key Establishment for Distributed Sensor Networks ABSTRACT Security schemes of pairwise key establishment, which enable sensors to communicate with each other securely, play a fundamental role in research on security issue in wireless sensor networks. A new kind of cluster deployed sensor networks distribution model is presented, and based on which, an innovative Hierarchical Hypercube model - H -LRB- k, u, m, v, n -RRB- and the mapping relationship between cluster deployed sensor networks and the H -LRB- k, u, m, v, n -RRB- are proposed. By utilizing nice properties of H -LRB- k, u, m, v, n -RRB- model, a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed, which combines the idea of KDC -LRB- Key Distribution Center -RRB- and polynomial pool schemes. Furthermore, the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected. Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key, compared with previous related works. 1. INTRODUCTION Security communication is an important requirement in many sensor network applications, so shared secret keys are used between communicating nodes to encrypt data. As one of the most fundamental security services, pairwise key establishment enables the sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the sensor nodes ' limited computational capabilities, battery energy, and available memory, it is not feasible for them to use traditional pairwise key establishment techniques such as public key cryptography and key distribution center -LRB- KDC -RRB-. Several alternative approaches have been developed recently to perform pairwise key establishment on resource-constrained sensor networks without involving the use of traditional cryptography -LSB- 14 -RSB-. Eschenauer and Gligor proposed a basic probabilistic key predistribution scheme for pairwise key establishment -LSB- 1 -RSB-. In the scheme, each sensor node randomly picks a set of keys from a key pool before the deployment so that any two of the sensor nodes have a certain probability to share at least one common key. Chan et al. further extended this idea and presented two key predistribution schemes : a q-composite key pre-distribution scheme and a random pairwise keys scheme. The q-composite scheme requires any two sensors share at least q pre-distributed keys. The random scheme randomly picks pair of sensors and assigns each pair a unique random key -LSB- 2 -RSB-. Based on such a framework, they presented two pairwise key pre-distribution schemes : a random subset assignment scheme and a grid-based scheme. A polynomial pool is used in those schemes, instead of using a key pool in the previous techniques. The random subset assignment scheme assigns each sensor node the secrets generated from a random subset of polynomials in the polynomial pool. The gridbased scheme associates polynomials with the rows and the columns of an artificial grid, assigns each sensor node to a unique coordinate in the grid, and gives the node the secrets generated from the corresponding row and column polynomials. Based on this grid, each sensor node can then identify whether it can directly establish a pairwise key with another node, and if not, what intermediate nodes it can contact to indirectly establish the pairwise key. A similar approach to those schemes described by Liu et al was independently developed by Du et a. -LSB- 5 -RSB-. Rather than on Blundo 's scheme their approach is based on Blom 's scheme -LSB- 6 -RSB-. All of those schemes above improve the security over the basic probabilistic key pre-distribution scheme. However, the pairwise key establishment problem in sensor networks is still not well solved. For the basic probabilistic and the q-composite key predistribution schemes, as the number of compromised nodes increases, the fraction of affected pairwise keys increases quickly. As a result, a small number of compromised nodes may affect a large fraction of pairwise keys -LSB- 3 -RSB-. Though the random pairwise keys scheme doses not suffer from the above security problem, it incurs a high memory overhead, which increases linearly with the number of nodes in the network if the level of security is kept constant -LSB- 2 -RSB- -LSB- 4 -RSB-. For the random subset assignment scheme, it suffers higher communication and computation overheads. In 2004, Liu proposed a new hypercube-based pairwise key predistribution scheme -LSB- 7 -RSB-, which extends the grid-based scheme from a two dimensional grid to a multi-dimensional hypercube. The analysis shows that hypercube-based scheme keeps some attractive properties of the grid-based scheme, including the guarantee of establishing pairwise keys and the resilience to node compromises. Also, when perfect security against node compromise is required, the hypercube-based scheme can support a larger network by adding more dimensions instead of increasing the storage overhead on sensor nodes. Though hypercube-based scheme -LRB- we consider the grid-based scheme is a special case of hypercube-based scheme -RRB- has many attractive properties, it requires any two nodes in sensor networks can communication directly with each other. This strong assumption is impractical in most of the actual applications of the sensor networks. In this paper, we present a kind of new cluster-based distribution model of sensor networks, and for which, we propose a new pairwise key pre-distribution scheme. Based on the topology, we propose a novel cluster distribution based hierarchical hypercube model to establish the pairwise key. We develop a kind of new pairwise key establishment algorithm with our hierarchical hypercube model. The structure of this paper is arranged as follows : In section 3, a new distribution model of cluster deployed sensor networks is presented. In section 4, a new Hierarchical Hypercube model is proposed. In section 5, the mapping relationship between the clusters deployed sensor network and Hierarchical Hypercube model is discussed. In section 6 and section 7, new pairwise key establishment algorithm are designed based on the Hierarchical Hypercube model and detailed analyses are described. Finally, section 8 presents a conclusion. 2. PRELIMINARY Definition 1 -LRB- Key Predistribution -RRB- : The procedure, which is used to encode the corresponding encryption and decryption algorithms in sensor nodes before distribution, is called Key Predistribution. Definition 2 -LRB- Pairwise Key -RRB- : For any two nodes A and B, if they have a common key E, then the key E is called a pairwise key between them. 8. CONCLUSION A new hierarchical hypercube model named H -LRB- k, u, m, v, n -RRB- is proposed, which can be used for pairwise key predistribution for cluster deployed sensor networks. And Based on the H -LRB- k, u, m, v, n -RRB- model, an innovative pairwise key predistribution scheme and algorithm are designed respectively, by combing the good properties of the Polynomial Key and Key Pool encryption schemes. So, the traditional pairwise key predistribution algorithm based on hypercube model -LSB- 7 -RSB- is only a special case of the new algorithm proposed in this paper. Theoretical and experimental analyses show that the newly proposed algorithm is an efficient pairwise key establishment algorithm that is suitable for the cluster deployed sensor networks.", "keyphrases": ["sensor network", "kei pool", "kei predistribut", "hierarch hypercub model", "secur", "pairwis kei establish algorithm", "cluster-base distribut model", "polynomi kei", "encrypt", "node code", "high fault-toler"]}
{"file_name": "H-32", "text": "Interesting Nuggets and Their Impact on Definitional Question Answering ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a `` Human Interest Model '' from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering. 1. DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003. The Definition questions, also called Other questions in recent years, are defined as follows. Given a question topic X, the task of a definitional QA system is akin to answering the question `` What is X? '' . The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic. Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as `` informative nuggets ''. Each informative nugget is a sentence fragment that describe some factual information about the topic. From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets can not simply be described as informative nuggets. Rather, these topic nuggets have a trivia-like quality associated with them. Typically, these are out of the ordinary pieces of information about a topic that can pique a human reader 's interest. For this reason, we decided to define answer nuggets that can evoke human interest as `` interesting nuggets ''. In essence, interesting nuggets answer the questions `` What is X famous for? '' , `` What defines X? '' . We now have two very different perspective as to what constitutes an answer to Definition questions. An answer can be some important factual information about the topic or some novel and interesting aspect about the topic. This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of `` George Foreman ''. Certain answer nuggets are more informative while other nuggets are more interesting in nature. Informative Nuggets - Became oldest world champion in boxing history. Interesting Nuggets - Returned to boxing after 10 yr hiatus. As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets. In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets. A '' Human Interest Model '' definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system. We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2. RELATED WORK There are currently two general methods for Definitional Question Answering. The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. -LSB- 1 -RSB- and Xu et al. -LSB- 14 -RSB-. Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets. For example, Xu et al. used 40 manually defined `` structured patterns '' in their 2003 definitional question answering system. Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created. A recent system by Harabagiu et al. -LSB- 6 -RSB- created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains. As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information. Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences. Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a person 's birthdate, or the name of a company 's CEO. However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations. This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities. For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being. Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets. This leads to the exploration of the second relevance-based approach that has been used in definitional question answering. Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets -LSB- 1 -RSB-. A similar approach has also been used as a baseline system for TREC 2003 -LSB- 14 -RSB-. More recently, Chen et al. -LSB- 3 -RSB- adapted a bi-gram or bi-term language model for definitional Question Answering. Generally, the relevance-based approach requires a `` definitional corpus '' that contain documents highly relevant to the topic. The baseline system in TREC 2003 simply uses the topic words as its definitional corpus. Blair-Goldensohn et al. -LSB- 1 -RSB- uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional. Chen et al. -LSB- 3 -RSB- collect snippets from Google to build its definitional corpus. From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected. This centroid vector or set of centroid words is taken to be highly indicative of the topic. Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic. BlairGoldensohn et al. -LSB- 1 -RSB- uses Cosine similarity to rank sentences by `` centrality ''. As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus. However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences. Thus such methods identify relevant sentences and not sentences containing definitional nuggets. Yet, the TREC 2003 baseline system -LSB- 14 -RSB- outperformed all but one other system. The bi-term language model -LSB- 3 -RSB- is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach. At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin -LSB- 7 -RSB-. We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords. This may explain why relevance-based method can perform competitively in definitional question answering. However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner. Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets. We will describe how we expand upon such methods to identify interesting nuggets in the next section. 7. CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets. Interesting nuggets are uncommon pieces of information about the topic that can evoke a human reader 's curiosity. The notion of an '' average human reader '' is an important consideration in our approach. This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering. Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings. Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems. We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers. What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic. Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic. Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features. As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model. We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers. Although the methods we used are simple, they have been shown experimentally to be effective. Our approach may also provide some insight into a few anomalies in past definitional question answering 's trials. For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets. We suspect the main contributor to the system 's performance Table 3 : TREC 2005 Topics Grouped by Entity Type In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers.", "keyphrases": ["us of linguist", "extern knowledg", "comput of human interest", "new corpu", "question topic", "inform nugget", "sentenc fragment", "human reader", "interest", "interest nugget", "uniqu qualiti", "surpris factor", "lexic pattern", "manual labor", "baselin system"]}
{"file_name": "I-4", "text": "Meta-Level Coordination for Solving Negotiation Chains in Semi-Cooperative Multi-Agent Systems ABSTRACT A negotiation chain is formed when multiple related negotiations are spread over multiple agents. In order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility, we present an extension to a singleagent concurrent negotiation framework. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. We introduce a pre-negotiation phase that allows agents to transfer meta-level information. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context. The agent can also use this information to allocate appropriate time for each negotiation, hence to find a good ordering of all related negotiations. The experimental data shows that these mechanisms improve the agents ' and the system 's overall performance significantly. 1. INTRODUCTION Sophisticated negotiation for task and resource allocation is crucial for the next generation of multi-agent systems -LRB- MAS -RRB- applications. Groups of agents need to efficiently negotiate over multiple related issues concurrently in a complex, distributed setting where there are deadlines by which the negotiations must be completed. This is an important research area where there has been very little work done. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. There is no single global goal in such systems, either because each agent represents a different organization/user, or because it is difficult/impossible to design one single global goal. This issue arises due to multiple concurrent tasks, resource constrains and uncertainties, and thus no agent has sufficient knowledge or computational resources to determine what is best for the whole system -LSB- 11 -RSB-. To accomplish tasks continuously arriving in the virtual organization, cooperation and sub-task relocation are needed and preferred. There is no single global goal since each agent may be involved in multiple virtual organizations. Meanwhile, the performance of each individual agent is tightly related to other agents ' cooperation and the virtual organization 's overall performance. The negotiation in such systems is not a zero-sum game, a deal that increases both agents ' utilities can be found through efficient negotiation. Additionally, there are multiple encounters among agents since new tasks are arriving all the time. In such negotiations, price may or may not be important, since it can be fixed resulting from a long-term contract. Other factors like quality and delivery time are important too. Reputation mechanisms in the system makes cheating not attractive from a long term viewpoint due to multiple encounters among agents. Another major difference between this work and other work on negotiation is that negotiation, here, is not viewed as a stand-alone process. Rather it is one part of the agent 's activity which is tightly interleaved with the planning, scheduling and executing of the agent 's activities, which also may relate to other negotiations. Based on this recognition, this work on negotiation is concerned more about the meta-level decision-making process in negotiation rather than the basic protocols or languages. the negotiations be performed. These macro-strategies are different from those micro-strategies that direct the individual negotiation thread, such as whether the agent should concede and how much the agent should concede, etc -LSB- 3 -RSB-. In this paper we extend a multi-linked negotiation model -LSB- 10 -RSB- from a single-agent perspective to a multi-agent perspective, so that a group of agents involved in chains of interrelated negotiations can find nearly-optimal macro negotiation strategies for pursuing their negotiations. Section 2 describes the basic negotiation process and briefly reviews a single agent 's model of multi-linked negotiation. Section 3 introduces a complex supply-chain scenario. Section 4 details how to solve those problems arising in the negotiation chain. Section 5 reports on the experimental work. Section 6 discusses related work and Section 7 presents conclusions and areas of future work. 2. BACKGROUND ON MULTI-LINKED NEGOTIATION This process can go back and forth until an agreement is reached or the agents decide to stop. If an agreement is reached and one agent can not fulfill the commitment, it needs to pay the other party a decommitment penalty as specified in the commitment. A negotiation starts with a proposal, which announces that a task -LRB- t -RRB- needs to be performed includes the following attributes : 1. deadline -LRB- dl -RRB- : the latest finish time of the task ; the task needs to be finished before the deadline dl. 3. minimum quality requirement -LRB- minq -RRB- : the task needs to be finished with a quality achievement no less than minq. 4. regular reward -LRB- r -RRB- : if the task is finished as the contract requested, the contractor agent will get reward r. 5. early finish reward rate -LRB- e -RRB- : if the contractor agent can finish the task earlier than dl, it will get the extra early finish reward proportional to this rate. 6. The multi-linked negotiation problem has two dimensions : the negotiations, and the subjects of negotiations. The negotiations are interrelated and the subjects are interrelated ; the attributes of negotiations and the attributes of the subjects are interrelated as well. This two-dimensional complexity of interrelationships distinguishes it from the classic project management problem or scheduling problem, where all tasks to be scheduled are local tasks and no negotiation is needed. 1. negotiation duration -LRB- \u03b4 -LRB- v -RRB- -RRB- : the maximum time allowed for negotiation v to complete, either reaching an agreed upon proposal -LRB- success -RRB- or no agreement -LRB- failure -RRB-. 2. negotiation start time -LRB- \u03b1 -LRB- v -RRB- -RRB- : the start time of negotiation v. \u03b1 -LRB- v -RRB- is an attribute that needs to be decided by the agent. 3. negotiation deadline -LRB- e -LRB- v -RRB- -RRB- : negotiation v needs to be finished before this deadline e -LRB- v -RRB-. The negotiation is no longer valid after time e -LRB- v -RRB-, which is the same as a failure outcome of this negotiation. 4. It depends on a set of attributes, including both attributes-in-negotiation -LRB- i.e. reward, flexibility, etc. -RRB- and attributes-of-negotiation -LRB- i.e. negotiation start time, negotiation deadline, etc. -RRB-. An agent involved in multiple related negotiation processes needs to reason on how to manage these negotiations in terms of ordering them and choosing the appropriate values for features. This is the multi-linked negotiation problem -LSB- 10 -RSB- : \u03c1 -LRB- v -RRB- -RRB-, which describes the relationship between negotiation v and its children. The AND relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires all its children nodes have successful accomplishments. The OR relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires at least one child node have successful accomplishment, where the multiple children nodes represent alternatives to accomplish the same goal. Multi-linked negotiation problem is a local optimization problem. To solve a multi-linked negotiation problem is to find a negotiation solution -LRB- 0, \u03d5 -RRB- with optimized expected utility Elf -LRB- 0, \u03d5 -RRB-, which is defined as : A negotiation ordering 0 defines a partial order of all negotiation issues. A feature assignment \u03d5 is a mapping function that assigns a value to each attribute that needs to be decided in the negotiation. A negotiation outcome \u03c7 for a set of negotiations -LCB- vj 1, -LRB- j = 1,..., n -RRB- specifies the result for each negotiation, either success or failure. There are a total of 2n different outcomes for n negotiations : -LCB- chii1, -LRB- i = 1,..., 2n -RRB-. P -LRB- \u03c7i, \u03d5 -RRB- denotes the probability of the outcome \u03c7i given the feature assignment \u03d5, which is calculated based on the success probability of each negotiation. The Sixth Intl.. Joint Conf. Figure 1 : A Complex Negotiation Chain Scenario A heuristic search algorithm -LSB- 10 -RSB- has been developed to solve the single agent 's multi-linked negotiation problem that produces nearly-optimal solutions. This algorithm is used as the core of the decision-making for each individual agent in the negotiation chain scenario. In the rest of the paper, we present our work on how to improve the local solution of a single agent in the global negotiation chain context. 6. RELATED WORK Fatima, Wooldridge and Jennings -LSB- 1 -RSB- studied the multiple issues in negotiation in terms of the agenda and negotiation procedure. However, this work is limited since it only involves a single agent 's perspective without any understanding that the agent may be part of a negotiation chain. Mailler and Lesser -LSB- 4 -RSB- have presented an approach to a distributed resource allocation problem where the negotiation chain scenario occurs. It models the negotiation problem as a distributed constraint optimization problem -LRB- DCOP -RRB- and a cooperative mediation mechanism is used to centralize relevant portions of the DCOP. In our work, the negotiation involves more complicated issues such as reward, penalty and utility ; also, we adopt a distribution approach where no centralized control is needed. A mediator-based partial centralized approach has been applied to the coordination and scheduling of complex task network -LSB- 8 -RSB-, which is different from our work since the system is a complete cooperative system and individual utility of single agent is not concerned at all. A combinatorial auction -LSB- 2, 9 -RSB- could be another approach to solving the negotiation chain problem. However, in a combinatorial auction, the agent does not reason about the ordering of negotiations. This would lead to a problem similar to those we discussed when the same-deadline policy is used. 7. CONCLUSION AND FUTURE WORK In this paper, we have solved negotiation chain problems by extending our multi-linked negotiation model from the perspective of a single agent to multiple agents. Instead of solving the negotiation chain problem in a centralized approach, we adopt a distributed approach where each agent has an extended local model and decisionmaking process. We have introduced a pre-negotiation phase that allows agents to transfer meta-level information on related negotiation issues. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing the appropriate negotiation solution. The experimental data shows that these mechanisms improve the agent 's and the system 's overall performance significantly. In future extension of this work, we would like to develop mechanisms to verify how reliable the agents are.", "keyphrases": ["multipl agent", "negoti framework", "negoti chain", "semi-cooper multi-agent system", "pre-negoti", "multi-link negoti", "agent", "distribut set", "multipl concurr task", "virtual organ", "sub-task reloc", "reput mechan", "complex suppli-chain scenario"]}
{"file_name": "I-14", "text": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems ABSTRACT The dominant existing routing strategies employed in peerto-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions. Specifically, agents maintain estimates on the downstream agents ' abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies. 1. INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems -LSB- 6, 13, 14, 15 -RSB-. In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents. In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches -LSB- 6, 13, 14, 15 -RSB-. While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors. Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms. In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms. Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' capabilities of providing relevant documents for specific types of incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. This process is conducted in an iterative manner. The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents. This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time. Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network -LRB- agent organization -RRB- based on the contentsimilarity measure among agents ' document collections in a bottom-up fashion. In the past work, we have shown that this organization improves search performance significantly. The intention of the reinforcement learning is to adapt the agents ' routing decisions to the dynamic network situations and learn from past search sessions. Specifically, the contributions of this paper include : -LRB- 1 -RRB- a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents ; -LRB- 2 -RRB- two strategies to speed up the learning process. The remainder of this paper is organized as follows : Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology. Section 3 describes a reinforcement learning based approach to direct the routing process ; Section 4 details the experimental settings and analyze the results. Section 5 discusses related studies and Section 6 concludes the paper. 5. RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks. In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process. IP-level Routing problems have been attacked from the reinforcement learning perspective -LSB- 2, 5, 11, 12 -RSB-. There are two major classes of adaptive, distributed packet routing algorithms in the literature : distance-vector algorithms and link-state algorithms. While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks. Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors. A variant of Q-Learning techniques is deployed The Sixth Intl.. Joint Conf. to update the estimations to converge to the real distances. It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies -LSB- 3 -RSB-. The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property. This is because the users ' traffic and query patterns can reduce the state space and speed up the learning process. Related work in taking advantage of this property include -LSB- 7 -RSB-, where the authors attempted to address this problem by user modeling techniques. 6. CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' ability to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents modify their routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time.", "keyphrases": ["peer-to-peer inform retriev system", "reinforc learn", "distribut search algorithm", "rout decis", "util", "network", "learn algorithm", "rout polici", "queri"]}
{"file_name": "C-23", "text": "Implementation of a Dynamic Adjustment Mechanism with Efficient Replica Selection in Data Grid Environments ABSTRACT The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers. Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers. In this paper, we propose a dynamic coallocation scheme, namely Recursive-Adjustment Co-Allocation scheme, to improve the performance of data transfer in Data Grids. Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide an effective scheme for reducing the cost of reassembling data blocks. 1. INTRODUCTION Data Grids aggregate distributed resources for solving large-size dataset management problems. Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment. Certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access. Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity. One way to improve download speeds is to determine the best replica locations using replica selection techniques -LSB- 19 -RSB-. This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet. Another way is to use co-allocation technology -LSB- 17 -RSB- to download data. Co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel. Several co-allocation strategies were provided in previous work -LSB- 17 -RSB-. An idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block. Therefore, it is important to reduce the differences in finishing time among replica servers. In this paper, we propose a dynamic co-allocation scheme based on co-allocation Grid data transfer architecture called RecursiveAdjustment Co-Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance -LSB- 24 -RSB-. Experimental results show that our approach is superior to previous methods and achieved the best overall performance. We also discuss combination cost and provide an effective scheme for reducing it. Related background review and studies are presented in Section 2 and the co-allocation architecture and related work are introduced in Section 3. In Section 4, an efficient replica selection service is proposed by us. Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6. Section 7 concludes this research paper. 2. BACKGROUND 2.1 Data Grid Data Grids -LSB- 1, 2, 16 -RSB- federate a lot of storage resources. Large collections of measured or computed data are emerging as important resources in many data intensive applications. 2.1.1 Replica Management Replica management involves creating or removing replicas at a data grid site -LSB- 19 -RSB-. In other words, the role of a replica manager is to create or delete replicas, within specified storage systems. Most often, these replicas are exact copies of the original files, created only to harness certain performance benefits. A replica manager typically maintains a replica catalog containing replica site addresses and the file instances. The replica management service is responsible for managing the replication of complete and partial copies of datasets, defined as collections of files. The replica management service is just one component in a Data Grid environment that provides support for high-performance, data-intensive applications. A replica or location is a subset of a collection that is stored on a particular physical storage system. There may be multiple possibly overlapping subsets of a collection stored on multiple storage systems in a Data Grid. These Grid storage systems may use a variety of underlying storage technologies and data movement protocols, which are independent of replica management. 2.1.2 Replica Catalog As mentioned above, the purpose of the replica catalog is to provide mappings between logical names for files or collections and one or more copies of the objects on physical storage systems. The replica catalog includes optional entries that describe individual logical files. Logical files are entities with globally unique names that may have one or more physical instances. The catalog may optionally contain one logical file entry in the replica catalog for each logical file in a collection. A Data Grid may contain multiple replica catalogs. For example, a community of researchers interested in a particular research topic might maintain a replica catalog for a collection of data sets of mutual interest. It is possible to create hierarchies of replica catalogs to impose a directory-like structure on related logical collections. In addition, the replica manager can perform access control on entire catalogs as well as on individual logical files. 2.1.3 Replica Selection The purpose of replica selection -LSB- 16 -RSB- is to select a replica from among the sites which constitute a Data Grid -LSB- 19 -RSB-. The criteria of selection depend on characteristics of the application. By using this mechanism, users of the Data Grid can easily manage replicas of data sets at their sites, with better performance. Much previous effort has been devoted to the replica selection problem. The common process of replica selection consists of three steps : data preparation, preprocessing and prediction. Then, applications can select a replica according to its specific attributes. Replica selection is important to data-intensive applications, and it can provide location transparency. When a user requests for accessing a data set, the system determines an appropriate way to deliver the replica to the user. 2.2 Globus Toolkit and GridFTP The Globus Project -LSB- 9, 11, 16 -RSB- provides software tools collectively called The Globus Toolkit that makes it easier to build computational Grids and Grid-based applications. Many organizations use the Globus Toolkit to build computational Grids to support their applications. The composition of the Globus Toolkit can be pictured as three pillars : Resource Management, Information Services, and Data Management. GRAM implements a resource management protocol, MDS implements an information services protocol, and GridFTP implements a data transfer protocol. The Globus alliance proposed a common data transfer and access protocol called GridFTP that provides secure, efficient data movement in Grid environments -LSB- 3 -RSB-. This protocol, which extends the standard FTP protocol, provides a superset of the features offered by the various Grid storage systems currently in use. In order to solve the appearing problems, the Data Grid community tries to develop a secure, efficient data transport mechanism and replica management services. GridFTP is a reliable, secure and efficient data transport protocol which is developed as a part of the Globus project. There is another key technology from Globus project, called replica catalog -LSB- 16 -RSB- which is used to register and manage complete and partial copies of data sets. The replica catalog contains the mapping information from a logical file or collection to one or more physical files. 2.3 Network Weather Service The Network Weather Service -LRB- NWS -RRB- -LSB- 22 -RSB- is a generalized and distributed monitoring system for producing short-term performance forecasts based on historical performance measurements. The goal of the system is to dynamically characterize and forecast the performance deliverable at the application level from a set of network and computational resources. 2.4 Sysstat Utilities The Sysstat -LSB- 15 -RSB- utilities are a collection of performance monitoring tools for the Linux OS. The Sysstat package incorporates the sar, mpstat, and iostat commands. The sar command collects and reports system activity information, which can also be saved in a system activity file for future inspection. The iostat command reports CPU statistics and I/O statistics for tty devices and disks. 7. CONCLUSIONS The co-allocation architecture provides a coordinated agent for assigning data blocks. A previous work showed that the dynamic co-allocation scheme leads to performance improvements. However, it can not handle the idle time of faster servers, which must wait for the slowest server to deliver its final block. We proposed the Recursive-Adjustment Co-Allocation scheme to improve data transfer performances using the co-allocation architecture in -LSB- 17 -RSB-. In this approach, the workloads of selected replica servers are continuously adjusted during data transfers, and we provide a function that enables users to define a final block threshold, according to their data grid environment. Experimental results show the effectiveness of our proposed technique in improving transfer time and reducing overall idle time spent waiting for the slowest server. We also discussed the re-combination cost and provided an effective scheme for reducing it.", "keyphrases": ["distribut resourc", "data grid applic", "replic", "co-alloc", "larg dataset", "resourc manag protocol", "replica", "co-alloc strategi", "server", "perform"]}
{"file_name": "J-28", "text": "Approximately-Strategyproof and Tractable Multi-Unit Auctions ABSTRACT We present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem. The bidding language in our auctions allows marginal-decreasing piecewise constant curves. First, we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem, which computes a -LRB- 1 + e -RRB- approximation in worst-case time T = O -LRB- n3/e -RRB-, given n bids each with a constant number of pieces. Second, we embed this approximation scheme within a Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism and compute payments to n agents for an asymptotic cost of O -LRB- T log n -RRB-. The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by e / -LRB- 1 + e -RRB- V, where V is the total surplus in the efficient outcome. 1. INTRODUCTION In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce ; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services -LRB- e.g. circuit boards, power suppliers, toner cartridges -RRB-, with flexible-sized lots. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. We consider the computational complexity of implementing the Vickrey-Clarke-Groves -LSB- 22, 5, 11 -RSB- mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer can not be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design -LSB- 17 -RSB-. The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to -LRB- weakly -RRB- intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the 1However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a -LRB- 1 + ~ -RRB- - approximation in worst-case time T = O -LRB- n3 / \u03b5 -RRB-, where each bid has a fixed number of piecewise constant pieces. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O -LRB- nT -RRB-. This upper-bound tends to 1 as the number of sellers increases. The approximate VCG mechanism is -LRB- \u03b5 1 + \u03b5 -RRB- - strategyproof for an approximation to within -LRB- 1 + ~ -RRB- of the optimal allocation. This means that a bidder can gain at most -LRB- \u03b5 1 + \u03b5 -RRB- V from a nontruthful bid, where V is the total surplus from the efficient allocation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about \u03b5-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme. Section 4 defines the approximation scheme for the payments in the VCG mechanism. Section 5 concludes. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem -LRB- CAP -RRB- is both NP-complete and inapproximable -LRB- e.g. -LSB- 6 -RSB- -RRB-. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language -- the bid taker in our setting is allowed to accept at most one point on the bid curve. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. For instance, Lehmann et al. -LSB- 15 -RSB- propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Feigenminimum-lot size constraints from the buyers. baum & Shenker -LSB- 8 -RSB- have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Eso et al. -LSB- 7 -RSB- have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. Kalagnanam et al. -LSB- 12 -RSB- address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different : it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel -LSB- 1 -RSB- has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values -LSB- 1 -RSB-, with an interpretation as a primal-dual algorithm -LSB- 2 -RSB-. 5. CONCLUSIONS We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language. Our scheme is both approximately efficient and approximately strategyproof within any specified factor \u03b5 > 0. As such it is an example of computationally tractable \u03b5-dominance result, as well as an example of a non-trivial but approximable allocation problem. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O -LRB- T log n -RRB-, where T is the time complexity to compute the solution to a single allocation problem.", "keyphrases": ["approxim-effici and approximatelystrategyproof auction mechan", "singl-good multi-unit alloc problem", "fulli polynomi-time approxim scheme", "vickrei-clark-grove", "forward auction", "revers auction", "equilibrium", "margin-decreas piecewis constant curv", "bid languag", "dynam program"]}
{"file_name": "C-40", "text": "Edge Indexing in a Grid for Highly Dynamic Virtual Environments \u2217 ABSTRACT Newly emerging game -- based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real -- time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client -- server based grid subspace division where the virtual worlds are partitioned into manageable sub -- worlds. In each sub -- world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user 's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real -- time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments. The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real -- time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next -- generation virtual frameworks that may merge into existing web -- based services in the near future. \u2217 This research has been funded in part by NSF grants EEC9529152 -LRB- IMSC ERC -RRB- and IIS-0534761, and equipment gifts from Intel Corporation, Hewlett-Packard, Sun Microsystems and Raptor Networks Technology. Categories and Subject Descriptors : C. 2.4 -LSB- Computer -- Com 1. INTRODUCTION Recently, Massively Multiplayer Online Games -LRB- MMOGs -RRB- have been studied as a framework for next -- generation virtual environments. In this paper, we mainly focus on the first two requirements. Dynamic extensibility allows regular game -- users to deploy their own created content. This is a powerful concept, but unfortunately, user -- created content tends to create imbalances among the existing scene complexity, causing system -- wide performance problems. Another important requirement is scalability. By carefully partitioning the world into multiple sub -- worlds or replicating worlds at geographically dispersed locations, massive numbers of concurrent users can be supported. Second Life -LSB- 4 -RSB- is the first successfully deployed MMOG system that meets both requirements. But we acknowledge that these requirements are also valid for new virtual environments. Figure 1 : Object popping occurred as a user moves forward -LRB- screenshots from Second Life -RRB- where \u0394 = 2 seconds. employs a client/server based 3D object streaming model -LSB- 5 -RSB-. In this model, a server continuously transmits both update events and geometry data to every connected user. As a result, this extensible gaming environment has accelerated the deployment of user -- created content and provides users with unlimited freedom to pursue a navigational experience in its space. One of the main operations in MMOG applications that stream 3D objects is to accurately calculate all objects that are visible to a user. The traditional visibility determination approach, however, has an object popping problem. For example, a house outside a user 's visible range is not drawn at time t, illustrated in Figure 1 -LRB- a -RRB-. As the user moves forward, the house will suddenly appear at time -LRB- t + \u0394 -RRB- as shown in Figure 1 -LRB- b -RRB-. The visibility calculation for each user not only needs to be accurate, but also fast. This challenge is illustrated by the fact that the maximum number of concurrent users per server of Second Life is still an order of magnitude smaller than for stationary worlds. To address these challenges, we propose a method that identifies the most relevant visible objects from a given geometry database -LRB- view model -RRB- and then put forth a fast indexing method that computes the visible objects for each user -LRB- spatial indexing -RRB-. Our two novel methods represent the main contributions of this work. Section 2 presents related work. Section 3 describes our new view method. In Section 4, we present assumptions on our target application and introduce a new spatial indexing method designed to support real -- time visibility computations. We also discuss its optimization issues. Section 5 reports on the quantitative analysis and Section 6 presents preliminary results of our simulation based experiments. Finally, we conclude and address future research directions in Section 7. 2. RELATED WORK Visibility determination has been widely explored in the field of 3D graphics. Various local rendering algorithms have been proposed to eliminate unnecessary objects before rendering or at any stage in the rendering pipeline. However, these algorithms assume that all the candidate visible objects have been stored locally. If the target objects are stored on remote servers, the clients receive the geometry items that are necessary for rendering from the server databases. However, these online optimization algorithms fail to address performance issue at the server in highly crowded environments. On the other hand, our visibility computation model, a representative of this category, is based on different assumptions on the data representation of virtual entities. In the graphics area, there has been little work on supporting real -- time visibility computations for a massive number of moving objects and users. Here we recognize that such graphics related issues have a very close similarity to spatial database problems. Recently, a number of publications have addressed the scalability issue on how to support massive numbers of objects and queries in highly dynamic environments. To support frequent updates, two partitioning policies have been studied in depth : -LRB- 1 -RRB- R-tree based spatial indexing, and -LRB- 2 -RRB- grid -- based spatial indexing. The grid -- based partitioning model is a special case of fixed partitioning. Recently, it has been re -- discovered since it can be efficient in highly dynamic environments. Q-Index -LSB- 13, 11 -RSB- is one of the earlier work that re -- discovers the usefulness of grid -- based space partitioning for emerging moving object environments. In contrast to traditional spatial indexing methods that construct an index on the moving objects, it builds an index on the continuous range queries, assuming that the queries move infrequently while the objects move freely. The basic idea of the Q+R tree -LSB- 14 -RSB- is to separate indexing structures for quasi -- stationary objects and moving objects : fast -- moving objects are indexed in a Quadtree and quasi -- stationary objects are stored in an R \u2217 - tree. SINA -LSB- 10 -RSB- was proposed to provide efficient query evaluations for any combination of stationary/moving objects and stationary/moving queries. Specifically, this approach only detects newly discovered -LRB- positive -RRB- or no longer relevant -LRB- negative -RRB- object updates efficiently. Unlike other spatial indexing methods that focus on reducing the query evaluation cost, Hu et al. -LSB- 12 -RSB- proposed a general framework that minimizes the communication cost for location updates by maintaining a rectangular area called a safe region around moving objects. As long as any object resides in this region, all the query results are guaranteed to be valid in the system. If objects move out of their region, location update requests should be delivered to the database server and the affected queries are re -- evaluated on the fly. Our indexing method is very similar to the above approaches. The major difference is that we are more concentrating on real -- time visibility determination while others assume loose timing constraints. 6. EVALUATION This section presents two simulation setups and their performance results. Section 6.1 examines whether our new view approach is superior to existing view models, in spite of its higher indexing complexity. Section 6.2 discusses the degree of practicality and scalability of our indexing method that is designed for our new view model. 6.1 Justification of Object-initiated View Model 6.1.1 Evaluation Metrics P is the ratio of relevant, retrieved items to all retrieved items. A lower value of P implies that the query result set contains a large number of unnecessary objects that do not have to be delivered to a client. A higher P value means a higher network traffic load than required. R is the ratio of relevant, retrieved items to all relevant items. A lower R value means that more objects that should be recognized are ignored. From the R measure, we can quantitatively estimate the occurrence of object popping. In addition to the P and R metrics, we use a standardized single -- valued query evaluation metric that combines P and R, called E -- measure -LSB- 15 -RSB-. The E -- measure is defined as : If \u03b2 is less than 1, P becomes more important. Otherwise, R will affect the E -- measure significantly. A lower E -- measure value implies that the tested view model has a higher quality. The best E -- measure value is zero, where the best values for P and R are both ones. 6.1.2 Simulation Setup We tested four query processing schemes, which use either a user -- initiated or an object -- initiated view model : \u2022 User-initiated visibility computation -- RQ -- OP : Region Query -- Object Point \u2022 Object-oriented visibility computation -- PQ-OR : Point Query -- Object Region -- RQ-OR : Region Query -- Object Region -- ACQ-OR : Approximate Cell Query -- Object Region RQ -- OP is the typical computation scheme that collects all objects whose location is inside a user defined AOI. PQ -- OR collects a set of objects whose AOI intersects with a given user point, formally -LCB- o | q.P \u2208 o.R -RCB-. RQ -- OR, an imaginary computation scheme, is the combination of RQ -- OP and PQ -- OR where the AOI of an object intersects with that of a user, -LCB- o | o.R \u2229 q.R = ~ \u2205 -RCB-. Lastly, ACQ -- OR, an approximate visibility computation model, is a special scheme designed for grid -- based space partitioning, which is our choice of cell evaluation methodology for edge indexing. If a virtual space is partitioned into tiled cells and a user point belongs to one of the cells, the ACQ -- OR searches the objects whose AOI Table 5 : P and R computations of different visibility determination schemes. Table 6 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a slowly moving environment would intersect with the region of the corresponding grid cell. It identifies any object o satisfying the condition c.R n o.R _ ~ 0 where the cell c satisfies q.P E c.R as well. Our simulation program populated 100K object entities and 10K user entities in a 2D unit space, -LSB- 0, 1 -RRB- x -LSB- 0, 1 -RRB-. The populated entities are uniformly located in the unit space. The program performs intersection tests between all user and all object entities exhaustively and computes the P, R, and E -- measure values -LRB- shown in Table 5 -RRB-. 6.1.3 Experimental Results Distribution of P and R measure : Figure 7 shows the distribution of P and R for RQ -- OP. We can observe that P and R are roughly inversely proportional to each other when varying a user AOI range. A smaller side length leads to higher accuracy but lower comprehensiveness. For example, 5 % of the side length of a user AOI detects all objects whose side length of the AOI is at least 5 %. Thus, every object retrieved by RQ -- OP is guaranteed to be all rendered at the client. But RQ -- OP can not detect the objects outside the AOI of the user, thus suffering from too many missing objects that should be rendered. Similarly, the user whose AOI is wider than any other AOI can not miss any objects that should be rendered, but detects too many unnecessary objects. To remove any object popping problem, the side length of any AOI should be greater than or equal to the maximum visible distance of any object in the system, which may incur significant system degradation. E-measure Distribution : Figure 8 reveals two trends. First, the precision values of RQ -- OP lie in between those of ACQ -- OR -LRB- 100 x 100 grid -RRB- and RQ -- OR. Second, the tendency curve of the Precision -- to -- E -- measure plot of RQ -- OR shows resemblance to that of ACQ -- OR. Effect of Different Grid Size : Figure 9 shows the statistical difference of E -- measure values of seven different grid partitioning schemes -LRB- using ACQ -- OR -RRB- and one RQ -- OP model. We use a box -- and -- whisker plot to show both median values and the variances of E-measure distributions and the outliers of each scheme. We also draw the median value of the RQ -- OP E -- measures -LRB- green line -RRB- for comparison purposes. While the ACQ -- OR schemes have some outliers, their E-measure values are heavily concentrated around the median values, thus, they are less sensitive to object AOI. As expected, fine-grained grid partitioning showed a smaller E-measure value. The RQ -- OP scheme showed a wider variance of its quality than other schemes, which is largely attributable to different user side lengths. As the R measure becomes more important, the query quality of ACQ -- OR is improved more evidently than that of RQ -- OP. From Figure 9, the 20x20 grid scheme had a better E-measure Table 7 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a highly dynamic environment -LRB- value in a prioritized environment than in an equal-prioritized environment. As a result, we can roughly anticipate that at least the 20x20 grid cell partitioning retrieves a higher quality of visible sets than the RQ -- OP. 6.2 Evaluation of Edge Indexing In this section, we present the preliminary results of the simulations that examine the applicability of our edge indexing implementation. To estimate the degree of real -- time support of our indexing method, we used the total elapsed time of updating all moving entities and computing visible sets for every cell. We also experimented with different grid partitioning policies and compared them with exhaustive search solutions. 6.2.1 Simulation Setup We implemented edge indexing algorithms in C and ran the experiments on a 64-bit 900MHz Itanium processor with 8 GBs of memory. We implemented a generalized hash table mechanism to store node and edge structures. 6.2.2 Experimental Results Periodic Monitoring Cost : Tables 6 and 7 show the performance numbers of different edge indexing methods by varying v. The moving speed of entities was also uniformly assigned between 0 and v. However, the two -- table method showed a slightly higher evaluation time than the two single -- table methods because of its sequential token removal. Table 7 exemplified the elapsed time of index updates and cell evaluations in a highly dynamic environment where slowly moving and dynamically moving objects co -- exist. Compared with the results shown in Table 6, the two -- table approach produced similar performance numbers regardless of the underlying moving environments. However, the performance gain obtained by the incremental policy of the single -- table is decreased compared with that in the slowly moving environment. Effect of Different Grid Size : How many object updates and cell evaluations can be supported in a given time period is an important performance metric to quantify system throughput. In this section, we evaluate the performance results of three different visibility computation models : two computation -- driven exhaustive search methods ; and one two -- table edge indexing method with different grid sizes. Figure 7 : Distribution of P and R measured by RQ -- OP. Figure 8 : E -- measure value as a function of Figure 9 : E -- measure value as a function of ACQ -- QR grid partitioning scheme when Figure 10 : Total elapsed time of different indexing schemes. Exhaustive search methods do not maintain any intermediate results. They simply compute whether a given user point is inside a given object AOI. They can tolerate unpredictable behavior of object movement. Figure 10 reveals the performance difference between the exhaustive solutions and the two -- table methods, a difference of up to two orders of magnitude. As shown in Section 5, the total elapsed time of object updates and cell evaluations is linear with respect to the average side length of object AOI. Because the side length is represented by cell units, an increase in the number of cells increases the side lengths proportionally. Figure 10 illustrates that the measured simulation results roughly match the expected performance gain computed from the analysis. 7. CONCLUSION AND FUTURE WORK To support dynamic extensibility and scalability in highly dynamic environments, we proposed a new view paradigm, the object-initiated view model, and its efficient indexing method, edge indexing. Compared with the traditional view model, our new view model promises to eliminate any object popping problem that can easily be observed in existing virtual environments at the expense of increased indexing complexity. Our edge indexing model, however, can overcome such higher indexing complexity by indexing spatial extensions at edge -- level not at node -- level in a grid partitioned sub -- world and was validated through quantitative analyses and simulations. However, for now our edge indexing still retains a higher complexity, even in a two -- dimensional domain. Currently, we are developing another edge indexing method to make the indexing complexity constant. Once indexing complexity becomes constant, we plan to index 3D spatial extensions and multi -- resolutional geometry data. We expect that our edge indexing can contribute to successful deployment of next -- generation gaming environments.", "keyphrases": ["edg index", "dynam virtual environ", "game-base applic", "mutabl virtual content", "spatial databas", "spatial index method", "real-time visibl test", "object-initi view model", "object pop", "3d spatial extens"]}
{"file_name": "C-86", "text": "Addressing Strategic Behavior in a Deployed Microeconomic Resource Allocator ABSTRACT While market-based systems have long been proposed as solutions for distributed resource allocation, few have been deployed for production use in real computer systems. Towards this end, we present our initial experience using Mirage, a microeconomic resource allocation system based on a repeated combinatorial auction. Mirage allocates time on a heavily-used 148-node wireless sensor network testbed. In particular, we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects. Based on these results, we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. Finally, we propose refinements to the system 's current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction. 1. INTRODUCTION Market-based systems have long been proposed as solutions for resource allocation in distributed systems including computational Grids -LSB- 2, 20 -RSB-, wide-area network testbeds -LSB- 9 -RSB-, and peer-to-peer systems -LSB- 17 -RSB-. Yet, while the theoretical underpinnings of market-based schemes have made significant strides in recent years, practical integration of market-based mechanisms into real computer systems and empirical observations of such systems under real workloads has remained an elusive goal. Towards this end, we have designed, implemented, and deployed a microeconomic resource allocation system called Mirage -LSB- 3 -RSB- for scheduling testbed time on a 148-node wireless sensor network -LRB- SensorNet -RRB- testbed at Intel Research. The system, which employs a repeated combinatorial auction -LSB- 5, 14 -RSB- to schedule allocations, has been in production use for over four months and has scheduled over 312,148 node hours across 11 research projects to date. In designing and deploying Mirage, we had three primary goals. First, we wanted to validate whether a market-based resource allocation scheme was necessary at all. An economic problem only exists when resources are scarce. Therefore, a key goal was to first measure both resource contention and the range of underlying valuations users place on the resources during periods of resource scarcity. Second, we wanted to observe how users would actually behave in a market-based environment. With Mirage, we wanted to observe to what extent rationality held and in what ways users would attempt to strategize and game the system. Finally, we wanted to identify what other practical problems would emerge in a deployment of a market based system. In this paper, we report briefly on our first goal while focusing primarily on the second. In deploying Mirage, we made the early decision to base the system on a repeated combinatorial auction known not to be strategyproof. That is, self-interested users could attempt to increase their personal gain, at the expense of others, by not revealing their true value to the system. We made this decision mainly because designing a strategyproof mechanism remains an open, challenging problem and we wanted to deploy a working system and gain experience with real users to address our three goals in a timely manner. Deploying a non-strategyproof mechanism also had the benefit of testing rationality and seeing how and to what extent users would try to game the system. The key contribution of this paper is an analysis of such strategic behavior as observed over a four-month time period and proposed refinements for mitigating such behavior en route to building an approximately strategyproof repeated combinatorial auction. The rest of this paper is organized as follows. In Section 2, we present an overview of Mirage including high-level observations on usage over a four-month period. In Section 3, we examine strategic user behavior, focusing on the four primary types of strategies employed by users in the system. Based on these results, Section 4 presents a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. As a first step in addressing some of these challenges, we describe refinements to Mirage 's current auction scheme that mitigate the strategies observed to date and also comment on some initial steps towards building an approximately strategyproof repeated combinatorial auction for Mirage. Finally, in Section 5, we conclude the paper. 5. CONCLUSION Despite initially using a repeated combinatorial auction known not to be strategyproof, Mirage has shown significant promise as a vehicle for SensorNet testbed allocation. Fully realizing these gains, however, requires addressing key problems in strategyproof mechanism design and combinatorial optimization. The temporal nature of computational resources and the combinatorial resource demands of distributed applications adds an additional layer of complexity.", "keyphrases": ["resourc alloc system", "combinatori auction", "market-base system", "distribut system", "strateg behavior", "ration", "auction-base scheme", "mirag system", "sensornet testb", "node-hour price", "usabl overhead", "batch schedul", "distribut applic"]}
