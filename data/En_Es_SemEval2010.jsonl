{"file_name": "C-38", "text": "Un marco para dise\u00f1ar superposiciones dirigidas por receptor de igual a igual RESUMEN Este art\u00edculo presenta un marco simple y escalable para dise\u00f1ar superposiciones de igual a igual llamado Superposici\u00f3n impulsada por receptor de igual a igual -LRB- o PRO -RRB-. PRO est\u00e1 dise\u00f1ado para aplicaciones de streaming no interactivas y su principal objetivo de dise\u00f1o es maximizar el ancho de banda entregado -LRB- y, por lo tanto, la calidad entregada -RRB- a pares con ancho de banda heterog\u00e9neo y asim\u00e9trico. Para lograr este objetivo, PRO adopta un enfoque impulsado por el receptor donde cada receptor -LRB- o par participante -RRB- -LRB- i -RRB- descubre de forma independiente a otros pares en la superposici\u00f3n a trav\u00e9s de chismes, y -LRB- ii -RRB- ego\u00edstamente. determina el mejor subconjunto de pares principales a trav\u00e9s del cual conectarse a la superposici\u00f3n para maximizar su propio ancho de banda entregado. Los pares participantes forman una superposici\u00f3n no estructurada que es inherentemente resistente a una alta tasa de abandono. Adem\u00e1s, cada receptor aprovecha el ancho de banda controlado por congesti\u00f3n de sus padres como se\u00f1al impl\u00edcita para detectar y reaccionar ante cambios a largo plazo en la red o en la condici\u00f3n de superposici\u00f3n sin ninguna coordinaci\u00f3n expl\u00edcita con otros pares participantes. La selecci\u00f3n de padres independiente por parte de pares individuales converge din\u00e1micamente hacia una estructura superpuesta eficiente. 1. INTRODUCCI\u00d3N heterogeneidad y asimetr\u00eda de la conectividad del ancho de banda entre los pares participantes -LSB- 19 -RSB-. Hacer frente a las variaciones del ancho de banda, la heterogeneidad y la asimetr\u00eda es particularmente importante en el dise\u00f1o de superposici\u00f3n entre pares para aplicaciones de streaming porque la calidad entregada a cada par est\u00e1 directamente determinada por su conectividad de ancho de banda con -LRB- otro par -LRB- s -RRB- en -RRB- la superposici\u00f3n. Este art\u00edculo presenta un marco simple para dise\u00f1ar una superposici\u00f3n dirigida por receptor punto a punto, llamada PRO. La principal filosof\u00eda de dise\u00f1o en PRO es que a cada par se le debe permitir determinar de forma independiente y ego\u00edsta la mejor manera de conectarse a la superposici\u00f3n para maximizar la calidad de su propia entrega. Con este fin, cada par puede conectarse a la topolog\u00eda de superposici\u00f3n en m\u00faltiples puntos -LRB-, es decir, recibir contenido a trav\u00e9s de m\u00faltiples pares padres -RRB-. Por lo tanto, los pares participantes forman una superposici\u00f3n no estructurada que puede hacer frente con gracia a una alta tasa de abandono -LSB- 5 -RSB-. Adem\u00e1s, tener varios padres pares se adapta a la heterogeneidad y asimetr\u00eda del ancho de banda y al mismo tiempo mejora la resiliencia frente a la din\u00e1mica de participaci\u00f3n de los pares. PRO consta de dos componentes clave: -LRB- i -RRB- Descubrimiento de pares basado en chismes: cada par intercambia peri\u00f3dicamente mensajes -LRB- es decir, chismes -RRB- con otros pares conocidos para aprender progresivamente sobre un subconjunto de pares participantes en la superposici\u00f3n. que probablemente sean buenos padres. -LRB- ii -RRB- Selecci\u00f3n de padres impulsada por el receptor: dada la informaci\u00f3n recopilada sobre otros pares participantes mediante un mecanismo de chismes,cada par -LRB- o receptor -RRB- mejora gradualmente su propia calidad entregada seleccionando din\u00e1micamente un subconjunto adecuado de pares padres que colectivamente maximizan el ancho de banda proporcionado al receptor. Dado que el ancho de banda disponible de diferentes pares participantes a un receptor -LRB- y la posible correlaci\u00f3n entre ellos -RRB- se pueden medir s\u00f3lo en ese receptor, un enfoque impulsado por el receptor es la soluci\u00f3n natural para maximizar el ancho de banda disponible para pares heterog\u00e9neos. Adem\u00e1s, el ancho de banda disponible de los pares principales sirve como una se\u00f1al impl\u00edcita para que un receptor detecte y reaccione a cambios en la red o condici\u00f3n de superposici\u00f3n sin ninguna coordinaci\u00f3n expl\u00edcita con otros pares participantes. La selecci\u00f3n independiente de padres por parte de pares individuales conduce a una superposici\u00f3n eficiente que maximiza la calidad entregada a cada par. PRO incorpora varias funciones de amortiguaci\u00f3n para garantizar la estabilidad de la superposici\u00f3n a pesar de acciones descoordinadas por parte de diferentes pares. PRO es parte de una arquitectura m\u00e1s amplia que hemos desarrollado para la transmisi\u00f3n de igual a igual. Por lo tanto, PRO y PALS est\u00e1n impulsados \u200b\u200bpor el receptor pero se complementan entre s\u00ed. M\u00e1s espec\u00edficamente, PRO determina un subconjunto adecuado de pares principales que colectivamente maximizan el ancho de banda entregado a cada receptor, mientras que PALS coordina la transmisi\u00f3n \"en el tiempo\" de diferentes segmentos de contenido multimedia de estos padres a pesar de las variaciones impredecibles en su ancho de banda disponible. Esta divisi\u00f3n de funcionalidad proporciona una gran flexibilidad porque desacopla la construcci\u00f3n superpuesta del mecanismo de entrega. En este art\u00edculo, nos centramos principalmente en el mecanismo de construcci\u00f3n de superposici\u00f3n, o PRO. El resto de este art\u00edculo est\u00e1 organizado de la siguiente manera: en la Secci\u00f3n 2, revisamos el problema de la construcci\u00f3n de superposiciones para la transmisi\u00f3n entre pares e identificamos sus dos componentes clave y exploramos su espacio de dise\u00f1o. Presentamos nuestro marco propuesto en la Secci\u00f3n 3. En las Secciones 4 y 5, los componentes clave de nuestro marco se describen con m\u00e1s detalle. Finalmente, la Secci\u00f3n 6 concluye el documento y presenta nuestros planes futuros. 6. CONCLUSIONES Y TRABAJO FUTURO En este art\u00edculo, presentamos un marco simple impulsado por un receptor para dise\u00f1ar estructuras superpuestas peer-to-pee llamado PRO. PRO permite que cada par determine de manera ego\u00edsta e independiente la mejor manera de conectarse a la superposici\u00f3n para maximizar su rendimiento. Por lo tanto, PRO deber\u00eda poder maximizar la calidad entregada a pares con conectividad de ancho de banda heterog\u00e9nea y asim\u00e9trica. Tanto el descubrimiento como la selecci\u00f3n de pares en este marco son escalables. Adem\u00e1s, PRO utiliza un ancho de banda controlado por congesti\u00f3n como una se\u00f1al impl\u00edcita para detectar cuellos de botella compartidos entre los padres existentes, as\u00ed como cambios en la red o condiciones de superposici\u00f3n para remodelar adecuadamente la estructura. Describimos el marco b\u00e1sico y sus componentes clave, y esbozamos nuestras soluciones de prueba. Este es un punto de partida para nuestro trabajo en PRO. Actualmente estamos evaluando varios aspectos de este marco mediante simulaci\u00f3n,y explorar el espacio de dise\u00f1o de componentes clave. Tambi\u00e9n estamos creando un prototipo de este marco para realizar experimentos del mundo real en Planet-Lab en un futuro pr\u00f3ximo.", "keyphrases": ["flujo de igual a igual", "control de congesti\u00f3n", "enfoque basado en la recepci\u00f3n", "superposici\u00f3n impulsada por recepci\u00f3n", "sistema de distribuci\u00f3n", "dise\u00f1o", "medir", "estructura superpuesta eficiente", "Pro", "subconjunto adecuado de pares padres", "descubrimiento de pares basado en chismes", "selecci\u00f3n de padres basada en recepci\u00f3n"]}
{"file_name": "H-17", "text": "Pol\u00edticas de poda para \u00edndices invertidos de dos niveles con garant\u00eda de correcci\u00f3n RESUMEN Los motores de b\u00fasqueda web mantienen \u00edndices invertidos a gran escala que son consultados miles de veces por segundo por usuarios ansiosos de informaci\u00f3n. Para hacer frente a las grandes cantidades de cargas de consultas, los motores de b\u00fasqueda podan su \u00edndice para mantener los documentos que probablemente se devolver\u00e1n como resultados principales y utilizan este \u00edndice podado para calcular los primeros lotes de resultados. Si bien este enfoque puede mejorar el rendimiento al reducir el tama\u00f1o del \u00edndice, si calculamos los resultados principales solo a partir del \u00edndice eliminado podemos notar una degradaci\u00f3n significativa en la calidad del resultado: si un documento deber\u00eda estar entre los resultados principales pero no se incluy\u00f3 en el \u00edndice podado, se colocar\u00e1 detr\u00e1s de los resultados calculados a partir del \u00edndice podado. Dada la feroz competencia en el mercado de las b\u00fasquedas online, este fen\u00f3meno es claramente indeseable. En este art\u00edculo, estudiamos c\u00f3mo podemos evitar cualquier degradaci\u00f3n de la calidad de los resultados debido a la optimizaci\u00f3n del rendimiento basada en la poda, sin dejar de aprovechar la mayor parte de sus beneficios. Nuestra contribuci\u00f3n es una serie de modificaciones en las t\u00e9cnicas de poda para crear el \u00edndice podado y un nuevo algoritmo de c\u00e1lculo de resultados que garantiza que las p\u00e1ginas con mejores coincidencias siempre se coloquen en los primeros resultados de b\u00fasqueda, aunque estemos calculando el primer lote de las p\u00e1ginas podadas. \u00edndice la mayor parte del tiempo. Tambi\u00e9n mostramos c\u00f3mo determinar el tama\u00f1o \u00f3ptimo de un \u00edndice podado y evaluamos experimentalmente nuestros algoritmos en una colecci\u00f3n de 130 millones de p\u00e1ginas web. 1. INTRODUCCI\u00d3N Seg\u00fan un estudio reciente -LSB- 13 -RSB-, se estima que el \u2217 Trabajo realizado mientras el autor estaba en el Departamento de Inform\u00e1tica de UCLA. \u2020 Este trabajo est\u00e1 parcialmente financiado por subvenciones NSF, IIS-0534784, IIS0347993 y CNS-0626702. Debido a esta inmensa cantidad de informaci\u00f3n disponible, los usuarios se vuelven cada vez m\u00e1s dependientes de los motores de b\u00fasqueda web para localizar informaci\u00f3n relevante en la Web. Normalmente, los motores de b\u00fasqueda web, al igual que otras aplicaciones de recuperaci\u00f3n de informaci\u00f3n, utilizan una estructura de datos llamada \u00edndice invertido. Un \u00edndice invertido permite la recuperaci\u00f3n eficaz de los documentos -LRB- o p\u00e1ginas web -RRB- que contienen una determinada palabra clave. En la mayor\u00eda de los casos, una consulta que realiza el usuario puede tener miles o incluso millones de documentos coincidentes. Para evitar abrumar a los usuarios con una gran cantidad de resultados, los motores de b\u00fasqueda presentan los resultados en lotes de 10 a 20 documentos relevantes. Luego, el usuario revisa el primer lote de resultados y, si no encuentra la respuesta que busca, puede solicitar ver el siguiente lote o decidir emitir una nueva consulta. Un estudio reciente -LSB- 16 -RSB- indic\u00f3 que aproximadamente el 80 % de los usuarios examinan como m\u00e1ximo los 3 primeros lotes de resultados. Es decir, el 80% de los usuarios suelen ver como m\u00e1ximo entre 30 y 60 resultados por cada consulta que realizan a un motor de b\u00fasqueda. Al mismo tiempo, dado el tama\u00f1o de la Web,el \u00edndice invertido que mantienen los motores de b\u00fasqueda puede crecer mucho. Una soluci\u00f3n natural a este problema es crear un peque\u00f1o \u00edndice en un subconjunto de los documentos que probablemente se devolver\u00e1n como resultados principales -LRB- utilizando, por ejemplo, las t\u00e9cnicas de poda en -LSB- 7, 20 -RSB- -RRB- y calcular el primer lote de respuestas utilizando el \u00edndice podado. Si bien se ha demostrado que este enfoque ofrece una mejora significativa en el rendimiento, tambi\u00e9n conduce a una degradaci\u00f3n notable en la calidad de los resultados de b\u00fasqueda, porque las respuestas principales se calculan \u00fanicamente a partir del \u00edndice recortado -LSB- 7, 20 -RSB-. Es decir, incluso si una p\u00e1gina debe colocarse como la p\u00e1gina con mejores coincidencias seg\u00fan la m\u00e9trica de clasificaci\u00f3n de un motor de b\u00fasqueda, la p\u00e1gina puede colocarse detr\u00e1s de las contenidas en el \u00edndice eliminado si la p\u00e1gina no pas\u00f3 a formar parte del \u00edndice eliminado. por diversos motivos -LSB- 7, 20 -RSB-. Dada la feroz competencia entre los motores de b\u00fasqueda actuales, esta degradaci\u00f3n es claramente indeseable y debe abordarse si es posible. En este art\u00edculo, estudiamos c\u00f3mo podemos evitar cualquier degradaci\u00f3n de la calidad de la b\u00fasqueda debido a la optimizaci\u00f3n del rendimiento anterior y al mismo tiempo aprovechar la mayor parte de sus beneficios. Es decir, presentamos una serie de cambios -LRB- simples pero importantes -RRB- en las t\u00e9cnicas de poda para crear el \u00edndice podado. Nuestra principal contribuci\u00f3n es un nuevo algoritmo de c\u00e1lculo de respuestas que garantiza que las p\u00e1ginas m\u00e1s coincidentes -LRB- seg\u00fan la m\u00e9trica de clasificaci\u00f3n del motor de b\u00fasqueda -RRB- siempre se coloquen en la parte superior de los resultados de b\u00fasqueda, incluso aunque estemos calculando la primera. lote de respuestas del \u00edndice eliminado la mayor parte del tiempo. Estas t\u00e9cnicas de poda mejoradas y algoritmos de c\u00e1lculo de respuestas se exploran en el context de la arquitectura de cl\u00faster com\u00fanmente empleada por los motores de b\u00fasqueda actuales. Finalmente, estudiamos y presentamos c\u00f3mo los motores de b\u00fasqueda pueden minimizar el costo operativo de responder consultas y al mismo tiempo proporcionar resultados de b\u00fasqueda de alta calidad. Figura 1: -LRB- un motor de b\u00fasqueda -RRB- replica su \u00edndice completo IF para aumentar la capacidad de respuesta a consultas. -LRB- b -RRB- En el primer nivel, los \u00edndices IP peque\u00f1os manejan la mayor\u00eda de las consultas. Cuando IP no puede responder una consulta, se redirige al segundo nivel, donde se utiliza el \u00edndice IF completo para calcular la respuesta. 6. TRABAJOS RELACIONADOS -LSB- 3, 30 -RSB- proporcionan una buena descripci\u00f3n general de la indexaci\u00f3n invertida en motores de b\u00fasqueda web y sistemas de IR. Los estudios experimentales y los an\u00e1lisis de varios esquemas de partici\u00f3n para un \u00edndice invertido se presentan en -LSB- 6, 23, 33 -RSB-. Los algoritmos de poda que hemos presentado en este art\u00edculo son independientes del esquema de partici\u00f3n utilizado. Sin embargo, -LSB- 1, 5, 7, 27 -RSB- no considera ninguna calidad independiente de la consulta -LRB- como el PageRank -RRB- en la funci\u00f3n de clasificaci\u00f3n. -LSB- 32 -RSB- presenta un marco gen\u00e9rico para calcular las respuestas top-k aproximadas con algunos l\u00edmites probabil\u00edsticos en la calidad de los resultados. Nuestro trabajo se extiende esencialmente -LSB- 1, 2, 4, 7, 20, 27,31 -RSB- proponiendo mecanismos para proporcionar la garant\u00eda de exactitud a los resultados top-k calculados. Los motores de b\u00fasqueda utilizan varios m\u00e9todos de almacenamiento en cach\u00e9 como medio para reducir el coste asociado a las consultas -LSB- 18, 19, 21, 31 -RSB-. Este hilo de trabajo tambi\u00e9n es ortogonal al nuestro porque un esquema de almacenamiento en cach\u00e9 puede operar sobre nuestro \u00edndice p para minimizar el costo de c\u00e1lculo de la respuesta. Las funciones de clasificaci\u00f3n exactas empleadas por los motores de b\u00fasqueda actuales son secretos muy bien guardados. Sin embargo, en general, las clasificaciones se basan en la relevancia dependiente de la consulta y en la \"calidad\" del documento independiente de la consulta. '' De manera similar, hay una serie de trabajos que miden la `` calidad '' de los documentos, t\u00edpicamente capturada a trav\u00e9s del an\u00e1lisis basado en enlaces -LSB- 17, 28, 26 -RSB-. Dado que nuestro trabajo no asume una forma particular de funci\u00f3n de clasificaci\u00f3n, es complementario a este cuerpo de trabajo. Ha habido una gran cantidad de trabajo en el c\u00e1lculo de los resultados top-k. 7. COMENTARIOS FINALES Los motores de b\u00fasqueda web suelen podar sus \u00edndices invertidos a gran escala para poder escalar a enormes cargas de consultas. Si bien este enfoque puede mejorar el rendimiento, al calcular los mejores resultados de un \u00edndice recortado podemos notar una degradaci\u00f3n significativa en la calidad de los resultados. En este art\u00edculo, proporcionamos un marco para nuevas t\u00e9cnicas de poda y algoritmos de c\u00e1lculo de respuestas que garantizan que las p\u00e1ginas con mayores coincidencias siempre se coloquen en la parte superior de los resultados de b\u00fasqueda en el orden correcto. Estudiamos dos t\u00e9cnicas de poda, a saber, la poda basada en palabras clave y la poda basada en documentos, as\u00ed como su combinaci\u00f3n. Nuestros resultados experimentales demostraron que nuestros algoritmos se pueden utilizar eficazmente para podar un \u00edndice invertido sin degradar la calidad de los resultados. En particular, un \u00edndice recortado por palabras clave puede garantizar el 73 % de las consultas con un tama\u00f1o del 30 % del \u00edndice completo, mientras que un \u00edndice recortado por documentos puede garantizar el 68 % de las consultas con el mismo tama\u00f1o. Cuando combinamos los dos algoritmos de poda podemos garantizar el 60% de las consultas con un tama\u00f1o de \u00edndice del 16%. Esperamos que nuestro trabajo ayude a los motores de b\u00fasqueda a desarrollar \u00edndices mejores, m\u00e1s r\u00e1pidos y m\u00e1s eficientes y as\u00ed proporcionar una mejor experiencia de b\u00fasqueda al usuario en la Web.es complementario a este cuerpo de trabajo. Ha habido una gran cantidad de trabajo en el c\u00e1lculo de los resultados top-k. 7. COMENTARIOS FINALES Los motores de b\u00fasqueda web suelen podar sus \u00edndices invertidos a gran escala para poder escalar a enormes cargas de consultas. Si bien este enfoque puede mejorar el rendimiento, al calcular los mejores resultados de un \u00edndice recortado podemos notar una degradaci\u00f3n significativa en la calidad de los resultados. En este art\u00edculo, proporcionamos un marco para nuevas t\u00e9cnicas de poda y algoritmos de c\u00e1lculo de respuestas que garantizan que las p\u00e1ginas con mayores coincidencias siempre se coloquen en la parte superior de los resultados de b\u00fasqueda en el orden correcto. Estudiamos dos t\u00e9cnicas de poda, a saber, la poda basada en palabras clave y la poda basada en documentos, as\u00ed como su combinaci\u00f3n. Nuestros resultados experimentales demostraron que nuestros algoritmos se pueden utilizar eficazmente para podar un \u00edndice invertido sin degradar la calidad de los resultados. En particular, un \u00edndice recortado por palabras clave puede garantizar el 73 % de las consultas con un tama\u00f1o del 30 % del \u00edndice completo, mientras que un \u00edndice recortado por documentos puede garantizar el 68 % de las consultas con el mismo tama\u00f1o. Cuando combinamos los dos algoritmos de poda podemos garantizar el 60% de las consultas con un tama\u00f1o de \u00edndice del 16%. Esperamos que nuestro trabajo ayude a los motores de b\u00fasqueda a desarrollar \u00edndices mejores, m\u00e1s r\u00e1pidos y m\u00e1s eficientes y as\u00ed proporcionar una mejor experiencia de b\u00fasqueda al usuario en la Web.es complementario a este cuerpo de trabajo. Ha habido una gran cantidad de trabajo en el c\u00e1lculo de los resultados top-k. 7. COMENTARIOS FINALES Los motores de b\u00fasqueda web suelen podar sus \u00edndices invertidos a gran escala para poder escalar a enormes cargas de consultas. Si bien este enfoque puede mejorar el rendimiento, al calcular los mejores resultados de un \u00edndice recortado podemos notar una degradaci\u00f3n significativa en la calidad de los resultados. En este art\u00edculo, proporcionamos un marco para nuevas t\u00e9cnicas de poda y algoritmos de c\u00e1lculo de respuestas que garantizan que las p\u00e1ginas con mayores coincidencias siempre se coloquen en la parte superior de los resultados de b\u00fasqueda en el orden correcto. Estudiamos dos t\u00e9cnicas de poda, a saber, la poda basada en palabras clave y la poda basada en documentos, as\u00ed como su combinaci\u00f3n. Nuestros resultados experimentales demostraron que nuestros algoritmos se pueden utilizar eficazmente para podar un \u00edndice invertido sin degradar la calidad de los resultados. En particular, un \u00edndice recortado por palabras clave puede garantizar el 73 % de las consultas con un tama\u00f1o del 30 % del \u00edndice completo, mientras que un \u00edndice recortado por documentos puede garantizar el 68 % de las consultas con el mismo tama\u00f1o. Cuando combinamos los dos algoritmos de poda podemos garantizar el 60% de las consultas con un tama\u00f1o de \u00edndice del 16%. Esperamos que nuestro trabajo ayude a los motores de b\u00fasqueda a desarrollar \u00edndices mejores, m\u00e1s r\u00e1pidos y m\u00e1s eficientes y as\u00ed proporcionar una mejor experiencia de b\u00fasqueda al usuario en la Web.", "keyphrases": ["motor de b\u00fasqueda web", "\u00edndice invertido a gran escala", "carga de consultas", "\u00edndice de poda", "mercado de b\u00fasqueda en l\u00ednea", "degradaci\u00f3n de la calidad del resultado", "base de poda realizar optimizaci\u00f3n", "t\u00e9cnica de poda", "algoritmo de c\u00e1lculo de resultados", "p\u00e1gina de mejores coincidencias", "resultado de b\u00fasqueda superior", "tama\u00f1o \u00f3ptimo"]}
{"file_name": "J-25", "text": "Apuestas de estilo booleano: un marco para la negociaci\u00f3n de valores basado en f\u00f3rmulas l\u00f3gicas RESUMEN Desarrollamos un marco para la negociaci\u00f3n de valores compuestos: instrumentos financieros que se amortizan dependiendo de los resultados de declaraciones arbitrarias en l\u00f3gica proposicional. Comprar o vender valores (que pueden considerarse como una apuesta a favor o en contra de un resultado futuro particular) permite a los agentes cubrir riesgos y obtener ganancias -LRB- con expectativa -RRB- de predicciones subjetivas. Un mercado de valores compuesto permite a los agentes realizar apuestas sobre combinaciones booleanas arbitrarias de eventos, lo que les permite alcanzar m\u00e1s estrechamente su exposici\u00f3n \u00f3ptima al riesgo y permite que el mercado en su conjunto alcance m\u00e1s estrechamente el \u00f3ptimo social. La desventaja de permitir tal expresividad est\u00e1 en la complejidad de los problemas de optimizaci\u00f3n de los agentes y del subastador. Desarrollamos y motivamos el concepto de mercado de valores compuesto, presentando el marco a trav\u00e9s de una serie de definiciones formales y ejemplos. Luego analizamos en detalle el problema de emparejamiento del subastador. Mostramos que, con n eventos, el problema de emparejamiento es co-NP-completo en el caso divisible y \u03a3p2-completo en el caso indivisible. Mostramos que este \u00faltimo resultado de dureza se mantiene incluso bajo severas restricciones de idioma en las ofertas. Con eventos log n, el problema es polin\u00f3mico en el caso divisible y NP-completo en el caso indivisible. Discutimos brevemente los algoritmos de coincidencia y los casos especiales manejables. 1. INTRODUCCI\u00d3N Los mercados de valores permiten efectivamente a los operadores hacer apuestas sobre los resultados de propuestas futuras inciertas. El valor econ\u00f3mico de los mercados de valores es doble. En primer lugar, permiten a los operadores cubrir riesgos o asegurarse contra resultados indeseables. Por ejemplo, el propietario de una acci\u00f3n podr\u00eda comprar una opci\u00f3n de venta -LRB- (el derecho a vender la acci\u00f3n a un precio determinado -RRB-) para asegurarse contra una ca\u00edda de la acci\u00f3n. En segundo lugar, los mercados de valores permiten a los operadores especular u obtener una ganancia esperada subjetiva cuando los precios de mercado no reflejan su evaluaci\u00f3n de la probabilidad de resultados futuros. Por ejemplo, un comerciante podr\u00eda comprar una opci\u00f3n de compra si cree que es alta la probabilidad de que el precio de la acci\u00f3n subyacente suba, independientemente de la exposici\u00f3n al riesgo de cambios en el precio de la acci\u00f3n. Dado que los comerciantes pueden obtener ganancias si pueden realizar evaluaciones de probabilidad efectivas, a menudo los precios en los mercados financieros producen pron\u00f3sticos agregados muy precisos de eventos futuros -LSB- 10, 29, 27, 28 -RSB-. Los mercados de valores reales tienen estructuras de pagos complejas con varios factores desencadenantes. Sin embargo, todos ellos pueden modelarse como colecciones de valores Arrow-Debreu m\u00e1s b\u00e1sicos o at\u00f3micos -LSB- 1, 8, 20 -RSB-. Una unidad de un valor Arrow-Debreu paga un d\u00f3lar si y s\u00f3lo si -LRB- iff -RRB- ocurre un evento binario correspondiente; no paga nada si el evento no ocurre. Entonces, por ejemplo, una unidad de un valor denominado -LRB- Acme100 -RRB- podr\u00eda pagar $1 si las acciones de Acme est\u00e1n por encima de $100 el 4 de enero.2004. Una opci\u00f3n sobre acciones de Acme, tal como se definir\u00eda en una bolsa financiera, puede considerarse como una cartera de dichos valores at\u00f3micos.1 En este art\u00edculo, desarrollamos y analizamos un marco para la negociaci\u00f3n en mercados de valores compuestos con pagos supeditados a decisiones arbitrarias. combinaciones l\u00f3gicas de eventos, incluidos los condicionales. Por ejemplo, dados los eventos binarios A, B y C, un operador podr\u00eda ofertar para comprar tres unidades de un valor denominado -LRB- A n B \u00af VC -RRB- que paga $ 1 si se produce el evento compuesto A n B \u00af VC ocurre por treinta centavos cada uno. Dado un conjunto de ofertas de este tipo, el subastador se enfrenta a un complejo problema de emparejamiento para decidir qu\u00e9 ofertas se aceptan, por cu\u00e1ntas unidades y a qu\u00e9 precio. Normalmente, el subastador no busca asumir ning\u00fan riesgo propio y s\u00f3lo busca igualar transacciones aceptables entre los postores, pero tambi\u00e9n consideramos formulaciones alternativas en las que el subastador act\u00faa como un creador de mercado dispuesto a aceptar alg\u00fan riesgo. Examinamos la complejidad computacional del problema de emparejamiento del subastador. Sea la longitud de la descripci\u00f3n de todos los valores disponibles O -LRB- n -RRB-. Con n eventos, el problema de coincidencia es co-NP-completo en el caso divisible y Ep2-completo en el caso indivisible. Esta dureza completa de Ep2 se mantiene incluso cuando el lenguaje de licitaci\u00f3n est\u00e1 significativamente restringido. Con eventos log n, el problema es polin\u00f3mico en el caso divisible y NP-completo en el caso indivisible. La Secci\u00f3n 2 presenta algunos antecedentes necesarios, motivaci\u00f3n y trabajo relacionado. La secci\u00f3n 3 describe formalmente nuestro marco para valores compuestos y define el problema de igualaci\u00f3n del subastador. La secci\u00f3n 4 analiza brevemente los algoritmos naturales para resolver el problema de emparejamiento. La secci\u00f3n 5 demuestra nuestros resultados centrales de complejidad computacional. La secci\u00f3n 6 analiza la posibilidad de casos especiales tratables. La secci\u00f3n 7 concluye con un resumen y algunas ideas de direcciones futuras. 2. PRELIMINARES 2.1 Antecedentes y notaci\u00f3n En este mundo simple hay cuatro posibles estados futuros: todas las combinaciones posibles de los resultados de los eventos binarios: golpeado n acme100, golpeado n acme100, golpeado n acme100, golpeado n acme100. El riesgo de cobertura puede considerarse como una acci\u00f3n de mover dinero entre varios estados futuros posibles. Por ejemplo, insur1. T\u00e9cnicamente, una opci\u00f3n es una cartera de infinitos valores at\u00f3micos, aunque se puede modelar aproximadamente con un n\u00famero finito. ing la propia casa transfiere dinero desde los estados futuros donde se golpea no es cierto hacia los estados donde se encuentra. Vender un valor denominado -LRB- acme100 -RRB- - que paga $ 1 si ocurre el evento acme100 - transfiere dinero de estados futuros donde el precio de Acme es superior a $ 100 el 4 de enero a estados donde no lo es. La especulaci\u00f3n tambi\u00e9n es un acto de transferir dinero entre estados futuros, aunque generalmente se asocia con maximizar el rendimiento esperado en lugar de reducir el riesgo. Por ejemplo, apostar en un equipo de f\u00fatbol mueve dinero del estado \"el equipo pierde\" al estado \"el equipo gana\".Todos los posibles resultados futuros forman un espacio de estados \u03a9, que consta de estados mutuamente excluyentes y exhaustivos \u03c9 E \u03a9. A menudo, una forma m\u00e1s natural de pensar en posibles resultados futuros es como un espacio de eventos A de eventos linealmente independientes AEA que pueden superponerse arbitrariamente. Entonces, en nuestro ejemplo de juguete, el golpe en acme100 es uno de los cuatro estados disjuntos, mientras que el golpe es uno de los dos eventos. Tenga en cuenta que un conjunto de n eventos linealmente independientes define un espacio de estados \u03a9 de tama\u00f1o 2 '' que consta de todas las combinaciones posibles de resultados de eventos. Por el contrario, cualquier espacio de estados \u03a9 se puede factorizar en eventos -LSB- log l\u03a9ll. Supongamos que A cubre exhaustivamente todos los resultados futuros significativos -LRB-, es decir, cubre todas las eventualidades contra las que los agentes pueden desear protegerse y/o especular sobre ellas -RRB-. Entonces, la existencia de 2'' valores linealmente independientes (llamado mercado completo) permite a los agentes distribuir su riqueza arbitrariamente entre estados futuros.2 Un agente puede crear cualquier cobertura o especulaci\u00f3n que desee. En condiciones cl\u00e1sicas, los agentes que operan en un mercado completo forman un equilibrio en el que el riesgo se asigna de manera \u00f3ptima en el sentido de Pareto. Si el mercado es incompleto, es decir, consta de menos de 2'' valores linealmente independientes, entonces en general los agentes no pueden construir coberturas arbitrarias y las asignaciones de equilibrio pueden ser no \u00f3ptimas -LSB- 1, 8, 19, 20 -RSB-. En entornos del mundo real, el n\u00famero de eventos significativos n es grande y, por lo tanto, el n\u00famero de seguridades necesarias para que est\u00e9 completo es intratable. No existe ni existir\u00e1 jam\u00e1s un mercado verdaderamente completo. Una motivaci\u00f3n detr\u00e1s de los mercados de valores compuestos es proporcionar un mecanismo que respalde la mayor transferencia de riesgo utilizando el menor n\u00famero de transacciones posible. Los valores compuestos permiten un alto grado de expresividad en la construcci\u00f3n de ofertas. La compensaci\u00f3n por una mayor expresividad es una mayor complejidad computacional, tanto desde el punto de vista del postor como del subastador. 2.2 Trabajo relacionado La b\u00fasqueda para reducir el n\u00famero de instrumentos financieros necesarios para respaldar una asignaci\u00f3n \u00f3ptima de riesgos data del trabajo original de Arrow -LSB- 1 -RSB-. El requisito mencionado anteriormente de \"s\u00f3lo\" 2 \"t\u00edtulos linealmente independientes es en s\u00ed mismo una reducci\u00f3n de la formulaci\u00f3n m\u00e1s sencilla. En una econom\u00eda con k bienes est\u00e1ndar, el mercado completo m\u00e1s sencillo contiene k \u2022 2 '' valores, cada uno de los cuales se amortiza en un bien bajo un estado de realizaci\u00f3n. La flecha -LSB- 1 -RSB- mostr\u00f3 que un mercado donde los valores y los bienes est\u00e1n esencialmente separados, con 2 '' valores que pagan en un solo bien numerario m\u00e1s k mercados al contado en los bienes est\u00e1ndar, tambi\u00e9n es completo. Para nuestros prop\u00f3sitos, necesitamos considerar s\u00f3lo el mercado de valores. 2Por valores linealmente independientes queremos decir que los vectores de pagos en todos los estados futuros de estos valores son linealmente independientes. Varian -LSB- 34 -RSB- muestra que se puede construir un mercado completo utilizando menos de 2n valores,reemplazar los valores faltantes con opciones. A\u00fan as\u00ed, el n\u00famero de instrumentos financieros linealmente independientes (valores m\u00e1s opciones) debe ser 2n para garantizar la integridad. Los autores muestran que en algunos casos el mercado puede estructurarse y \"compactarse\" en analog\u00eda con las representaciones de redes bayesianas de distribuciones de probabilidad conjuntas -LSB- 23 -RSB-. Muestran que, si las independencias neutrales al riesgo de todos los agentes concuerdan con las independencias codificadas en la estructura del mercado, entonces el mercado est\u00e1 operativamente completo. Para conjuntos de agentes con aversi\u00f3n absoluta y constante al riesgo, el acuerdo sobre las independencias de Markov es suficiente. Bossaerts, Fine y Ledyard -LSB- 2 -RSB- desarrollan un mecanismo que llaman negociaci\u00f3n de valor combinado -LRB- CVT -RRB- que permite a los operadores ordenar una cartera arbitraria de valores en una sola oferta, en lugar de dividir la orden en una secuencia de ofertas sobre valores individuales. Si se acepta la orden de cartera, todas las operaciones impl\u00edcitas sobre valores individuales se ejecutan simult\u00e1neamente, eliminando as\u00ed el llamado riesgo de ejecuci\u00f3n de que los precios cambien en medio de una secuencia planificada de \u00f3rdenes. Los autores llevan a cabo experimentos de laboratorio que muestran que, incluso en mercados reducidos donde el comercio secuencial ordinario fracasa, la CVT respalda la fijaci\u00f3n de precios y la asignaci\u00f3n eficiente. Tenga en cuenta que la CVT difiere significativamente de la negociaci\u00f3n de valores compuestos. CVT permite la negociaci\u00f3n instant\u00e1nea de cualquier combinaci\u00f3n lineal de valores, mientras que los valores compuestos permiten valores m\u00e1s expresivos que pueden codificar combinaciones booleanas no lineales de eventos. Por ejemplo, CVT puede permitir a un agente ordenar valores -LRB- A -RRB- y -LRB- B -RRB- en un paquete que se amortiza como una combinaci\u00f3n lineal de A y B,3 pero CVT no permitir\u00e1 el construcci\u00f3n de un valor compuesto -LRB- A n B -RRB- que paga $ 1 si ocurren tanto A como B, o un valor compuesto -LRB- AIB -RRB-. Las subastas combinatorias permiten a los postores asignar valores distintos a todos los posibles paquetes de bienes en lugar de solo a bienes individuales. Los valores compuestos se diferencian de las subastas combinatorias en concepto y complejidad. Los valores compuestos permiten a los postores construir una apuesta arbitraria sobre cualquiera de los 22n eventos compuestos posibles expresables como funciones l\u00f3gicas de los n eventos base, condicionados a cualquier otro de los 22n eventos compuestos. Los agentes optimizan en funci\u00f3n de sus propias probabilidades subjetivas y actitud de riesgo -LRB- y, en general, de sus creencias sobre las creencias y utilidades de otros agentes, ad infinitum -RRB-. El problema central del subastador es identificar oportunidades de arbitraje: es decir, igualar apuestas sin asumir ning\u00fan riesgo. Las subastas combinatorias, por otro lado, permiten pujar por cualquiera de los 2n paquetes de n bienes. La incertidumbre (y por tanto el riesgo) no se considera. El problema central del subastador es maximizar el bienestar social. Tenga en cuenta tambi\u00e9n que los problemas se encuentran en diferentes clases de complejidad.Mientras que la compensaci\u00f3n de una subasta combinatoria es polin\u00f3mica en el caso divisible y NP-completa en el caso indivisible, la casaci\u00f3n en un mercado de valores compuesto es NP-completa en el caso divisible y Ep2-completa en el caso indivisible. De hecho, incluso el problema de decidir si dos ofertas sobre valores compuestos coinciden, incluso en el caso divisible, es NP-completo -LRB- ver Secci\u00f3n 5.2 -RRB-. En cierto sentido es posible traducir nuestro problema de emparejamiento de valores compuestos en un problema an\u00e1logo para compensar intercambios combinatorios bilaterales -LSB- 31 -RSB- de tama\u00f1o exponencial. Espec\u00edficamente, si consideramos el pago en un estado particular como un bien, entonces los valores compuestos pueden verse como paquetes de -LRB- cantidades fraccionarias de -RRB- dichos bienes. La restricci\u00f3n de equilibrio de materiales que enfrenta el subastador combinatorio corresponde a una restricci\u00f3n que impide al subastador de valores compuestos asumir cualquier riesgo. Tenga en cuenta que esta traducci\u00f3n no es del todo \u00fatil para abordar el problema de emparejamiento de valores compuestos, ya que el intercambio combinatorio resultante tiene un n\u00famero exponencial de bienes. Hanson -LSB- 15 -RSB- desarrolla un mecanismo de mercado llamado regla de puntuaci\u00f3n de mercado que es especialmente adecuado para permitir apuestas sobre un n\u00famero combinatorio de resultados. El mecanismo mantiene una distribuci\u00f3n de probabilidad conjunta en los 2n estados, ya sea expl\u00edcita o impl\u00edcitamente utilizando una red bayesiana u otra representaci\u00f3n compacta. En el l\u00edmite de un \u00fanico operador, el mecanismo se comporta como una regla de puntuaci\u00f3n, adecuada para sondear a un \u00fanico agente su distribuci\u00f3n de probabilidad. En el caso de muchos comerciantes, produce una estimaci\u00f3n combinada. Dado que el mercado esencialmente siempre tiene un conjunto completo de precios publicados para todos los resultados posibles, el mecanismo evita el problema de los mercados delgados o la iliquidez, que necesariamente afecta a cualquier mercado que contenga un n\u00famero exponencial de inversiones alternativas. Las ofertas de valores compuestos pueden considerarse expresiones de desigualdades probabil\u00edsticas: por ejemplo, una oferta para comprar -LRB- A n B -RRB- al precio 0,3 es una afirmaci\u00f3n de que la probabilidad de A n B es mayor que 0,3. Si un conjunto de ofertas unitarias corresponde a un conjunto de desigualdades probabil\u00edsticas inconsistentes, entonces hay una coincidencia. Abordamos estas cuestiones a continuaci\u00f3n.La restricci\u00f3n de equilibrio de materiales que enfrenta el subastador combinatorio corresponde a una restricci\u00f3n que impide al subastador de valores compuestos asumir cualquier riesgo. Tenga en cuenta que esta traducci\u00f3n no es del todo \u00fatil para abordar el problema de emparejamiento de valores compuestos, ya que el intercambio combinatorio resultante tiene un n\u00famero exponencial de bienes. Hanson -LSB- 15 -RSB- desarrolla un mecanismo de mercado llamado regla de puntuaci\u00f3n de mercado que es especialmente adecuado para permitir apuestas sobre un n\u00famero combinatorio de resultados. El mecanismo mantiene una distribuci\u00f3n de probabilidad conjunta en los 2n estados, ya sea expl\u00edcita o impl\u00edcitamente utilizando una red bayesiana u otra representaci\u00f3n compacta. En el l\u00edmite de un \u00fanico operador, el mecanismo se comporta como una regla de puntuaci\u00f3n, adecuada para sondear a un \u00fanico agente su distribuci\u00f3n de probabilidad. En el caso de muchos comerciantes, produce una estimaci\u00f3n combinada. Dado que el mercado esencialmente siempre tiene un conjunto completo de precios publicados para todos los resultados posibles, el mecanismo evita el problema de los mercados delgados o la iliquidez, que necesariamente afecta a cualquier mercado que contenga un n\u00famero exponencial de inversiones alternativas. Las ofertas de valores compuestos pueden considerarse expresiones de desigualdades probabil\u00edsticas: por ejemplo, una oferta para comprar -LRB- A n B -RRB- al precio 0,3 es una afirmaci\u00f3n de que la probabilidad de A n B es mayor que 0,3. Si un conjunto de ofertas unitarias corresponde a un conjunto de desigualdades probabil\u00edsticas inconsistentes, entonces hay una coincidencia. Abordamos estas cuestiones a continuaci\u00f3n.La restricci\u00f3n de equilibrio de materiales que enfrenta el subastador combinatorio corresponde a una restricci\u00f3n que impide al subastador de valores compuestos asumir cualquier riesgo. Tenga en cuenta que esta traducci\u00f3n no es del todo \u00fatil para abordar el problema de emparejamiento de valores compuestos, ya que el intercambio combinatorio resultante tiene un n\u00famero exponencial de bienes. Hanson -LSB- 15 -RSB- desarrolla un mecanismo de mercado llamado regla de puntuaci\u00f3n de mercado que es especialmente adecuado para permitir apuestas sobre un n\u00famero combinatorio de resultados. El mecanismo mantiene una distribuci\u00f3n de probabilidad conjunta en los 2n estados, ya sea expl\u00edcita o impl\u00edcitamente utilizando una red bayesiana u otra representaci\u00f3n compacta. En el l\u00edmite de un \u00fanico operador, el mecanismo se comporta como una regla de puntuaci\u00f3n, adecuada para sondear a un \u00fanico agente su distribuci\u00f3n de probabilidad. En el caso de muchos comerciantes, produce una estimaci\u00f3n combinada. Dado que el mercado esencialmente siempre tiene un conjunto completo de precios publicados para todos los resultados posibles, el mecanismo evita el problema de los mercados delgados o la iliquidez, que necesariamente afecta a cualquier mercado que contenga un n\u00famero exponencial de inversiones alternativas. Las ofertas de valores compuestos pueden considerarse expresiones de desigualdades probabil\u00edsticas: por ejemplo, una oferta para comprar -LRB- A n B -RRB- al precio 0,3 es una afirmaci\u00f3n de que la probabilidad de A n B es mayor que 0,3. Si un conjunto de ofertas unitarias corresponde a un conjunto de desigualdades probabil\u00edsticas inconsistentes, entonces hay una coincidencia. Abordamos estas cuestiones a continuaci\u00f3n.Abordamos estas cuestiones a continuaci\u00f3n.Abordamos estas cuestiones a continuaci\u00f3n.", "keyphrases": ["apuesta combinatoria", "efecto probable evaluar", "combinaci\u00f3n l\u00f3gica arbitraria", "seguro compuesto", "red bayesiana", "comercio de valor combinado", "algoritmo aproximado", "vector de pago", "caso manejable", "base segura"]}
{"file_name": "J-15", "text": "Descomposici\u00f3n generalizada del valor y subastas estructuradas de m\u00faltiples atributos RESUMEN Los mecanismos de subasta de m\u00faltiples atributos generalmente permanecen agn\u00f3sticos sobre las preferencias de los comerciantes o asumen formas altamente restrictivas, como la aditividad total. Las preferencias reales a menudo exhiben dependencias entre atributos, pero pueden poseer alguna estructura que puede explotarse de manera \u00fatil para agilizar la comunicaci\u00f3n y simplificar el funcionamiento de una subasta de m\u00faltiples atributos. Desarrollamos dicha estructura utilizando la teor\u00eda de funciones de valor mensurables, una representaci\u00f3n de utilidad cardinal basada en un orden subyacente sobre las diferencias de preferencia. Un conjunto de relaciones de independencia condicional local sobre tales diferencias respalda una representaci\u00f3n de preferencia aditiva generalizada, que descompone la utilidad en grupos superpuestos de atributos relacionados. Introducimos un mecanismo de subasta iterativo que mantiene los precios en grupos locales de atributos en lugar del espacio completo de configuraciones conjuntas. Cuando las preferencias de los comerciantes son consistentes con la estructura aditiva generalizada de la subasta, el mecanismo produce asignaciones aproximadamente \u00f3ptimas, a precios VCG aproximados. 1. INTRODUCCI\u00d3N Los mecanismos de negociaci\u00f3n de atributos m\u00faltiples ampl\u00edan los mecanismos tradicionales basados \u200b\u200b\u00fanicamente en el precio al facilitar la negociaci\u00f3n sobre un conjunto de atributos predefinidos que representan varios aspectos del acuerdo no relacionados con el precio. En lugar de negociar sobre un bien o servicio completamente definido, un mecanismo de atributos m\u00faltiples retrasa el compromiso con configuraciones espec\u00edficas hasta que se identifican los candidatos m\u00e1s prometedores. Por ejemplo, el departamento de adquisiciones de una empresa puede utilizar una subasta de atributos m\u00faltiples para seleccionar un proveedor de discos duros. Para tener en cuenta las preferencias de los comerciantes, el mecanismo de subasta debe extraer informaci\u00f3n evaluativa sobre un dominio complejo de configuraciones multidimensionales. Construir y comunicar una especificaci\u00f3n de preferencias completa puede ser una carga severa incluso para un n\u00famero moderado de atributos; por lo tanto, las subastas pr\u00e1cticas de atributos m\u00faltiples deben acomodar especificaciones parciales o soportar la expresi\u00f3n compacta de preferencias asumiendo alguna forma simplificada. Con diferencia, la forma de atributos m\u00faltiples m\u00e1s popular a adoptar es la m\u00e1s simple: una representaci\u00f3n aditiva donde el valor general es una combinaci\u00f3n lineal de valores asociados con cada atributo. Por ejemplo, varias propuestas recientes de subastas iterativas de atributos m\u00faltiples -LSB- 2, 3, 8, 19 -RSB- requieren representaciones de preferencias aditivas. Tal aditividad reduce exponencialmente la complejidad de la especificaci\u00f3n de preferencias -LRB- en comparaci\u00f3n con el caso discreto general -RRB-, pero excluye la expresi\u00f3n de cualquier interdependencia entre los atributos. Sin embargo, en la pr\u00e1ctica las interdependencias entre los atributos naturales son bastante comunes. En tales casos, una funci\u00f3n de valor aditivo puede no ser capaz de proporcionar ni siquiera una aproximaci\u00f3n razonable de las preferencias reales. Por otra parte, los modelos totalmente generales son intratables,y es razonable esperar que las preferencias multiatributo exhiban alguna estructura. Nuestro objetivo, por lo tanto, es identificar las representaciones estructuradas m\u00e1s sutiles pero de mayor aplicaci\u00f3n y explotar estas propiedades de las preferencias en los mecanismos comerciales. Proponemos un mecanismo de subasta iterativo basado precisamente en una estructura de preferencias flexible. Nuestro enfoque est\u00e1 inspirado en el dise\u00f1o de una subasta de adquisici\u00f3n iterativa de atributos m\u00faltiples para preferencias de aditivos, debido a Parkes y Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone dos tipos de subastas iterativas: la primera -LRB- NLD -RRB- no hace suposiciones sobre las preferencias de los comerciantes y permite a los vendedores ofertar en todo el espacio de atributos multidimensionales. Debido a que NLD mantiene una estructura de precios exponencial, solo es adecuado para dominios peque\u00f1os. La otra subasta -LRB- AD -RRB- asume funciones aditivas de valoraci\u00f3n del comprador y coste del vendedor. Recopila ofertas de venta por nivel de atributo y para un \u00fanico plazo de descuento. El precio de una configuraci\u00f3n se define como la suma de los precios de los niveles de atributos elegidos menos el descuento. La subasta que proponemos tambi\u00e9n admite espacios de precios compactos, aunque para niveles de grupos de atributos en lugar de \u00fanicos. Dadas sus ra\u00edces en la teor\u00eda de la utilidad multiatributo -LSB- 13 -RSB-, la condici\u00f3n GAI se define con respecto a la funci\u00f3n de utilidad esperada. Por lo tanto, aplicarlo para modelar valores para ciertos resultados requiere una reinterpretaci\u00f3n de la preferencia bajo certeza. Para este fin, aprovechamos el hecho de que los resultados de las subastas est\u00e1n asociados con precios continuos, lo que proporciona una escala natural para evaluar la magnitud de la preferencia. Primero, presentamos un marco de representaci\u00f3n para las preferencias que captura, adem\u00e1s de ordenamientos simples entre los valores de configuraci\u00f3n de los atributos, la diferencia en la disposici\u00f3n a pagar -LRB- wtp -RRB- por cada uno. A continuaci\u00f3n, construimos un v\u00ednculo directo y formalmente justificado desde declaraciones de preferencia sobre resultados con precio hasta una descomposici\u00f3n aditiva generalizada de la funci\u00f3n dap. Despu\u00e9s de dise\u00f1ar esta infraestructura, empleamos esta herramienta de representaci\u00f3n para el desarrollo de un mecanismo de subasta iterativo de atributos m\u00faltiples que permite a los comerciantes expresar sus preferencias complejas en formato GAI. Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.Nuestro enfoque est\u00e1 inspirado en el dise\u00f1o de una subasta de adquisici\u00f3n iterativa de atributos m\u00faltiples para preferencias de aditivos, debido a Parkes y Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone dos tipos de subastas iterativas: la primera -LRB- NLD -RRB- no hace suposiciones sobre las preferencias de los comerciantes y permite a los vendedores ofertar en todo el espacio de atributos multidimensionales. Debido a que NLD mantiene una estructura de precios exponencial, solo es adecuado para dominios peque\u00f1os. La otra subasta -LRB- AD -RRB- asume funciones aditivas de valoraci\u00f3n del comprador y coste del vendedor. Recopila ofertas de venta por nivel de atributo y para un \u00fanico plazo de descuento. El precio de una configuraci\u00f3n se define como la suma de los precios de los niveles de atributos elegidos menos el descuento. La subasta que proponemos tambi\u00e9n admite espacios de precios compactos, aunque para niveles de grupos de atributos en lugar de \u00fanicos. Dadas sus ra\u00edces en la teor\u00eda de la utilidad multiatributo -LSB- 13 -RSB-, la condici\u00f3n GAI se define con respecto a la funci\u00f3n de utilidad esperada. Por lo tanto, aplicarlo para modelar valores para ciertos resultados requiere una reinterpretaci\u00f3n de la preferencia bajo certeza. Para este fin, aprovechamos el hecho de que los resultados de las subastas est\u00e1n asociados con precios continuos, lo que proporciona una escala natural para evaluar la magnitud de la preferencia. Primero, presentamos un marco de representaci\u00f3n para las preferencias que captura, adem\u00e1s de ordenamientos simples entre los valores de configuraci\u00f3n de los atributos, la diferencia en la disposici\u00f3n a pagar -LRB- wtp -RRB- por cada uno. A continuaci\u00f3n, construimos un v\u00ednculo directo y formalmente justificado desde declaraciones de preferencia sobre resultados con precio hasta una descomposici\u00f3n aditiva generalizada de la funci\u00f3n dap. Despu\u00e9s de dise\u00f1ar esta infraestructura, empleamos esta herramienta de representaci\u00f3n para el desarrollo de un mecanismo de subasta iterativo de atributos m\u00faltiples que permite a los comerciantes expresar sus preferencias complejas en formato GAI. Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.Nuestro enfoque est\u00e1 inspirado en el dise\u00f1o de una subasta de adquisici\u00f3n iterativa de atributos m\u00faltiples para preferencias de aditivos, debido a Parkes y Kalagnanam -LRB- PK -RRB- -LSB- 19 -RSB-. PK propone dos tipos de subastas iterativas: la primera -LRB- NLD -RRB- no hace suposiciones sobre las preferencias de los comerciantes y permite a los vendedores ofertar en todo el espacio de atributos multidimensionales. Debido a que NLD mantiene una estructura de precios exponencial, solo es adecuado para dominios peque\u00f1os. La otra subasta -LRB- AD -RRB- asume funciones aditivas de valoraci\u00f3n del comprador y coste del vendedor. Recopila ofertas de venta por nivel de atributo y para un \u00fanico plazo de descuento. El precio de una configuraci\u00f3n se define como la suma de los precios de los niveles de atributos elegidos menos el descuento. La subasta que proponemos tambi\u00e9n admite espacios de precios compactos, aunque para niveles de grupos de atributos en lugar de \u00fanicos. Dadas sus ra\u00edces en la teor\u00eda de la utilidad multiatributo -LSB- 13 -RSB-, la condici\u00f3n GAI se define con respecto a la funci\u00f3n de utilidad esperada. Por lo tanto, aplicarlo para modelar valores para ciertos resultados requiere una reinterpretaci\u00f3n de la preferencia bajo certeza. Para este fin, aprovechamos el hecho de que los resultados de las subastas est\u00e1n asociados con precios continuos, lo que proporciona una escala natural para evaluar la magnitud de la preferencia. Primero, presentamos un marco de representaci\u00f3n para las preferencias que captura, adem\u00e1s de ordenamientos simples entre los valores de configuraci\u00f3n de los atributos, la diferencia en la disposici\u00f3n a pagar -LRB- wtp -RRB- por cada uno. A continuaci\u00f3n, construimos un v\u00ednculo directo y formalmente justificado desde declaraciones de preferencia sobre resultados con precio hasta una descomposici\u00f3n aditiva generalizada de la funci\u00f3n dap. Despu\u00e9s de dise\u00f1ar esta infraestructura, empleamos esta herramienta de representaci\u00f3n para el desarrollo de un mecanismo de subasta iterativo de atributos m\u00faltiples que permite a los comerciantes expresar sus preferencias complejas en formato GAI. Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.El precio de una configuraci\u00f3n se define como la suma de los precios de los niveles de atributos elegidos menos el descuento. La subasta que proponemos tambi\u00e9n admite espacios de precios compactos, aunque para niveles de grupos de atributos en lugar de \u00fanicos. Dadas sus ra\u00edces en la teor\u00eda de la utilidad multiatributo -LSB- 13 -RSB-, la condici\u00f3n GAI se define con respecto a la funci\u00f3n de utilidad esperada. Por lo tanto, aplicarlo para modelar valores para ciertos resultados requiere una reinterpretaci\u00f3n de la preferencia bajo certeza. Para este fin, aprovechamos el hecho de que los resultados de las subastas est\u00e1n asociados con precios continuos, lo que proporciona una escala natural para evaluar la magnitud de la preferencia. Primero, presentamos un marco de representaci\u00f3n para las preferencias que captura, adem\u00e1s de ordenamientos simples entre los valores de configuraci\u00f3n de los atributos, la diferencia en la disposici\u00f3n a pagar -LRB- wtp -RRB- por cada uno. A continuaci\u00f3n, construimos un v\u00ednculo directo y formalmente justificado desde declaraciones de preferencia sobre resultados con precio hasta una descomposici\u00f3n aditiva generalizada de la funci\u00f3n dap. Despu\u00e9s de dise\u00f1ar esta infraestructura, empleamos esta herramienta de representaci\u00f3n para el desarrollo de un mecanismo de subasta iterativo de atributos m\u00faltiples que permite a los comerciantes expresar sus preferencias complejas en formato GAI. Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.El precio de una configuraci\u00f3n se define como la suma de los precios de los niveles de atributos elegidos menos el descuento. La subasta que proponemos tambi\u00e9n admite espacios de precios compactos, aunque para niveles de grupos de atributos en lugar de \u00fanicos. Dadas sus ra\u00edces en la teor\u00eda de la utilidad multiatributo -LSB- 13 -RSB-, la condici\u00f3n GAI se define con respecto a la funci\u00f3n de utilidad esperada. Por lo tanto, aplicarlo para modelar valores para ciertos resultados requiere una reinterpretaci\u00f3n de la preferencia bajo certeza. Para este fin, aprovechamos el hecho de que los resultados de las subastas est\u00e1n asociados con precios continuos, lo que proporciona una escala natural para evaluar la magnitud de la preferencia. Primero, presentamos un marco de representaci\u00f3n para las preferencias que captura, adem\u00e1s de ordenamientos simples entre los valores de configuraci\u00f3n de los atributos, la diferencia en la disposici\u00f3n a pagar -LRB- wtp -RRB- por cada uno. A continuaci\u00f3n, construimos un v\u00ednculo directo y formalmente justificado desde declaraciones de preferencia sobre resultados con precio hasta una descomposici\u00f3n aditiva generalizada de la funci\u00f3n dap. Despu\u00e9s de dise\u00f1ar esta infraestructura, empleamos esta herramienta de representaci\u00f3n para el desarrollo de un mecanismo de subasta iterativo de atributos m\u00faltiples que permite a los comerciantes expresar sus preferencias complejas en formato GAI. Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.Luego estudiamos las propiedades pr\u00e1cticas, computacionales y de asignaci\u00f3n de la subasta. En la Secci\u00f3n 2 presentamos antecedentes esenciales de nuestro marco de representaci\u00f3n, la funci\u00f3n de valor medible -LRB- MVF -RRB-. La secci\u00f3n 3 desarrolla nuevas estructuras de atributos m\u00faltiples para MVF, que respaldan descomposiciones aditivas generalizadas. A continuaci\u00f3n, mostramos la aplicabilidad del marco te\u00f3rico a las preferencias en el comercio. El resto del art\u00edculo est\u00e1 dedicado al mecanismo de subasta propuesto.", "keyphrases": ["subasta", "subasta multiatributo", "prefiero manejar", "teor\u00eda de la funci\u00f3n de valor de medida", "mecanismo de subasta iter", "mvf", "ga\u00fa", "subasta de base gai"]}
{"file_name": "I-7", "text": "Compromiso y extorsi\u00f3n * RESUMEN Hacer compromisos, por ejemplo, mediante promesas y amenazas, permite a un jugador explotar las fortalezas de su propia posici\u00f3n estrat\u00e9gica, as\u00ed como las debilidades de la de sus oponentes. Los compromisos que un jugador puede asumir con credibilidad dependen de las circunstancias. En algunos, un jugador s\u00f3lo puede comprometerse a realizar una acci\u00f3n, en otros, puede comprometerse condicionalmente a las acciones de los dem\u00e1s jugadores. Algunas situaciones incluso permiten compromisos sobre compromisos o compromisos con acciones aleatorias. Exploramos las propiedades formales de estos tipos de compromiso -LRB- condicional -RRB- y sus interrelaciones. Para evitar inconsistencias entre los compromisos condicionales, asumimos un orden en el que los jugadores asumen sus compromisos. Central para nuestros an\u00e1lisis es la noci\u00f3n de extorsi\u00f3n, que definimos, para un orden dado de jugadores, como un perfil que contiene, para cada jugador, un compromiso \u00f3ptimo dados los compromisos de los jugadores que se comprometieron anteriormente. Sobre esta base, investigamos para diferentes tipos de compromiso si es ventajoso comprometerse m\u00e1s temprano que tarde, y c\u00f3mo los resultados obtenidos a trav\u00e9s de extorsiones se relacionan con la inducci\u00f3n hacia atr\u00e1s y la eficiencia de Pareto. 1. INTRODUCCI\u00d3N Desde un punto de vista, lo menos que uno puede esperar de la teor\u00eda de juegos es que proporcione una respuesta a la pregunta de qu\u00e9 acciones maximizan la utilidad esperada de un agente en situaciones de toma de decisiones interactiva. Desde esta perspectiva, el modelo formal de un juego en forma estrat\u00e9gica s\u00f3lo esboza las caracter\u00edsticas estrat\u00e9gicas de una situaci\u00f3n interactiva. Adem\u00e1s de simplemente elegir y realizar una acci\u00f3n de un conjunto de acciones, tambi\u00e9n puede haber otros caminos abiertos a un agente. Por ejemplo, la situaci\u00f3n estrat\u00e9gica del terreno puede ser tal que una promesa, una amenaza o una combinaci\u00f3n de ambas ser\u00eda m\u00e1s conducente a sus fines. Del mismo modo, una amenaza s\u00f3lo logra disuadir a un agente si se le puede hacer creer que el amenazador est\u00e1 obligado a ejecutar la amenaza, en caso de que sea ignorada. En este sentido, las promesas y amenazas implican esencialmente un compromiso por parte de quien las hace, restringiendo as\u00ed deliberadamente su libertad de elecci\u00f3n. Las promesas y las amenazas personifican uno de los fen\u00f3menos fundamentales y quiz\u00e1s m\u00e1s sorprendentes a primera vista en la teor\u00eda de juegos: puede ocurrir que un jugador pueda mejorar su posici\u00f3n estrat\u00e9gica limitando su propia libertad de acci\u00f3n. Por compromisos entenderemos tales limitaciones del propio espacio de acci\u00f3n. La acci\u00f3n en s\u00ed misma podr\u00eda verse como el compromiso \u00faltimo. Realizar una acci\u00f3n particular significa hacerlo con exclusi\u00f3n de todas las dem\u00e1s acciones. Los compromisos se presentan de diferentes formas y puede depender de las circunstancias cu\u00e1les se pueden contraer de manera cre\u00edble y cu\u00e1les no. Adem\u00e1s de simplemente comprometerse a realizar una acci\u00f3n, un agente podr\u00eda condicionar su compromiso a las acciones de otros agentes, como lo hace, por ejemplo, el secuestrador cuando promete liberar a un reh\u00e9n al recibir un rescate.mientras amenaza con cortarle otro dedo del pie, de lo contrario. Algunas situaciones incluso permiten compromisos sobre compromisos o compromisos con acciones aleatorias. Al centrarse en la selecci\u00f3n de acciones m\u00e1s que en los compromisos, podr\u00eda parecer que la concepci\u00f3n de la teor\u00eda de juegos como mera teor\u00eda de la decisi\u00f3n interactiva es demasiado estrecha. En este sentido, la visi\u00f3n de Schelling podr\u00eda parecer evidenciar una comprensi\u00f3n m\u00e1s amplia de lo que la teor\u00eda de juegos intenta lograr. Se podr\u00eda objetar que los compromisos podr\u00edan verse como las acciones de un juego m\u00e1s amplio. -LSB-... -RSB- Lo que queremos es una teor\u00eda que sistematice el estudio de los diversos ingredientes universales que conforman la estructura de movimientos de los juegos; un modelo demasiado abstracto los perder\u00e1. -LSB- 9, pp. 156-7 -RSB- Nuestra preocupaci\u00f3n son estas t\u00e1cticas de compromiso, ya sea que nuestro an\u00e1lisis se limite a situaciones en las que los jugadores pueden comprometerse en un orden determinado y donde asumimos los compromisos que los jugadores pueden hacer. son dados. A pesar de la advertencia de Schelling sobre un marco demasiado abstracto, nuestro enfoque se basar\u00e1 en la noci\u00f3n formal de extorsi\u00f3n, que propondremos en la Secci\u00f3n 4 como una t\u00e1ctica uniforme para una clase integral de situaciones en las que los compromisos pueden hacerse secuencialmente. Sobre esta base abordamos cuestiones como la utilidad de determinados tipos de compromiso en diferentes situaciones -LRB-, juegos estrat\u00e9gicos -RRB- o si es mejor comprometerse temprano que tarde. Tambi\u00e9n proporcionamos un marco para la evaluaci\u00f3n de cuestiones m\u00e1s generales de la teor\u00eda de juegos, como la relaci\u00f3n de las extorsiones con la inducci\u00f3n hacia atr\u00e1s o la eficiencia de Pareto. Por ejemplo, se ha argumentado que los compromisos son importantes para los agentes de software que interact\u00faan, as\u00ed como para el dise\u00f1o de mecanismos. En el primer entorno, la incapacidad de reprogramar un agente de software sobre la marcha puede verse como un compromiso con su especificaci\u00f3n y, por lo tanto, explotarse para fortalecer su posici\u00f3n estrat\u00e9gica en un entorno multiagente. Un mecanismo, por otro lado, podr\u00eda verse como un conjunto de compromisos que dirige el comportamiento de los jugadores de una determinada manera deseada -LRB- ver, por ejemplo, -LSB- 2 -RSB- -RRB-. Estos juegos analizan situaciones en las que un l\u00edder apuesta por una estrategia pura o mixta, y un n\u00famero de seguidores, que luego act\u00faan simult\u00e1neamente. Despu\u00e9s de discutir brevemente el trabajo relacionado en la Secci\u00f3n 2, presentamos el marco formal de la teor\u00eda de juegos, en el que definimos las nociones de un tipo de compromiso as\u00ed como compromisos condicionales e incondicionales -LRB- Secci\u00f3n 3 -RRB-. En la Secci\u00f3n 4 proponemos el concepto gen\u00e9rico de extorsi\u00f3n, que para cada tipo de compromiso captura la idea de un perfil de compromiso \u00f3ptimo. La secci\u00f3n 5 analiza brevemente algunos otros tipos de compromiso, como los compromisos inductivos, mixtos y condicionales mixtos. 2. TRABAJOS RELACIONADOS El compromiso es un concepto central en la teor\u00eda de juegos. La posibilidad de asumir compromisos distingue la teor\u00eda de juegos cooperativa de la no cooperativa -LSB- 4, 6 -RSB-. Los juegos de liderazgo, como se mencion\u00f3 en la introducci\u00f3n,analizar las apuestas por estrategias puras o mixtas en lo que es esencialmente un escenario de dos jugadores -LSB- 15, 16 -RSB-. Informalmente, Schelling -LSB- 9 -RSB- ha enfatizado la importancia de las promesas, amenazas y similares para una comprensi\u00f3n adecuada de la interacci\u00f3n social. En un nivel m\u00e1s formal, las amenazas tambi\u00e9n han figurado en la teor\u00eda de la negociaci\u00f3n. El juego de amenazas de Nash -LSB- 5 -RSB- y las amenazas racionales de Harsanyi -LSB- 3 -RSB- son dos importantes ejemplos tempranos. Adem\u00e1s, los compromisos han jugado un papel importante en la teor\u00eda de la selecci\u00f3n del equilibrio -LRB- v\u00e9ase, por ejemplo, -LSB- 13 -RSB-. En los \u00faltimos a\u00f1os, la teor\u00eda de juegos se ha vuelto casi indispensable como herramienta de investigaci\u00f3n para la inform\u00e1tica y la investigaci\u00f3n de m\u00faltiples agentes -LRB-. Los compromisos nunca han pasado desapercibidos -LRB- ver Figura 1: Comprometerse con una estrategia dominada puede ser ventajoso. por ejemplo, -LSB- 1, 11 -RSB- -RRB-. \u00daltimamente tambi\u00e9n los aspectos estrat\u00e9gicos de los compromisos han atra\u00eddo la atenci\u00f3n de los inform\u00e1ticos. As\u00ed, Conitzer y Sandholm -LSB- 2 -RSB- han estudiado la complejidad computacional de calcular la estrategia \u00f3ptima a seguir en forma normal y en juegos bayesianos. Sandholm and Lesser -LSB- 8 -RSB- emplean compromisos nivelados para el dise\u00f1o de sistemas multiagente en los que los acuerdos contractuales no son totalmente vinculantes. Otra conexi\u00f3n entre los compromisos y la inform\u00e1tica fue se\u00f1alada por Samet -LSB- 7 -RSB- y Tennenholtz -LSB- 12 -RSB-. Su punto de partida es la observaci\u00f3n de que los programas pueden utilizarse para formular compromisos que est\u00e9n condicionados a los programas de otros sistemas. Nuestro enfoque es similar al escenario de Stackleberg en el sentido de que asumimos un orden en el que los jugadores se comprometen. Sin embargo, consideramos varios tipos diferentes de compromisos, entre ellos los compromisos condicionales, y proponemos un concepto de soluci\u00f3n gen\u00e9rico. 6. RESUMEN Y CONCLUSI\u00d3N En algunas situaciones los agentes pueden fortalecer su posici\u00f3n estrat\u00e9gica comprometi\u00e9ndose con un curso de acci\u00f3n particular. Existen varios tipos de compromiso, por ejemplo, puro, mixto y condicional. El tipo de compromiso que un agente est\u00e1 en condiciones de asumir depende esencialmente de la situaci\u00f3n que se est\u00e9 considerando. Si los agentes se comprometen en un orden determinado, existe una t\u00e1ctica com\u00fan a la realizaci\u00f3n de compromisos de cualquier tipo, que hemos formalizado mediante el concepto de extorsi\u00f3n. Este concepto gen\u00e9rico de extorsi\u00f3n puede analizarse en abstracto. Adem\u00e1s, sobre esta base se pueden comparar formal y sistem\u00e1ticamente los distintos tipos de compromisos. Hemos visto que el tipo de compromiso que un agente puede asumir tiene un profundo impacto en lo que un agente puede lograr en una situaci\u00f3n similar a la de un juego. En algunas situaciones, a un jugador le ayuda mucho estar en condiciones de comprometerse condicionalmente, mientras que en otras, los compromisos mixtos ser\u00edan m\u00e1s rentables. Esto plantea la cuesti\u00f3n de los rasgos formales caracter\u00edsticos de las situaciones en las que es ventajoso para un actor poder contraer compromisos de un tipo particular.Otro tema que dejamos para futuras investigaciones es la complejidad computacional de encontrar una extorsi\u00f3n para los diferentes tipos de compromiso.", "keyphrases": ["comprometerse", "cre\u00edble", "teor\u00eda de juegos", "tomar decisiones", "posici\u00f3n estrat\u00e9gica", "libertad de acci\u00f3n", "sistema multiag", "distribuci\u00f3n de computaci\u00f3n", "mercado de electrones", "extorsionar", "conjunto de stackleberg", "compromiso de condici\u00f3n \u00f3ptima", "tipo de confirmaci\u00f3n secuencial", "inducir hip\u00f3tesis", "eficiencia de Pareto", "Pareto Effici Condit extorsionar"]}
{"file_name": "J-10", "text": "Comprender el comportamiento del usuario en los informes de comentarios en l\u00ednea RESUMEN Las rese\u00f1as en l\u00ednea se han vuelto cada vez m\u00e1s populares como una forma de juzgar la calidad de diversos productos y servicios. Trabajos anteriores han demostrado que los informes contradictorios y los sesgos subyacentes de los usuarios dificultan juzgar el verdadero valor de un servicio. En este art\u00edculo, investigamos los factores subyacentes que influyen en el comportamiento de los usuarios al enviar comentarios. Examinamos dos fuentes de informaci\u00f3n adem\u00e1s de las calificaciones num\u00e9ricas: la evidencia ling\u00fc\u00edstica del comentario textual que acompa\u00f1a a una rese\u00f1a y los patrones en la secuencia temporal de los informes. Primero mostramos que los grupos de usuarios que discuten ampliamente una determinada caracter\u00edstica tienen m\u00e1s probabilidades de ponerse de acuerdo sobre una calificaci\u00f3n com\u00fan para esa caracter\u00edstica. En segundo lugar, mostramos que la calificaci\u00f3n de un usuario refleja en parte la diferencia entre la calidad real y la expectativa previa de calidad inferida de revisiones anteriores. Ambos nos brindan una forma menos ruidosa de producir estimaciones de calificaci\u00f3n y revelar las razones detr\u00e1s del sesgo de los usuarios. Nuestras hip\u00f3tesis fueron validadas por evidencia estad\u00edstica de rese\u00f1as de hoteles en el sitio web de TripAdvisor. 1. MOTIVACIONES Consideran seriamente los comentarios en l\u00ednea al tomar decisiones de compra y est\u00e1n dispuestos a pagar primas de reputaci\u00f3n por productos o servicios que tienen buena reputaci\u00f3n. Sin embargo, an\u00e1lisis recientes plantean cuestiones importantes sobre la capacidad de los foros existentes para reflejar la calidad real de un producto. En ausencia de incentivos claros, los usuarios con una perspectiva moderada no se molestar\u00e1n en expresar sus opiniones, lo que da lugar a una muestra de rese\u00f1as no representativa. En estas circunstancias, utilizar la media aritm\u00e9tica para predecir la calidad -LRB- como lo hacen la mayor\u00eda de los foros -RRB- le da al usuario t\u00edpico un estimador con alta varianza que a menudo es falso. Mejorar la forma en que agregamos la informaci\u00f3n disponible a partir de rese\u00f1as en l\u00ednea requiere una comprensi\u00f3n profunda de los factores subyacentes que sesgan el comportamiento de calificaci\u00f3n de los usuarios. Hu et al. -LSB- 12 -RSB- propone el ``Modelo de alardear y gemir'' donde los usuarios califican s\u00f3lo si su utilidad del producto -LRB- extra\u00eddo de una distribuci\u00f3n normal -RRB- cae fuera de un intervalo mediano. Los autores concluyen que el modelo explica la distribuci\u00f3n emp\u00edrica de los informes y ofrece informaci\u00f3n sobre formas m\u00e1s inteligentes de estimar la verdadera calidad del producto. En el presente art\u00edculo ampliamos esta l\u00ednea de investigaci\u00f3n e intentamos explicar m\u00e1s hechos sobre el comportamiento de los usuarios cuando informan comentarios en l\u00ednea. Utilizando rese\u00f1as de hoteles reales del sitio web TripAdvisor2, consideramos dos fuentes adicionales de informaci\u00f3n adem\u00e1s de las calificaciones num\u00e9ricas b\u00e1sicas enviadas por los usuarios. La primera es la simple evidencia ling\u00fc\u00edstica procedente de la revisi\u00f3n textual que suele acompa\u00f1ar a las calificaciones num\u00e9ricas. Descubrimos que los usuarios que comentan m\u00e1s sobre la misma caracter\u00edstica tienen m\u00e1s probabilidades de ponerse de acuerdo sobre una calificaci\u00f3n num\u00e9rica com\u00fan para esa caracter\u00edstica en particular. De manera intuitiva, los comentarios extensos revelan la importancia de la funci\u00f3n para el usuario.Dado que las personas tienden a tener m\u00e1s conocimientos en los aspectos que consideran importantes, se podr\u00eda suponer que los usuarios que analizan una caracter\u00edstica determinada con m\u00e1s detalle tienen m\u00e1s autoridad para evaluar esa caracter\u00edstica. En segundo lugar, investigamos la relaci\u00f3n entre una rese\u00f1a Figura 1: La p\u00e1gina de TripAdvisor que muestra rese\u00f1as de un hotel popular de Boston. El nombre del hotel y los anuncios fueron borrados deliberadamente. y las revisiones que lo precedieron. Un examen detenido de las rese\u00f1as en l\u00ednea muestra que las calificaciones a menudo son parte de hilos de discusi\u00f3n, donde una publicaci\u00f3n no es necesariamente independiente de otras publicaciones. Se pueden ver, por ejemplo, usuarios que se esfuerzan por contradecir o estar de acuerdo con vehemencia con los comentarios de usuarios anteriores. Al analizar la secuencia temporal de los informes, concluimos que las revisiones pasadas influyen en los informes futuros, ya que crean alguna expectativa previa sobre la calidad del servicio. La percepci\u00f3n subjetiva del usuario est\u00e1 influenciada por la brecha entre la expectativa previa y el desempe\u00f1o real del servicio -LSB- 17, 18, 16, 21 -RSB- que luego se reflejar\u00e1 en la calificaci\u00f3n del usuario. Proponemos un modelo que captura la dependencia de las calificaciones de las expectativas previas y lo validamos utilizando los datos emp\u00edricos que recopilamos. Ambos resultados se pueden utilizar para mejorar la forma en que los mecanismos de reputaci\u00f3n agregan la informaci\u00f3n de rese\u00f1as individuales. Nuestro primer resultado se puede utilizar para determinar una estimaci\u00f3n de calidad caracter\u00edstica por caracter\u00edstica, donde para cada caracter\u00edstica, se considera un subconjunto diferente de rese\u00f1as -LRB-, es decir, aquellas con comentarios extensos sobre esa caracter\u00edstica -RRB-. El segundo conduce a un algoritmo que genera una estimaci\u00f3n m\u00e1s precisa de la calidad real.Nuestro primer resultado se puede utilizar para determinar una estimaci\u00f3n de calidad caracter\u00edstica por caracter\u00edstica, donde para cada caracter\u00edstica, se considera un subconjunto diferente de rese\u00f1as -LRB-, es decir, aquellas con comentarios extensos sobre esa caracter\u00edstica -RRB-. El segundo conduce a un algoritmo que genera una estimaci\u00f3n m\u00e1s precisa de la calidad real.Nuestro primer resultado se puede utilizar para determinar una estimaci\u00f3n de calidad caracter\u00edstica por caracter\u00edstica, donde para cada caracter\u00edstica, se considera un subconjunto diferente de rese\u00f1as -LRB-, es decir, aquellas con comentarios extensos sobre esa caracter\u00edstica -RRB-. El segundo conduce a un algoritmo que genera una estimaci\u00f3n m\u00e1s precisa de la calidad real.", "keyphrases": ["revisi\u00f3n en l\u00ednea", "mec\u00e1nico de reputaci\u00f3n", "estimaci\u00f3n de calidad caracter\u00edstica por caracter\u00edstica", "ausencia de incentivo claro", "utilidad del producto", "modelo de alardear y gemir", "tasa", "gran probabilidad bimodal", "distribuci\u00f3n en forma de u", "orientaci\u00f3n sem\u00e1ntica de la evaluaci\u00f3n del producto", "correlacionar", "gran lapso de tiempo"]}
{"file_name": "C-1", "text": "Descubrimiento de servicios de red escalable basado en UDDI * RESUMEN El descubrimiento eficiente de servicios de red es esencial para el \u00e9xito de la computaci\u00f3n en red. La estandarizaci\u00f3n de grids basados \u200b\u200ben servicios web ha resultado en la necesidad de implementar mecanismos escalables de descubrimiento de servicios web en grids. Aunque UDDI ha sido el est\u00e1ndar de facto de la industria para el descubrimiento de servicios web, impuso requisitos de replicaci\u00f3n estricta entre registros y falta de del control aut\u00f3nomo ha dificultado gravemente su despliegue y uso generalizados. Con la llegada de la computaci\u00f3n grid, la cuesti\u00f3n de la escalabilidad de UDDI se convertir\u00e1 en un obst\u00e1culo que impedir\u00e1 su implementaci\u00f3n en grids. En este art\u00edculo presentamos nuestra arquitectura de descubrimiento de servicios web distribuidos, llamada DUDE -LRB- Distributed UDDI Deployment Engine -RRB-. DUDE aprovecha DHT -LRB- Distributed Hash Tables -RRB- como mecanismo de encuentro entre m\u00faltiples registros UDDI. DUDE permite a los consumidores consultar m\u00faltiples registros y, al mismo tiempo, permite a las organizaciones tener control aut\u00f3nomo sobre sus registros. . Basado en un prototipo preliminar en PlanetLab, creemos que la arquitectura DUDE puede soportar la distribuci\u00f3n efectiva de registros UDDI, haciendo as\u00ed que UDDI sea m\u00e1s robusto y tambi\u00e9n abordando sus problemas de escala. Adem\u00e1s, la arquitectura DUDE para distribuci\u00f3n escalable se puede aplicar m\u00e1s all\u00e1 de UDDI a cualquier mecanismo de descubrimiento de servicios de red. 1. INTRODUCCI\u00d3N El descubrimiento eficiente de servicios grid es esencial para el \u00e9xito de la computaci\u00f3n grid. Mecanismos de descubrimiento que se implementar\u00e1n en las redes. Los servicios de descubrimiento de redes brindan la capacidad de monitorear y descubrir recursos y servicios en las redes. Proporcionan la capacidad de consultar y suscribirse a informaci\u00f3n de recursos/servicios. El estado de los datos debe mantenerse en un estado suave para que la informaci\u00f3n m\u00e1s reciente est\u00e9 siempre disponible. La informaci\u00f3n recopilada debe proporcionarse a una variedad de sistemas con el fin de utilizar la cuadr\u00edcula o proporcionar informaci\u00f3n resumida. Sin embargo, el problema fundamental es la necesidad de ser escalable para manejar enormes cantidades de datos de m\u00faltiples fuentes. La comunidad de servicios web ha abordado la necesidad de descubrimiento de servicios, antes de que se anticiparan las redes, a trav\u00e9s de un est\u00e1ndar industrial llamado UDDI. Sin embargo, aunque UDDI ha sido el est\u00e1ndar de facto de la industria para el descubrimiento de servicios web, ha impuesto requisitos de replicaci\u00f3n estricta entre registros y la falta de control aut\u00f3nomo, entre otras cosas, ha obstaculizado gravemente su implementaci\u00f3n y uso generalizados -LSB- 7 -RSB- . Con la llegada de la computaci\u00f3n grid, el problema de la escalabilidad de UDDI se convertir\u00e1 en un obst\u00e1culo que impedir\u00e1 su implementaci\u00f3n en grids. Este art\u00edculo aborda el problema de la escalabilidad y una forma de encontrar servicios en m\u00faltiples registros en UDDI mediante el desarrollo de una arquitectura de descubrimiento de servicios web distribuidos. La distribuci\u00f3n de la funcionalidad UDDI se puede lograr de m\u00faltiples maneras y quiz\u00e1s utilizando diferentes infraestructuras/plataformas inform\u00e1ticas distribuidas -LRB-, por ejemplo, CORBA, DCE, etc. -RRB-.En este art\u00edculo exploramos c\u00f3mo se puede aprovechar la tecnolog\u00eda Distributed Hash Table -LRB-DHT-RRB- para desarrollar una arquitectura de descubrimiento de servicios web distribuidos escalable. Un DHT es un sistema distribuido peer-to-peer -LRB-P2P-RRB- que forma una superposici\u00f3n estructurada que permite un enrutamiento m\u00e1s eficiente que la red subyacente. El primer factor motivador es la simplicidad inherente de la abstracci\u00f3n put/get que proporcionan los DHT, lo que facilita la creaci\u00f3n r\u00e1pida de aplicaciones sobre los DHT. Otras plataformas inform\u00e1ticas distribuidas/middleware, aunque proporcionan m\u00e1s funcionalidad, tienen una sobrecarga y una complejidad mucho mayores. El segundo factor motivador surge del hecho de que los DHT son una herramienta relativamente nueva para crear aplicaciones distribuidas y nos gustar\u00eda probar su potencial aplic\u00e1ndolo al problema de la distribuci\u00f3n de UDDI. En la siguiente secci\u00f3n, brindamos una breve descripci\u00f3n general de los servicios de informaci\u00f3n de red, UDDI y sus limitaciones, seguida de una descripci\u00f3n general de los DHT en la Secci\u00f3n 3. La Secci\u00f3n 4 describe nuestra arquitectura propuesta con detalles sobre los casos de uso. En la Secci\u00f3n 5, el Art\u00edculo 2 describe nuestra implementaci\u00f3n actual, seguida de nuestras conclusiones en la Secci\u00f3n 6. La Secci\u00f3n 7 analiza el trabajo relacionado en esta \u00e1rea y la Secci\u00f3n 8 contiene nuestras observaciones finales. 2. ANTECEDENTES 2.1 Descubrimiento del servicio Grid La computaci\u00f3n Grid se basa en est\u00e1ndares que utilizan tecnolog\u00eda de servicios web. En la arquitectura presentada en -LSB- 6 -RSB-, la funci\u00f3n de descubrimiento de servicios se asigna a un servicio Grid especializado llamado Registro. Su funci\u00f3n b\u00e1sica lo hace similar al registro UDDI. Para lograr escalabilidad, los servicios Index de diferentes contenedores Globus pueden registrarse entre s\u00ed de forma jer\u00e1rquica para agregar datos. Espec\u00edficamente, este enfoque no es una buena opci\u00f3n para los sistemas que intentan explotar la convergencia de la computaci\u00f3n grid y peer-to-peer -LSB- 5 -RSB-. 2.2 UDDI M\u00e1s all\u00e1 de la computaci\u00f3n grid, el problema del descubrimiento de servicios debe abordarse de manera m\u00e1s general en la comunidad de servicios web. Una vez m\u00e1s, la escalabilidad es una preocupaci\u00f3n importante, ya que millones de compradores que buscan servicios espec\u00edficos necesitan encontrar todos los vendedores potenciales del servicio que puedan satisfacer sus necesidades. Aunque existen diferentes formas de hacerlo, los comit\u00e9s de est\u00e1ndares de servicios web abordan este requisito a trav\u00e9s de una especificaci\u00f3n denominada UDDI -LRB- Universal Descripci\u00f3n, Descubrimiento e Integraci\u00f3n -RRB-. Un registro UDDI permite a una empresa ingresar tres tipos de informaci\u00f3n en un registro UDDI: p\u00e1ginas blancas, p\u00e1ginas amarillas y p\u00e1ginas verdes. La intenci\u00f3n de la UDDI es funcionar como un registro de servicios del mismo modo que las p\u00e1ginas amarillas son un registro de empresas. Al igual que en las p\u00e1ginas amarillas, las empresas se registran a s\u00ed mismas y a sus servicios en diferentes categor\u00edas. En UDDI, las P\u00e1ginas Blancas son un listado de entidades comerciales. Las p\u00e1ginas verdes representan la informaci\u00f3n t\u00e9cnica necesaria para invocar un servicio determinado. As\u00ed, al navegar por un registro UDDI,un desarrollador deber\u00eda poder localizar un servicio y una empresa y descubrir c\u00f3mo invocar el servicio. Cuando se ofreci\u00f3 inicialmente UDDI, ofrec\u00eda un gran potencial. Sin embargo, hoy nos encontramos con que UDDI no se ha implementado ampliamente en Internet. De hecho, los \u00fanicos usos conocidos de UDDI son los llamados registros UDDI privados dentro de los l\u00edmites de una empresa. Los lectores pueden consultar -LSB- 7 -RSB- para ver un art\u00edculo reciente que analiza las deficiencias de UDDI y las propiedades de un registro de servicios ideal. La mejora del est\u00e1ndar UDDI contin\u00faa con toda su fuerza y \u200b\u200b\u200b\u200bla versi\u00f3n 3 de UDDI -LRB- V3 -RRB- fue aprobada recientemente como est\u00e1ndar OASIS. Sin embargo, la UDDI hoy tiene problemas que no han sido abordados, como la escalabilidad y la autonom\u00eda de los registros individuales. UDDI V3 proporciona mayor soporte para entornos de registros m\u00faltiples basados \u200b\u200ben la portabilidad de claves. Al permitir que las claves se vuelvan a registrar en m\u00faltiples registros, se habilita de manera efectiva la capacidad de vincular registros en varias topolog\u00edas. Sin embargo, en este momento no se proporciona ninguna descripci\u00f3n normativa de estas topolog\u00edas en la especificaci\u00f3n UDDI. Las mejoras dentro de UDDI V3 que permiten el soporte para entornos de registros m\u00faltiples son significativas y abren la posibilidad de realizar investigaciones adicionales sobre c\u00f3mo se pueden implementar entornos de registros m\u00faltiples. Un escenario de implementaci\u00f3n recomendado propuesto por la especificaci\u00f3n UDDI V3.0.2 es utilizar los registros comerciales UDDI como registros ra\u00edz, y es posible habilitar esto usando nuestra soluci\u00f3n. 2.3 Tablas Hash Distribuidas Una Tabla Hash Distribuida -LRB-DHT-RRB- es un sistema distribuido peer-to-peer -LRB-P2P-RRB- que forma una superposici\u00f3n estructurada que permite un enrutamiento m\u00e1s eficiente que la red subyacente. Mantiene una colecci\u00f3n de pares clave-valor en los nodos que participan en esta estructura gr\u00e1fica. Para nuestra implementaci\u00f3n, una clave es el hash de una palabra clave de un nombre o descripci\u00f3n de un servicio. Habr\u00e1 varios valores para esta clave, uno para cada servicio que contenga la palabra clave. Al igual que cualquier otra estructura de datos de tabla hash, proporciona una interfaz simple que consta de operaciones put -LRB- -RRB- y get -LRB- -RRB-. Esto debe hacerse con solidez debido a la naturaleza transitoria de los nodos en los sistemas P2P. Las claves DHT se obtienen de un gran espacio de identificadores. Se aplica una funci\u00f3n hash, como MD5 o SHA-1, al nombre de un objeto para obtener su clave DHT. Los nodos en un DHT tambi\u00e9n se asignan al mismo espacio de identificador aplicando la funci\u00f3n hash a su identificador, como la direcci\u00f3n IP y el n\u00famero de puerto, o la clave p\u00fablica. El espacio de identificador se asigna a los nodos de forma distribuida y determinista, de modo que el enrutamiento y la b\u00fasqueda se puedan realizar de manera eficiente. Los nodos de un DHT mantienen enlaces con algunos de los otros nodos del DHT. El patr\u00f3n de estos enlaces se conoce como geometr\u00eda del DHT. Por ejemplo, en el Bamboo DHT -LSB- 11 -RSB-, y en el Pasteler\u00eda DHT -LSB- 8 -RSB- en el que se basa Bamboo,Los nodos mantienen enlaces con nodos vecinos y con otros nodos distantes que se encuentran en una tabla de enrutamiento. La tabla de enrutamiento permite un enrutamiento superpuesto eficiente. Para lograr un enrutamiento o b\u00fasqueda consistente, se debe enrutar una clave DHT al nodo con el identificador num\u00e9ricamente m\u00e1s cercano. Para obtener detalles sobre c\u00f3mo se construyen y mantienen las tablas de enrutamiento, se remite al lector a -LSB- 8, 11 -RSB-. 5. TRABAJO RELACIONADO En -LSB-18-RSB- se ha propuesto un marco para el descubrimiento de servicios basados \u200b\u200ben QoS en redes. UDDIe, un registro UDDI extendido para publicar y descubrir servicios basados \u200b\u200ben par\u00e1metros de QoS, se propone en -LSB-19-RSB-. Nuestro trabajo es complementario ya que nos enfocamos en c\u00f3mo federar los registros UDDI y abordar el problema de escalabilidad con UDDI. El proxy DUDE puede publicar las propiedades de servicio admitidas por UDDIe en DHT y admitir consultas de rango utilizando t\u00e9cnicas propuestas para dichas consultas en DHT. Entonces podremos ofrecer los beneficios de escalabilidad de nuestra soluci\u00f3n actual a los registros UDDI y UDDIe. Se ha estudiado el descubrimiento de servicios que cumplan con los requisitos de QoS y precios en el context de una econom\u00eda de red, de modo que los programadores de redes puedan utilizar varios modelos de mercado, como mercados de productos b\u00e1sicos y subastas. Para ello se propuso el Directorio de Mercados Grid -LSB- 20 -RSB-. Las descripciones de recursos y solicitudes se expresan en RDF Schema, un lenguaje de marcado sem\u00e1ntico. Las reglas de emparejamiento se expresan en TRIPLE, un lenguaje basado en Horn Logic. Aunque nuestra implementaci\u00f3n actual se centra en UDDI versi\u00f3n 2, en el futuro consideraremos extensiones sem\u00e1nticas a UDDI, WS-Discovery -LSB- 16 -RSB- y otros est\u00e1ndares de computaci\u00f3n Grid como Monitoring and Discovery Service -LRB- MDS -RRB- -LSB. - 10 -RSB-. Entonces, la extensi\u00f3n m\u00e1s simple de nuestro trabajo podr\u00eda implicar el uso de DHT para realizar una b\u00fasqueda inicial basada en sintaxis para identificar los registros locales con los que es necesario contactar. La convergencia de la computaci\u00f3n grid y P2P se ha explorado en -LSB- 5 -RSB-. Se ha construido un servicio UDDI federado -LSB- 4 -RSB- sobre el sistema de publicaci\u00f3n-suscripci\u00f3n PlanetP -LSB- 3 -RSB- para comunidades P2P no estructuradas. El foco de este trabajo ha estado en la manejabilidad del servicio federado. El servicio UDDI se trata como un servicio de aplicaci\u00f3n del art\u00edculo 2 que debe gestionarse en su marco. Por lo tanto, no abordan la cuesti\u00f3n de la escalabilidad en UDDI y, en su lugar, utilizan una replicaci\u00f3n simple. En -LSB- 21 -RSB-, los autores describen un sistema de extensi\u00f3n UDDI -LRB- UX -RRB- que lanza una consulta federada solo si los resultados encontrados localmente no son adecuados. Si bien el servidor UX se posiciona como un intermediario similar al proxy UDDI descrito en nuestro marco DUDE, se centra m\u00e1s en el marco QoS y no intenta implementar un mecanismo de federaci\u00f3n transparente como nuestro enfoque basado en DHT. En -LSB-22-RSB-D2HT se describe un marco de descubrimiento construido sobre DHT. Sin embargo, hemos optado por utilizar UDDI adem\u00e1s de DHT. 6. CONCLUSIONES Y TRABAJO FUTURO En este art\u00edculo,Hemos descrito una arquitectura distribuida para soportar el descubrimiento a gran escala de servicios web. Nuestra arquitectura permitir\u00e1 a las organizaciones mantener un control aut\u00f3nomo sobre sus registros UDDI y al mismo tiempo permitir\u00e1 a los clientes consultar m\u00faltiples registros simult\u00e1neamente. Con base en las pruebas de prototipos iniciales, creemos que la arquitectura DUDE puede respaldar la distribuci\u00f3n efectiva de registros UDDI, lo que hace que UDDI sea m\u00e1s s\u00f3lido y tambi\u00e9n aborda sus problemas de escalamiento. El documento ha resuelto los problemas de escalabilidad con UDDI pero no excluye la aplicaci\u00f3n de este enfoque a otros mecanismos de descubrimiento de servicios. Un ejemplo de otro mecanismo de descubrimiento de servicios que podr\u00eda beneficiarse de este enfoque es el MDS de Globus Toolkit. Adem\u00e1s, planeamos investigar otros aspectos del descubrimiento de servicios de red que ampl\u00eden este trabajo. Adem\u00e1s, planeamos revisar las API de servicio para una soluci\u00f3n Grid Service Discovery aprovechando las soluciones y especificaciones disponibles, as\u00ed como el trabajo presentado en este documento.", "keyphrases": ["descubrimiento de servicio de red", "udd\u00ed", "distribuir servicio web descubrimiento arquitectura", "dht base uddi registros jer\u00e1rquicos", "implementar problema", "c\u00f3digo dht de bamb\u00fa", "b\u00fasqueda que no distingue entre may\u00fasculas y min\u00fasculas", "pregunta", "prefijo de disponibilidad m\u00e1s largo", "descubrimiento de servicios qo-base", "control aut\u00f3nomo", "registro uddi", "emisi\u00f3n escalable", "estado blando"]}
{"file_name": "J-22", "text": "Apuestas a permutaciones RESUMEN Consideramos un escenario de apuestas de permutaci\u00f3n, donde las personas apuestan al orden final de n candidatos: por ejemplo, el resultado de una carrera de caballos. Examinamos el problema del subastador de igualar apuestas sin riesgo o, de manera equivalente, encontrar oportunidades de arbitraje entre las apuestas propuestas. Exigir a los postores que enumeren expl\u00edcitamente los pedidos en los que les gustar\u00eda apostar es antinatural e intratable, \u00a1porque el n\u00famero de pedidos es n! \u00a1y el n\u00famero de subconjuntos de pedidos es 2n! . Proponemos dos lenguajes de apuestas expresivos que parecen naturales para los postores y examinamos la complejidad computacional del problema del subastador en cada caso. Las apuestas de subconjunto permiten a los operadores apostar a que un candidato terminar\u00e1 clasificado entre alg\u00fan subconjunto de posiciones en el orden final, por ejemplo, `` el caballo A terminar\u00e1 en las posiciones 4, 9 o 13-21 '', o que una posici\u00f3n ser\u00e1 tomado por alg\u00fan subconjunto de candidatos, por ejemplo `` el caballo A, B o D terminar\u00e1 en la posici\u00f3n 2 ''. Para las apuestas de subconjuntos, mostramos que el problema del subastador se puede resolver en tiempo polin\u00f3mico si las \u00f3rdenes son divisibles. Las apuestas por pares permiten a los operadores apostar sobre si un candidato terminar\u00e1 en una clasificaci\u00f3n m\u00e1s alta que otro candidato, por ejemplo, \"el caballo A vencer\u00e1 al caballo B\". Demostramos que el problema del subastador se vuelve NP-dif\u00edcil para las apuestas por parejas. Identificamos una condici\u00f3n suficiente para la existencia de un partido de apuestas por parejas que puede verificarse en tiempo polin\u00f3mico. Tambi\u00e9n mostramos que un algoritmo codicioso natural proporciona una mala aproximaci\u00f3n para \u00f3rdenes indivisibles. 1. INTRODUCCI\u00d3N Comprar o vender un t\u00edtulo financiero en efecto es una apuesta sobre el valor del t\u00edtulo. Por ejemplo, comprar una acci\u00f3n es apostar a que el valor de la acci\u00f3n es mayor que su precio actual. Cada comerciante eval\u00faa su beneficio esperado para decidir la cantidad a comprar o vender de acuerdo con su propia informaci\u00f3n y evaluaci\u00f3n de probabilidad subjetiva. La interacci\u00f3n colectiva de todas las apuestas conduce a un equilibrio que refleja una agregaci\u00f3n de la informaci\u00f3n y creencias de todos los traders. Considere comprar un valor a un precio de cincuenta y dos centavos, que pagar\u00e1 1 d\u00f3lar si y s\u00f3lo si un dem\u00f3crata gana las elecciones presidenciales de Estados Unidos en 2008. En este caso de un valor contingente a un evento, el precio (el valor de mercado del valor) corresponde directamente a la probabilidad estimada del evento. Casi todos los intercambios financieros y de apuestas existentes vinculan a socios comerciales bilaterales. Por ejemplo, un operador dispuesto a aceptar una p\u00e9rdida de x d\u00f3lares si un dem\u00f3crata no gana a cambio de una ganancia en d\u00f3lares si un dem\u00f3crata gana se enfrenta a un segundo operador dispuesto a aceptar lo contrario. Sin embargo, en muchos escenarios, incluso si no existen acuerdos bilaterales entre los comerciantes, los acuerdos multilaterales pueden ser posibles. Proponemos un intercambio donde los comerciantes tengan una flexibilidad considerable para expresar sus apuestas de forma natural y sucinta.y examinar la complejidad computacional del problema de emparejamiento resultante del subastador al identificar acuerdos bilaterales y multilaterales. En particular, nos centramos en un escenario donde los comerciantes apuestan por el resultado de una competencia entre n candidatos. Por ejemplo, supongamos que hay n candidatos en una elecci\u00f3n -LRB- o n caballos en una carrera, etc. -RRB- y por tanto n! posibles ordenamientos de candidatos despu\u00e9s del recuento final de votos. Como veremos, el problema de correspondencia se puede plantear como un programa lineal o entero, dependiendo de si los \u00f3rdenes son divisibles o indivisibles, respectivamente. Intentar reducir el problema a un problema de coincidencia bilateral creando expl\u00edcitamente n! valores, uno para cada orden final posible, es engorroso para los comerciantes y computacionalmente inviable incluso para n de tama\u00f1o modesto. Adem\u00e1s, la atenci\u00f3n de los comerciantes se repartir\u00eda entre n! opciones independientes, lo que hace que la probabilidad de que dos operadores converjan al mismo tiempo y en el mismo lugar parezca remota. Existe un equilibrio entre la expresividad del lenguaje de licitaci\u00f3n y la complejidad computacional del problema de emparejamiento. Queremos ofrecer a los operadores el lenguaje de ofertas m\u00e1s expresivo posible manteniendo al mismo tiempo la viabilidad computacional. Exploramos dos lenguajes de oferta que parecen naturales desde la perspectiva del comerciante. Las apuestas de subconjuntos, descritas en la Secci\u00f3n 3.2, permiten a los operadores apostar sobre qu\u00e9 posiciones en la clasificaci\u00f3n caer\u00e1 un candidato, por ejemplo, \"el candidato D terminar\u00e1 en la posici\u00f3n 1, 3-5 o 10\". De manera sim\u00e9trica, los operadores tambi\u00e9n pueden apostar sobre qu\u00e9 candidatos se ubicar\u00e1n en una posici\u00f3n particular. En la Secci\u00f3n 4, derivamos un algoritmo de tiempo polinomial para igualar apuestas de subconjuntos -LRB- divisibles -RRB-. Las apuestas por pares, descritas en la Secci\u00f3n 3.3, permiten a los operadores apostar sobre la clasificaci\u00f3n final de dos candidatos cualesquiera, por ejemplo, \"el candidato D derrotar\u00e1 al candidato R\". En la Secci\u00f3n 5, mostramos que el emparejamiento \u00f3ptimo de apuestas de pares -LRB- divisibles o indivisibles -RRB- es NP-dif\u00edcil, a trav\u00e9s de una reducci\u00f3n del problema del conjunto de arco de retroalimentaci\u00f3n m\u00ednimo no ponderado. Tambi\u00e9n proporcionamos una condici\u00f3n suficiente polinomialmente verificable para la existencia de un partido de apuestas por pares y mostramos que un algoritmo codicioso ofrece una mala aproximaci\u00f3n para apuestas por pares indivisibles. 2. ANTECEDENTES Y TRABAJOS RELACIONADOS Consideramos apuestas de permutaci\u00f3n, o apuestas sobre el resultado de una competencia entre n candidatos. El resultado final del ES del estado es una clasificaci\u00f3n ordinal de los n candidatos. Por ejemplo, los candidatos podr\u00edan ser caballos en una carrera y el resultado ser\u00eda la lista de caballos en orden creciente de sus tiempos de finalizaci\u00f3n. El espacio de estados S contiene todos los n! permutaciones de candidatos mutuamente excluyentes y exhaustivas. En la pr\u00e1ctica, en los hip\u00f3dromos, cada uno de estos diferentes tipos de apuestas se procesa en grupos o grupos separados. En cambio, describimos un intercambio central donde todas las apuestas sobre el resultado se procesan juntas, agregando as\u00ed liquidez y garantizando que la inferencia informativa se produzca autom\u00e1ticamente. Idealmente,Nos gustar\u00eda permitir que los operadores apuesten por cualquier propiedad del orden final que deseen, expresada exactamente en el idioma que prefieran. En la pr\u00e1ctica, permitir un lenguaje demasiado flexible crea una carga computacional para el subastador que intenta emparejar a los comerciantes dispuestos. Exploramos el equilibrio entre la expresividad del lenguaje de licitaci\u00f3n y la complejidad computacional del problema de emparejamiento. Consideramos un marco en el que las personas proponen comprar valores que pagan 1 d\u00f3lar si y s\u00f3lo si alguna propiedad del orden final es verdadera. Los comerciantes indican el precio que est\u00e1n dispuestos a pagar por acci\u00f3n y la cantidad de acciones que les gustar\u00eda comprar. Una orden divisible permite al comerciante recibir menos acciones de las solicitadas, siempre que se cumpla la restricci\u00f3n de precio; un orden indivisible es un orden de todo o nada. valores, uno para cada estado s ES -LRB- o de hecho cualquier conjunto de n! valores linealmente independientes -RRB-. Se trata del denominado mercado de valores completo Arrow-Debreu -LSB- 1 -RSB- para nuestro entorno. En la pr\u00e1ctica, los traders no quieren lidiar con especificaciones de bajo nivel de \u00f3rdenes completas: la gente piensa de manera m\u00e1s natural en t\u00e9rminos de propiedades de alto nivel de las \u00f3rdenes. Adem\u00e1s, operar n! valores es inviable en la pr\u00e1ctica desde un punto de vista computacional a medida que n crece. Un lenguaje de oferta muy simple podr\u00eda permitir a los operadores apostar s\u00f3lo sobre qui\u00e9n gana la competencia, como se hace en el grupo \"ganador\" en los hip\u00f3dromos. El problema de correspondencia correspondiente es polin\u00f3mico, sin embargo el lenguaje no es muy expresivo. Un operador que cree que A derrotar\u00e1 a B, pero que ninguno de los dos ganar\u00e1 directamente, no puede transmitir su informaci\u00f3n al mercado de manera \u00fatil. El espacio de precios del mercado revela las estimaciones colectivas de las probabilidades de ganar, pero nada m\u00e1s. Nuestro objetivo es encontrar lenguajes que sean lo m\u00e1s expresivos e intuitivos posible y que revelen la mayor cantidad de informaci\u00f3n posible, manteniendo la viabilidad computacional. Nuestro trabajo est\u00e1 en analog\u00eda directa con el trabajo de Fortnow et. Mientras que exploramos la combinatoria de permutaciones, Fortnow et. Alabama. Explora la combinatoria booleana. Los autores consideran un espacio de estados de los 2n resultados posibles de n variables binarias. Los comerciantes expresan sus apuestas en l\u00f3gica booleana. Los autores muestran que el emparejamiento divisible es co-NP-completo y el emparejamiento indivisible es p2-completo. Hanson -LSB- 9 -RSB- describe un mecanismo de reglas de puntuaci\u00f3n del mercado que puede permitir apostar sobre un n\u00famero combinatorio de resultados. El mercado comienza con una distribuci\u00f3n de probabilidad conjunta entre todos los resultados. Funciona como una versi\u00f3n secuencial de una regla de puntuaci\u00f3n. Cualquier operador puede cambiar la distribuci\u00f3n de probabilidad siempre que acepte pagar al operador m\u00e1s reciente de acuerdo con la regla de puntuaci\u00f3n. El creador de mercado paga al \u00faltimo operador. Por lo tanto, asume riesgos y puede incurrir en p\u00e9rdidas. Los mecanismos de reglas de puntuaci\u00f3n del mercado tienen la buena propiedad de que la p\u00e9rdida del creador de mercado en el peor de los casos es limitada. Sin embargo, los aspectos computacionales sobre c\u00f3mo operar el mecanismo no se han explorado completamente.Nuestros mecanismos cuentan con un subastador que no asume ning\u00fan riesgo y s\u00f3lo iguala pedidos. Las subastas combinatorias permiten a los postores asignar valores distintos a paquetes de bienes en lugar de solo a bienes individuales. Por lo general, no se consideran la incertidumbre y el riesgo y el problema central del subastador es maximizar el bienestar social. Nuestros mecanismos permiten a los operadores construir apuestas para un evento con n! resultados. Se tienen en cuenta la incertidumbre y el riesgo, y el problema del subastador es explorar oportunidades de arbitraje y igualar las apuestas sin riesgo. 6. CONCLUSI\u00d3N Consideramos un escenario de apuestas de permutaci\u00f3n, donde los operadores apuestan por el orden final de n candidatos. Si bien es antinatural e intratable permitir que los operadores apuesten directamente al n! diferentes ordenamientos finales, proponemos dos lenguajes de apuestas expresivos, apuestas de subconjuntos y apuestas de pares. En un mercado de apuestas de subconjuntos, los operadores pueden apostar en un subconjunto de posiciones que ocupa un candidato o en un subconjunto de candidatos que ocupan una posici\u00f3n espec\u00edfica en el pedido final. Las apuestas por pares permiten a los operadores apostar sobre si un candidato determinado ocupa un lugar m\u00e1s alto que otro candidato determinado. Examinamos el problema del subastador de igualar \u00f3rdenes sin incurrir en riesgos. Encontramos que en un mercado de apuestas de subconjuntos un subastador puede encontrar el conjunto \u00f3ptimo y la cantidad de \u00f3rdenes a aceptar de modo que su beneficio en el peor de los casos se maximice en tiempo polin\u00f3mico si las \u00f3rdenes son divisibles. La complejidad cambia dr\u00e1sticamente en las apuestas por parejas. Demostramos que el problema de emparejamiento \u00f3ptimo para el subastador es NP-dif\u00edcil para apuestas por pares con \u00f3rdenes tanto indivisibles como divisibles mediante reducciones al problema del conjunto de arco de retroalimentaci\u00f3n m\u00ednimo. Identificamos una condici\u00f3n suficiente para la existencia de una coincidencia, que puede verificarse en tiempo polin\u00f3mico. Se ha demostrado que un algoritmo codicioso natural proporciona una mala aproximaci\u00f3n para las apuestas de pares indivisibles. Interesantes preguntas abiertas para nuestras apuestas de permutaci\u00f3n incluyen la complejidad computacional del emparejamiento indivisible \u00f3ptimo para apuestas de subconjuntos y la condici\u00f3n necesaria para la existencia de un partido en los mercados de apuestas de pares. Estamos interesados \u200b\u200ben explorar m\u00e1s a fondo mejores algoritmos de aproximaci\u00f3n para los mercados de apuestas por parejas.En un mercado de apuestas de subconjuntos, los operadores pueden apostar en un subconjunto de posiciones que ocupa un candidato o en un subconjunto de candidatos que ocupan una posici\u00f3n espec\u00edfica en el pedido final. Las apuestas por pares permiten a los operadores apostar sobre si un candidato determinado ocupa un lugar m\u00e1s alto que otro candidato determinado. Examinamos el problema del subastador de igualar \u00f3rdenes sin incurrir en riesgos. Encontramos que en un mercado de apuestas de subconjuntos un subastador puede encontrar el conjunto \u00f3ptimo y la cantidad de \u00f3rdenes a aceptar de modo que su beneficio en el peor de los casos se maximice en tiempo polin\u00f3mico si las \u00f3rdenes son divisibles. La complejidad cambia dr\u00e1sticamente en las apuestas por parejas. Demostramos que el problema de emparejamiento \u00f3ptimo para el subastador es NP-dif\u00edcil para apuestas por pares con \u00f3rdenes tanto indivisibles como divisibles mediante reducciones al problema del conjunto de arco de retroalimentaci\u00f3n m\u00ednimo. Identificamos una condici\u00f3n suficiente para la existencia de una coincidencia, que puede verificarse en tiempo polin\u00f3mico. Se ha demostrado que un algoritmo codicioso natural proporciona una mala aproximaci\u00f3n para las apuestas de pares indivisibles. Interesantes preguntas abiertas para nuestras apuestas de permutaci\u00f3n incluyen la complejidad computacional del emparejamiento indivisible \u00f3ptimo para apuestas de subconjuntos y la condici\u00f3n necesaria para la existencia de un partido en los mercados de apuestas de pares. Estamos interesados \u200b\u200ben explorar m\u00e1s a fondo mejores algoritmos de aproximaci\u00f3n para los mercados de apuestas por parejas.En un mercado de apuestas de subconjuntos, los operadores pueden apostar en un subconjunto de posiciones que ocupa un candidato o en un subconjunto de candidatos que ocupan una posici\u00f3n espec\u00edfica en el pedido final. Las apuestas por pares permiten a los operadores apostar sobre si un candidato determinado ocupa un lugar m\u00e1s alto que otro candidato determinado. Examinamos el problema del subastador de igualar \u00f3rdenes sin incurrir en riesgos. Encontramos que en un mercado de apuestas de subconjuntos un subastador puede encontrar el conjunto \u00f3ptimo y la cantidad de \u00f3rdenes a aceptar de modo que su beneficio en el peor de los casos se maximice en tiempo polin\u00f3mico si las \u00f3rdenes son divisibles. La complejidad cambia dr\u00e1sticamente en las apuestas por parejas. Demostramos que el problema de emparejamiento \u00f3ptimo para el subastador es NP-dif\u00edcil para apuestas por pares con \u00f3rdenes tanto indivisibles como divisibles mediante reducciones al problema del conjunto de arco de retroalimentaci\u00f3n m\u00ednimo. Identificamos una condici\u00f3n suficiente para la existencia de una coincidencia, que puede verificarse en tiempo polin\u00f3mico. Se ha demostrado que un algoritmo codicioso natural proporciona una mala aproximaci\u00f3n para las apuestas de pares indivisibles. Interesantes preguntas abiertas para nuestras apuestas de permutaci\u00f3n incluyen la complejidad computacional del emparejamiento indivisible \u00f3ptimo para apuestas de subconjuntos y la condici\u00f3n necesaria para la existencia de un partido en los mercados de apuestas de pares. Estamos interesados \u200b\u200ben explorar m\u00e1s a fondo mejores algoritmos de aproximaci\u00f3n para los mercados de apuestas por parejas.", "keyphrases": ["apuesta permuta", "apuesta de subconjunto", "socio comercial bilateral", "algoritmo polin\u00f3mico-tiempo", "informar agregado", "combinador permutado", "mercado de apuestas de par", "gr\u00e1fico bipartito", "retroalimentaci\u00f3n m\u00ednima", "algoritmo codicioso", "transformada polinomial compleja"]}
{"file_name": "I-31", "text": "Razonamiento sobre juicios y agregaci\u00f3n de preferencias \u25e6 RESUMEN Los agentes que deben llegar a acuerdos con otros agentes necesitan razonar sobre c\u00f3mo sus preferencias, juicios y creencias podr\u00edan agregarse con los de otros mediante los mecanismos de elecci\u00f3n social que gobiernan sus interacciones. El campo recientemente emergente de agregaci\u00f3n de juicios estudia la agregaci\u00f3n desde una perspectiva l\u00f3gica y considera c\u00f3mo se pueden agregar m\u00faltiples conjuntos de f\u00f3rmulas l\u00f3gicas en un \u00fanico conjunto consistente. Como caso especial, se puede considerar que la agregaci\u00f3n de juicios subsume la agregaci\u00f3n de preferencias cl\u00e1sica. Presentamos una l\u00f3gica modal que pretende apoyar el razonamiento sobre escenarios de agregaci\u00f3n de juicios -LRB- y, por tanto, como caso especial, sobre agregaci\u00f3n de preferencias -RRB-: el lenguaje l\u00f3gico se interpreta directamente en reglas de agregaci\u00f3n de juicios. Presentamos una axiomatizaci\u00f3n s\u00f3lida y completa de tales reglas. Mostramos que la l\u00f3gica puede expresar reglas de agregaci\u00f3n como la votaci\u00f3n por mayor\u00eda; propiedades de las reglas como la independencia; y resultados como la paradoja discursiva, el teorema de Arrow y la paradoja de Condorcet, que son derivables como teoremas formales de la l\u00f3gica. La l\u00f3gica est\u00e1 parametrizada de tal manera que puede usarse como marco general para comparar las propiedades l\u00f3gicas de diferentes tipos de agregaci\u00f3n, incluida la agregaci\u00f3n de preferencias cl\u00e1sica. 1. INTRODUCCI\u00d3N En este art\u00edculo, nos interesan los formalismos de representaci\u00f3n del conocimiento para sistemas en los que los agentes necesitan agregar sus preferencias, juicios, creencias, etc. Por ejemplo, un agente puede necesitar razonar sobre la votaci\u00f3n mayoritaria en un grupo en el que est\u00e1. un miembro de. La agregaci\u00f3n de preferencias (combinar las relaciones de preferencia de los individuos sobre un conjunto de alternativas en una relaci\u00f3n de preferencia que representa las preferencias conjuntas del grupo mediante las llamadas funciones de bienestar social) ha sido ampliamente estudiada en la teor\u00eda de la elecci\u00f3n social -LSB- 2 -RSB- . El campo recientemente emergente de agregaci\u00f3n de juicios estudia la agregaci\u00f3n desde una perspectiva l\u00f3gica y analiza c\u00f3mo, dado un conjunto consistente de f\u00f3rmulas l\u00f3gicas para cada agente, que representan las creencias o juicios del agente, podemos agregarlos en un \u00fanico conjunto consistente de f\u00f3rmulas. Con este fin se han desarrollado una variedad de reglas de agregaci\u00f3n de juicios. Como caso especial, se puede considerar que la agregaci\u00f3n de juicios subsume la agregaci\u00f3n de preferencias -LSB- 5 -RSB-. En este art\u00edculo presentamos una l\u00f3gica, denominada L\u00f3gica de Agregaci\u00f3n de Juicios -LRB- jal -RRB-, para razonar sobre la agregaci\u00f3n de juicios. Las f\u00f3rmulas de la l\u00f3gica se interpretan como afirmaciones sobre reglas de agregaci\u00f3n de juicios, y damos una axiomatizaci\u00f3n s\u00f3lida y completa de todas esas reglas. La axiomatizaci\u00f3n est\u00e1 parametrizada de tal manera que podemos instanciarla para obtener una variedad de l\u00f3gicas de agregaci\u00f3n de juicios diferentes. Por ejemplo, un ejemplo es una axiomatizaci\u00f3n, en nuestro lenguaje, de todas las funciones de bienestar social; por lo tanto, obtenemos tambi\u00e9n una l\u00f3gica de agregaci\u00f3n de preferencias cl\u00e1sica.Y esta es una de las principales contribuciones de este art\u00edculo: identificamos las propiedades l\u00f3gicas de la agregaci\u00f3n de juicios y podemos comparar las propiedades l\u00f3gicas de diferentes clases de agregaci\u00f3n de juicios, y de la agregaci\u00f3n de juicios en general y de la agregaci\u00f3n de preferencias en particular. Por supuesto, una l\u00f3gica s\u00f3lo es interesante mientras sea expresiva. Uno de los objetivos de este art\u00edculo es investigar las capacidades l\u00f3gicas y de representaci\u00f3n que necesita un agente para juzgar y agregar preferencias; es decir, \u00bfqu\u00e9 tipo de lenguaje l\u00f3gico podr\u00eda usarse para representar y razonar sobre la agregaci\u00f3n de juicios? El lenguaje de representaci\u00f3n del conocimiento de un agente deber\u00eda poder expresar: reglas de agregaci\u00f3n comunes como la votaci\u00f3n por mayor\u00eda; propiedades com\u00fanmente discutidas de las reglas de agregaci\u00f3n de juicios y funciones de bienestar social como la independencia; paradojas com\u00fanmente utilizadas para ilustrar la agregaci\u00f3n de juicios y la agregaci\u00f3n de preferencias, a saber. la paradoja discursiva y la paradoja de Condorcet respectivamente; y otras propiedades importantes como el teorema de Arrow. De este ejemplo parece que un lenguaje formal para SWF deber\u00eda poder expresar: \u2022 Propiedades de relaciones de preferencia para diferentes agentes y propiedades de varias relaciones de preferencia diferentes para el mismo agente en la misma f\u00f3rmula. \u2022 Comparaci\u00f3n de diferentes relaciones de preferencia. \u2022 La relaci\u00f3n de preferencia resultante de aplicar un SWF a otras relaciones de preferencia. Desde estos puntos podr\u00eda parecer que tal lenguaje ser\u00eda bastante complejo -LRB- en particular, estos requisitos parecen descartar una l\u00f3gica modal proposicional est\u00e1ndar -RRB-. En la siguiente secci\u00f3n revisamos los conceptos b\u00e1sicos de la agregaci\u00f3n de juicios, as\u00ed como de la agregaci\u00f3n de preferencias, y mencionamos algunas propiedades com\u00fanmente discutidas de las reglas de agregaci\u00f3n de juicios y las funciones de bienestar social. Las f\u00f3rmulas de JAL son interpretadas directamente por las reglas de agregaci\u00f3n de juicios y, por lo tanto, representan propiedades de ellas. En la Secci\u00f3n 4 demostramos que la l\u00f3gica puede expresar propiedades com\u00fanmente discutidas de las reglas de agregaci\u00f3n de juicios, como la paradoja discursiva. En la Secci\u00f3n 5 damos una axiomatizaci\u00f3n s\u00f3lida y completa de la l\u00f3gica, bajo el supuesto de que la agenda sobre la que los agentes emiten juicios es finita. Como se mencion\u00f3 anteriormente, la agregaci\u00f3n de preferencias puede verse como un caso especial de agregaci\u00f3n de juicios, y en la Secci\u00f3n 6 introducimos una interpretaci\u00f3n alternativa de las f\u00f3rmulas JAL directamente en funciones de bienestar social. Tambi\u00e9n obtenemos una axiomatizaci\u00f3n s\u00f3lida y completa de la l\u00f3gica de la agregaci\u00f3n de preferencias. Las secciones 7 y 8 analizan el trabajo relacionado y concluyen. 7. TRABAJOS RELACIONADOS Las l\u00f3gicas formales relacionadas con la elecci\u00f3n social se han centrado principalmente en la representaci\u00f3n l\u00f3gica de las preferencias cuando el conjunto de alternativas es grande y en las propiedades de c\u00e1lculo de las preferencias agregadas para una representaci\u00f3n dada -LSB- 6, 7, 8 -RSB- . Una excepci\u00f3n notable y reciente es un marco l\u00f3gico para la agregaci\u00f3n de juicios desarrollado por Marc Pauly en -LSB- 10 -RSB-,para poder caracterizar las relaciones l\u00f3gicas entre diferentes reglas de agregaci\u00f3n de juicios. La l\u00f3gica modal de flechas -LSB- 11 -RSB- est\u00e1 dise\u00f1ada para razonar sobre cualquier objeto que pueda representarse gr\u00e1ficamente como una flecha, y tiene varios operadores modales para expresar propiedades y relaciones entre estas flechas. En la l\u00f3gica de agregaci\u00f3n de preferencias jal -LRB- LK -RRB- interpretamos f\u00f3rmulas en pares de alternativas, que pueden verse como flechas. Por lo tanto, -LRB- al menos -RRB- la variante de agregaci\u00f3n de preferencias de nuestra l\u00f3gica est\u00e1 relacionada con la l\u00f3gica de flechas. Sin embargo, si bien los operadores modales de la l\u00f3gica de flechas pueden expresar propiedades de relaciones de preferencia como la transitividad, no pueden expresar directamente la mayor\u00eda de las propiedades que hemos analizado en este art\u00edculo. Sin embargo, la relaci\u00f3n con la l\u00f3gica de flechas podr\u00eda investigarse m\u00e1s a fondo en trabajos futuros. En particular, las l\u00f3gicas de flechas suelen demostrarse completas. un \u00e1lgebra. Esto podr\u00eda significar que ser\u00eda posible utilizar dichas \u00e1lgebras como estructura subyacente para representar las preferencias individuales y colectivas. Luego, cambiar el perfil de preferencia nos lleva de un \u00e1lgebra a otra, y un SWF determina la preferencia colectiva, en cada una de las \u00e1lgebras. 8. DISCUSI\u00d3N Hemos presentado una l\u00f3gica s\u00f3lida y completa para representar y razonar sobre la agregaci\u00f3n de juicios. jal es expresivo: puede expresar reglas de agregaci\u00f3n de juicios como la votaci\u00f3n por mayor\u00eda; propiedades complicadas como la independencia; y resultados importantes como la paradoja discursiva, el teorema de Arrow y la paradoja de Condorcet. Sostenemos que estos resultados muestran exactamente qu\u00e9 capacidades l\u00f3gicas necesita un agente para poder razonar sobre la agregaci\u00f3n de juicios. Quiz\u00e1s resulte sorprendente que un lenguaje relativamente simple proporcione estas capacidades. La axiomatizaci\u00f3n describe los principios l\u00f3gicos de la agregaci\u00f3n de juicios y tambi\u00e9n se puede instanciar para razonar sobre instancias espec\u00edficas de agregaci\u00f3n de juicios, como la agregaci\u00f3n de preferencias arroviana cl\u00e1sica. As\u00ed, nuestro marco arroja luz sobre las diferencias entre los principios l\u00f3gicos detr\u00e1s de la agregaci\u00f3n de juicios generales, por un lado, y la agregaci\u00f3n de preferencias cl\u00e1sica, por el otro. En trabajos futuros ser\u00eda interesante flexibilizar los requisitos de integridad y coherencia de los conjuntos de juicios y tratar de caracterizarlos en el lenguaje l\u00f3gico, como propiedades de conjuntos de juicios generales.-LRB- al menos -RRB- la variante de agregaci\u00f3n de preferencias de nuestra l\u00f3gica est\u00e1 relacionada con la l\u00f3gica de flechas. Sin embargo, si bien los operadores modales de la l\u00f3gica de flechas pueden expresar propiedades de relaciones de preferencia como la transitividad, no pueden expresar directamente la mayor\u00eda de las propiedades que hemos analizado en este art\u00edculo. Sin embargo, la relaci\u00f3n con la l\u00f3gica de flechas podr\u00eda investigarse m\u00e1s a fondo en trabajos futuros. En particular, las l\u00f3gicas de flechas suelen demostrarse completas. un \u00e1lgebra. Esto podr\u00eda significar que ser\u00eda posible utilizar dichas \u00e1lgebras como estructura subyacente para representar las preferencias individuales y colectivas. Luego, cambiar el perfil de preferencia nos lleva de un \u00e1lgebra a otra, y un SWF determina la preferencia colectiva, en cada una de las \u00e1lgebras. 8. DISCUSI\u00d3N Hemos presentado una l\u00f3gica s\u00f3lida y completa para representar y razonar sobre la agregaci\u00f3n de juicios. jal es expresivo: puede expresar reglas de agregaci\u00f3n de juicios como la votaci\u00f3n por mayor\u00eda; propiedades complicadas como la independencia; y resultados importantes como la paradoja discursiva, el teorema de Arrow y la paradoja de Condorcet. Sostenemos que estos resultados muestran exactamente qu\u00e9 capacidades l\u00f3gicas necesita un agente para poder razonar sobre la agregaci\u00f3n de juicios. Quiz\u00e1s resulte sorprendente que un lenguaje relativamente simple proporcione estas capacidades. La axiomatizaci\u00f3n describe los principios l\u00f3gicos de la agregaci\u00f3n de juicios y tambi\u00e9n se puede instanciar para razonar sobre instancias espec\u00edficas de agregaci\u00f3n de juicios, como la agregaci\u00f3n de preferencias arroviana cl\u00e1sica. As\u00ed, nuestro marco arroja luz sobre las diferencias entre los principios l\u00f3gicos detr\u00e1s de la agregaci\u00f3n de juicios generales, por un lado, y la agregaci\u00f3n de preferencias cl\u00e1sica, por el otro. En trabajos futuros ser\u00eda interesante flexibilizar los requisitos de integridad y coherencia de los conjuntos de juicios y tratar de caracterizarlos en el lenguaje l\u00f3gico, como propiedades de conjuntos de juicios generales.-LRB- al menos -RRB- la variante de agregaci\u00f3n de preferencias de nuestra l\u00f3gica est\u00e1 relacionada con la l\u00f3gica de flechas. Sin embargo, si bien los operadores modales de la l\u00f3gica de flechas pueden expresar propiedades de relaciones de preferencia como la transitividad, no pueden expresar directamente la mayor\u00eda de las propiedades que hemos analizado en este art\u00edculo. Sin embargo, la relaci\u00f3n con la l\u00f3gica de flechas podr\u00eda investigarse m\u00e1s a fondo en trabajos futuros. En particular, las l\u00f3gicas de flechas suelen demostrarse completas. un \u00e1lgebra. Esto podr\u00eda significar que ser\u00eda posible utilizar dichas \u00e1lgebras como estructura subyacente para representar las preferencias individuales y colectivas. Luego, cambiar el perfil de preferencia nos lleva de un \u00e1lgebra a otra, y un SWF determina la preferencia colectiva, en cada una de las \u00e1lgebras. 8. DISCUSI\u00d3N Hemos presentado una l\u00f3gica s\u00f3lida y completa para representar y razonar sobre la agregaci\u00f3n de juicios. jal es expresivo: puede expresar reglas de agregaci\u00f3n de juicios como la votaci\u00f3n por mayor\u00eda; propiedades complicadas como la independencia; y resultados importantes como la paradoja discursiva, el teorema de Arrow y la paradoja de Condorcet. Sostenemos que estos resultados muestran exactamente qu\u00e9 capacidades l\u00f3gicas necesita un agente para poder razonar sobre la agregaci\u00f3n de juicios. Quiz\u00e1s resulte sorprendente que un lenguaje relativamente simple proporcione estas capacidades. La axiomatizaci\u00f3n describe los principios l\u00f3gicos de la agregaci\u00f3n de juicios y tambi\u00e9n se puede instanciar para razonar sobre instancias espec\u00edficas de agregaci\u00f3n de juicios, como la agregaci\u00f3n de preferencias arroviana cl\u00e1sica. As\u00ed, nuestro marco arroja luz sobre las diferencias entre los principios l\u00f3gicos detr\u00e1s de la agregaci\u00f3n de juicios generales, por un lado, y la agregaci\u00f3n de preferencias cl\u00e1sica, por el otro. En trabajos futuros ser\u00eda interesante flexibilizar los requisitos de integridad y coherencia de los conjuntos de juicios y tratar de caracterizarlos en el lenguaje l\u00f3gico, como propiedades de conjuntos de juicios generales.Teorema de Arrow y paradoja de Condorcet. Sostenemos que estos resultados muestran exactamente qu\u00e9 capacidades l\u00f3gicas necesita un agente para poder razonar sobre la agregaci\u00f3n de juicios. Quiz\u00e1s resulte sorprendente que un lenguaje relativamente simple proporcione estas capacidades. La axiomatizaci\u00f3n describe los principios l\u00f3gicos de la agregaci\u00f3n de juicios y tambi\u00e9n se puede instanciar para razonar sobre instancias espec\u00edficas de agregaci\u00f3n de juicios, como la agregaci\u00f3n de preferencias arroviana cl\u00e1sica. As\u00ed, nuestro marco arroja luz sobre las diferencias entre los principios l\u00f3gicos detr\u00e1s de la agregaci\u00f3n de juicios generales, por un lado, y la agregaci\u00f3n de preferencias cl\u00e1sica, por el otro. En trabajos futuros ser\u00eda interesante flexibilizar los requisitos de integridad y coherencia de los conjuntos de juicios y tratar de caracterizarlos en el lenguaje l\u00f3gico, como propiedades de conjuntos de juicios generales.Teorema de Arrow y paradoja de Condorcet. Sostenemos que estos resultados muestran exactamente qu\u00e9 capacidades l\u00f3gicas necesita un agente para poder razonar sobre la agregaci\u00f3n de juicios. Quiz\u00e1s resulte sorprendente que un lenguaje relativamente simple proporcione estas capacidades. La axiomatizaci\u00f3n describe los principios l\u00f3gicos de la agregaci\u00f3n de juicios y tambi\u00e9n se puede instanciar para razonar sobre instancias espec\u00edficas de agregaci\u00f3n de juicios, como la agregaci\u00f3n de preferencias arroviana cl\u00e1sica. As\u00ed, nuestro marco arroja luz sobre las diferencias entre los principios l\u00f3gicos detr\u00e1s de la agregaci\u00f3n de juicios generales, por un lado, y la agregaci\u00f3n de preferencias cl\u00e1sica, por el otro. En trabajos futuros ser\u00eda interesante flexibilizar los requisitos de integridad y coherencia de los conjuntos de juicios y tratar de caracterizarlos en el lenguaje l\u00f3gico, como propiedades de conjuntos de juicios generales.", "keyphrases": ["conocimiento representa formal", "funci\u00f3n de bienestar social", "axiomatis completa", "sintaxis y sem\u00e1ntica de jal", "discurre la paradoja", "regla agregada de juicio", "teorema de la flecha", "expresar", "no dictadura", "unanimidad", "prefiero agregar", "l\u00f3gica de flecha", "jala"]}
{"file_name": "C-27", "text": "Un sistema de localizaci\u00f3n de alta precisi\u00f3n y bajo costo para redes de sensores inal\u00e1mbricos RESUMEN El problema de la localizaci\u00f3n de nodos de sensores inal\u00e1mbricos se ha considerado durante mucho tiempo como muy dif\u00edcil de resolver, cuando se consideran las realidades de los entornos del mundo real. En este art\u00edculo, describimos, dise\u00f1amos, implementamos y evaluamos formalmente un novedoso sistema de localizaci\u00f3n, llamado Spotlight. Nuestro sistema utiliza las propiedades espacio-temporales de eventos bien controlados en la red -LRB-, por ejemplo, luz -RRB-, para obtener las ubicaciones de los nodos sensores. Demostramos que se puede lograr una alta precisi\u00f3n en la localizaci\u00f3n sin la ayuda de hardware costoso en los nodos sensores, como lo requieren otros sistemas de localizaci\u00f3n. Evaluamos el desempe\u00f1o de nuestro sistema en implementaciones de motas Mica2 y XSM. Mediante evaluaciones de desempe\u00f1o de un sistema real implementado en exteriores, obtenemos un error de localizaci\u00f3n de 20 cm. Una red de sensores, con cualquier n\u00famero de nodos, desplegada en un \u00e1rea de 2.500 m2, se puede localizar en menos de 10 minutos, utilizando un dispositivo que cuesta menos de 1.000 d\u00f3lares. Hasta donde sabemos, este es el primer informe de una sub- error de localizaci\u00f3n del medidor, obtenido en un entorno exterior, sin equipar los nodos de sensores inal\u00e1mbricos con hardware de alcance especializado. 1. INTRODUCCI\u00d3N Recientemente, los sistemas de redes de sensores inal\u00e1mbricos se han utilizado en muchas aplicaciones prometedoras, incluida la vigilancia militar, el monitoreo del h\u00e1bitat, el seguimiento de la vida silvestre, etc. -LSB- 12 -RSB- -LSB- 22 -RSB- -LSB- 33 -RSB- -LSB - 36 -RSB-. Si bien se han dise\u00f1ado e implementado con \u00e9xito muchos servicios de middleware para respaldar estas aplicaciones, la localizaci\u00f3n (encontrar la posici\u00f3n de los nodos sensores) sigue siendo uno de los desaf\u00edos de investigaci\u00f3n m\u00e1s dif\u00edciles de resolver en la pr\u00e1ctica. Un GPS integrado -LSB- 23 -RSB- es una soluci\u00f3n t\u00edpica de alta gama, que requiere un hardware sofisticado para lograr una sincronizaci\u00f3n horaria de alta resoluci\u00f3n con los sat\u00e9lites. Las limitaciones de energ\u00eda y costo de los peque\u00f1os nodos sensores impiden que esta sea una soluci\u00f3n viable. Otras soluciones requieren dispositivos por nodo que puedan realizar mediciones entre nodos vecinos. Las dificultades de estos enfoques son dobles. En primer lugar, bajo limitaciones de factor de forma y suministro de energ\u00eda, los alcances efectivos de dichos dispositivos son muy limitados. Por ejemplo, el alcance efectivo de los transductores ultras\u00f3nicos utilizados en el sistema Cricket es inferior a 2 metros cuando el emisor y el receptor no est\u00e1n uno frente al otro -LSB- 26 -RSB-. En segundo lugar, dado que la mayor\u00eda de los nodos sensores son est\u00e1ticos, es decir, no se espera que cambie la ubicaci\u00f3n, no es rentable equipar estos sensores con circuitos especiales s\u00f3lo para una localizaci\u00f3n \u00fanica. Para superar estas limitaciones, se han propuesto muchos esquemas de localizaci\u00f3n sin alcance. La mayor\u00eda de estos esquemas estiman la ubicaci\u00f3n de los nodos sensores explotando la informaci\u00f3n de conectividad de radio entre los nodos vecinos. Estos enfoques eliminan la necesidad de hardware especializado de alto costo, a costa de una localizaci\u00f3n menos precisa. Adem\u00e1s,Las caracter\u00edsticas de propagaci\u00f3n de radio var\u00edan con el tiempo y dependen del entorno, lo que impone altos costos de calibraci\u00f3n para los esquemas de localizaci\u00f3n sin alcance. Nuestra respuesta a este desaf\u00edo es un sistema de localizaci\u00f3n llamado Spotlight. Este sistema emplea una arquitectura asim\u00e9trica, en la que los nodos sensores no necesitan ning\u00fan hardware adicional al que tienen actualmente. Todo el hardware y la computaci\u00f3n sofisticados residen en un \u00fanico dispositivo Spotlight. El dispositivo Spotlight utiliza una fuente de luz l\u00e1ser orientable que ilumina los nodos de sensores ubicados dentro de un terreno conocido. Al mismo tiempo, dado que s\u00f3lo se necesita un dispositivo sofisticado para localizar toda la red, el costo amortizado es mucho menor que el costo de agregar componentes de hardware a los sensores individuales. 2. TRABAJOS RELACIONADOS El problema de la localizaci\u00f3n es un problema de investigaci\u00f3n fundamental en muchos dominios. Los errores de localizaci\u00f3n reportados son del orden de decenas de cent\u00edmetros, cuando se utiliza hardware de medici\u00f3n especializado, es decir, tel\u00e9metro l\u00e1ser o ultrasonido. Debido al alto coste y al factor de forma no despreciable del hardware de alcance, estas soluciones no se pueden aplicar simplemente a las redes de sensores. El RSSI ha resultado una soluci\u00f3n atractiva para estimar la distancia entre el emisor y el receptor. El sistema RADAR -LSB- 2 -RSB- utiliza el RSSI para construir un dep\u00f3sito centralizado de intensidades de se\u00f1al en varias posiciones con respecto a un conjunto de nodos de baliza. La ubicaci\u00f3n de un usuario de m\u00f3vil se estima en unos pocos metros. De manera similar, MoteTrack -LSB- 17 -RSB- distribuye los valores RSSI de referencia a los nodos de baliza. Tambi\u00e9n se han propuesto soluciones que utilizan RSSI y no requieren nodos de baliza -LSB- 5 -RSB- -LSB- 14 -RSB- -LSB- 24 -RSB- -LSB- 26 -RSB- -LSB- 29 -RSB-. Todos comparten la idea de utilizar una baliza m\u00f3vil. Los nodos sensores que reciben las balizas aplican diferentes algoritmos para inferir su ubicaci\u00f3n. En -LSB- 29 -RSB-, Sichitiu propone una soluci\u00f3n en la que los nodos que reciben la baliza construyen, en funci\u00f3n del valor RSSI, una restricci\u00f3n en su estimaci\u00f3n de posici\u00f3n. En -LSB-24-RSB-, Pathirana et al. Formule el problema de localizaci\u00f3n como una estimaci\u00f3n en l\u00ednea en un sistema din\u00e1mico no lineal y propone un filtro de Kalman extendido robusto para resolverlo. Elnahrawy -LSB- 8 -RSB- proporciona pruebas s\u00f3lidas de las limitaciones inherentes a la precisi\u00f3n de la localizaci\u00f3n utilizando RSSI en entornos interiores. Una t\u00e9cnica de determinaci\u00f3n de distancias m\u00e1s precisa utiliza la diferencia de tiempo entre una se\u00f1al de radio y una onda ac\u00fastica para obtener distancias por pares entre los nodos sensores. Este enfoque produce errores de localizaci\u00f3n m\u00e1s peque\u00f1os, a costa de hardware adicional. El sistema de soporte de localizaci\u00f3n Cricket -LSB- 25 -RSB- puede alcanzar una granularidad de localizaci\u00f3n de decenas de cent\u00edmetros con transceptores de ultrasonido de corto alcance. AHLoS, propuesto por Savvides et al. -LSB- 27 -RSB-, emplea t\u00e9cnicas de rango de tiempo de llegada -LRB- ToA -RRB- que requieren hardware extenso y la resoluci\u00f3n de sistemas de ecuaciones no lineales relativamente grandes.En -LSB-30-RSB-, Simon et al. implementar un sistema distribuido -LRB- mediante alcance ac\u00fastico -RRB- que localiza a un francotirador en un terreno urbano. Kwon et al. tambi\u00e9n utilizan el rango ac\u00fastico para la localizaci\u00f3n. -LSB- 15 -RSB-. Los errores reportados en la localizaci\u00f3n var\u00edan de 2,2 m a 9,5 m, dependiendo del tipo -LRB- centralizado vs. distribuido -RRB- del algoritmo de Escalado de M\u00ednimos Cuadrados utilizado. Para las redes de sensores inal\u00e1mbricos, el alcance es una opci\u00f3n dif\u00edcil. Sin embargo, la alta precisi\u00f3n de localizaci\u00f3n que se puede lograr con estos esquemas es muy deseable. Para superar los desaf\u00edos que plantean los esquemas de localizaci\u00f3n basados \u200b\u200ben rangos, cuando se aplican a redes de sensores, en el pasado se ha propuesto y evaluado un enfoque diferente. Este enfoque se denomina rango libre e intenta obtener informaci\u00f3n de ubicaci\u00f3n a partir de la proximidad a un conjunto de nodos de baliza conocidos. Bulusu et al. proponemos en -LSB- 4 -RSB- un esquema de localizaci\u00f3n, llamado Centroide, en el que cada nodo se localiza en el centroide de sus nodos de baliza pr\u00f3ximos. El Sistema Global de Coordenadas -LSB- 20 -RSB-, desarrollado en el MIT, utiliza el conocimiento a priori de la densidad de nodos en la red, para estimar la distancia promedio de salto. La familia DV - * de esquemas de localizaci\u00f3n -LSB- 21 -RSB-, utiliza el recuento de saltos desde nodos de baliza conocidos a los nodos de la red para inferir la distancia. La mayor\u00eda de los esquemas de localizaci\u00f3n sin alcance se han evaluado en simulaciones o entornos controlados. Langendoen y Reijers presentan un estudio comparativo detallado de varios esquemas de localizaci\u00f3n en -LSB- 16 -RSB-. Hasta donde sabemos, Spotlight es el primer esquema de localizaci\u00f3n sin alcance que funciona muy bien en un entorno exterior. Nuestro sistema requiere una l\u00ednea de visi\u00f3n entre un solo dispositivo y los nodos sensores, y el mapa del terreno donde se encuentra el campo sensor. El sistema Spotlight tiene un largo alcance efectivo -LRB- 1000 metros -RRB- y no requiere ninguna infraestructura ni hardware adicional para nodos sensores. El sistema Spotlight combina las ventajas y no sufre las desventajas de las dos clases de localizaci\u00f3n. 7. CONCLUSIONES Y TRABAJO FUTURO En este art\u00edculo presentamos el dise\u00f1o, implementaci\u00f3n y evaluaci\u00f3n de un sistema de localizaci\u00f3n para redes de sensores inal\u00e1mbricos, denominado Spotlight. Nuestra soluci\u00f3n de localizaci\u00f3n no requiere ning\u00fan hardware adicional para los nodos de sensores, aparte del que ya existe. Toda la complejidad del sistema est\u00e1 encapsulada en un \u00fanico dispositivo Spotlight. Nuestro sistema de localizaci\u00f3n es reutilizable, es decir, los costes se pueden amortizar mediante varias implementaciones y su rendimiento no se ve afectado por el n\u00famero de nodos de sensores en la red. Nuestros resultados experimentales, obtenidos de un sistema real implementado en exteriores, muestran que el error de localizaci\u00f3n es inferior a 20 cm. Este error es actualmente de \u00faltima generaci\u00f3n,incluso para sistemas de localizaci\u00f3n basados \u200b\u200ben rango y es un 75 % menor que el error obtenido cuando se utilizan dispositivos GPS o cuando el despliegue manual de nodos sensores es una opci\u00f3n factible -LSB- 31 -RSB-. Como trabajo futuro, nos gustar\u00eda explorar la autocalibraci\u00f3n y el autoajuste del sistema Spotlight. La precisi\u00f3n del sistema se puede mejorar a\u00fan m\u00e1s si se informa la distribuci\u00f3n del evento, en lugar de una \u00fanica marca de tiempo. Se podr\u00eda obtener una generalizaci\u00f3n reformulando el problema como un problema de estimaci\u00f3n angular que proporcione los componentes b\u00e1sicos para t\u00e9cnicas de localizaci\u00f3n m\u00e1s generales.", "keyphrases": ["red de sensores inal\u00e1mbricos", "local", "base de rango local", "esquema sin sonar", "transmitir", "llevar a cabo", "precisi\u00f3n", "error local", "red de sensores", "sistema de foco", "t\u00e9cnica local", "distribuir"]}
{"file_name": "C-17", "text": "Problemas de implementaci\u00f3n de un sistema de conferencias VoIP en un entorno de conferencias virtuales RESUMEN Los servicios en tiempo real han sido admitidos en gran medida en redes de conmutaci\u00f3n de circuitos. Las tendencias recientes favorecen los servicios portados en redes de conmutaci\u00f3n de paquetes. Para las audioconferencias, debemos considerar muchos aspectos: escalabilidad, calidad de la aplicaci\u00f3n de conferencia, control de sala y carga en los clientes/servidores, por nombrar algunos. En este art\u00edculo, describimos un marco de servicio de audio dise\u00f1ado para proporcionar un entorno de conferencia virtual -LRB-VCE-RRB-. El sistema est\u00e1 dise\u00f1ado para dar cabida a un gran n\u00famero de usuarios finales que hablan al mismo tiempo y repartidos por Internet. El framework se basa en Servidores de Conferencias -LSB- 14 -RSB-, que facilitan el manejo del audio, mientras explotamos las capacidades SIP para fines de se\u00f1alizaci\u00f3n. La selecci\u00f3n de clientes se basa en un cuantificador reciente llamado ``N\u00famero de volumen'' que ayuda a imitar una conferencia f\u00edsica cara a cara. Abordamos los problemas de implementaci\u00f3n de la soluci\u00f3n propuesta tanto en t\u00e9rminos de escalabilidad como de interactividad, al tiempo que explicamos las t\u00e9cnicas que utilizamos para reducir el tr\u00e1fico. Hemos implementado una aplicaci\u00f3n Conference Server -LRB- CS -RRB- en una red que abarca todo el campus de nuestro Instituto. 1. INTRODUCCI\u00d3N La Internet actual utiliza el conjunto de protocolos IP que fue dise\u00f1ado principalmente para el transporte de datos y proporciona la mejor entrega de datos. Las limitaciones y caracter\u00edsticas de los retrasos separan los datos tradicionales, por un lado, de las aplicaciones de voz y v\u00eddeo, por el otro. Por lo tanto, a medida que se van implementando en Internet aplicaciones de voz y v\u00eddeo cada vez m\u00e1s urgentes, se pone de manifiesto la insuficiencia de Internet. Adem\u00e1s, buscamos trasladar los servicios telef\u00f3nicos a Internet. Entre ellos, la instalaci\u00f3n de conferencia virtual -LRB- teleconferencia -RRB- est\u00e1 a la vanguardia. Las conferencias de audio y v\u00eddeo en Internet son populares -LSB- 25 -RSB- por las diversas ventajas que conllevan -LSB- 3,6 -RSB-. Es evidente que el ancho de banda necesario para una teleconferencia a trav\u00e9s de Internet aumenta r\u00e1pidamente con el n\u00famero de participantes; Reducir el ancho de banda sin comprometer la calidad del audio es un desaf\u00edo en la Telefon\u00eda por Internet. Hay mucha discusi\u00f3n entre la comunidad de HCI y CSCW sobre el uso de la etnometodolog\u00eda para el dise\u00f1o de aplicaciones CSCW. El enfoque b\u00e1sico es proporcionar mayor ancho de banda, m\u00e1s instalaciones y mecanismos de control m\u00e1s avanzados, con miras a una mejor calidad de interacci\u00f3n. Este enfoque ignora la utilidad funcional del entorno que se utiliza para la colaboraci\u00f3n. Por lo tanto, es necesario adoptar un enfoque que considere ambos aspectos: el t\u00e9cnico y el funcional. En este trabajo, no hablamos de videoconferencias; su inclusi\u00f3n no beneficia significativamente la calidad de la conferencia -LSB- 4 -RSB-. Nuestro enfoque est\u00e1 en entornos de audio virtuales. Primero describimos los desaf\u00edos encontrados en las audioconferencias virtuales. Luego analizamos las motivaciones seguidas por la literatura relevante. En la Secci\u00f3n 5,Te explicamos la arquitectura de nuestro sistema. La secci\u00f3n 6 comprende la descripci\u00f3n de los diversos algoritmos utilizados en nuestra configuraci\u00f3n. Abordamos problemas de implementaci\u00f3n. A continuaci\u00f3n se realiza una discusi\u00f3n sobre el desempe\u00f1o. Concluimos considerando algunos problemas de implementaci\u00f3n. 4. TRABAJO RELACIONADO El est\u00e1ndar SIP definido en RFC 3261 -LSB- 22 -RSB- y en extensiones posteriores como -LSB- 21 -RSB- no ofrece servicios de control de conferencias como control de sala o votaci\u00f3n y no prescribe c\u00f3mo una Fig. 1. Ejemplo de conferencia: 3 dominios que contienen las entidades necesarias para que se pueda realizar la conferencia. se va a gestionar la conferencia. Sin embargo, se puede utilizar SIP para iniciar una sesi\u00f3n que utilice alg\u00fan otro protocolo de control de conferencia. La especificaci\u00f3n SIP principal admite muchos modelos para conferencias -LSB- 26, 23 -RSB-. En los modelos basados \u200b\u200ben servidor, un servidor mezcla flujos de medios, mientras que en una conferencia sin servidor, la mezcla se realiza en los sistemas finales. SDP -LSB- 7 -RSB- se puede utilizar para definir capacidades de medios y proporcionar otra informaci\u00f3n sobre la conferencia. Ahora consideraremos algunos modelos de conferencias en SIP que se han propuesto recientemente -LSB-23-RSB-. Primero, analicemos los modelos sin servidor. En End-System Mixing, solo un cliente -LRB- SIP UA -RRB- maneja la se\u00f1alizaci\u00f3n y mezcla de medios para todos los dem\u00e1s, lo que claramente no es escalable y causa problemas cuando ese cliente en particular abandona la conferencia. Esto conduce a un n\u00famero cada vez mayor de saltos para las hojas remotas y no es escalable. Otra opci\u00f3n ser\u00eda utilizar la multidifusi\u00f3n para conferencias, pero la multidifusi\u00f3n no est\u00e1 habilitada a trav\u00e9s de Internet y actualmente solo es posible en una LAN. Entre los modelos basados \u200b\u200ben servidor, en una conferencia telef\u00f3nica, los UA se conectan a un servidor central que maneja toda la mezcla. Este modelo no es escalable ya que est\u00e1 limitado por la potencia de procesamiento del servidor y el ancho de banda de la red. Las conferencias centralizadas adhoc y los servidores de conferencias telef\u00f3nicas tienen mecanismos y problemas similares. Los modelos h\u00edbridos que involucran se\u00f1alizaci\u00f3n centralizada y medios distribuidos, este \u00faltimo usando unidifusi\u00f3n o multidifusi\u00f3n, plantean problemas de escalabilidad como antes. Sin embargo, una ventaja es que el control de la conferencia puede ser una soluci\u00f3n de terceros. La p\u00e9rdida de espacialismo cuando se mezclan y el aumento del ancho de banda cuando no lo hacen son problemas abiertos. Un estudio relacionado -LSB- 19 -RSB- del mismo autor propone una arquitectura de conferencia para entornos virtuales colaborativos -LRB-CVE-RRB- pero no proporciona el \u00e1ngulo de escalabilidad sin la disponibilidad de multidifusi\u00f3n. Teniendo en cuenta las limitaciones de los sistemas de conferencias propuestos, ahora detallaremos nuestra propuesta. 9. CONCLUSI\u00d3N En este art\u00edculo, hemos presentado una discusi\u00f3n sobre un entorno de conferencia virtual s\u00f3lo de voz. Hemos argumentado que la naturaleza distribuida del despliegue aqu\u00ed lo hace escalable. La interactividad se logra adaptando un esquema de selecci\u00f3n de transmisi\u00f3n reciente basado en el n\u00famero de sonoridad. De este modo se consigue una utilizaci\u00f3n significativamente eficaz del ancho de banda.Estos hacen que el discurso improvisado en una teleconferencia virtual a trav\u00e9s de VoIP sea una realidad, como en una conferencia real cara a cara. El tr\u00e1fico en la WAN -LRB- Internet -RRB- est\u00e1 limitado superiormente por el cuadrado del n\u00famero de dominios, reducido a\u00fan m\u00e1s mediante el uso de algoritmos heur\u00edsticos, que est\u00e1 muy por debajo del n\u00famero total de clientes en la conferencia. Esto se debe al uso de un servidor de conferencias local para cada dominio. Las t\u00e9cnicas VAD ayudan a reducir a\u00fan m\u00e1s el tr\u00e1fico. El uso del est\u00e1ndar SIP para la se\u00f1alizaci\u00f3n hace que esta soluci\u00f3n sea altamente interoperable. Hemos implementado una aplicaci\u00f3n CS en una red de todo el campus. Creemos que esta nueva generaci\u00f3n de entornos de conferencias virtuales ganar\u00e1 m\u00e1s popularidad en el futuro a medida que su facilidad de implementaci\u00f3n est\u00e9 garantizada gracias a tecnolog\u00edas f\u00e1cilmente disponibles y marcos escalables.", "keyphrases": ["sistema de conferencias voip", "red de conmutaci\u00f3n de paquetes", "marco de servicio de audio", "entorno de conferencia virtual", "conferir servidor", "numero fuerte", "mezcla parcial", "detecci\u00f3n activa de voz", "basta con tres altavoces simult\u00e1neos", "t\u00e9cnica vad"]}
{"file_name": "H-30", "text": "Expansi\u00f3n de conceptos latentes utilizando campos aleatorios de Markov RESUMEN La expansi\u00f3n de consultas, en forma de retroalimentaci\u00f3n de pseudo-relevancia o retroalimentaci\u00f3n de relevancia, es una t\u00e9cnica com\u00fan utilizada para mejorar la efectividad de la recuperaci\u00f3n. La mayor\u00eda de los enfoques anteriores han ignorado cuestiones importantes, como el papel de las caracter\u00edsticas y la importancia de modelar las dependencias de t\u00e9rminos. En este art\u00edculo, proponemos una t\u00e9cnica robusta de expansi\u00f3n de consultas basada en el modelo de campos aleatorios de Markov para la recuperaci\u00f3n de informaci\u00f3n. La t\u00e9cnica, denominada expansi\u00f3n de conceptos latentes, proporciona un mecanismo para modelar dependencias de t\u00e9rminos durante la expansi\u00f3n. Adem\u00e1s, el uso de caracter\u00edsticas arbitrarias dentro del modelo proporciona un marco poderoso para ir m\u00e1s all\u00e1 de las simples caracter\u00edsticas de ocurrencia de t\u00e9rminos que son utilizadas impl\u00edcitamente por la mayor\u00eda de las otras t\u00e9cnicas de expansi\u00f3n. Evaluamos nuestra t\u00e9cnica frente a modelos de relevancia, una t\u00e9cnica de expansi\u00f3n de consultas de modelado de lenguaje de \u00faltima generaci\u00f3n. Nuestro modelo demuestra mejoras consistentes y significativas en la efectividad de la recuperaci\u00f3n en varios conjuntos de datos TREC. Tambi\u00e9n describimos c\u00f3mo se puede utilizar nuestra t\u00e9cnica para generar conceptos significativos de varios t\u00e9rminos para tareas como la sugerencia/reformulaci\u00f3n de consultas. 1. INTRODUCCI\u00d3N posiblemente una narrativa m\u00e1s larga. Se pierde una gran cantidad de informaci\u00f3n durante el proceso de traducci\u00f3n de la informaci\u00f3n necesaria a la consulta real. Por esta raz\u00f3n, ha habido un gran inter\u00e9s en las t\u00e9cnicas de expansi\u00f3n de consultas. Estas t\u00e9cnicas se utilizan para aumentar la consulta original y producir una representaci\u00f3n que refleje mejor la necesidad de informaci\u00f3n subyacente. Las t\u00e9cnicas de expansi\u00f3n de consultas han sido bien estudiadas para varios modelos en el pasado y han demostrado mejorar significativamente la efectividad tanto en la configuraci\u00f3n de retroalimentaci\u00f3n de relevancia como en la retroalimentaci\u00f3n de pseudorelevancia -LSB- 12, 21, 28, 29 -RSB-. El modelo MRF generaliza el unigrama, el bigrama y otros modelos de dependencia diversos -LSB- 14 -RSB-. La mayor\u00eda de los modelos de dependencia de t\u00e9rminos anteriores no han logrado mostrar mejoras consistentes y significativas con respecto a las l\u00edneas de base de unigrama, con pocas excepciones -LSB- 8 -RSB-. Hasta ahora, el modelo se ha utilizado \u00fanicamente para clasificar documentos en respuesta a una consulta determinada. En este trabajo, mostramos c\u00f3mo el modelo se puede extender y utilizar para la expansi\u00f3n de consultas utilizando una t\u00e9cnica que llamamos expansi\u00f3n de conceptos latentes -LRB-LCE-RRB-. Hay tres contribuciones principales de nuestro trabajo. Primero, LCE proporciona un mecanismo para combinar la dependencia de t\u00e9rminos con la expansi\u00f3n de consultas. Las t\u00e9cnicas de expansi\u00f3n de consultas anteriores se basan en modelos de bolsa de palabras. Por lo tanto, al realizar la expansi\u00f3n de consultas utilizando el modelo MRF, podemos estudiar la din\u00e1mica entre la dependencia de t\u00e9rminos y la expansi\u00f3n de consultas. A continuaci\u00f3n, como mostraremos, el modelo MRF permite utilizar caracter\u00edsticas arbitrarias dentro del modelo. Las t\u00e9cnicas de expansi\u00f3n de consultas en el pasado impl\u00edcitamente solo hac\u00edan uso de caracter\u00edsticas de ocurrencia de t\u00e9rminos. Al utilizar conjuntos de caracter\u00edsticas m\u00e1s s\u00f3lidos, es posible producir mejores t\u00e9rminos de expansi\u00f3n que discriminen mejor entre documentos relevantes y no relevantes. Finalmente,Nuestro enfoque propuesto proporciona perfectamente un mecanismo para generar conceptos tanto de un solo t\u00e9rmino como de varios t\u00e9rminos. La mayor\u00eda de las t\u00e9cnicas anteriores, por defecto, generan t\u00e9rminos de forma independiente. Ha habido varios enfoques que hacen uso de conceptos generalizados, sin embargo dichos enfoques fueron algo heur\u00edsticos y se realizaron fuera del modelo -LSB- 19, 28 -RSB-. Nuestro enfoque tiene una motivaci\u00f3n formal y una extensi\u00f3n natural del modelo subyacente. En la Secci\u00f3n 2 describimos enfoques de expansi\u00f3n de consultas relacionados. La Secci\u00f3n 3 proporciona una descripci\u00f3n general del modelo MRF y detalla nuestra t\u00e9cnica de expansi\u00f3n de conceptos latentes propuesta. En la Secci\u00f3n 4 evaluamos nuestro modelo propuesto y analizamos los resultados. 2. TRABAJOS RELACIONADOS Uno de los enfoques cl\u00e1sicos y m\u00e1s utilizados para la expansi\u00f3n de consultas es el algoritmo de Rocchio -LSB- 21 -RSB-. El enfoque de Rocchio, que se desarroll\u00f3 dentro del modelo de espacio vectorial, vuelve a ponderar el vector de consulta original moviendo los pesos hacia el conjunto de documentos relevantes o pseudorelevantes y alej\u00e1ndolos de los documentos no relevantes. Desafortunadamente, no es posible aplicar formalmente el enfoque de Rocchio a un modelo de recuperaci\u00f3n estad\u00edstica, como el modelado del lenguaje para la recuperaci\u00f3n de informaci\u00f3n. Se han desarrollado una serie de t\u00e9cnicas de expansi\u00f3n de consultas formalizadas para el marco de modelado del lenguaje, incluida la retroalimentaci\u00f3n basada en modelos de Zhai y Lafferty y los modelos de relevancia de Lavrenko y Croft -LSB- 12, 29 -RSB-. Ambos enfoques intentan utilizar documentos pseudorelevantes o relevantes para estimar un mejor modelo de consulta. La retroalimentaci\u00f3n basada en modelos encuentra el modelo que mejor describe los documentos relevantes teniendo en cuenta un modelo de fondo -LRB- ruido -RRB-. Esto separa el modelo de contenido del modelo de fondo. Luego, el modelo de contenido se interpola con el modelo de consulta original para formar la consulta expandida. La otra t\u00e9cnica, los modelos de relevancia, est\u00e1 m\u00e1s estrechamente relacionada con nuestro trabajo. Por tanto, entramos en los detalles del modelo. Al igual que la retroalimentaci\u00f3n basada en modelos, los modelos de relevancia estiman un modelo de consulta mejorado. La \u00fanica diferencia entre los dos enfoques es que los modelos de relevancia no modelan expl\u00edcitamente los documentos relevantes o pseudorelevantes. En cambio, modelan una noci\u00f3n m\u00e1s generalizada de relevancia, como mostramos ahora. Dada una consulta Q, un modelo de relevancia es una distribuci\u00f3n multinomial, P -LRB- \u00b7 | Q -RRB-, que codifica la probabilidad de cada t\u00e9rmino dado la consulta como evidencia. Se calcula como: donde RQ es el conjunto de documentos que son relevantes o pseudorelevantes para consultar Q. Estos supuestos leves hacen que calcular el posterior bayesiano sea m\u00e1s pr\u00e1ctico. Despu\u00e9s de estimar el modelo, los documentos se clasifican recortando el modelo de relevancia eligiendo los k t\u00e9rminos m\u00e1s probables de P -LRB- \u00b7 | Q -RRB-. Esta distribuci\u00f3n recortada luego se interpola con el modelo de consulta de m\u00e1xima verosimilitud original -LSB- 1 -RSB-. Se puede considerar que esto es una expansi\u00f3n de la consulta original en k t\u00e9rminos ponderados. A lo largo del resto de este trabajo,Nos referimos a esta instanciaci\u00f3n de modelos de relevancia como RM3. Se ha trabajado relativamente poco en el \u00e1rea de expansi\u00f3n de consultas en el context de los modelos de dependencia -LSB- 9 -RSB-. Sin embargo, ha habido varios intentos de expandirse utilizando conceptos de t\u00e9rminos m\u00faltiples. El m\u00e9todo de an\u00e1lisis del context local -LRB- LCA -RRB- de Xu y Croft combin\u00f3 la recuperaci\u00f3n a nivel de pasaje con la expansi\u00f3n de conceptos, donde los conceptos eran t\u00e9rminos y frases individuales -LSB- 28 -RSB-. Los conceptos de expansi\u00f3n se eligieron y ponderaron utilizando una m\u00e9trica basada en estad\u00edsticas de coocurrencia. Papka y Allan investigan el uso de comentarios de relevancia para realizar una expansi\u00f3n de conceptos de m\u00faltiples t\u00e9rminos para el enrutamiento de documentos -LSB- 19 -RSB-. Los resultados mostraron que la combinaci\u00f3n de conceptos de un solo t\u00e9rmino y de ventana grande y de m\u00faltiples t\u00e9rminos mejor\u00f3 significativamente la efectividad. 5. CONCLUSIONES En este art\u00edculo propusimos una t\u00e9cnica robusta de expansi\u00f3n de consultas llamada expansi\u00f3n de conceptos latentes. Se demostr\u00f3 que la t\u00e9cnica es una extensi\u00f3n natural del modelo de campo aleatorio de Markov para la recuperaci\u00f3n de informaci\u00f3n y una generalizaci\u00f3n de modelos de relevancia. Demostramos que la t\u00e9cnica se puede utilizar para producir conceptos de expansi\u00f3n de m\u00faltiples t\u00e9rminos de alta calidad, bien formados y relevantes para el tema. Los conceptos generados se pueden utilizar en un m\u00f3dulo de sugerencia de consultas alternativo. Tambi\u00e9n demostramos que el modelo es muy eficaz. De hecho, logra mejoras significativas en la precisi\u00f3n promedio sobre los modelos de relevancia en una selecci\u00f3n de conjuntos de datos TREC. Tambi\u00e9n se demostr\u00f3 que el modelo MRF en s\u00ed, sin ninguna expansi\u00f3n de consultas, supera a los modelos de relevancia en grandes conjuntos de datos web. Finalmente, reiteramos la importancia de elegir t\u00e9rminos de expansi\u00f3n que modelen la relevancia, en lugar de los documentos relevantes, y mostramos c\u00f3mo LCE captura dependencias tanto sint\u00e1cticas como sem\u00e1nticas del lado de la consulta. El trabajo futuro tambi\u00e9n buscar\u00e1 incorporar dependencias del lado del documento.Los conceptos generados se pueden utilizar en un m\u00f3dulo de sugerencia de consultas alternativo. Tambi\u00e9n demostramos que el modelo es muy eficaz. De hecho, logra mejoras significativas en la precisi\u00f3n promedio sobre los modelos de relevancia en una selecci\u00f3n de conjuntos de datos TREC. Tambi\u00e9n se demostr\u00f3 que el modelo MRF en s\u00ed, sin ninguna expansi\u00f3n de consultas, supera a los modelos de relevancia en grandes conjuntos de datos web. Finalmente, reiteramos la importancia de elegir t\u00e9rminos de expansi\u00f3n que modelen la relevancia, en lugar de los documentos relevantes, y mostramos c\u00f3mo LCE captura dependencias tanto sint\u00e1cticas como sem\u00e1nticas del lado de la consulta. El trabajo futuro tambi\u00e9n buscar\u00e1 incorporar dependencias del lado del documento.Los conceptos generados se pueden utilizar en un m\u00f3dulo de sugerencia de consultas alternativo. Tambi\u00e9n demostramos que el modelo es muy eficaz. De hecho, logra mejoras significativas en la precisi\u00f3n promedio sobre los modelos de relevancia en una selecci\u00f3n de conjuntos de datos TREC. Tambi\u00e9n se demostr\u00f3 que el modelo MRF en s\u00ed, sin ninguna expansi\u00f3n de consultas, supera a los modelos de relevancia en grandes conjuntos de datos web. Finalmente, reiteramos la importancia de elegir t\u00e9rminos de expansi\u00f3n que modelen la relevancia, en lugar de los documentos relevantes, y mostramos c\u00f3mo LCE captura dependencias tanto sint\u00e1cticas como sem\u00e1nticas del lado de la consulta. El trabajo futuro tambi\u00e9n buscar\u00e1 incorporar dependencias del lado del documento.", "keyphrases": ["consulta robusta expande la t\u00e9cnica", "modelo de lenguaje queri expande la t\u00e9cnica", "retroalimentaci\u00f3n relevante", "retroalimentaci\u00f3n pseudo-relevante", "informar recuperar", "enfoque del modelo de lenguaje", "b\u00fasqueda Web", "la consulta se expande", "mrf", "algoritmo de rocchio", "marco del modelo de lenguaje", "rm3", "ruta de documentos", "recuperaci\u00f3n ad hoc", "modelo mrf", "distribuci\u00f3n relevante"]}
{"file_name": "I 10", "text": "SMILE: Aprendizaje incremental multiagente sonoro;--RRB- * RESUMEN Este art\u00edculo aborda el problema del aprendizaje colaborativo en un sistema multiagente. Aqu\u00ed cada agente puede actualizar incrementalmente sus creencias B -LRB- la representaci\u00f3n conceptual -RRB- de manera que se mantenga consistente con todo el conjunto de informaci\u00f3n K -LRB- los ejemplos -RRB- que ha recibido del entorno o otros agentes. Extendemos esta noci\u00f3n de consistencia -LRB- o solidez -RRB- a todo el MAS y discutimos c\u00f3mo lograr que, en cualquier momento, est\u00e9 presente en cada agente una misma representaci\u00f3n conceptual consistente. El protocolo correspondiente se aplica al concepto de aprendizaje supervisado. El m\u00e9todo resultante SMILE -LRB- que significa Sound Multiagent Incremental LEarning -RRB- se describe y experimenta aqu\u00ed. Sorprendentemente, algunas f\u00f3rmulas booleanas dif\u00edciles se aprenden mejor, dado el mismo conjunto de aprendizaje, con un sistema multiagente que con un solo agente. 1. INTRODUCCI\u00d3N Este art\u00edculo aborda el problema del aprendizaje colaborativo de conceptos en un sistema multiagente. -LSB- 6 -RSB- introduce una caracterizaci\u00f3n del aprendizaje en un sistema multiagente seg\u00fan el nivel de conciencia de los agentes. En el nivel 1, los agentes aprenden * El autor principal de este art\u00edculo es un estudiante. en el sistema sin tener en cuenta la presencia de otros agentes, excepto a trav\u00e9s de la modificaci\u00f3n que su acci\u00f3n produce en el medio ambiente. El nivel 2 implica una interacci\u00f3n directa entre los agentes ya que pueden intercambiar mensajes para mejorar su aprendizaje. En este art\u00edculo nos centramos en el nivel 2, estudiando la interacci\u00f3n directa entre agentes involucrados en un proceso de aprendizaje. Se supone que cada agente es capaz de aprender incrementalmente a partir de los datos que recibe, lo que significa que cada agente puede actualizar su conjunto de creencias B para mantenerlo consistente con todo el conjunto de informaci\u00f3n K que ha recibido del entorno o de otros agentes. Adem\u00e1s, suponemos que al menos una parte Bc de las creencias de cada agente es com\u00fan a todos los agentes y debe permanecer as\u00ed. Por tanto, una actualizaci\u00f3n de este conjunto com\u00fan Bc por parte del agente r debe provocar una actualizaci\u00f3n de Bc para toda la comunidad de agentes. Nos lleva a definir cu\u00e1l es la mas-consistencia de un agente respecto de la comunidad. El proceso de actualizaci\u00f3n de las creencias de la comunidad cuando uno de sus miembros obtiene nueva informaci\u00f3n puede definirse como el proceso de mantenimiento de la coherencia que garantiza que todos los agentes de la comunidad se mantengan masconsistentes. Este proceso de mantenimiento de la coherencia masiva en el que un agente obtiene nueva informaci\u00f3n le otorga el papel de aprendiz e implica comunicaci\u00f3n con otros agentes que act\u00faan como cr\u00edticos. Sin embargo, los agentes no est\u00e1n especializados y pueden a su vez ser aprendices o cr\u00edticos, sin que ninguno de ellos cumpla un rol espec\u00edfico. La informaci\u00f3n se distribuye entre los agentes, pero puede ser redundante. No hay una memoria central. El trabajo descrito aqu\u00ed tiene su origen en un trabajo anterior sobre el aprendizaje en un sistema multiagente intencional utilizando un formalismo BDI -LSB- 6 -RSB-. En ese trabajo,Los agentes ten\u00edan planes, cada uno de ellos asociado a un context que define en qu\u00e9 condiciones puede desencadenarse. Los planes -LRB- teniendo cada uno de ellos su propio context -RRB- eran comunes a todo el conjunto de agentes de la comunidad. Los agentes deb\u00edan adaptar los contexts de sus planes en funci\u00f3n del fracaso o \u00e9xito de los planes ejecutados, utilizando un mecanismo de aprendizaje y pidiendo ejemplos a otros agentes -LRB- de planes de \u00e9xito o fracasos -RRB-. Sin embargo, este trabajo carec\u00eda de un protocolo de aprendizaje colectivo que permitiera una autonom\u00eda real del sistema multiagente. El estudio de dicho protocolo es el objeto del presente art\u00edculo. En la secci\u00f3n 2 definimos formalmente la consistencia mas de un mecanismo de actualizaci\u00f3n para todo el MAS y proponemos un mecanismo de actualizaci\u00f3n gen\u00e9rico que demostr\u00f3 ser consistente mas. En la secci\u00f3n 3 describimos SMILE, un aprendiz de conceptos incremental de m\u00faltiples agentes que aplica nuestro mecanismo de actualizaci\u00f3n mas consistente al aprendizaje de conceptos colaborativo. La Secci\u00f3n 4 describe varios experimentos con SMILE y analiza varias cuestiones, incluido c\u00f3mo var\u00edan la precisi\u00f3n y la simplicidad de la hip\u00f3tesis actual al comparar el aprendizaje de un solo agente y el aprendizaje masivo. En la secci\u00f3n 5 presentamos brevemente algunos trabajos relacionados y luego concluimos en la secci\u00f3n 6 discutiendo investigaciones adicionales sobre el aprendizaje m\u00e1s consistente. 5. TRABAJOS RELACIONADOS Desde el a\u00f1o 96 -LSB- 15 -RSB- se han realizado diversos trabajos sobre aprendizaje en MAS, pero pocos sobre aprendizaje de conceptos. En -LSB- 11 -RSB- el MAS realiza una forma de aprendizaje conjunto en el que los agentes son aprendices perezosos -LRB- no se mantiene ninguna representaci\u00f3n expl\u00edcita -RRB- y venden ejemplos in\u00fatiles a otros agentes. En -LSB- 10 -RSB- cada agente observa todos los ejemplos pero s\u00f3lo percibe una parte de su representaci\u00f3n. En el aprendizaje mutuo de conceptos en l\u00ednea -LSB- 14 -RSB- los agentes convergen hacia una hip\u00f3tesis \u00fanica, pero cada agente produce ejemplos a partir de su propia representaci\u00f3n conceptual, lo que resulta en una especie de sincronizaci\u00f3n en lugar de un aprendizaje puro de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.Los agentes deb\u00edan adaptar los contexts de sus planes en funci\u00f3n del fracaso o \u00e9xito de los planes ejecutados, utilizando un mecanismo de aprendizaje y pidiendo ejemplos a otros agentes -LRB- de planes de \u00e9xito o fracasos -RRB-. Sin embargo, este trabajo carec\u00eda de un protocolo de aprendizaje colectivo que permitiera una autonom\u00eda real del sistema multiagente. El estudio de dicho protocolo es el objeto del presente art\u00edculo. En la secci\u00f3n 2 definimos formalmente la consistencia mas de un mecanismo de actualizaci\u00f3n para todo el MAS y proponemos un mecanismo de actualizaci\u00f3n gen\u00e9rico que demostr\u00f3 ser consistente mas. En la secci\u00f3n 3 describimos SMILE, un aprendiz de conceptos incremental de m\u00faltiples agentes que aplica nuestro mecanismo de actualizaci\u00f3n mas consistente al aprendizaje de conceptos colaborativo. La Secci\u00f3n 4 describe varios experimentos con SMILE y analiza varias cuestiones, incluido c\u00f3mo var\u00edan la precisi\u00f3n y la simplicidad de la hip\u00f3tesis actual al comparar el aprendizaje de un solo agente y el aprendizaje masivo. En la secci\u00f3n 5 presentamos brevemente algunos trabajos relacionados y luego concluimos en la secci\u00f3n 6 discutiendo investigaciones adicionales sobre el aprendizaje m\u00e1s consistente. 5. TRABAJOS RELACIONADOS Desde el a\u00f1o 96 -LSB- 15 -RSB- se han realizado diversos trabajos sobre aprendizaje en MAS, pero pocos sobre aprendizaje de conceptos. En -LSB- 11 -RSB- el MAS realiza una forma de aprendizaje conjunto en el que los agentes son aprendices perezosos -LRB- no se mantiene ninguna representaci\u00f3n expl\u00edcita -RRB- y venden ejemplos in\u00fatiles a otros agentes. En -LSB- 10 -RSB- cada agente observa todos los ejemplos pero s\u00f3lo percibe una parte de su representaci\u00f3n. En el aprendizaje mutuo de conceptos en l\u00ednea -LSB- 14 -RSB- los agentes convergen hacia una hip\u00f3tesis \u00fanica, pero cada agente produce ejemplos a partir de su propia representaci\u00f3n conceptual, lo que resulta en una especie de sincronizaci\u00f3n en lugar de un aprendizaje puro de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.Los agentes deb\u00edan adaptar los contexts de sus planes en funci\u00f3n del fracaso o \u00e9xito de los planes ejecutados, utilizando un mecanismo de aprendizaje y pidiendo ejemplos a otros agentes -LRB- de planes de \u00e9xito o fracasos -RRB-. Sin embargo, este trabajo carec\u00eda de un protocolo de aprendizaje colectivo que permitiera una autonom\u00eda real del sistema multiagente. El estudio de dicho protocolo es el objeto del presente art\u00edculo. En la secci\u00f3n 2 definimos formalmente la consistencia mas de un mecanismo de actualizaci\u00f3n para todo el MAS y proponemos un mecanismo de actualizaci\u00f3n gen\u00e9rico que demostr\u00f3 ser consistente mas. En la secci\u00f3n 3 describimos SMILE, un aprendiz de conceptos incremental de m\u00faltiples agentes que aplica nuestro mecanismo de actualizaci\u00f3n mas consistente al aprendizaje de conceptos colaborativo. La Secci\u00f3n 4 describe varios experimentos con SMILE y analiza varias cuestiones, incluido c\u00f3mo var\u00edan la precisi\u00f3n y la simplicidad de la hip\u00f3tesis actual al comparar el aprendizaje de un solo agente y el aprendizaje masivo. En la secci\u00f3n 5 presentamos brevemente algunos trabajos relacionados y luego concluimos en la secci\u00f3n 6 discutiendo investigaciones adicionales sobre el aprendizaje m\u00e1s consistente. 5. TRABAJOS RELACIONADOS Desde el a\u00f1o 96 -LSB- 15 -RSB- se han realizado diversos trabajos sobre aprendizaje en MAS, pero pocos sobre aprendizaje de conceptos. En -LSB- 11 -RSB- el MAS realiza una forma de aprendizaje conjunto en el que los agentes son aprendices perezosos -LRB- no se mantiene ninguna representaci\u00f3n expl\u00edcita -RRB- y venden ejemplos in\u00fatiles a otros agentes. En -LSB- 10 -RSB- cada agente observa todos los ejemplos pero s\u00f3lo percibe una parte de su representaci\u00f3n. En el aprendizaje mutuo de conceptos en l\u00ednea -LSB- 14 -RSB- los agentes convergen hacia una hip\u00f3tesis \u00fanica, pero cada agente produce ejemplos a partir de su propia representaci\u00f3n conceptual, lo que resulta en una especie de sincronizaci\u00f3n en lugar de un aprendizaje puro de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.un aprendiz de conceptos incremental de m\u00faltiples agentes que aplica nuestro mecanismo de actualizaci\u00f3n m\u00e1s consistente al aprendizaje de conceptos colaborativo. La Secci\u00f3n 4 describe varios experimentos con SMILE y analiza varias cuestiones, incluido c\u00f3mo var\u00edan la precisi\u00f3n y la simplicidad de la hip\u00f3tesis actual al comparar el aprendizaje de un solo agente y el aprendizaje masivo. En la secci\u00f3n 5 presentamos brevemente algunos trabajos relacionados y luego concluimos en la secci\u00f3n 6 discutiendo investigaciones adicionales sobre el aprendizaje m\u00e1s consistente. 5. TRABAJOS RELACIONADOS Desde el a\u00f1o 96 -LSB- 15 -RSB- se han realizado diversos trabajos sobre aprendizaje en MAS, pero pocos sobre aprendizaje de conceptos. En -LSB- 11 -RSB- el MAS realiza una forma de aprendizaje conjunto en el que los agentes son aprendices perezosos -LRB- no se mantiene ninguna representaci\u00f3n expl\u00edcita -RRB- y venden ejemplos in\u00fatiles a otros agentes. En -LSB- 10 -RSB- cada agente observa todos los ejemplos pero s\u00f3lo percibe una parte de su representaci\u00f3n. En el aprendizaje mutuo de conceptos en l\u00ednea -LSB- 14 -RSB- los agentes convergen hacia una hip\u00f3tesis \u00fanica, pero cada agente produce ejemplos a partir de su propia representaci\u00f3n conceptual, lo que resulta en una especie de sincronizaci\u00f3n en lugar de un aprendizaje puro de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.un aprendiz de conceptos incremental de m\u00faltiples agentes que aplica nuestro mecanismo de actualizaci\u00f3n m\u00e1s consistente al aprendizaje de conceptos colaborativo. La Secci\u00f3n 4 describe varios experimentos con SMILE y analiza varias cuestiones, incluido c\u00f3mo var\u00edan la precisi\u00f3n y la simplicidad de la hip\u00f3tesis actual al comparar el aprendizaje de un solo agente y el aprendizaje masivo. En la secci\u00f3n 5 presentamos brevemente algunos trabajos relacionados y luego concluimos en la secci\u00f3n 6 discutiendo investigaciones adicionales sobre el aprendizaje m\u00e1s consistente. 5. TRABAJOS RELACIONADOS Desde el a\u00f1o 96 -LSB- 15 -RSB- se han realizado diversos trabajos sobre aprendizaje en MAS, pero pocos sobre aprendizaje de conceptos. En -LSB- 11 -RSB- el MAS realiza una forma de aprendizaje conjunto en el que los agentes son aprendices perezosos -LRB- no se mantiene ninguna representaci\u00f3n expl\u00edcita -RRB- y venden ejemplos in\u00fatiles a otros agentes. En -LSB- 10 -RSB- cada agente observa todos los ejemplos pero s\u00f3lo percibe una parte de su representaci\u00f3n. En el aprendizaje mutuo de conceptos en l\u00ednea -LSB- 14 -RSB- los agentes convergen hacia una hip\u00f3tesis \u00fanica, pero cada agente produce ejemplos a partir de su propia representaci\u00f3n conceptual, lo que resulta en una especie de sincronizaci\u00f3n en lugar de un aprendizaje puro de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.resultando as\u00ed en una especie de sincronizaci\u00f3n m\u00e1s que en un puro aprendizaje de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.resultando as\u00ed en una especie de sincronizaci\u00f3n m\u00e1s que en un puro aprendizaje de conceptos. 6. CONCLUSI\u00d3N Hemos presentado aqu\u00ed y experimentado un protocolo para el aprendizaje de conceptos en l\u00ednea MAS. Sin embargo, nuestro marco es abierto, es decir, los agentes pueden salir del sistema o entrar en \u00e9l mientras se preserva el mecanismo de coherencia. Por ejemplo, si introducimos un mecanismo de tiempo de espera, incluso cuando un agente cr\u00edtico falla u omite responder, se implica la coherencia con los dem\u00e1s cr\u00edticos -LRB- dentro del resto de agentes -RRB-. El trabajo adicional se refiere, en primer lugar, a combinar la inducci\u00f3n y la abducci\u00f3n para realizar un aprendizaje colaborativo de conceptos cuando cada agente observa solo parcialmente los ejemplos y, en segundo lugar, investigar el aprendizaje de la memoria parcial: c\u00f3mo se preserva el aprendizaje cuando un agente o todo el MAS olvida algunos ejemplos seleccionados.", "keyphrases": ["aprendizaje multiagente", "concepto de colaboraci\u00f3n aprender", "aprender proceso", "conocimiento", "ma-consiste", "incremento aprender", "agente", "Mecanismo de actualizaci\u00f3n", "sincr\u00f3nico"]}
{"file_name": "H-31", "text": "Un estudio del modelo de generaci\u00f3n de consultas de Poisson para la recuperaci\u00f3n de informaci\u00f3n RESUMEN Se han propuesto muchas variantes de modelos de lenguaje para la recuperaci\u00f3n de informaci\u00f3n. La mayor\u00eda de los modelos existentes se basan en una distribuci\u00f3n multinomial y calificar\u00edan los documentos seg\u00fan la probabilidad de la consulta calculada seg\u00fan un modelo probabil\u00edstico de generaci\u00f3n de consultas. En este art\u00edculo, proponemos y estudiamos una nueva familia de modelos de generaci\u00f3n de consultas basados \u200b\u200ben la distribuci\u00f3n de Poisson. Mostramos que si bien en sus formas m\u00e1s simples, la nueva familia de modelos y los modelos multinomiales existentes son equivalentes, se comportan de manera diferente para muchos m\u00e9todos de suavizado. Mostramos que el modelo de Poisson tiene varias ventajas sobre el modelo multinomial, incluida la adaptaci\u00f3n natural al suavizado por t\u00e9rminos y la posibilidad de un modelado de fondo m\u00e1s preciso. Presentamos varias variantes del nuevo modelo correspondientes a diferentes m\u00e9todos de suavizado y las evaluamos en cuatro colecciones de pruebas TREC representativas. Los resultados muestran que, si bien sus modelos b\u00e1sicos funcionan de manera comparable, el modelo de Poisson puede superar al modelo multinomial con suavizamiento por t\u00e9rminos. El rendimiento se puede mejorar a\u00fan m\u00e1s con un suavizado de dos etapas. 1. INTRODUCCI\u00d3N Como un nuevo tipo de modelos de recuperaci\u00f3n probabil\u00edsticos, los modelos de lenguaje han demostrado ser efectivos para muchas tareas de recuperaci\u00f3n -LSB- 21, 28, 14, 4 -RSB-. Luego podemos clasificar los documentos seg\u00fan la probabilidad de generar la consulta. Pr\u00e1cticamente todos los modelos de lenguajes de generaci\u00f3n de consultas existentes se basan en la distribuci\u00f3n multinomial -LSB- 19, 6, 28 -RSB- o en la distribuci\u00f3n multivariada de Bernoulli -LSB- 21, 18 -RSB-. La distribuci\u00f3n multinomial es especialmente popular y tambi\u00e9n ha demostrado ser bastante eficaz. Tenga en cuenta que la ausencia de t\u00e9rminos tambi\u00e9n se captura indirectamente en un modelo multinomial a trav\u00e9s de la restricci\u00f3n de que todas las probabilidades de los t\u00e9rminos deben sumar 1. En este art\u00edculo, proponemos y estudiamos una nueva familia de modelos de generaci\u00f3n de consultas basados \u200b\u200ben la distribuci\u00f3n de Poisson. En esta nueva familia de modelos, modelamos la frecuencia de cada t\u00e9rmino de forma independiente con una distribuci\u00f3n de Poisson. Para calificar un documento, primero estimar\u00edamos un modelo de Poisson multivariado basado en el documento y luego lo calificar\u00edamos seg\u00fan la probabilidad de la consulta dada por el modelo de Poisson estimado. En cierto sentido, el modelo de Poisson combina la ventaja del multinomial al modelar la frecuencia de los t\u00e9rminos y la ventaja del multivariado de Bernoulli al acomodar el suavizado por t\u00e9rminos. Como en el trabajo existente sobre modelos de lenguaje multinomial, el suavizado es fundamental para esta nueva familia de modelos. Derivamos varios m\u00e9todos de suavizado para el modelo de Poisson en paralelo a los utilizados para distribuciones multinomiales, y comparamos los modelos de recuperaci\u00f3n correspondientes con aquellos basados \u200b\u200ben distribuciones multinomiales. Encontramos que, si bien con algunos m\u00e9todos de suavizado, el nuevo modelo y el modelo multinomial conducen a exactamente la misma f\u00f3rmula, con algunos otros m\u00e9todos de suavizado divergen, y el modelo de Poisson aporta m\u00e1s flexibilidad para el suavizado. En particular,una diferencia clave es que el modelo de Poisson puede acomodar naturalmente el suavizado permanente, lo cual es dif\u00edcil de lograr con un modelo multinomial sin un giro heur\u00edstico de la sem\u00e1ntica de un modelo generativo. Explotamos esta ventaja potencial para desarrollar un nuevo algoritmo de suavizado dependiente de t\u00e9rminos para el modelo de Poisson y mostramos que este nuevo algoritmo de suavizado puede mejorar el rendimiento con respecto a los algoritmos de suavizado independientes de t\u00e9rminos que utilizan el modelo Poisson o multinomial. Esta ventaja se observa tanto para el suavizado de una etapa como para el de dos etapas. Otra ventaja potencial del modelo de Poisson es que su correspondiente modelo de fondo para suavizado se puede mejorar mediante el uso de un modelo de mezcla que tenga una f\u00f3rmula de forma cerrada. Se ha demostrado que este nuevo modelo de fondo supera al modelo de fondo est\u00e1ndar y reduce la sensibilidad del rendimiento de recuperaci\u00f3n al par\u00e1metro de suavizado. En la Secci\u00f3n 2, presentamos la nueva familia de modelos de generaci\u00f3n de consultas con distribuci\u00f3n de Poisson y presentamos varios m\u00e9todos de suavizado que conducen a diferentes funciones de recuperaci\u00f3n. En la Secci\u00f3n 3, comparamos anal\u00edticamente el modelo de lenguaje de Poisson con el modelo de lenguaje multinomial, desde la perspectiva de la recuperaci\u00f3n. Luego dise\u00f1amos experimentos emp\u00edricos para comparar las dos familias de modelos de lenguaje en la Secci\u00f3n 4. Discutimos el trabajo relacionado en 5 y concluimos en 6. 5. TRABAJO RELACIONADO Hasta donde sabemos, no se han realizado estudios sobre modelos de generaci\u00f3n de consultas basados \u200b\u200ben sobre la distribuci\u00f3n de Poisson. Los modelos de lenguaje han demostrado ser eficaces para muchas tareas de recuperaci\u00f3n -LSB- 21, 28, 14, 4 -RSB-. El m\u00e1s popular y fundamental es el modelo de lenguaje de generaci\u00f3n de consultas -LSB- 21, 13 -RSB-. Todos los modelos de lenguaje de generaci\u00f3n de consultas existentes se basan en la distribuci\u00f3n multinomial -LSB- 19, 6, 28, 13 -RSB- o en la distribuci\u00f3n multivariada de Bernoulli -LSB- 21, 17, 18 -RSB-. Introducimos una nueva familia de modelos de lenguaje, basados \u200b\u200ben la distribuci\u00f3n de Poisson. La distribuci\u00f3n de Poisson ha sido estudiada previamente en los modelos de generaci\u00f3n de documentos -LSB- 16, 22, 3, 24 -RSB-, dando lugar al desarrollo de una de las f\u00f3rmulas de recuperaci\u00f3n m\u00e1s efectivas BM25 -LSB- 23 -RSB-. -LSB- 24 -RSB- estudia la derivaci\u00f3n paralela de tres modelos de recuperaci\u00f3n diferentes que est\u00e1 relacionada con nuestra comparaci\u00f3n de Poisson y multinomial. Sin embargo, el modelo de Poisson en su art\u00edculo todav\u00eda se encuentra dentro del marco de generaci\u00f3n de documentos y tampoco tiene en cuenta la variaci\u00f3n de la longitud del documento. -LSB- 26 -RSB- introduce una forma de buscar emp\u00edricamente un modelo exponencial para los documentos. Las mezclas de Poisson -LSB- 3 -RSB- como 2-Poisson -LSB- 22 -RSB-, multinomial negativo y KMixture de Katz -LSB- 9 -RSB- han demostrado ser efectivas para modelar y recuperar documentos. Una vez m\u00e1s, nada de este trabajo explora la distribuci\u00f3n de Poisson en el marco de generaci\u00f3n de consultas. El suavizado de modelos de lenguaje -LSB- 2, 28, 29 -RSB- y las estructuras de fondo -LSB- 15, 10, 25, 27 -RSB- se han estudiado con modelos de lenguaje multinomiales.-LSB- 7 -RSB- muestra anal\u00edticamente que la suavizaci\u00f3n espec\u00edfica de t\u00e9rminos podr\u00eda ser \u00fatil. Mostramos que el modelo de lenguaje Poisson es natural para acomodar el suavizado por t\u00e9rminos sin giro heur\u00edstico de la sem\u00e1ntica de un modelo generativo, y es capaz de modelar mejor de manera eficiente el fondo de la mezcla, tanto anal\u00edtica como emp\u00edricamente. 6. CONCLUSIONES Presentamos una nueva familia de modelos de lenguaje de generaci\u00f3n de consultas para recuperaci\u00f3n basados \u200b\u200ben la distribuci\u00f3n de Poisson. Derivamos varios m\u00e9todos de suavizado para esta familia de modelos, incluido el suavizado de una sola etapa y el suavizado de dos etapas. Comparamos los nuevos modelos con los populares modelos de recuperaci\u00f3n multinomial tanto de forma anal\u00edtica como experimental. Nuestro an\u00e1lisis muestra que, si bien nuestros nuevos modelos y modelos multinomiales son equivalentes seg\u00fan algunos supuestos, en general son diferentes con algunas diferencias importantes. En particular, mostramos que Poisson tiene una ventaja sobre el multinomial en la adaptaci\u00f3n natural al suavizado por t\u00e9rmino. Explotamos esta propiedad para desarrollar un nuevo algoritmo de suavizado por t\u00e9rmino para los modelos de lenguaje Poisson, que ha demostrado superar el suavizado independiente de los t\u00e9rminos tanto para los modelos Poisson como para los multinomiales. Adem\u00e1s, mostramos que se puede utilizar un modelo de fondo mixto para Poisson para mejorar el rendimiento y la solidez del modelo de fondo est\u00e1ndar de Poisson. Nuestro trabajo abre muchas direcciones interesantes para una mayor exploraci\u00f3n en esta nueva familia de modelos. Explorar m\u00e1s a fondo las flexibilidades de los modelos de lenguaje multinomiales, como la normalizaci\u00f3n de longitud y la pseudorretroalimentaci\u00f3n, podr\u00eda ser un buen trabajo futuro.Explorar m\u00e1s a fondo las flexibilidades de los modelos de lenguaje multinomiales, como la normalizaci\u00f3n de longitud y la pseudorretroalimentaci\u00f3n, podr\u00eda ser un buen trabajo futuro.Explorar m\u00e1s a fondo las flexibilidades de los modelos de lenguaje multinomiales, como la normalizaci\u00f3n de longitud y la pseudorretroalimentaci\u00f3n, podr\u00eda ser un buen trabajo futuro.", "keyphrases": ["distribuci\u00f3n multinomi", "modelo probabilista de queri gener", "distribuci\u00f3n de veneno", "suave de dos etapas", "distribuci\u00f3n multivari bernoullu", "reconocimiento de voz", "frecuencia de t\u00e9rmino", "suave permanente", "nuevo algoritmo suave dependiente de t\u00e9rminos", "conjunto de vocabulario", "proceso de veneno homog\u00e9neo", "pseudot\u00e9rmino \u00fanico"]}
{"file_name": "I-35", "text": "Gesti\u00f3n distribuida de normas en sistemas regulados de m\u00faltiples agentes * RESUMEN Las normas son ampliamente reconocidas como un medio para coordinar sistemas de m\u00faltiples agentes. La gesti\u00f3n distribuida de normas es un tema desafiante y observamos una falta de realizaciones computacionales verdaderamente distribuidas de modelos normativos. Para regular el comportamiento de agentes aut\u00f3nomos que participan en m\u00faltiples actividades relacionadas, proponemos un modelo normativo, la Estructura Normativa -LRB- NS -RRB-, un artefacto que se basa en la propagaci\u00f3n de posiciones normativas -LRB- obligaciones. , prohibiciones, permisos -RRB-, como consecuencias de la actuaci\u00f3n de los agentes. Dentro de una SN, pueden surgir conflictos debido a la naturaleza din\u00e1mica del MAS y la concurrencia de acciones de los agentes. Sin embargo, garantizar la ausencia de conflictos de una SN en el momento del dise\u00f1o es computacionalmente intratable. Mostramos esto formalizando la noci\u00f3n de conflicto, proporcionando un mapeo de las NS en redes de Petri coloreadas y tomando prestados resultados te\u00f3ricos bien conocidos de ese campo. Dado que se requiere la resoluci\u00f3n de conflictos en l\u00ednea, presentamos un algoritmo manejable para ser empleado de forma distribuida. Luego demostramos que este algoritmo es primordial para la implementaci\u00f3n distribuida de una NS. 1. INTRODUCCI\u00d3N Una caracter\u00edstica fundamental de los sistemas multiagente abiertos y regulados en los que interact\u00faan agentes aut\u00f3nomos es que los agentes participantes deben cumplir con las convenciones del sistema. Las normas pueden usarse para modelar tales convenciones y, por tanto, como un medio para regular el comportamiento observable de los agentes -LSB- 6, 29 -RSB-. Existen numerosos aportes sobre el tema de las normas por parte de soci\u00f3logos, fil\u00f3sofos y l\u00f3gicos -LRB- ej., -LSB- 15, 28 -RSB- -RRB-. Sin embargo, hay muy pocas propuestas para realizaciones computacionales de modelos normativos: la forma en que las normas pueden integrarse en el dise\u00f1o y ejecuci\u00f3n de MAS. Hasta donde sabemos, ninguna propuesta realmente apoya la promulgaci\u00f3n distribuida de entornos normativos. En nuestro art\u00edculo abordamos ese problema y proponemos medios para manejar compromisos conflictivos en sistemas abiertos, regulados y multiagente de manera distribuida. El tipo de MAS regulado que contemplamos consiste en m\u00faltiples actividades concurrentes y relacionadas donde los agentes interact\u00faan. Cada agente puede participar simult\u00e1neamente en varias actividades y cambiar de una actividad a otra. Las acciones de un agente dentro de una actividad pueden tener consecuencias en forma de posiciones normativas -LRB- es decir, obligaciones, permisos y prohibiciones -RRB- -LSB- 26 -RSB- que pueden limitar su comportamiento futuro. Suponemos que los agentes pueden optar por no cumplir con todas sus obligaciones y, por tanto, pueden ser sancionados por el MAS. Obs\u00e9rvese que, cuando se distribuyen las actividades, las posiciones normativas deben fluir desde las actividades en las que se generan hasta aquellas en las que tienen efecto. Dado que en un MAS abierto y regulado no se pueden incorporar aspectos normativos en el dise\u00f1o de los agentes,Adoptamos la opini\u00f3n de que la MAS deber\u00eda complementarse con un conjunto separado de normas que regule a\u00fan m\u00e1s el comportamiento de los agentes participantes. Para modelar la separaci\u00f3n de preocupaciones entre el nivel de coordinaci\u00f3n -LRB-, interacciones de agentes -RRB- y el nivel normativo -LRB- propagaci\u00f3n de posiciones normativas -RRB-, proponemos un artefacto llamado Estructura Normativa -LRB- NS -RRB. -. Dentro de una SN pueden surgir conflictos debido a la naturaleza din\u00e1mica del MAS y la concurrencia de acciones de los agentes. Por ejemplo, un agente puede verse obligado y prohibido a realizar la misma acci\u00f3n en una actividad. Sin embargo, garantizar la ausencia de conflictos de una SN en el momento del dise\u00f1o es computacionalmente intratable. Mostramos esto formalizando la noci\u00f3n de conflicto, proporcionando un mapeo de las NS en Redes de Petri de colores -LRB-CPN-RRB- y tomando prestados resultados te\u00f3ricos bien conocidos del campo de las CPN. Creemos que es necesaria la detecci\u00f3n y resoluci\u00f3n de conflictos en l\u00ednea. Por lo tanto, presentamos un algoritmo manejable para la resoluci\u00f3n de conflictos. Este algoritmo es fundamental para la implementaci\u00f3n distribuida de una Sociedad Nacional. El documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2 detallamos un escenario que servir\u00e1 como ejemplo a lo largo del art\u00edculo. A continuaci\u00f3n, en la Secci\u00f3n 3 definimos formalmente el artefacto de estructura normativa. M\u00e1s adelante, en la Secci\u00f3n 4 formalizamos la noci\u00f3n de conflicto para analizar posteriormente la complejidad de la detecci\u00f3n de conflictos en t\u00e9rminos de CPN en la Secci\u00f3n 5. La Secci\u00f3n 6 describe la gesti\u00f3n computacional de las NS describiendo su implementaci\u00f3n y presentando un algoritmo para la resoluci\u00f3n de conflictos. Finalmente, comentamos el trabajo relacionado, sacamos conclusiones e informamos sobre el trabajo futuro en la Secci\u00f3n 7. 7. TRABAJO RELACIONADO Y CONCLUSIONES Nuestras contribuciones en este documento son triples. En primer lugar, introducimos un enfoque para la gesti\u00f3n y el razonamiento sobre normas de forma distribuida. Hasta donde sabemos, hay pocos trabajos publicados en esta direcci\u00f3n. En -LSB- 8, 21 -RSB-, se presentan dos lenguajes para la aplicaci\u00f3n distribuida de normas en MAS. Sin embargo, en ambos trabajos, cada agente tiene una interfaz de mensajes local que reenv\u00eda mensajes legales seg\u00fan un conjunto de normas. Dado que estas interfaces son locales para cada agente, las normas s\u00f3lo pueden expresarse en t\u00e9rminos de acciones de ese agente. Esta es una seria desventaja, por ejemplo cuando uno necesita activar una obligaci\u00f3n hacia un agente debido a un determinado mensaje de otro. El segundo aporte es la propuesta de una estructura normativa. La noci\u00f3n es fruct\u00edfera porque permite separar las preocupaciones normativas y procesales. La estructura normativa que proponemos hace evidente la similitud entre la propagaci\u00f3n de posiciones normativas y la propagaci\u00f3n de 642 The Sixth Intl.. Joint Conf. sobre Agentes Aut\u00f3nomos y Sistemas Multi-Agente -LRB- AAMAS 07 -RRB- de tokens en Redes de Petri de Colores. Esa similitud sugiere f\u00e1cilmente una correlaci\u00f3n entre los dos y da base para un tratamiento anal\u00edtico conveniente de la estructura normativa, en general,y la complejidad de la detecci\u00f3n de conflictos, en particular. En -LSB- 5 -RSB-, las conversaciones se dise\u00f1an y analizan primero a nivel de CPN y luego se traducen en protocolos. Lin et al. -LSB- 20 -RSB- asigna esquemas de conversaci\u00f3n a CPN. Hasta donde sabemos, el uso de esta representaci\u00f3n para apoyar la detecci\u00f3n de conflictos en MAS regulados no se ha informado en ning\u00fan otro lugar. Finalmente, presentamos un mecanismo distribuido para resolver conflictos normativos. Sartor -LSB- 25 -RSB- trata los conflictos normativos desde el punto de vista de la teor\u00eda jur\u00eddica y sugiere una manera de ordenar las normas involucradas. Su idea se implementa en -LSB- 12 -RSB- pero requiere un recurso central para el mantenimiento de la norma. El enfoque para la detecci\u00f3n y resoluci\u00f3n de conflictos es una adaptaci\u00f3n y extensi\u00f3n del trabajo sobre gr\u00e1ficos de creaci\u00f3n de instancias informado en -LSB- 17 -RSB- y un algoritmo relacionado en -LSB- 27 -RSB-. Estas tres contribuciones que presentamos en este art\u00edculo abren muchas posibilidades para trabajos futuros. Esperamos que tal acoplamiento dote a las instituciones electr\u00f3nicas de un entorno normativo m\u00e1s flexible (y m\u00e1s expresivo). Desde el punto de vista te\u00f3rico, pretendemos utilizar t\u00e9cnicas de an\u00e1lisis de CPN para caracterizar clases de CPN -LRB- por ejemplo, ac\u00edclicas, sim\u00e9tricas, etc. -RRB- correspondientes a familias de Estructuras Normativas que son susceptibles de detecci\u00f3n de conflictos manejables fuera de l\u00ednea. La combinaci\u00f3n de estas t\u00e9cnicas junto con nuestros mecanismos de resoluci\u00f3n de conflictos en l\u00ednea tiene como objetivo dotar a los dise\u00f1adores de MAS de la capacidad de incorporar normas en sus sistemas de forma basada en principios.La combinaci\u00f3n de estas t\u00e9cnicas junto con nuestros mecanismos de resoluci\u00f3n de conflictos en l\u00ednea tiene como objetivo dotar a los dise\u00f1adores de MAS de la capacidad de incorporar normas en sus sistemas de forma basada en principios.La combinaci\u00f3n de estas t\u00e9cnicas junto con nuestros mecanismos de resoluci\u00f3n de conflictos en l\u00ednea tiene como objetivo dotar a los dise\u00f1adores de MAS de la capacidad de incorporar normas en sus sistemas de forma basada en principios.", "keyphrases": ["algoritmo", "activo", "gui\u00f3n", "norma postulada", "protocolo", "escena normal", "regla de tr\u00e1nsito norma", "estructura normativa", "gr\u00e1fico bipartito", "prohibir", "permitir superposici\u00f3n", "simb\u00f3lico", "conflicto"]}
{"file_name": "I-33", "text": "Un camino formal desde las normas institucionales a las estructuras organizativas RESUMEN Hasta ahora, la forma en que se han utilizado las instituciones y organizaciones en el desarrollo de sistemas abiertos no ha ido m\u00e1s all\u00e1 de una heur\u00edstica \u00fatil. Para desarrollar sistemas que realmente implementen instituciones y organizaciones, los m\u00e9todos formales deben reemplazar a los heur\u00edsticos. El art\u00edculo presenta una sem\u00e1ntica formal para la noci\u00f3n de instituci\u00f3n y sus componentes -LRB-, normas abstractas y concretas, empoderamiento de agentes, roles -RRB- y define una relaci\u00f3n formal entre instituciones y estructuras organizacionales. Como resultado, se muestra c\u00f3mo las normas institucionales pueden refinarse hasta convertirse en constructos (estructuras organizativas) que se aproximan m\u00e1s a un sistema implementado. Tambi\u00e9n se muestra c\u00f3mo dicho proceso de refinamiento puede formalizarse completamente y, por tanto, ser susceptible de una verificaci\u00f3n rigurosa. 1. INTRODUCCI\u00d3N La oportunidad de una `` transferencia de tecnolog\u00eda '' desde el campo de la teor\u00eda organizacional y social a la IA distribuida y los sistemas multiagente -LRB- MASs -RRB- ha sido defendida durante mucho tiempo -LRB- -LSB- 8 -RSB- -RRB -. En las MAS, la aplicaci\u00f3n de met\u00e1foras organizacionales e institucionales al dise\u00f1o de sistemas ha demostrado ser \u00fatil para el desarrollo de metodolog\u00edas y herramientas. En muchos casos, sin embargo, la aplicaci\u00f3n de estos aparatos conceptuales equivale a meras heur\u00edsticas que gu\u00edan el dise\u00f1o de alto nivel de los sistemas. tratados formalmente, es decir, una vez que nociones como norma, rol, estructura, etc. obtienen una sem\u00e1ntica formal. El objetivo del presente art\u00edculo es llenar este vac\u00edo con respecto a la noci\u00f3n de instituci\u00f3n proporcionando fundamentos formales para la aplicaci\u00f3n de la met\u00e1fora institucional y para su relaci\u00f3n con la met\u00e1fora organizacional. El principal resultado del art\u00edculo consiste en mostrar c\u00f3mo las restricciones abstractas -LRB- instituciones -RRB- pueden ser refinadas paso a paso hasta descripciones estructurales concretas -LRB- estructuras organizativas -RRB- del sistema que se va a implementar, cerrando as\u00ed la brecha entre normas abstractas y especificaciones concretas del sistema. Concretamente, en la Secci\u00f3n 2 se presenta un marco l\u00f3gico que proporciona una sem\u00e1ntica formal para las nociones de instituci\u00f3n, norma y rol, y que respalda la explicaci\u00f3n de caracter\u00edsticas clave de las instituciones, tales como la traducci\u00f3n de normas abstractas en normas concretas e implementables, la empoderamiento institucional de los agentes, y algunos aspectos del dise\u00f1o de la aplicaci\u00f3n de normas. En la Secci\u00f3n 3 el marco se ampl\u00eda para abordar la noci\u00f3n de infraestructura de una instituci\u00f3n. Luego se estudia el marco ampliado en relaci\u00f3n con el formalismo para representar estructuras organizativas presentado en -LSB- 11 -RSB-. En la Secci\u00f3n 4 se presentan algunas conclusiones. 4. CONCLUSIONES El art\u00edculo tuvo como objetivo proporcionar un an\u00e1lisis formal integral de la met\u00e1fora institucional y su relaci\u00f3n con la organizacional. La herramienta formal predominante ha sido la l\u00f3gica descriptiva.TBoxes se ha utilizado para representar las especificaciones de instituciones -LRB- Definici\u00f3n 3 -RRB- y sus infraestructuras -LRB- Definici\u00f3n 6 -RRB-, proporcionando por lo tanto una sem\u00e1ntica de sistema de transici\u00f3n para una serie de nociones institucionales -LRB- Ejemplos 1-7 - RRB-. Luego se han utilizado multigr\u00e1ficos para representar la especificaci\u00f3n de estructuras organizativas -LRB- Definici\u00f3n 6 -RRB-. El \u00faltimo resultado presentado se refiere a la definici\u00f3n de una correspondencia formal entre las especificaciones de instituci\u00f3n y organizaci\u00f3n -LRB- Definici\u00f3n 7 -RRB-, que proporciona una forma formal para cambiar entre los dos paradigmas. En definitiva, estos resultados ofrecen una manera de relacionar especificaciones abstractas de sistemas -LRB- es decir, instituciones como conjuntos de normas -RRB- con especificaciones m\u00e1s cercanas a un sistema implementado -LRB- es decir, estructuras organizativas -RRB-.", "keyphrases": ["m\u00e9todo formal", "norma del instituto", "restricci\u00f3n abstracta", "formal para repres organiz structur", "entidad", "propiedad", "l\u00f3gica descriptiva", "l\u00f3gica din\u00e1mica", "axioma terminol\u00f3gico", "role", "infraestructura"]}
{"file_name": "C-36", "text": "Control de acceso mediante cifrado en redes din\u00e1micas de publicaci\u00f3n/suscripci\u00f3n multidominio RESUMEN Los sistemas de publicaci\u00f3n/suscripci\u00f3n proporcionan una infraestructura de comunicaciones distribuida de \u00e1rea amplia, eficiente y basada en eventos. Es probable que los sistemas de publicaci\u00f3n/suscripci\u00f3n a gran escala empleen componentes de la red de transporte de eventos propiedad de organizaciones cooperantes pero independientes. A medida que aumenta el n\u00famero de participantes en la red, la seguridad se convierte en una preocupaci\u00f3n cada vez mayor. Este art\u00edculo ampl\u00eda el trabajo anterior para presentar y evaluar una infraestructura segura de publicaci\u00f3n/suscripci\u00f3n multidominio que admita y aplique un control de acceso detallado sobre los atributos individuales de los tipos de eventos. La actualizaci\u00f3n de claves nos permite garantizar la seguridad hacia adelante y hacia atr\u00e1s cuando los agentes de eventos se unen y abandonan la red. Demostramos que los gastos generales de tiempo y espacio se pueden minimizar mediante una cuidadosa consideraci\u00f3n de las t\u00e9cnicas de cifrado y mediante el uso de almacenamiento en cach\u00e9 para disminuir los descifrados innecesarios. Mostramos que nuestro enfoque tiene una sobrecarga de comunicaci\u00f3n general menor que los enfoques existentes para lograr el mismo grado de control sobre la seguridad en redes de publicaci\u00f3n/suscripci\u00f3n. 1. INTRODUCCI\u00d3N La publicaci\u00f3n/suscripci\u00f3n es muy adecuada como mecanismo de comunicaci\u00f3n para crear aplicaciones distribuidas basadas en eventos a escala de Internet. de participantes proviene de su desacoplamiento de editores y suscriptores al colocar un servicio de entrega de eventos asincr\u00f3nicos entre ellos. En sistemas de publicaci\u00f3n/suscripci\u00f3n verdaderamente a escala de Internet, el servicio de entrega de eventos incluir\u00e1 un gran conjunto de nodos de intermediarios interconectados que abarcan una amplia \u00e1rea geogr\u00e1fica -LRB- y, por lo tanto, de red -RRB-. Si bien las capacidades de comunicaci\u00f3n de los sistemas de publicaci\u00f3n/suscripci\u00f3n est\u00e1n bien probadas, es probable que sea necesario abordar consideraciones de seguridad para abarcar m\u00faltiples dominios administrativos. Como la seguridad y el control de acceso son casi la ant\u00edtesis del desacoplamiento, hasta ahora relativamente poca investigaci\u00f3n de publicaci\u00f3n/suscripci\u00f3n se ha centrado en la seguridad. El objetivo general de nuestra investigaci\u00f3n es desarrollar redes de publicaci\u00f3n/suscripci\u00f3n a escala de Internet que proporcionen una entrega segura y eficiente de eventos, tolerancia a fallas y autorreparaci\u00f3n en la infraestructura de entrega, y una interfaz de eventos conveniente. En -LSB-12-RSB- Pesonen et al. proponer una arquitectura de control de acceso multidominio basada en capacidades para sistemas de publicaci\u00f3n/suscripci\u00f3n. La arquitectura proporciona un mecanismo para autorizar a los clientes de eventos a publicar y suscribirse a tipos de eventos. Los privilegios del cliente son verificados por el corredor local al que se conecta el cliente para acceder al sistema de publicaci\u00f3n/suscripci\u00f3n. El enfoque implementa el control de acceso en el borde de la red de intermediarios y supone que se puede confiar en que todos los intermediarios aplicar\u00e1n las pol\u00edticas de control de acceso correctamente. Cualquier corredor malicioso, comprometido o no autorizado es libre de leer y escribir cualquier evento que pase por \u00e9l en su camino desde los editores hasta los suscriptores. Proponemos hacer cumplir el control de acceso dentro de la red de corredores cifrando el contenido del evento,y esa pol\u00edtica dicta controles sobre las claves de cifrado necesarias. Con el contenido del evento cifrado, solo aquellos intermediarios que est\u00e1n autorizados a acceder a las claves de cifrado pueden acceder al contenido del evento -LRB-, es decir, publicar, suscribirse o filtrar -RRB-. Trasladamos efectivamente la aplicaci\u00f3n del control de acceso de los intermediarios a los administradores de claves de cifrado. Esperamos que sea necesario aplicar el control de acceso en un sistema de publicaci\u00f3n/suscripci\u00f3n multidominio cuando varias organizaciones forman un sistema de publicaci\u00f3n/suscripci\u00f3n compartido pero ejecutan m\u00faltiples aplicaciones independientes. El control de acceso tambi\u00e9n podr\u00eda ser necesario cuando una sola organizaci\u00f3n consta de m\u00faltiples subdominios que entregan datos confidenciales a trav\u00e9s del sistema de publicaci\u00f3n/suscripci\u00f3n de toda la organizaci\u00f3n. Ambos casos requieren control de acceso porque la entrega de eventos en una infraestructura din\u00e1mica de publicaci\u00f3n/suscripci\u00f3n basada en una red de intermediarios compartida bien puede llevar a que los eventos se enruten a trav\u00e9s de dominios no autorizados a lo largo de sus rutas desde los editores hasta los suscriptores. Hay dos beneficios particulares al compartir la infraestructura de publicaci\u00f3n/suscripci\u00f3n, los cuales se relacionan con la red de corredores. En primer lugar, los corredores de intercambio crear\u00e1n una red f\u00edsicamente m\u00e1s grande que proporcionar\u00e1 un mayor alcance geogr\u00e1fico. En segundo lugar, aumentar la interconectividad de los corredores permitir\u00e1 que el sistema de publicaci\u00f3n/suscripci\u00f3n proporcione una mayor tolerancia a fallos. La Figura 1 muestra la red de publicaci\u00f3n/suscripci\u00f3n multidominio que utilizamos como ejemplo en este documento. Este dominio contiene un conjunto de c\u00e1maras CCTV que publican informaci\u00f3n sobre los movimientos de veh\u00edculos en el \u00e1rea de Londres. Hemos incluido al Detective Smith como suscriptor en este dominio. Dominio del servicio de cargo por congesti\u00f3n. Los sistemas dentro de este dominio emiten los cargos que se aplican a los veh\u00edculos que han pasado por la zona de Tasa de Congesti\u00f3n de Londres cada d\u00eda. Los datos de origen del reconocimiento de matr\u00edculas provienen de las c\u00e1maras del dominio de la Polic\u00eda Metropolitana. El hecho de que los CCS solo est\u00e9n autorizados a leer un subconjunto de los datos de eventos del veh\u00edculo ejercer\u00e1 algunas de las caracter\u00edsticas clave del control de acceso al sistema de publicaci\u00f3n/suscripci\u00f3n exigible presentado en este documento. Dominio PITO. Es el propietario del tipo de evento en este escenario particular. El cifrado protege la confidencialidad de los eventos en caso de que se transporten a trav\u00e9s de dominios no autorizados. Sin embargo, cifrar eventos completos significa que los agentes no autorizados no pueden tomar decisiones de enrutamiento eficientes. Nuestro enfoque es aplicar cifrado a los atributos individuales de los eventos. De esta manera, nuestra pol\u00edtica de control de acceso a m\u00faltiples dominios funciona con una granularidad m\u00e1s fina: los editores y suscriptores pueden tener acceso autorizado a un subconjunto de atributos disponibles. En los casos en los que se utilizan eventos no cifrados para el enrutamiento, podemos reducir la cantidad total de eventos enviados a trav\u00e9s del sistema sin revelar los valores de los atributos confidenciales. De este modo, preservamos la privacidad de los conductores y al mismo tiempo permitimos que el CCS haga su trabajo utilizando la infraestructura compartida de publicaci\u00f3n/suscripci\u00f3n.La detective obtiene una orden judicial que la autoriza a suscribirse a eventos de matr\u00edcula espec\u00edficos relacionados con su caso. Los sistemas actuales de control de acceso de publicaci\u00f3n/suscripci\u00f3n imponen la seguridad en el borde de la red de intermediarios donde los clientes se conectan a ella. Sin embargo, este enfoque a menudo no ser\u00e1 aceptable en sistemas a escala de Internet. Proponemos reforzar la seguridad dentro de la red de corredores, as\u00ed como en los bordes a los que se conectan los clientes de eventos, mediante el cifrado del contenido del evento. Las publicaciones se cifrar\u00e1n con sus claves de cifrado espec\u00edficas del tipo de evento. Al controlar el acceso a las claves de cifrado, podemos controlar el acceso a los tipos de eventos. El enfoque propuesto permite a los agentes de eventos enrutar eventos incluso cuando solo tienen acceso a un subconjunto de posibles claves de cifrado. Introducimos sistemas descentralizados de publicaci\u00f3n/suscripci\u00f3n y criptograf\u00eda relevante en la Secci\u00f3n 2. En la Secci\u00f3n 3 presentamos nuestro modelo para cifrar el contenido del evento tanto a nivel de evento como de atributo. La secci\u00f3n 4 analiza la gesti\u00f3n de claves de cifrado en sistemas de publicaci\u00f3n/suscripci\u00f3n multidominio. Finalmente, la Secci\u00f3n 6 analiza el trabajo relacionado con la seguridad de los sistemas de publicaci\u00f3n/suscripci\u00f3n y la Secci\u00f3n 7 proporciona comentarios finales. 2. ANTECEDENTES En esta secci\u00f3n proporcionamos una breve introducci\u00f3n a los sistemas descentralizados de publicaci\u00f3n/suscripci\u00f3n. Indicamos nuestras suposiciones sobre los sistemas de publicaci\u00f3n/suscripci\u00f3n multidominio y describimos c\u00f3mo estas suposiciones influyen en los desarrollos que hemos realizado a partir de nuestro trabajo publicado anteriormente. 2.1 Sistemas descentralizados de publicaci\u00f3n/suscripci\u00f3n Un sistema de publicaci\u00f3n/suscripci\u00f3n incluye editores, suscriptores y un servicio de eventos. Los editores publican eventos, los suscriptores se suscriben a eventos de su inter\u00e9s y el servicio de eventos es responsable de entregar los eventos publicados a todos los suscriptores cuyos intereses coincidan con el evento determinado. El servicio de eventos en un sistema de publicaci\u00f3n/suscripci\u00f3n descentralizado se distribuye entre varios nodos de intermediario. Juntos, estos corredores forman una red que es responsable de mantener las rutas de enrutamiento necesarias desde los editores hasta los suscriptores. Los clientes -LRB-, los editores y los suscriptores -RRB- se conectan a un corredor local, en el que el cliente conf\u00eda plenamente. En nuestra discusi\u00f3n nos referimos a los brokers de hosting de clientes como brokers de hosting de editores -LRB- PHB -RRB- o brokers de hosting de suscriptores -LRB- SHB -RRB- dependiendo de si el cliente conectado es un editor o un editor. implementaci\u00f3n de publicaci\u00f3n/suscripci\u00f3n multidominio por suscriptor, respectivamente. Un corredor local suele formar parte del mismo dominio que el cliente o es propiedad de un proveedor de servicios en el que el cliente conf\u00eda. Una red de intermediarios puede tener una topolog\u00eda est\u00e1tica -LRB-, por ejemplo, Siena -LSB- 3 -RSB- y Gryphon -LSB- 14 -RSB- -RRB- o una topolog\u00eda din\u00e1mica -LRB-, por ejemplo, Scribe -LSB- 4 -RSB- y Hermes. -LSB- 13 -RSB- -RRB-. Nuestro enfoque propuesto funcionar\u00e1 en ambos casos.Una topolog\u00eda est\u00e1tica permite al administrador del sistema crear dominios confiables y de esa manera mejorar la eficiencia del enrutamiento evitando cifrados innecesarios -LRB- ver Secci\u00f3n. Nuestro trabajo se basa en el sistema Hermes. Hermes es un middleware de publicaci\u00f3n/suscripci\u00f3n basado en contenido que incluye un s\u00f3lido soporte para tipos de eventos. En otras palabras, cada publicaci\u00f3n es una instancia de un tipo de evento predefinido particular. Las publicaciones se verifican en el corredor local de cada editorial. Nuestro esquema de cifrado a nivel de atributo supone que los eventos se escriben. Hermes utiliza una red superpuesta estructurada como transporte y, por lo tanto, tiene una topolog\u00eda din\u00e1mica. Una publicaci\u00f3n de Hermes consta de un identificador de tipo de evento y un conjunto de pares de valores de atributos. El identificador de tipo es el hash SHA-1 del nombre del tipo de evento. Se utiliza para enrutar la publicaci\u00f3n a trav\u00e9s de la red de agentes de eventos. Oculta convenientemente el tipo de publicaci\u00f3n, es decir, los corredores no pueden ver qu\u00e9 eventos fluyen a trav\u00e9s de ellos a menos que conozcan el nombre y el identificador del tipo de evento espec\u00edfico. 2.2 Tipos de eventos seguros Pesonen et al. introdujo tipos de eventos seguros en -LSB- 11 -RSB-, cuya integridad y autenticidad se puede confirmar mediante la verificaci\u00f3n de sus firmas digitales. Un efecto secundario \u00fatil de los tipos de eventos seguros es su tipo de evento y sus nombres de atributos \u00fanicos a nivel mundial. Las pol\u00edticas de control de acceso pueden hacer referencia a estos nombres. En este documento utilizamos el nombre seguro del tipo de evento o atributo para referirnos a la clave de cifrado utilizada para cifrar el evento o atributo. 2.3 Control de acceso basado en capacidades Pesonen et al. propuso una arquitectura de control de acceso basada en capacidades para sistemas de publicaci\u00f3n/suscripci\u00f3n multidominio en -LSB- 12 -RSB-. El modelo trata los tipos de eventos como recursos a los que los editores, suscriptores y agentes de eventos desean acceder. El propietario del tipo de evento es responsable de gestionar el control de acceso para un tipo de evento mediante la emisi\u00f3n de certificados de autorizaci\u00f3n de Infraestructura de clave p\u00fablica simple -LRB-SPKI-RRB- que otorgan al titular acceso al tipo de evento especificado. Por ejemplo, a los editores autorizados se les habr\u00e1 emitido un certificado de autorizaci\u00f3n que especifica que el editor, identificado por clave p\u00fablica, est\u00e1 autorizado a publicar instancias del tipo de evento especificado en el certificado. En este documento aprovechamos el mecanismo de control de acceso mencionado anteriormente controlando el acceso a las claves de cifrado utilizando los mismos certificados de autorizaci\u00f3n. Es decir, un editor que est\u00e1 autorizado a publicar un tipo de evento determinado tambi\u00e9n est\u00e1 autorizado a acceder a las claves de cifrado utilizadas para proteger eventos de ese tipo. 4. 2.4 Modelo de amenaza El objetivo del mecanismo propuesto es hacer cumplir el control de acceso para los participantes autorizados en el sistema. En nuestro caso, el primer nivel de control de acceso se aplica cuando el participante intenta unirse a la red de publicaci\u00f3n/suscripci\u00f3n. Los corredores de eventos no autorizados no pueden unirse a la red de corredores. De manera similar, los clientes de eventos no autorizados no pueden conectarse a un corredor de eventos.Todas las conexiones en la red de corredores entre los corredores de eventos y los clientes de eventos utilizan Transport Layer Security -LRB- TLS -RRB- -LSB- 5 -RSB- para evitar el acceso no autorizado en la capa de transporte. La arquitectura del sistema de publicaci\u00f3n/suscripci\u00f3n significa que los clientes de eventos deben conectarse a los agentes de eventos para poder acceder al sistema de publicaci\u00f3n/suscripci\u00f3n. Por tanto, asumimos que estos clientes no son una amenaza. El cliente de eventos depende completamente del corredor de eventos local para acceder a la red de corredores. Por lo tanto, el cliente del evento no puede acceder a ning\u00fan evento sin la ayuda del corredor local. Por otro lado, los corredores pueden analizar todos los eventos del sistema que pasan por ellos. Un broker puede analizar tanto el tr\u00e1fico de eventos como el n\u00famero y nombres de atributos que se pueblan en un evento -LRB- en el caso de cifrado a nivel de atributo -RRB-. Existen enfoques viables para prevenir el an\u00e1lisis del tr\u00e1fico mediante la inserci\u00f3n de eventos aleatorios en el flujo de eventos para producir un patr\u00f3n de tr\u00e1fico uniforme. 6. TRABAJOS RELACIONADOS Wang et al. han categorizado los diversos problemas de seguridad que deben abordarse en los sistemas de publicaci\u00f3n/suscripci\u00f3n en el futuro en -LSB- 20 -RSB-. El documento es una descripci\u00f3n general completa de los problemas de seguridad en los sistemas de publicaci\u00f3n/suscripci\u00f3n y, como tal, intenta llamar la atenci\u00f3n sobre los problemas en lugar de proporcionar soluciones. Tocino y cols. en -LSB- 1 -RSB- examina el uso del control de acceso basado en roles en sistemas de publicaci\u00f3n/suscripci\u00f3n distribuidos y multidominio. Opyrchal y Prakash abordan el problema de la confidencialidad de eventos en el \u00faltimo enlace entre el suscriptor y el SHB en -LSB- 10 -RSB-. Afirman correctamente que un enfoque de comunicaci\u00f3n grupal segura no es factible en un entorno como el de publicaci\u00f3n/suscripci\u00f3n que tiene membres\u00edas grupales altamente din\u00e1micas. En nuestro trabajo asumimos que el SHB es lo suficientemente potente como para mantener una conexi\u00f3n segura TLS para cada suscriptor local. Tanto Srivatsa et al. -LSB- 19 -RSB- y Raiciu et al. -LSB- 16 -RSB- presentan mecanismos para proteger la confidencialidad de los mensajes en infraestructuras descentralizadas de publicaci\u00f3n/suscripci\u00f3n. En comparaci\u00f3n con nuestro trabajo, ambos documentos tienen como objetivo proporcionar los medios para proteger la integridad y confidencialidad de los mensajes, mientras que el objetivo de nuestro trabajo es hacer cumplir el control de acceso dentro de la red de corredores. Raiciu et al. asumen en su trabajo que ninguno de los corredores de la red es confiable y, por lo tanto, todos los eventos est\u00e1n cifrados desde el editor hasta el suscriptor y que todas las coincidencias se basan en eventos cifrados. Por el contrario, suponemos que se conf\u00eda en que algunos de los intermediarios en el camino de una publicaci\u00f3n accedan a esa publicaci\u00f3n y, por lo tanto, pueden implementar la coincidencia de eventos. Tambi\u00e9n asumimos que siempre se conf\u00eda en el editor y los agentes de alojamiento de suscriptores para acceder a la publicaci\u00f3n. Finalmente, Fiege et al. aborde el tema relacionado de la visibilidad de eventos en -LSB- 6 -RSB-.Si bien el trabajo se concentr\u00f3 en el uso de \u00e1mbitos como mecanismo para estructurar sistemas basados \u200b\u200ben eventos a gran escala, la noci\u00f3n de visibilidad de eventos resuena con el control de acceso hasta cierto punto. 7. CONCLUSIONES El cifrado del contenido de eventos se puede utilizar para aplicar una pol\u00edtica de control de acceso mientras los eventos est\u00e1n en tr\u00e1nsito en la red de intermediarios de un sistema de publicaci\u00f3n/suscripci\u00f3n multidominio. Se puede implementar cifrado a nivel de atributo para aplicar pol\u00edticas de control de acceso detalladas. Adem\u00e1s de proporcionar control de acceso a nivel de atributos, el cifrado de atributos permite a los intermediarios parcialmente autorizados implementar enrutamiento basado en contenido seg\u00fan los atributos a los que tienen acceso.", "keyphrases": ["sistema seguro de publicaci\u00f3n/suscripci\u00f3n", "distribuir control de acceso", "dominio de administrador m\u00faltiple", "cifrado de atributo", "multidominio", "gastos generales comunes generales", "sistema de distribuci\u00f3n-aplicaci\u00f3n de distribuci\u00f3n", "llevar a cabo", "cifrar", "servicio de carga por congesti\u00f3n"]}
{"file_name": "C-29", "text": "Implementaci\u00f3n y evaluaci\u00f3n del rendimiento de CONFLEX-G: programa de b\u00fasqueda de espacio conformacional molecular habilitado para cuadr\u00edcula con OmniRPC RESUMEN CONFLEX-G es la versi\u00f3n habilitada para cuadr\u00edcula de un programa de b\u00fasqueda de espacio conformacional molecular llamado CONFLEX. Hemos implementado CONFLEX-G utilizando un sistema RPC de red llamado OmniRPC. En este art\u00edculo, informamos el rendimiento de CONFLEX-G en un banco de pruebas grid de varios cl\u00fasteres de PC distribuidos geogr\u00e1ficamente. Para explorar muchas conformaciones de biomol\u00e9culas grandes, CONFLEX-G genera estructuras de prueba de las mol\u00e9culas y asigna trabajos para optimizar una estructura de prueba con un m\u00e9todo confiable de mec\u00e1nica molecular en la red. OmniRPC proporciona un modelo de persistencia restringido para admitir aplicaciones de b\u00fasqueda param\u00e9trica. En este modelo, cuando el procedimiento de inicializaci\u00f3n se define en el m\u00f3dulo RPC, el m\u00f3dulo se inicializa autom\u00e1ticamente en el momento de la invocaci\u00f3n llamando al procedimiento de inicializaci\u00f3n. Esto puede eliminar comunicaciones e inicializaciones innecesarias en cada llamada en CONFLEX-G. CONFLEXG puede lograr un rendimiento comparable al de CONFLEX MPI y puede explotar m\u00e1s recursos inform\u00e1ticos al permitir el uso de un cl\u00faster de m\u00faltiples cl\u00fasteres en la red. El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces en el caso de la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere un tiempo significativo. El desequilibrio de carga del tiempo de optimizaci\u00f3n de la estructura de prueba tambi\u00e9n puede causar una degradaci\u00f3n del rendimiento. 1. INTRODUCCI\u00d3N Recientemente, el concepto de red computacional ha comenzado a atraer un inter\u00e9s significativo en el campo de la computaci\u00f3n en red de alto rendimiento. CONFLEX es uno de los programas de b\u00fasqueda de espacios conformacionales m\u00e1s eficientes y confiables -LSB- 1 -RSB-. Hemos aplicado este programa a la paralelizaci\u00f3n utilizando computaci\u00f3n global. El rendimiento del CONFLEX paralelizado permite la exploraci\u00f3n de la regi\u00f3n de menor energ\u00eda del espacio conformacional de p\u00e9ptidos peque\u00f1os dentro de un tiempo transcurrido disponible utilizando un grupo de PC local. Dado que la optimizaci\u00f3n de la estructura de prueba en CONFLEX se calcula mediante mec\u00e1nica molecular, la b\u00fasqueda del espacio conformacional se puede realizar r\u00e1pidamente en comparaci\u00f3n con la que se realiza mediante el c\u00e1lculo de orbitales moleculares. Aunque se utiliz\u00f3 la versi\u00f3n paralelizada de CONFLEX para calcular en paralelo la optimizaci\u00f3n de la estructura, que ocupa m\u00e1s del 90 % del procesamiento en la b\u00fasqueda de conformaci\u00f3n molecular, no se pudo lograr una mejora suficiente en la aceleraci\u00f3n solo con este m\u00e9todo. Esto requiere los vastos recursos inform\u00e1ticos de un entorno inform\u00e1tico en red. En este art\u00edculo, describimos CONFLEX-G, un programa de b\u00fasqueda conformacional molecular habilitado para cuadr\u00edculas, que utiliza OmniRPC e informamos su rendimiento en una cuadr\u00edcula de varios grupos de PC que est\u00e1n distribuidos geogr\u00e1ficamente. El prototipo CONFLEX-G asigna la optimizaci\u00f3n de las estructuras de prueba de c\u00e1lculo, que es una tarea que requiere mucho tiempo, a los nodos trabajadores en el entorno de la red para obtener un alto rendimiento.Adem\u00e1s, comparamos el rendimiento de CONFLEX-G en un cl\u00faster de PC local con el de un banco de pruebas grid. OmniRPC -LSB- 2, 3, 4 -RSB- es una implementaci\u00f3n segura para subprocesos de Ninf RPC -LSB- 5, 6 -RSB- que es una instalaci\u00f3n Grid RPC para inform\u00e1tica en entornos grid. Varios sistemas adoptan el concepto de RPC como modelo b\u00e1sico para la computaci\u00f3n en entorno grid, incluidos Ninf-G -LSB- 7 -RSB-, NetSolve -LSB- 8 -RSB- y CORBA -LSB- 9 -RSB-. El sistema RPCstyle proporciona una interfaz de programaci\u00f3n intuitiva y f\u00e1cil de usar, que permite a los usuarios del sistema grid crear f\u00e1cilmente aplicaciones habilitadas para grid. Para admitir la programaci\u00f3n paralela, un cliente RPC puede emitir solicitudes de llamadas as\u00edncronas a una computadora remota diferente para explotar el paralelismo en toda la red a trav\u00e9s de OmniRPC. En este art\u00edculo, proponemos el modelo de persistencia OmniRPC a un sistema Grid RPC y demostramos su efectividad. Para admitir una aplicaci\u00f3n t\u00edpica para un entorno de grilla, como una aplicaci\u00f3n de b\u00fasqueda param\u00e9trica, en la que la misma funci\u00f3n se ejecuta con diferentes par\u00e1metros de entrada en el mismo conjunto de datos. En el sistema GridRPC actual -LSB- 10 -RSB-, los datos establecidos por la llamada anterior no pueden ser utilizados por llamadas posteriores. Este art\u00edculo demuestra que CONFLEX-G es capaz de explotar los enormes recursos inform\u00e1ticos de un entorno de red y buscar conf\u00f3rmeros moleculares a gran escala. Demostramos CONFLEX-G en nuestro banco de pruebas de cuadr\u00edcula utilizando la prote\u00edna real como mol\u00e9cula de muestra. La funci\u00f3n OmniRPC del m\u00f3dulo inicializable autom\u00e1tico -LRB- AIM -RRB- permite al sistema calcular de manera eficiente numerosos conformadores. Adem\u00e1s, al utilizar OmniRPC, el usuario puede paralelizar en grid la aplicaci\u00f3n existente y pasar del cl\u00faster al entorno de grid sin modificar el c\u00f3digo del programa ni compilar el programa. Adem\u00e1s, el usuario puede crear f\u00e1cilmente un entorno de red privada. Una descripci\u00f3n general Figura 1: Algoritmo de b\u00fasqueda de espacio conformacional en el CONFLEX original. del sistema CONFLEX se presenta en la Secci\u00f3n 2, y la implementaci\u00f3n y el dise\u00f1o de CONFLEX-G se describen en la Secci\u00f3n 3. Informamos los resultados experimentales obtenidos usando CONFLEX-G y discutimos su desempe\u00f1o en la Secci\u00f3n 4. En la Secci\u00f3n 6, presentamos conclusiones y discutimos temas para futuros estudios. 5. TRABAJOS RELACIONADOS Recientemente se ha desarrollado un algoritmo que resuelve los problemas de paralelizaci\u00f3n y comunicaci\u00f3n en procesadores mal conectados para ser utilizados en simulaci\u00f3n. Esto nos ha permitido simular el plegamiento por primera vez y examinar directamente las enfermedades relacionadas con el plegamiento. SETI@home[14] es un programa para buscar vida extraterrestre mediante el an\u00e1lisis de se\u00f1ales de radiotelescopios utilizando datos de radiotelescopios por transformada de Fourier procedentes de telescopios de diferentes sitios. SETI@home aborda problemas inmensamente paralelos, en los que el c\u00e1lculo se puede dividir f\u00e1cilmente entre varios ordenadores. Los fragmentos de datos del radiotelescopio se pueden asignar f\u00e1cilmente a diferentes computadoras. Sin embargo, es posible que OmniRPC no requiera las habilidades y el esfuerzo necesarios para desarrollar una aplicaci\u00f3n grid.Nimrod/G -LSB- 15 -RSB- es una herramienta para modelado param\u00e9trico distribuido e implementa una granja de tareas paralela para simulaciones que requieren varios par\u00e1metros de entrada variables. Nimrod se ha aplicado a aplicaciones que incluyen bioinform\u00e1tica, investigaci\u00f3n de operaciones y modelado molecular para el dise\u00f1o de f\u00e1rmacos. NetSolve -LSB- 8 -RSB- es una instalaci\u00f3n RPC similar a OmniRPC y Ninf, que proporciona una interfaz de programaci\u00f3n similar y un mecanismo de equilibrio de carga autom\u00e1tico. Matsuoka et al. -LSB- 16 -RSB- tambi\u00e9n ha discutido varias cuestiones de dise\u00f1o relacionadas con los sistemas RPC de red. 6. CONCLUSIONES Y TRABAJO FUTURO Hemos dise\u00f1ado e implementado CONFLEX-G utilizando OmniRPC. Informamos su rendimiento en un banco de pruebas en red de varios cl\u00fasteres de PC distribuidos geogr\u00e1ficamente. Para explorar la conformaci\u00f3n de biomol\u00e9culas grandes, se utiliz\u00f3 CONFLEXG para generar estructuras de prueba de las mol\u00e9culas y asignar tareas para optimizarlas mediante la mec\u00e1nica molecular en la red. OmniRPC proporciona un modelo de persistencia restringido para que el m\u00f3dulo se inicialice autom\u00e1ticamente en el momento de la invocaci\u00f3n llamando al procedimiento de inicializaci\u00f3n. Esto puede eliminar la comunicaci\u00f3n innecesaria y la inicializaci\u00f3n en cada llamada en CONFLEX-G. CONFLEX-G puede lograr un rendimiento comparable al de CONFLEX MPI y explotar m\u00e1s recursos inform\u00e1ticos al permitir el uso de m\u00faltiples cl\u00fasteres de PC en la red. El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces para la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere una gran cantidad de tiempo. El desequilibrio de carga de las optimizaciones de la estructura de prueba puede provocar una degradaci\u00f3n del rendimiento. Necesitamos refinar el algoritmo utilizado para generar la estructura de prueba para mejorar la optimizaci\u00f3n del equilibrio de carga para las estructuras de prueba en CONFLEX. Los estudios futuros incluir\u00e1n el desarrollo de herramientas de implementaci\u00f3n y un examen de la tolerancia a fallos. En el OmniRPC actual, el registro de un programa de ejecuci\u00f3n en hosts remotos y las implementaciones de programas de trabajo se configuran manualmente. Se necesitar\u00e1n herramientas de implementaci\u00f3n a medida que aumente la cantidad de hosts remotos. En entornos de red en los que el entorno cambia din\u00e1micamente, tambi\u00e9n es necesario admitir tolerancia a fallos. Esta caracter\u00edstica es especialmente importante en aplicaciones a gran escala que requieren c\u00e1lculos prolongados en un entorno de red. Planeamos refinar el algoritmo de optimizaci\u00f3n conformacional en CONFLEX para explorar la b\u00fasqueda en el espacio de conformaci\u00f3n de biomol\u00e9culas m\u00e1s grandes, como la proteasa del VIH, utilizando hasta 1000 trabajadores en un entorno de red.-LSB- 16 -RSB- tambi\u00e9n ha discutido varias cuestiones de dise\u00f1o relacionadas con los sistemas RPC de red. 6. CONCLUSIONES Y TRABAJO FUTURO Hemos dise\u00f1ado e implementado CONFLEX-G utilizando OmniRPC. Informamos su rendimiento en un banco de pruebas en red de varios cl\u00fasteres de PC distribuidos geogr\u00e1ficamente. Para explorar la conformaci\u00f3n de biomol\u00e9culas grandes, se utiliz\u00f3 CONFLEXG para generar estructuras de prueba de las mol\u00e9culas y asignar tareas para optimizarlas mediante la mec\u00e1nica molecular en la red. OmniRPC proporciona un modelo de persistencia restringido para que el m\u00f3dulo se inicialice autom\u00e1ticamente en el momento de la invocaci\u00f3n llamando al procedimiento de inicializaci\u00f3n. Esto puede eliminar la comunicaci\u00f3n innecesaria y la inicializaci\u00f3n en cada llamada en CONFLEX-G. CONFLEX-G puede lograr un rendimiento comparable al de CONFLEX MPI y explotar m\u00e1s recursos inform\u00e1ticos al permitir el uso de m\u00faltiples cl\u00fasteres de PC en la red. El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces para la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere una gran cantidad de tiempo. El desequilibrio de carga de las optimizaciones de la estructura de prueba puede provocar una degradaci\u00f3n del rendimiento. Necesitamos refinar el algoritmo utilizado para generar la estructura de prueba para mejorar la optimizaci\u00f3n del equilibrio de carga para las estructuras de prueba en CONFLEX. Los estudios futuros incluir\u00e1n el desarrollo de herramientas de implementaci\u00f3n y un examen de la tolerancia a fallos. En el OmniRPC actual, el registro de un programa de ejecuci\u00f3n en hosts remotos y las implementaciones de programas de trabajo se configuran manualmente. Se necesitar\u00e1n herramientas de implementaci\u00f3n a medida que aumente la cantidad de hosts remotos. En entornos de red en los que el entorno cambia din\u00e1micamente, tambi\u00e9n es necesario admitir tolerancia a fallos. Esta caracter\u00edstica es especialmente importante en aplicaciones a gran escala que requieren c\u00e1lculos prolongados en un entorno de red. Planeamos refinar el algoritmo de optimizaci\u00f3n conformacional en CONFLEX para explorar la b\u00fasqueda en el espacio de conformaci\u00f3n de biomol\u00e9culas m\u00e1s grandes, como la proteasa del VIH, utilizando hasta 1000 trabajadores en un entorno de red.-LSB- 16 -RSB- tambi\u00e9n ha discutido varias cuestiones de dise\u00f1o relacionadas con los sistemas RPC de red. 6. CONCLUSIONES Y TRABAJO FUTURO Hemos dise\u00f1ado e implementado CONFLEX-G utilizando OmniRPC. Informamos su rendimiento en un banco de pruebas en red de varios cl\u00fasteres de PC distribuidos geogr\u00e1ficamente. Para explorar la conformaci\u00f3n de biomol\u00e9culas grandes, se utiliz\u00f3 CONFLEXG para generar estructuras de prueba de las mol\u00e9culas y asignar tareas para optimizarlas mediante la mec\u00e1nica molecular en la red. OmniRPC proporciona un modelo de persistencia restringido para que el m\u00f3dulo se inicialice autom\u00e1ticamente en el momento de la invocaci\u00f3n llamando al procedimiento de inicializaci\u00f3n. Esto puede eliminar la comunicaci\u00f3n innecesaria y la inicializaci\u00f3n en cada llamada en CONFLEX-G. CONFLEX-G puede lograr un rendimiento comparable al de CONFLEX MPI y explotar m\u00e1s recursos inform\u00e1ticos al permitir el uso de m\u00faltiples cl\u00fasteres de PC en la red. El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces para la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere una gran cantidad de tiempo. El desequilibrio de carga de las optimizaciones de la estructura de prueba puede provocar una degradaci\u00f3n del rendimiento. Necesitamos refinar el algoritmo utilizado para generar la estructura de prueba para mejorar la optimizaci\u00f3n del equilibrio de carga para las estructuras de prueba en CONFLEX. Los estudios futuros incluir\u00e1n el desarrollo de herramientas de implementaci\u00f3n y un examen de la tolerancia a fallos. En el OmniRPC actual, el registro de un programa de ejecuci\u00f3n en hosts remotos y las implementaciones de programas de trabajo se configuran manualmente. Se necesitar\u00e1n herramientas de implementaci\u00f3n a medida que aumente la cantidad de hosts remotos. En entornos de red en los que el entorno cambia din\u00e1micamente, tambi\u00e9n es necesario admitir tolerancia a fallos. Esta caracter\u00edstica es especialmente importante en aplicaciones a gran escala que requieren c\u00e1lculos prolongados en un entorno de red. Planeamos refinar el algoritmo de optimizaci\u00f3n conformacional en CONFLEX para explorar la b\u00fasqueda en el espacio de conformaci\u00f3n de biomol\u00e9culas m\u00e1s grandes, como la proteasa del VIH, utilizando hasta 1000 trabajadores en un entorno de red.El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces para la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere una gran cantidad de tiempo. El desequilibrio de carga de las optimizaciones de la estructura de prueba puede provocar una degradaci\u00f3n del rendimiento. Necesitamos refinar el algoritmo utilizado para generar la estructura de prueba para mejorar la optimizaci\u00f3n del equilibrio de carga para las estructuras de prueba en CONFLEX. Los estudios futuros incluir\u00e1n el desarrollo de herramientas de implementaci\u00f3n y un examen de la tolerancia a fallos. En el OmniRPC actual, el registro de un programa de ejecuci\u00f3n en hosts remotos y las implementaciones de programas de trabajo se configuran manualmente. Se necesitar\u00e1n herramientas de implementaci\u00f3n a medida que aumente la cantidad de hosts remotos. En entornos de red en los que el entorno cambia din\u00e1micamente, tambi\u00e9n es necesario admitir tolerancia a fallos. Esta caracter\u00edstica es especialmente importante en aplicaciones a gran escala que requieren c\u00e1lculos prolongados en un entorno de red. Planeamos refinar el algoritmo de optimizaci\u00f3n conformacional en CONFLEX para explorar la b\u00fasqueda en el espacio de conformaci\u00f3n de biomol\u00e9culas m\u00e1s grandes, como la proteasa del VIH, utilizando hasta 1000 trabajadores en un entorno de red.El resultado experimental muestra que CONFLEX-G logr\u00f3 una aceleraci\u00f3n de 56,5 veces para la mol\u00e9cula 1BL1, donde la mol\u00e9cula consta de una gran cantidad de \u00e1tomos y cada optimizaci\u00f3n de la estructura de prueba requiere una gran cantidad de tiempo. El desequilibrio de carga de las optimizaciones de la estructura de prueba puede provocar una degradaci\u00f3n del rendimiento. Necesitamos refinar el algoritmo utilizado para generar la estructura de prueba para mejorar la optimizaci\u00f3n del equilibrio de carga para las estructuras de prueba en CONFLEX. Los estudios futuros incluir\u00e1n el desarrollo de herramientas de implementaci\u00f3n y un examen de la tolerancia a fallos. En el OmniRPC actual, el registro de un programa de ejecuci\u00f3n en hosts remotos y las implementaciones de programas de trabajo se configuran manualmente. Se necesitar\u00e1n herramientas de implementaci\u00f3n a medida que aumente la cantidad de hosts remotos. En entornos de red en los que el entorno cambia din\u00e1micamente, tambi\u00e9n es necesario admitir tolerancia a fallos. Esta caracter\u00edstica es especialmente importante en aplicaciones a gran escala que requieren c\u00e1lculos prolongados en un entorno de red. Planeamos refinar el algoritmo de optimizaci\u00f3n conformacional en CONFLEX para explorar la b\u00fasqueda en el espacio de conformaci\u00f3n de biomol\u00e9culas m\u00e1s grandes, como la proteasa del VIH, utilizando hasta 1000 trabajadores en un entorno de red.", "keyphrases": ["conflex-g", "omnirpc", "b\u00fasqueda de espacio conforme", "biomol\u00e9cula", "m\u00f3dulo rpc", "procedimiento inicial", "mpu", "grupo de computadoras", "computaci\u00f3n en red", "sistema rpc de red", "mecanismo molecular", "m\u00f3dulo de inicializaci\u00f3n autom\u00e1tica"]}
{"file_name": "C-9", "text": "EDAS: Proporcionar un entorno para servicios adaptativos descentralizados RESUMEN A medida que la idea de la virtualizaci\u00f3n de la potencia inform\u00e1tica, el almacenamiento y el ancho de banda se vuelve cada vez m\u00e1s importante, la computaci\u00f3n grid evoluciona y se aplica a un n\u00famero cada vez mayor de aplicaciones. El entorno para servicios adaptativos descentralizados -LRB- EDAS -RRB- proporciona una infraestructura similar a una red para servicios a largo plazo a los que acceden los usuarios -LRB-, por ejemplo, servidor web, repositorio de c\u00f3digo fuente, etc. -RRB-. Su objetivo es apoyar la ejecuci\u00f3n aut\u00f3noma y la evoluci\u00f3n de los servicios en t\u00e9rminos de escalabilidad y distribuci\u00f3n consciente de los recursos. EDAS ofrece modelos de servicios flexibles basados \u200b\u200ben objetos m\u00f3viles distribuidos que van desde un escenario cliente-servidor tradicional hasta un enfoque totalmente basado en peer-to-peer. La gesti\u00f3n autom\u00e1tica y din\u00e1mica de recursos permite un uso optimizado de los recursos disponibles y al mismo tiempo minimiza la complejidad administrativa. 1. INTRODUCCI\u00d3N Las infraestructuras para computaci\u00f3n grid tienen como objetivo virtualizar un grupo de computadoras, servidores y almacenamiento como un gran sistema inform\u00e1tico. La gesti\u00f3n de recursos es una cuesti\u00f3n clave en estos sistemas, necesaria para una distribuci\u00f3n eficiente y automatizada de tareas en la red. Estas infraestructuras de red a menudo se implementan a nivel empresarial, pero proyectos como SETI@home -LSB- 1 -RSB- tambi\u00e9n han demostrado la viabilidad de redes m\u00e1s descentralizadas. Las infraestructuras de computaci\u00f3n grid actuales no brindan soporte suficiente para la ejecuci\u00f3n de servicios distribuidos a largo plazo a los que acceden los usuarios, ya que est\u00e1n dise\u00f1adas para resolver tareas inform\u00e1ticas o de uso intensivo de datos con un conjunto de par\u00e1metros m\u00e1s o menos fijo. En cambio, una infraestructura para servicios a largo plazo tiene que ubicar los servicios en funci\u00f3n de su demanda actual y sus necesidades futuras estimadas. Sin embargo, la migraci\u00f3n es costosa ya que es necesario transferir todo el estado de un servicio. Adem\u00e1s, no se puede acceder a un servicio no replicado durante la migraci\u00f3n. Por lo tanto, la gesti\u00f3n de recursos debe evitar la migraci\u00f3n si es posible. Adem\u00e1s, debe proporcionarse un concepto de servicio que, en primer lugar, evite la sobrecarga y, en segundo lugar, inhiba la indisponibilidad del servicio si no se puede evitar la migraci\u00f3n. EDAS -LSB- 2 -RSB- tiene como objetivo proporcionar una infraestructura similar a una red para servicios a largo plazo a los que acceden los usuarios que permite la adaptaci\u00f3n din\u00e1mica en tiempo de ejecuci\u00f3n, proporciona una infraestructura de gesti\u00f3n y ofrece soporte a nivel de sistema para escalabilidad y fallas. tolerancia. Los nodos pueden unirse y salir din\u00e1micamente de la infraestructura, y todas las tareas de gesti\u00f3n, especialmente la gesti\u00f3n de recursos, est\u00e1n descentralizadas. El entorno se basa en nuestra infraestructura de middleware AspectIX -LSB- 3 -RSB-, que admite directamente la reconfiguraci\u00f3n din\u00e1mica de servicios basada en QoS. La gesti\u00f3n de recursos se centra en la ejecuci\u00f3n de servicios que tienen un tiempo de funcionamiento largo y potencialmente infinito. Estos servicios est\u00e1n organizados en proyectos. Cada proyecto tiene un alcance de ejecuci\u00f3n distribuido llamado entorno de servicio. Un entorno as\u00ed posiblemente abarque m\u00faltiples instituciones.Cada instituci\u00f3n representa un dominio administrativo que puede respaldar un proyecto con un conjunto fijo de recursos. Nuestro enfoque apoya la gesti\u00f3n adaptativa de recursos de todos los proyectos en el \u00e1mbito de una instituci\u00f3n basado en un algoritmo inspirado en los algoritmos difusos para el equilibrio de carga descentralizado -LSB- 4 -RSB-. No se sabe c\u00f3mo subdividir de manera \u00f3ptima estos recursos para los servicios, ya que la demanda de recursos de los servicios puede cambiar con el tiempo o incluso fluctuar con frecuencia. Para proporcionar recursos seg\u00fan sea necesario, nuestro enfoque vuelve a dedicar autom\u00e1ticamente recursos gratuitos o no necesarios entre instancias de servicio en proyectos y nodos. En los casos en los que no sea posible la rededicaci\u00f3n, se inicia la migraci\u00f3n del servicio demandante. En una infraestructura grid de servicios a largo plazo, la replicaci\u00f3n activa tiene varios beneficios: las r\u00e9plicas pueden unirse y abandonar el grupo de objetos y, por lo tanto, se pueden migrar sin que el servicio est\u00e9 disponible. Finalmente, se puede tolerar una cierta cantidad de fallas de nodos. La secci\u00f3n 4 explica los conceptos de autogesti\u00f3n y rededicaci\u00f3n de la gesti\u00f3n adaptativa distribuida de recursos. La Secci\u00f3n 5 describe el marco para los servicios adaptativos descentralizados. La Secci\u00f3n 6 describe el trabajo relacionado y finalmente la Secci\u00f3n 7 concluye el art\u00edculo. 6. TRABAJO RELACIONADO Las infraestructuras grid como Globus-Toolkit -LSB- 11 -RSB- proporcionan servicios y mecanismos para entornos heterog\u00e9neos distribuidos para combinar recursos bajo demanda para resolver tareas que consumen recursos y computan de manera intensiva. Debido a esta orientaci\u00f3n, se centran en diferentes modelos de servicio y no brindan soporte para la movilidad de objetos, si es que siquiera admiten un enfoque de objetos distribuidos. Pero lo m\u00e1s importante es que siguen un enfoque diferente de gesti\u00f3n de recursos, ya que apuntan a la ejecuci\u00f3n paralela de un gran n\u00famero de tareas a corto y mediano plazo. Los objetos replicados activamente los proporciona Jgroup -LSB- 14 -RSB- basado en RMI. Sobre este middleware b\u00e1sico se ha implementado una capa de gesti\u00f3n de replicaci\u00f3n denominada ARM -LSB- 15 -RSB-. JGroup se centra en la replicaci\u00f3n activa de objetos pero carece de soporte para servicios m\u00e1s flexibles como lo hace EDAS. ARM se puede comparar con EDAS pero no admite distribuci\u00f3n que tenga en cuenta los recursos. Fog -LSB- 16 -RSB- y Globe -LSB- 17 -RSB- son entornos de middleware b\u00e1sicos que admiten el enfoque de objetos fragmentados. Globe considera la replicaci\u00f3n y el almacenamiento en cach\u00e9. Ambos sistemas carecen de soporte para la distribuci\u00f3n consciente de los recursos. 7. CONCLUSI\u00d3N Y TRABAJO EN CURSO Basado en el modelo de objetos fragmentados y la arquitectura del entorno EDAS, los servicios adaptativos descentralizados pueden dise\u00f1arse, implementarse y ejecutarse f\u00e1cilmente. Como se describi\u00f3, la gesti\u00f3n de recursos se puede descomponer en dos problemas principales que deben resolverse. Control y gesti\u00f3n de los l\u00edmites de recursos, incluyendo garantizar que los recursos asignados est\u00e9n disponibles -LRB- incluso en el context de ca\u00eddas de nodos -RRB- y la colocaci\u00f3n aut\u00f3noma de servicios. Para ambos problemas ofrecemos una soluci\u00f3n,un entorno de simulaci\u00f3n actualmente implementado verificar\u00e1 su viabilidad. En un siguiente paso, la gesti\u00f3n de recursos se integrar\u00e1 en un prototipo ya implementado de la arquitectura EDAS. Como se describi\u00f3, ya tenemos una implementaci\u00f3n temprana del marco para los servicios adaptativos descentralizados. Este marco debe ampliarse para interactuar sin problemas con la gesti\u00f3n de recursos y la arquitectura EDAS. En un paso final necesitamos implementar algunos servicios que verifiquen la usabilidad de todo el proyecto EDAS.", "keyphrases": ["Servicio de adaptaci\u00f3n decente.", "gesti\u00f3n de recursos", "ambiente hogare\u00f1o", "infraestructura", "cliente", "servicio a largo plazo", "eda", "l\u00edmite local", "l\u00edmite global", "recurso", "nodo"]}
{"file_name": "H-14", "text": "Estudio del uso de destinos populares para mejorar la interacci\u00f3n de b\u00fasqueda web RESUMEN Presentamos una novedosa caracter\u00edstica de interacci\u00f3n de b\u00fasqueda web que, para una consulta determinada, proporciona enlaces a sitios web visitados frecuentemente por otros usuarios con necesidades de informaci\u00f3n similares. Estos destinos populares complementan los resultados de b\u00fasqueda tradicionales, permitiendo la navegaci\u00f3n directa a recursos autorizados para el tema de consulta. Los destinos se identifican utilizando el historial de comportamiento de b\u00fasqueda y navegaci\u00f3n de muchos usuarios durante un per\u00edodo de tiempo prolongado, cuyo comportamiento colectivo proporciona una base para calcular la autoridad de la fuente. Describimos un estudio de usuarios que compar\u00f3 la sugerencia de destinos con la sugerencia previamente propuesta de consultas relacionadas, as\u00ed como con la b\u00fasqueda web tradicional sin ayuda. Los resultados muestran que la b\u00fasqueda mejorada por sugerencias de destinos supera a otros sistemas para tareas exploratorias, y el mejor rendimiento se obtiene al analizar el comportamiento anterior de los usuarios en granularidad a nivel de consulta. 1. INTRODUCCI\u00d3N El problema de la mejora de las consultas enviadas a los sistemas de Recuperaci\u00f3n de Informaci\u00f3n -LRB- IR -RRB- ha sido ampliamente estudiado en la investigaci\u00f3n de RI -LSB- 4 -RSB- -LSB- 11 -RSB-. Se pueden ofrecer formulaciones de consulta alternativas, conocidas como sugerencias de consulta, a los usuarios despu\u00e9s de una consulta inicial, lo que les permite modificar la especificaci\u00f3n de sus necesidades proporcionada al sistema, lo que conduce a un mejor rendimiento de recuperaci\u00f3n. La reciente popularidad de los motores de b\u00fasqueda web ha permitido sugerencias de consultas que se basan en el comportamiento de reformulaci\u00f3n de consultas de muchos usuarios para hacer recomendaciones de consultas basadas en interacciones anteriores de los usuarios -LSB- 10 -RSB-. Aprovechar los procesos de toma de decisiones de muchos usuarios para la reformulaci\u00f3n de consultas tiene sus ra\u00edces en la indexaci\u00f3n adaptativa -LSB- 8 -RSB-. Sin embargo, los enfoques basados \u200b\u200ben la interacci\u00f3n para la sugerencia de consultas pueden ser menos potentes cuando la necesidad de informaci\u00f3n es exploratoria, ya que una gran proporci\u00f3n de la actividad del usuario para tales necesidades de informaci\u00f3n puede ocurrir m\u00e1s all\u00e1 de las interacciones con los motores de b\u00fasqueda. En los casos en los que la b\u00fasqueda dirigida es s\u00f3lo una fracci\u00f3n del comportamiento de b\u00fasqueda de informaci\u00f3n de los usuarios, la utilidad de los clics de otros usuarios en el espacio de los resultados mejor clasificados puede ser limitada, ya que no cubre el comportamiento de navegaci\u00f3n posterior. Al mismo tiempo, la navegaci\u00f3n del usuario que sigue las interacciones del motor de b\u00fasqueda proporciona un respaldo impl\u00edcito a los recursos web preferidos por los usuarios, lo que puede ser particularmente valioso para las tareas de b\u00fasqueda exploratoria. Por lo tanto, proponemos explotar una combinaci\u00f3n de b\u00fasquedas pasadas y comportamiento de navegaci\u00f3n del usuario para mejorar las interacciones de b\u00fasqueda web de los usuarios. Los complementos del navegador y los registros del servidor proxy brindan acceso a los patrones de navegaci\u00f3n de los usuarios que trascienden las interacciones con los motores de b\u00fasqueda. En trabajos anteriores, Agichtein et al. han utilizado dichos datos para mejorar la clasificaci\u00f3n de los resultados de b\u00fasqueda. -LSB- 1 -RSB-. Radlinski y Joachims -LSB- 13 -RSB- han utilizado dicha inteligencia colectiva del usuario para mejorar la precisi\u00f3n de la recuperaci\u00f3n mediante el uso de secuencias de reformulaciones de consultas consecutivas.sin embargo, su enfoque no considera las interacciones de los usuarios m\u00e1s all\u00e1 de la p\u00e1gina de resultados de b\u00fasqueda. En este art\u00edculo, presentamos un estudio de usuarios de una t\u00e9cnica que explota el comportamiento de b\u00fasqueda y navegaci\u00f3n de muchos usuarios para sugerir p\u00e1ginas web populares, denominadas destinos en adelante, adem\u00e1s de los resultados de b\u00fasqueda habituales. Es posible que los destinos no est\u00e9n entre los resultados mejor clasificados, que no contengan los t\u00e9rminos consultados o que ni siquiera est\u00e9n indexados por el motor de b\u00fasqueda. En cambio, son p\u00e1ginas a las que otros usuarios terminan con frecuencia despu\u00e9s de enviar consultas iguales o similares y luego navegar lejos de los resultados de b\u00fasqueda en los que inicialmente hicieron clic. Conjeturamos que los destinos populares entre una gran cantidad de usuarios pueden capturar la experiencia colectiva del usuario para las necesidades de informaci\u00f3n, y nuestros resultados respaldan esta hip\u00f3tesis. En -LSB- 19 -RSB-, Wexelblat y Maes describen un sistema para soportar la navegaci\u00f3n dentro del dominio basada en las rutas de navegaci\u00f3n de otros usuarios. Sin embargo, no tenemos conocimiento de que dichos principios se apliquen a la b\u00fasqueda en la Web. Quiz\u00e1s la instancia m\u00e1s cercana de teletransportaci\u00f3n sea la oferta de los motores de b\u00fasqueda de varios atajos dentro del dominio debajo del t\u00edtulo de un resultado de b\u00fasqueda. Si bien estos pueden basarse en el comportamiento del usuario y posiblemente en la estructura del sitio, el usuario guarda como m\u00e1ximo un clic en esta funci\u00f3n. Por el contrario, nuestro enfoque propuesto puede transportar a los usuarios a ubicaciones con muchos clics m\u00e1s all\u00e1 del resultado de la b\u00fasqueda, ahorrando tiempo y brind\u00e1ndoles una perspectiva m\u00e1s amplia de la informaci\u00f3n relacionada disponible. El estudio de usuarios realizado investiga la eficacia de incluir enlaces a destinos populares como una caracter\u00edstica adicional de la interfaz en las p\u00e1ginas de resultados de los motores de b\u00fasqueda. Comparamos dos variantes de este enfoque con la sugerencia de consultas relacionadas y b\u00fasqueda web sin ayuda, y buscamos respuestas a preguntas sobre: \u200b\u200b-LRB- i -RRB- preferencia del usuario y efectividad de la b\u00fasqueda para tareas de b\u00fasqueda exploratoria y de elementos conocidos, y -LRB- ii -RRB- la distancia preferida entre la consulta y el destino utilizada para identificar destinos populares a partir de registros de comportamiento anteriores. Los resultados indican que sugerir destinos populares a los usuarios que intentan tareas exploratorias proporciona mejores resultados en aspectos clave de la experiencia de b\u00fasqueda de informaci\u00f3n, mientras que proporcionar sugerencias para refinar las consultas es m\u00e1s deseable para tareas de elementos conocidos. En la Secci\u00f3n 2 describimos la extracci\u00f3n de rutas de b\u00fasqueda y navegaci\u00f3n de los registros de actividad del usuario y su uso para identificar los principales destinos para nuevas consultas. La Secci\u00f3n 3 describe el dise\u00f1o del estudio de usuarios, mientras que las Secciones 4 y 5 presentan los hallazgos del estudio y su discusi\u00f3n, respectivamente. 6. CONCLUSIONES Presentamos un enfoque novedoso para mejorar la interacci\u00f3n de b\u00fasqueda web de los usuarios al proporcionar enlaces a sitios web visitados con frecuencia por buscadores anteriores con necesidades de informaci\u00f3n similares. Se realiz\u00f3 un estudio de usuarios en el que evaluamos la efectividad de la t\u00e9cnica propuesta en comparaci\u00f3n con un sistema de refinamiento de consultas y una b\u00fasqueda web sin ayuda. Los resultados de nuestro estudio revelaron que:Los sistemas -LRB- i -RRB- que sugieren refinamientos de consultas fueron preferidos para tareas de elementos conocidos, -LRB- ii -RRB- sistemas que ofrecen destinos populares fueron preferidos para tareas de b\u00fasqueda exploratoria, y los destinos -LRB- iii -RRB- deber\u00edan extraerse de el final de los senderos de consulta, no de los senderos de sesi\u00f3n. En general, las sugerencias de destinos populares influyeron estrat\u00e9gicamente en las b\u00fasquedas de una manera que no se puede lograr mediante los enfoques de sugerencia de consultas, al ofrecer una nueva forma de resolver problemas de informaci\u00f3n y mejorar la experiencia de b\u00fasqueda de informaci\u00f3n para muchos buscadores en la Web.", "keyphrases": ["destino popular", "interacci\u00f3n de b\u00fasqueda web", "consulta de improvisaci\u00f3n", "recuperar realizar", "pregunta relacionada", "experiencia de b\u00fasqueda de informaci\u00f3n", "sendero de consulta", "rastro de sesi\u00f3n", "enfoque de base de b\u00fasqueda", "evaluaci\u00f3n de base de registro"]}
{"file_name": "I-20", "text": "Calcular el \u00edndice de poder de Banzhaf en juegos de flujo de red RESUMEN La agregaci\u00f3n de preferencias se utiliza en una variedad de aplicaciones multiagente y, como resultado, la teor\u00eda de la votaci\u00f3n se ha convertido en un tema importante en la investigaci\u00f3n de sistemas multiagente. Sin embargo, los \u00edndices de poder -LRB- que reflejan cu\u00e1nto ``poder real'' tiene un votante en un sistema de votaci\u00f3n ponderado -RRB- han recibido relativamente poca atenci\u00f3n, aunque han sido estudiados durante mucho tiempo en ciencias pol\u00edticas y econom\u00eda. El \u00edndice de poder de Banzhaf es uno de los m\u00e1s populares; tambi\u00e9n est\u00e1 bien definido para cualquier juego de coalici\u00f3n simple. En este art\u00edculo, examinamos la complejidad computacional de calcular el \u00edndice de potencia de Banzhaf dentro de un dominio multiagente particular, un juego de flujo de red. Los agentes controlan los bordes de un gr\u00e1fico; una coalici\u00f3n gana si puede enviar un flujo de un tama\u00f1o determinado desde un v\u00e9rtice de origen a un v\u00e9rtice de destino. El poder relativo de cada borde/agente refleja su importancia para permitir dicho flujo, y en las redes del mundo real podr\u00eda usarse, por ejemplo, para asignar recursos para mantener partes de la red. Mostramos que el c\u00e1lculo del \u00edndice de potencia de Banzhaf de cada agente en este dominio de flujo de red es #P - completo. Tambi\u00e9n mostramos que para algunos dominios de flujo de red restringidos existe un algoritmo polin\u00f3mico para calcular los \u00edndices de poder de Banzhaf de los agentes. 1. INTRODUCCI\u00d3N \u00bfCu\u00e1l es la complejidad del proceso? \u00bfSe puede utilizar la complejidad para protegerse contra fen\u00f3menos no deseados? \u00bfLa complejidad del c\u00e1lculo impide la implementaci\u00f3n realista de una t\u00e9cnica? Las aplicaciones pr\u00e1cticas de la votaci\u00f3n entre agentes automatizados ya est\u00e1n muy extendidas. De hecho, para ver la generalidad del escenario de votaci\u00f3n -LRB- automatizado -RRB-, considere la b\u00fasqueda web moderna. En este art\u00edculo, consideramos un tema que ha sido menos estudiado en el context de la votaci\u00f3n automatizada de agentes: los \u00edndices de poder. Un \u00edndice de poder es una medida del poder que tiene un subgrupo, o equivalentemente un votante en un entorno de votaci\u00f3n ponderado, sobre las decisiones de un grupo m\u00e1s grande. El \u00edndice de poder de Banzhaf es una de las medidas m\u00e1s populares del poder de voto y, aunque se ha utilizado principalmente para medir el poder en juegos de votaci\u00f3n ponderada, est\u00e1 bien definido para cualquier juego de coalici\u00f3n simple. Analizamos algunos aspectos computacionales del \u00edndice de poder de Banzhaf en un entorno espec\u00edfico, es decir, un juego de flujo de red. En este juego, una coalici\u00f3n de agentes gana si puede enviar un flujo de tama\u00f1o k desde un v\u00e9rtice de origen s a un v\u00e9rtice de destino t, donde el poder relativo de cada borde refleja su importancia al permitir dicho flujo. Mostramos que el c\u00e1lculo del \u00edndice de potencia de Banzhaf de cada agente en este dominio de flujo de red general es #P - completo. En los juegos de conectividad en gr\u00e1ficos de capas acotadas -RRB-, existe un algoritmo polinomial para calcular el \u00edndice de potencia de Banzhaf de un agente. El documento procede de la siguiente manera. En la Secci\u00f3n 2 damos algunos antecedentes sobre los juegos de coalici\u00f3n y el \u00edndice de poder de Banzhaf, y en la Secci\u00f3n 3 presentamos nuestro juego de flujo de red espec\u00edfico.En la Secci\u00f3n 4 analizamos el \u00edndice de potencia de Banzhaf en juegos de flujo de redes y presentamos nuestro resultado de complejidad en el caso general. En la Secci\u00f3n 5 consideramos un caso restringido del juego del flujo de red y presentamos los resultados. En la Secci\u00f3n 6 analizamos trabajos relacionados y concluimos en la Secci\u00f3n 7. 6. TRABAJO RELACIONADO La medici\u00f3n del poder de los jugadores individuales en juegos de coalici\u00f3n se ha estudiado durante muchos a\u00f1os. Los \u00edndices m\u00e1s populares sugeridos para dicha medici\u00f3n son el \u00edndice de Banzhaf -LSB- 1 -RSB- y el \u00edndice de Shapley-Shubik -LSB- 19 -RSB-. En su art\u00edculo fundamental, Shapley -LSB- 18 -RSB- consider\u00f3 los juegos de coalici\u00f3n y la asignaci\u00f3n justa de la utilidad obtenida por la gran coalici\u00f3n -LRB- la coalici\u00f3n de todos los agentes -RRB- para sus miembros. El \u00edndice Shapley-Shubik -LSB- 19 -RSB- es la aplicaci\u00f3n directa del valor de Shapley a juegos de coalici\u00f3n simples. El \u00edndice de Banzhaf surgi\u00f3 directamente del estudio de la votaci\u00f3n en los \u00f3rganos de toma de decisiones. El \u00edndice de Banzhaf normalizado mide la proporci\u00f3n de coaliciones en las que un jugador es swinger, entre todas las coaliciones ganadoras. Este \u00edndice es similar al \u00edndice de Banzhaf analizado en la Secci\u00f3n 1 y se define como: El \u00edndice de Banzhaf se analiz\u00f3 matem\u00e1ticamente en -LSB- 3 -RSB-, donde se demostr\u00f3 que esta normalizaci\u00f3n carece de ciertas propiedades deseables, y el \u00edndice de Banzhaf m\u00e1s natural Se introduce el \u00edndice. Tanto el \u00edndice de Shapley-Shubik como el de Banzhaf han sido ampliamente estudiados, y Straffin -LSB-20-RSB- ha demostrado que cada \u00edndice refleja condiciones espec\u00edficas en un \u00f3rgano de votaci\u00f3n. -LSB- 11 -RSB- considera estos dos \u00edndices junto con varios otros, y describe los axiomas que caracterizan los diferentes \u00edndices. La ingenua implementaci\u00f3n de un algoritmo para calcular el \u00edndice de Banzhaf de un agente i enumera todas las coaliciones que contienen i. Hay 2n \u2212 1 de tales coaliciones, por lo que el desempe\u00f1o es exponencial en el n\u00famero de agentes. -LSB- 12 -RSB- contiene un estudio de algoritmos para calcular \u00edndices de poder de juegos mayoritarios ponderados. Deng y Papadimitriou -LSB- 2 -RSB- muestran que calcular el valor de Shapley en juegos de mayor\u00eda ponderada es #P - completo, utilizando una reducci\u00f3n de KNAPSACK. Dado que el valor de Shapley de cualquier juego simple tiene el mismo valor que su \u00edndice Shapley-Shubik, esto muestra que calcular el \u00edndice Shapley-Shubik en juegos de mayor\u00eda ponderada es #Pcompleto. Matsui y Matsui -LSB- 13 -RSB- han demostrado que el c\u00e1lculo de los \u00edndices de Banzhaf y Shapley-Shubik en juegos de votaci\u00f3n ponderada es NP-completo. El problema de calcular los \u00edndices de potencia en juegos simples depende de la representaci\u00f3n elegida del juego. Dado que el n\u00famero de coaliciones posibles es exponencial en el n\u00famero de agentes, el c\u00e1lculo de \u00edndices de poder en tiempo polin\u00f3mico en el n\u00famero de agentes s\u00f3lo se puede lograr en dominios espec\u00edficos. En este art\u00edculo, hemos considerado el dominio del flujo de red, donde una coalici\u00f3n de agentes debe lograr un flujo m\u00e1s all\u00e1 de un cierto valor. El juego de flujo de red que hemos definido es un juego sencillo. -LSB- 10,9 -RSB- han considerado un dominio de flujo de red similar, donde cada agente controla un borde de un gr\u00e1fico de flujo de red. Sin embargo, introdujeron un juego no simple, donde el valor que logra una coalici\u00f3n de agentes es el flujo total m\u00e1ximo. Han demostrado que ciertas familias de juegos de flujo de red y juegos similares tienen n\u00facleos no vac\u00edos. 7. CONCLUSIONES Y DIRECCIONES FUTURAS Hemos considerado juegos de flujo de red, donde una coalici\u00f3n de agentes gana si logra enviar un flujo de m\u00e1s de cierto valor k entre dos v\u00e9rtices. Hemos evaluado el poder relativo de cada agente en este escenario utilizando el \u00edndice de Banzhaf. Este \u00edndice de potencia se puede utilizar para decidir c\u00f3mo asignar recursos de mantenimiento en redes del mundo real, con el fin de maximizar nuestra capacidad de mantener un cierto flujo de informaci\u00f3n entre dos sitios. Aunque te\u00f3ricamente el \u00edndice de Banzhaf nos permite medir el poder de los agentes en el juego de flujo de red, hemos demostrado que el problema de calcular el \u00edndice de Banzhaf en este dominio en #P es completo. A pesar de este resultado desalentador para el dominio de flujo de red general, tambi\u00e9n hemos proporcionado un resultado m\u00e1s alentador para un dominio restringido. En el caso de los juegos de conectividad -LRB- donde s\u00f3lo se requiere que una coalici\u00f3n contenga un camino desde el origen hasta el destino -RRB- jugados en gr\u00e1ficos de capas acotadas, es posible calcular el \u00edndice de Banzhaf de un agente en tiempo polinomial. . Sigue siendo un problema abierto encontrar formas de aproximar de manera manejable el \u00edndice de Banzhaf en el dominio general del flujo de la red. Tambi\u00e9n es posible encontrar otros dominios restringidos \u00fatiles donde sea posible calcular exactamente el \u00edndice de Banzhaf. S\u00f3lo hemos considerado la complejidad del c\u00e1lculo del \u00edndice de Banzhaf; sigue siendo un problema abierto encontrar la complejidad de calcular el \u00edndice de Shapley-Shubik u otros \u00edndices en el dominio del flujo de red. Finalmente, creemos que hay muchos dominios interesantes adicionales adem\u00e1s de los juegos de votaci\u00f3n ponderada y los juegos de flujo de red, y valdr\u00eda la pena investigar la complejidad de calcular el \u00edndice de Banzhaf u otros \u00edndices de poder en dichos dominios.Hemos demostrado que el problema de calcular el \u00edndice de Banzhaf en este dominio en #P est\u00e1 completo. A pesar de este resultado desalentador para el dominio de flujo de red general, tambi\u00e9n hemos proporcionado un resultado m\u00e1s alentador para un dominio restringido. En el caso de los juegos de conectividad -LRB- donde s\u00f3lo se requiere que una coalici\u00f3n contenga un camino desde el origen hasta el destino -RRB- jugados en gr\u00e1ficos de capas acotadas, es posible calcular el \u00edndice de Banzhaf de un agente en tiempo polinomial. . Sigue siendo un problema abierto encontrar formas de aproximar de manera manejable el \u00edndice de Banzhaf en el dominio general del flujo de la red. Tambi\u00e9n es posible encontrar otros dominios restringidos \u00fatiles donde sea posible calcular exactamente el \u00edndice de Banzhaf. S\u00f3lo hemos considerado la complejidad del c\u00e1lculo del \u00edndice de Banzhaf; sigue siendo un problema abierto encontrar la complejidad de calcular el \u00edndice de Shapley-Shubik u otros \u00edndices en el dominio del flujo de red. Finalmente, creemos que hay muchos dominios interesantes adicionales adem\u00e1s de los juegos de votaci\u00f3n ponderada y los juegos de flujo de red, y valdr\u00eda la pena investigar la complejidad de calcular el \u00edndice de Banzhaf u otros \u00edndices de poder en dichos dominios.Hemos demostrado que el problema de calcular el \u00edndice de Banzhaf en este dominio en #P est\u00e1 completo. A pesar de este resultado desalentador para el dominio de flujo de red general, tambi\u00e9n hemos proporcionado un resultado m\u00e1s alentador para un dominio restringido. En el caso de los juegos de conectividad -LRB- donde s\u00f3lo se requiere que una coalici\u00f3n contenga un camino desde el origen hasta el destino -RRB- jugados en gr\u00e1ficos de capas acotadas, es posible calcular el \u00edndice de Banzhaf de un agente en tiempo polinomial. . Sigue siendo un problema abierto encontrar formas de aproximar de manera manejable el \u00edndice de Banzhaf en el dominio general del flujo de la red. Tambi\u00e9n es posible encontrar otros dominios restringidos \u00fatiles donde sea posible calcular exactamente el \u00edndice de Banzhaf. S\u00f3lo hemos considerado la complejidad del c\u00e1lculo del \u00edndice de Banzhaf; sigue siendo un problema abierto encontrar la complejidad de calcular el \u00edndice de Shapley-Shubik u otros \u00edndices en el dominio del flujo de red. Finalmente, creemos que hay muchos dominios interesantes adicionales adem\u00e1s de los juegos de votaci\u00f3n ponderada y los juegos de flujo de red, y valdr\u00eda la pena investigar la complejidad de calcular el \u00edndice de Banzhaf u otros \u00edndices de poder en dichos dominios.", "keyphrases": ["prefiero agregar", "aplicaci\u00f3n multiag", "teor\u00eda del voto", "\u00edndice de poder de banzhaf", "An\u00e1lisis de algoritmos y problemas complejos.", "teor\u00eda de la elecci\u00f3n social", "voto del agente autom\u00e1tico", "juego de flujo de red", "modelo probabilista", "conectar juego"]}
{"file_name": "J-7", "text": "El papel de la compatibilidad en la difusi\u00f3n de tecnolog\u00edas a trav\u00e9s de las redes sociales RESUMEN En muchos entornos, se puede ver que las tecnolog\u00edas en competencia (por ejemplo, sistemas operativos, sistemas de mensajer\u00eda instant\u00e1nea o formatos de documentos) adoptan una cantidad limitada de compatibilidad entre s\u00ed; en otras palabras, la dificultad de utilizar m\u00faltiples tecnolog\u00edas se equilibra en alg\u00fan punto entre los dos extremos de la imposibilidad y la interoperabilidad sin esfuerzo. Hay una variedad de razones por las que ocurre este fen\u00f3meno, muchas de las cuales, basadas en consideraciones legales, sociales o comerciales, parecen desafiar modelos matem\u00e1ticos concisos. Pese a ello, mostramos que las ventajas de una compatibilidad limitada pueden surgir en un modelo muy simple de difusi\u00f3n en redes sociales, ofreciendo as\u00ed una explicaci\u00f3n b\u00e1sica de este fen\u00f3meno en t\u00e9rminos puramente estrat\u00e9gicos. Nuestro enfoque se basa en el trabajo sobre la difusi\u00f3n de innovaciones en la literatura econ\u00f3mica, que busca modelar c\u00f3mo una nueva tecnolog\u00eda A podr\u00eda difundirse a trav\u00e9s de una red social de individuos que actualmente son usuarios de la tecnolog\u00eda B. Consideramos varias formas de capturar la compatibilidad de A. y B, centr\u00e1ndose principalmente en un modelo en el que los usuarios pueden elegir adoptar A, adoptar B o, con un costo adicional, adoptar tanto A como B. Caracterizamos c\u00f3mo la capacidad de A para propagarse depende tanto de su calidad relativa a B, y tambi\u00e9n este costo adicional de adoptar ambas, y encontrar algunas propiedades sorprendentes de no monotonicidad en la dependencia de estos par\u00e1metros: en algunos casos, para que una tecnolog\u00eda sobreviva a la introducci\u00f3n de otra, el costo de adoptar ambas tecnolog\u00edas debe estar equilibrado dentro de un rango estrecho e intermedio. Tambi\u00e9n ampliamos el marco al caso de m\u00faltiples tecnolog\u00edas, donde encontramos que un simple Este trabajo ha sido apoyado en parte por subvenciones NSF CCF0325453, IIS-0329064, CNS-0403340 y BCS-0537606, una subvenci\u00f3n de investigaci\u00f3n de Google, una subvenci\u00f3n de Yahoo ! Beca Research Alliance, el Instituto de Ciencias Sociales de Cornell y la Fundaci\u00f3n John D. y Catherine T. MacArthur. El modelo captura el fen\u00f3meno de dos empresas que adoptan una \"alianza estrat\u00e9gica\" limitada para defenderse de una tercera tecnolog\u00eda nueva. 1. INTRODUCCI\u00d3N Juegos de Difusi\u00f3n y Coordinaci\u00f3n en Red. Tales cuestiones surgen, por ejemplo, en la adopci\u00f3n de nuevas tecnolog\u00edas, la aparici\u00f3n de nuevas normas sociales o convenciones organizativas, o la difusi\u00f3n de lenguajes humanos -LSB- 2, 14, 15, 16, 17 -RSB-. Una l\u00ednea activa de investigaci\u00f3n en econom\u00eda y sociolog\u00eda matem\u00e1tica se preocupa por modelar este tipo de procesos de difusi\u00f3n como un juego de coordinaci\u00f3n en una red social -LSB- 1, 5, 7, 13, 19 -RSB-. Comenzamos analizando uno de los modelos de difusi\u00f3n de la teor\u00eda de juegos m\u00e1s b\u00e1sicos, propuesto en un influyente art\u00edculo de Morris -LSB-13-RSB-, que constituir\u00e1 el punto de partida de nuestro trabajo aqu\u00ed. Lo describimos en t\u00e9rminos del siguiente escenario de adopci\u00f3n de tecnolog\u00eda, aunque hay muchos otros ejemplos que servir\u00edan para el mismo prop\u00f3sito.Obs\u00e9rvese que A es la tecnolog\u00eda \"mejor\" si q < 21, en el sentido de que los beneficios de AA exceder\u00edan entonces a los de BB, mientras que A es la peor tecnolog\u00eda si q > 21. Se pueden derivar varios conocimientos cualitativos a partir de una difusi\u00f3n modelo incluso en este nivel de simplicidad. Espec\u00edficamente, considere una red G y deje que todos los nodos jueguen inicialmente con B. Ahora suponga que una peque\u00f1a cantidad de nodos comienzan a adoptar la estrategia A. Compatibilidad, Interoperabilidad y Biling\u00fcismo. Sin embargo, una pieza importante que posiblemente falta en los modelos b\u00e1sicos de difusi\u00f3n de la teor\u00eda de juegos es una imagen m\u00e1s detallada de lo que sucede en el l\u00edmite de coexistencia, donde la forma b\u00e1sica del modelo postula nodos que adoptan A vinculados a nodos que adoptan A. B. En estos entornos motivadores para los modelos, por supuesto, uno ve muy a menudo regiones de interfaz en las que los individuos esencialmente se vuelven \"biling\u00fces\". '' En el caso de la difusi\u00f3n del lenguaje humano, esta biling\u00fcismo se entiende literalmente: las regiones geogr\u00e1ficas donde hay una interacci\u00f3n sustancial con hablantes de dos idiomas diferentes tienden a tener habitantes que hablan ambos. Desde este punto de vista, es natural preguntarse c\u00f3mo se comportan los modelos de difusi\u00f3n cuando se extienden de modo que ciertos nodos puedan ser biling\u00fces en este sentido tan general, adoptando ambas estrategias con alg\u00fan costo para ellos mismos. \u00bfQu\u00e9 podr\u00edamos aprender de tal extensi\u00f3n? Para empezar, tiene el potencial de proporcionar una perspectiva valiosa sobre la cuesti\u00f3n de la compatibilidad e incompatibilidad que sustenta la competencia entre las empresas de tecnolog\u00eda. Existe una amplia literatura sobre c\u00f3mo la compatibilidad entre tecnolog\u00edas afecta la competencia entre empresas y, en particular, c\u00f3mo la incompatibilidad puede ser una decisi\u00f3n estrat\u00e9gica beneficiosa para ciertos participantes en un mercado -LSB- 3, 4, 8, 9, 12 -RSB-. Si bien estos modelos existentes de compatibilidad capturan los efectos de red en el sentido de que los usuarios en el mercado prefieren usar tecnolog\u00eda que est\u00e1 m\u00e1s extendida, no capturan el fen\u00f3meno de red m\u00e1s detallado representado por la difusi\u00f3n: que cada usuario incluye su visi\u00f3n local en la decisi\u00f3n, en base a lo que est\u00e1n haciendo sus propios vecinos de la red social. Un modelo de difusi\u00f3n que incorporara tales extensiones podr\u00eda proporcionar informaci\u00f3n sobre la estructura de los l\u00edmites en la red entre tecnolog\u00edas; Potencialmente, podr\u00eda ofrecer una base te\u00f3rica de grafos sobre c\u00f3mo la incompatibilidad puede beneficiar a una tecnolog\u00eda existente, fortaleciendo estos l\u00edmites e impidiendo la incursi\u00f3n de una tecnolog\u00eda nueva y mejor. El presente trabajo: Difusi\u00f3n con conducta biling\u00fce. En este art\u00edculo, desarrollamos un conjunto de modelos de difusi\u00f3n que incorporan nociones de compatibilidad y biling\u00fcismo, y encontramos que algunos fen\u00f3menos inesperados surgen incluso de versiones muy simples de los modelos. Comenzamos con la que tal vez sea la forma m\u00e1s sencilla de ampliar el modelo de Morris analizado anteriormente para incorporar el comportamiento biling\u00fce. Consideremos nuevamente el ejemplo de los sistemas IM A y B, con la estructura de pagos como antes,pero supongamos ahora que cada nodo puede adoptar una tercera estrategia, denominada AB, en la que decide utilizar tanto A como B. Finalmente, quien adopta AB paga una penalizaci\u00f3n de costo fijo de c -LRB- es decir, c se suma a su pago total -RRB- para representar el costo de tener que mantener ambas tecnolog\u00edas. As\u00ed, en este modelo, hay dos par\u00e1metros que pueden variar: las cualidades relativas de las dos tecnolog\u00edas -LRB- codificadas por q -RRB-, y el coste de ser biling\u00fce, que refleja un tipo de incompatibilidad -LRB- codificada por c -RRB-. Tambi\u00e9n introducimos una notaci\u00f3n adicional que ser\u00e1 \u00fatil en las secciones siguientes: definimos r = c / \u0394, la penalizaci\u00f3n fija por adoptar AB, escalada de modo que sea un costo por borde. En el modelo de Morris, donde las \u00fanicas opciones estrat\u00e9gicas son A y B, un par\u00e1metro clave es el umbral de contagio de G, denotado q \u2217 -LRB- G -RRB-: este es el supremo de q para el cual A puede volverse epid\u00e9mico en G con par\u00e1metro q en la estructura de pagos. Un resultado central de -LSB- 13 -RSB- es que 21 es el umbral de contagio m\u00e1ximo posible para cualquier gr\u00e1fico: supG q \u2217 -LRB- G -RRB- = 21. De hecho, existen gr\u00e1ficos en los que el umbral de contagio es tan grande como 21 -LRB- incluyendo la l\u00ednea infinita - el \u00fanico gr\u00e1fico 2-regular conectado infinito -RRB-; por otro lado, se puede mostrar que no existe ning\u00fan gr\u00e1fico con un umbral de contagio mayor que Figura 1: La regi\u00f3n del plano -LRB- q, r -RRB- para la cual la tecnolog\u00eda A puede volverse epid\u00e9mica en la l\u00ednea infinita. Nuestros resultados. -LRB- Encontramos formas an\u00e1logas que se vuelven a\u00fan m\u00e1s complejas para otras estructuras de gr\u00e1ficos infinitos simples; v\u00e9anse, por ejemplo, las Figuras 3 y 4. -RRB- En particular, esto significa que para valores de q cercanos pero menores que 21, la estrategia A puede volverse epid\u00e9mica en la l\u00ednea infinita si r es suficientemente peque\u00f1o o suficientemente grande, pero no si r toma valores en alg\u00fan intervalo intermedio. En otras palabras, la estrategia B -LRB- que representa la peor tecnolog\u00eda, ya que q < 21 -RRB- sobrevivir\u00e1 si y s\u00f3lo si el costo de ser biling\u00fce se calibra para que se encuentre en este intervalo medio. Esto es un reflejo de una compatibilidad limitada -que puede ser de inter\u00e9s para una tecnolog\u00eda establecida dificultar, pero no demasiado, el uso de una nueva tecnolog\u00eda- y nos sorprende que surja de un modelo b\u00e1sico sobre tales una estructura de red simple. Es natural preguntarse si existe una interpretaci\u00f3n cualitativa de c\u00f3mo esto surge del modelo y, de hecho, no es dif\u00edcil dar tal interpretaci\u00f3n, de la siguiente manera. Cuando r es muy peque\u00f1o, a los nodos les resulta barato adoptar AB como estrategia, por lo que AB se propaga por toda la red. Una vez que AB est\u00e1 en todas partes, las actualizaciones de mejor respuesta hacen que todos los nodos cambien a A, ya que obtienen los mismos beneficios de interacci\u00f3n sin pagar la penalizaci\u00f3n de r. Cuando r es muy grande, a los nodos en la interfaz, con un vecino A y un vecino B, les resultar\u00e1 demasiado caro elegir AB, por lo que elegir\u00e1n A -LRB-, la mejor tecnolog\u00eda -RRB-,y por tanto A se propagar\u00e1 paso a paso a trav\u00e9s de la red. Cuando r toma un valor intermedio, un nodo v en la interfaz, con un vecino A y un vecino B, encontrar\u00e1 m\u00e1s beneficioso adoptar AB como estrategia. Por lo tanto, este valor intermedio de r permite que se forme una `` frontera '' de AB entre los adoptantes de A y los adoptantes de B. Pero si tiene el equilibrio correcto en el valor de r, entonces las adopciones de A llegan a un det\u00e9ngase en un l\u00edmite biling\u00fce donde los nodos adoptan AB. Yendo m\u00e1s all\u00e1 de los gr\u00e1ficos espec\u00edficos G, encontramos que esta no convexidad tambi\u00e9n se cumple en un sentido mucho m\u00e1s general, al considerar la regi\u00f3n epid\u00e9mica general \u03a9 = UG\u03a9 -LRB- G -RRB-. Para cualquier valor dado de \u0394, la regi\u00f3n \u03a9 es una uni\u00f3n complicada de pol\u00edgonos acotados y no acotados, y no tenemos una descripci\u00f3n simple y cerrada para ello. Sin embargo, podemos demostrar mediante un argumento de funci\u00f3n potencial que ning\u00fan punto -LRB- q, r -RRB- con q > 21 pertenece a \u03a9. Adem\u00e1s, podemos mostrar la existencia de un punto -LRB- q, r -RRB- E ~ \u03a9 para el cual q < 21. Por otro lado, la consideraci\u00f3n de la regi\u00f3n epid\u00e9mica para la l\u00ednea infinita muestra que -LRB- 21, r -RRB- E \u03a9 para r = 0 y para r suficientemente grande. Por tanto, ni \u03a9 ni su complemento son convexos en el cuadrante positivo. Finalmente, tambi\u00e9n ampliamos una caracterizaci\u00f3n que Morris dio para el umbral de contagio -LSB- 13 -RSB-, produciendo una caracterizaci\u00f3n algo m\u00e1s intrincada de la regi\u00f3n \u03a9 -LRB- G -RRB-. En el marco de Morris, sin una estrategia AB, demostr\u00f3 que A no puede volverse epid\u00e9mico con el par\u00e1metro q si y s\u00f3lo si cada conjunto cofinito de nodos contiene un subconjunto S que funciona como una \"comunidad\" bien conectada: cada nodo en S tiene al menos una fracci\u00f3n -LRB- 1 -- q -RRB- de sus vecinos en S. En otras palabras, las comunidades muy unidas son los obst\u00e1culos naturales a la difusi\u00f3n en su entorno. Con la estrategia AB como opci\u00f3n adicional, una estructura m\u00e1s compleja se convierte en el obst\u00e1culo: demostramos que A no puede volverse epid\u00e9mico con par\u00e1metros -LRB- q, r -RRB- si y s\u00f3lo si cada conjunto cofinito contiene una estructura que consiste en una -Comunidad unida con un tipo particular de \"interfaz\" de nodos vecinos. Mostramos que dicha estructura permite a los nodos adoptar AB en la interfaz y B dentro de la propia comunidad, evitando una mayor propagaci\u00f3n de A; y a la inversa, \u00e9sta es la \u00fanica manera de bloquear la propagaci\u00f3n de A. Ampliaciones adicionales. Otra forma de modelar la compatibilidad y la interoperabilidad en los modelos de difusi\u00f3n es a trav\u00e9s de los t\u00e9rminos \"fuera de la diagonal\" que representan el beneficio de las interacciones entre un nodo que adopta A y un nodo que adopta B. En lugar de establecerlos en 0, podemos considerar establecerlos en un valor x < min -LRB- q, 1 -- q -RRB-. Encontramos que para el caso de dos tecnolog\u00edas, el modelo no se vuelve m\u00e1s general, en el sentido de que cualquier caso de este tipo es equivalente, mediante un reescalamiento de q y r, a uno donde x = 0. Adem\u00e1s,Utilizando nuestra caracterizaci\u00f3n de la regi\u00f3n \u03a9 -LRB- G -RRB- en t\u00e9rminos de comunidades e interfaces, mostramos un resultado de monoton\u00eda: si A puede volverse epid\u00e9mica en un gr\u00e1fico G con par\u00e1metros -LRB- q, r, x -RRB-, y luego se aumenta x, entonces A a\u00fan puede volverse epid\u00e9mico con los nuevos par\u00e1metros. Tambi\u00e9n consideramos el efecto de estos t\u00e9rminos fuera de la diagonal en una extensi\u00f3n a k > 2 tecnolog\u00edas competidoras; para las tecnolog\u00edas X e Y, sea qX el beneficio de una interacci\u00f3n XX en un borde y qXY el beneficio de una interacci\u00f3n XY en un borde. Consideramos un escenario en el que dos tecnolog\u00edas B y C, que inicialmente coexisten con qBC = 0, se enfrentan a la introducci\u00f3n de una tercera tecnolog\u00eda, mejor, A en un conjunto finito de nodos. Mostramos un ejemplo en el que B y C sobreviven en equilibrio si fijan qBC en un rango particular de valores, pero no si fijan qBC demasiado bajo o demasiado alto para estar en este rango. As\u00ed, incluso en un modelo de difusi\u00f3n b\u00e1sico con tres tecnolog\u00edas, se encuentran casos en los que dos empresas tienen un incentivo para adoptar una \"alianza estrat\u00e9gica\" limitada, aumentando parcialmente su interoperabilidad para defenderse de un nuevo participante en el mercado. 6. COMPATIBILIDAD LIMITADA Consideremos ahora algunas formas adicionales de modelar la compatibilidad y la interoperabilidad. Primero consideramos dos tecnolog\u00edas, como en las secciones anteriores, e introducimos pagos \"fuera de la diagonal\" para capturar un beneficio positivo en las interacciones AB directas. Encontramos que, de hecho, esto no es m\u00e1s general que el modelo con beneficios cero para las interacciones AB. Luego consideramos extensiones a tres tecnolog\u00edas, identificando situaciones en las que dos tecnolog\u00edas existentes coexistentes pueden o no querer aumentar su compatibilidad mutua frente a una tercera tecnolog\u00eda nueva. Dos tecnolog\u00edas. Una relajaci\u00f3n natural del modelo de dos tecnolog\u00edas es introducir pagos -LRB- peque\u00f1os -RRB- positivos para la interacci\u00f3n AB; es decir, la comunicaci\u00f3n entre tecnolog\u00edas produce un valor menor para ambos agentes. Podemos modelar esto usando una variable xAB que representa la recompensa obtenida por un agente con tecnolog\u00eda A cuando su vecino tiene tecnolog\u00eda B, y de manera similar, una variable xBA que representa la recompensa obtenida por un agente con B cuando su vecino tiene A. Aqu\u00ed consideramos la Caso especial en el que estas entradas ``fuera de la diagonal'' son sim\u00e9tricas, es decir, xAB = xBA = x. Tambi\u00e9n suponemos que x < q < 1 -- q. Primero mostramos que el juego con entradas fuera de la diagonal es equivalente a un juego sin estas entradas, bajo un simple reescalamiento de q y r. Tenga en cuenta que si redimensionamos todos los pagos mediante una constante aditiva o multiplicativa, el comportamiento del juego no se ve afectado. Dado un juego con entradas fuera de la diagonal parametrizadas por q, r y x, considere restar x de todos los pagos y aumentarlo en un factor de 1 / -LRB- 1 -- 2x -RRB-. Como puede verse al examinar la Tabla 1, los pagos resultantes son exactamente los de un juego sin entradas fuera de la diagonal,parametrizado por q' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- y r ' = r / -LRB- 1 -- 2x -RRB-. Por tanto, la adici\u00f3n de entradas sim\u00e9tricas fuera de la diagonal no ampl\u00eda la clase de juegos que se consideran. La Tabla 1 representa los beneficios del juego de coordinaci\u00f3n en t\u00e9rminos de estos par\u00e1metros. Sin embargo, todav\u00eda podemos preguntarnos c\u00f3mo la adici\u00f3n de una entrada fuera de la diagonal podr\u00eda afectar el resultado de un juego en particular. Como muestra el siguiente ejemplo, aumentar la compatibilidad entre dos tecnolog\u00edas puede permitir que una tecnolog\u00eda que inicialmente no era epid\u00e9mica lo sea. EJEMPLO 6.1. Considere el juego de contagio que se juega en un gr\u00e1fico de l\u00edneas gruesas -LRB- ver Secci\u00f3n 3 -RRB- con r = 5/32 y q = 3/8. En este caso, A no es epid\u00e9mica, como se puede ver al examinar la Figura 1, ya que 2r < q y q + r > 1/2. Sin embargo, si insertamos pagos sim\u00e9tricos fuera de la diagonal x = 1/4, tenemos un nuevo juego, equivalente a un juego parametrizado por r ' = 5/16 y q ' = 1/4. Dado que q ' < 1/2 y q ' < 2r ', A es epid\u00e9mica en este juego y, por tanto, tambi\u00e9n en el juego con compatibilidad limitada. Ahora mostramos que, en general, si A es la tecnolog\u00eda superior -LRB-, es decir, q < 1/2 -RRB-, agregar un t\u00e9rmino de compatibilidad x solo puede ayudar a que A se expanda. TEOREMA 6.2. Sea G un juego sin compatibilidad, parametrizado por r y q en una red particular. Sea G ' el mismo juego, pero con un t\u00e9rmino de compatibilidad sim\u00e9trico agregado x. Si A es epid\u00e9mica para G, entonces A es epid\u00e9mica para G'. PRUEBA. Demostraremos que cualquier estructura de bloqueo en G ' es tambi\u00e9n una estructura de bloqueo en G. Seg\u00fan nuestro teorema de caracterizaci\u00f3n, el Teorema 4.6, esto implica el resultado deseado. Tenemos que G' equivale a un juego sin compatibilidad parametrizado por q' = -LRB- q -- x -RRB- / -LRB- 1 -- 2x -RRB- y r ' = r / -LRB- 1 -- 2x -RRB-. Considere una estructura de bloqueo -LRB- SB, SAB -RRB- para G'. Por tanto, m\u00e1s de dos tecnolog\u00edas. Dada la compleja estructura inherente a los juegos de contagio con dos tecnolog\u00edas, la comprensi\u00f3n de los juegos de contagio con tres o m\u00e1s tecnolog\u00edas est\u00e1 en gran medida abierta. Aqu\u00ed indicamos algunos de los problemas t\u00e9cnicos que surgen con m\u00faltiples tecnolog\u00edas, a trav\u00e9s de una serie de resultados iniciales. La configuraci\u00f3n b\u00e1sica que estudiamos es aquella en la que inicialmente coexisten dos tecnolog\u00edas existentes, B y C, y una tercera tecnolog\u00eda A, superior a ambas, se introduce inicialmente en un conjunto finito de nodos. Primero presentamos un teorema que establece que para cualquier \u0394 par, hay un juego de contagio en un \u0394: gr\u00e1fico regular en el que las dos tecnolog\u00edas actuales B y C pueden encontrar beneficioso aumentar su compatibilidad para evitar ser eliminadas por el nueva tecnolog\u00eda superior A. En particular, consideramos una situaci\u00f3n en la que inicialmente, dos tecnolog\u00edas B y C con compatibilidad cero se encuentran en un estado estable. Por estado estable queremos decir que ninguna perturbaci\u00f3n finita de los estados actuales puede provocar una epidemia ni para B ni para C. Tambi\u00e9n tenemos una tecnolog\u00eda A que es superior tanto a B como a C,y puede convertirse en una epidemia al obligar a un solo nodo a elegir A. Sin embargo, al aumentar su compatibilidad, B y C pueden mantener su estabilidad y resistir una epidemia de A. Sea qA denota los beneficios de dos nodos adyacentes que eligen la tecnolog\u00eda A, y defina qB y qC de manera an\u00e1loga. Supondremos qA > qB > qC. Tambi\u00e9n suponemos que r, el costo de seleccionar tecnolog\u00edas adicionales, es lo suficientemente grande como para garantizar que los nodos nunca adopten m\u00e1s de una tecnolog\u00eda. Finalmente, consideramos un par\u00e1metro de compatibilidad qBC que representa los pagos a dos nodos adyacentes cuando uno selecciona B y el otro selecciona C. Por lo tanto, nuestro juego de contagio ahora se describe mediante cinco par\u00e1metros -LRB- G, qA, qB, qC, qBC -RRB -. PRUEBA. -LRB- Boceto. -RRB- Dado \u0394, defina G comenzando con una cuadr\u00edcula infinita y conectando cada nodo a su \u0394 m\u00e1s cercano: 2 vecinos que est\u00e1n en la misma fila. El estado inicial s asigna la estrategia B a filas pares y la estrategia C a filas impares. Las afirmaciones primera, tercera y cuarta del teorema se pueden verificar verificando las desigualdades correspondientes. La segunda afirmaci\u00f3n se deriva de la primera y de la observaci\u00f3n de que las filas alternas contienen cualquier posible epidemia que crezca verticalmente. El teorema anterior muestra que dos tecnolog\u00edas pueden sobrevivir a la introducci\u00f3n de una nueva tecnolog\u00eda aumentando su nivel de compatibilidad entre s\u00ed. Como era de esperar, Tabla 1: Los pagos en el juego de coordinaci\u00f3n. La entrada -LRB- x, y -RRB- en la fila i, columna j indica que el jugador de la fila obtiene un pago de x y el jugador de la columna obtiene un pago de y cuando el jugador de la fila juega la estrategia i y el jugador de la columna juega la estrategia j. Hay casos en los que una mayor compatibilidad entre dos tecnolog\u00edas ayuda a una tecnolog\u00eda a expensas de la otra. Pero, sorprendentemente, tambi\u00e9n hay casos en los que la compatibilidad resulta perjudicial para ambas partes; el siguiente ejemplo considera una configuraci\u00f3n inicial fija con las tecnolog\u00edas A, B y C que est\u00e1 en equilibrio cuando qBC = 0. Sin embargo, si este t\u00e9rmino de compatibilidad aumenta lo suficiente, el equilibrio se pierde y A se vuelve epid\u00e9mica. EJEMPLO 6.4. Considere la uni\u00f3n de un gr\u00e1fico de cuadr\u00edcula bidimensional infinito con nodos u -LRB- x, y -RRB- y un gr\u00e1fico de l\u00edneas infinitas con nodos v -LRB- y -RRB-. Agregue una arista entre u -LRB- 1, y -RRB- y v -LRB- y -RRB- para todo y. Para esta red, consideramos la configuraci\u00f3n inicial en la que todos los nodos v -LRB- y -RRB- seleccionan A, y el nodo u -LRB- x, y -RRB- selecciona B si x < 0 y selecciona C en caso contrario. Ahora definimos los par\u00e1metros de este juego de la siguiente manera. Se verifica f\u00e1cilmente que para estos valores, la configuraci\u00f3n inicial dada anteriormente es un equilibrio. Sin embargo, supongamos ahora que aumentamos el t\u00e9rmino de coordinaci\u00f3n, estableciendo qBC = 0,9. Esto no es un equilibrio, ya que cada nodo de la forma u -LRB- 0, y -RRB- ahora tiene un incentivo para cambiar de C -LRB- generando un pago de 3.9 -RRB- a B -LRB- generando as\u00ed un pago de 3,95 -RRB-. Sin embargo,una vez que estos nodos han adoptado B, la mejor respuesta para cada nodo de la forma u -LRB- 1, y -RRB- es A -LRB- A genera un pago de 4 mientras que B solo genera un pago de 3,95 -RRB- . A partir de aqu\u00ed, no es dif\u00edcil demostrar que A se propaga directamente por toda la red.", "keyphrases": ["proceso difuso", "modelo difuso de teor\u00eda de juegos", "estrategia incompatible", "biling\u00fce", "l\u00edmite de compatibilidad", "interoperar", "propiedad no convexa", "personaje", "teorema de morri", "umbral de contagio", "juego de contagio", "funci\u00f3n potencia"]}
{"file_name": "I-34", "text": "Resoluci\u00f3n de conflictos e inconsistencias en organizaciones virtuales reguladas por normas RESUMEN Las organizaciones virtuales gobernadas por normas definen, gobiernan y facilitan el intercambio coordinado de recursos y la resoluci\u00f3n de problemas en sociedades de agentes. Con una descripci\u00f3n expl\u00edcita de las normas, se puede lograr la apertura en las organizaciones virtuales: se pueden acomodar sin problemas nuevos componentes, dise\u00f1ados por varias partes. Nos centramos en organizaciones virtuales realizadas como sistemas multiagente, en las que agentes humanos y de software interact\u00faan para lograr objetivos individuales y globales. Sin embargo, cualquier explicaci\u00f3n realista de las normas debe abordar su naturaleza din\u00e1mica: las normas cambiar\u00e1n a medida que los agentes interact\u00faan entre s\u00ed y con su entorno. Debido a la naturaleza cambiante de las normas o a normas derivadas de diferentes organizaciones virtuales, habr\u00e1 situaciones en las que una acci\u00f3n se permite y se proh\u00edbe simult\u00e1neamente, es decir, surge un conflicto. Asimismo, habr\u00e1 situaciones en las que una acci\u00f3n sea a la vez obligada y prohibida, es decir, surja una inconsistencia. Introducimos un enfoque, basado en la unificaci\u00f3n de primer orden, para detectar y resolver dichos conflictos e inconsistencias. En nuestra soluci\u00f3n propuesta, anotamos una norma con el conjunto de valores que sus variables no deber\u00edan tener para evitar un conflicto o una inconsistencia con otra norma. Nuestro enfoque se adapta perfectamente a las interrelaciones dependientes del dominio entre acciones y los conflictos/inconsistencias indirectas que estas pueden causar. De manera m\u00e1s general, podemos capturar una noci\u00f3n \u00fatil de delegaci\u00f3n entre agentes -LRB- y entre roles -RRB- de acciones y normas asociadas a ellas, y utilizarla para abordar conflictos/inconsistencias causados \u200b\u200bpor la delegaci\u00f3n de acciones. Ilustramos nuestro enfoque con un ejemplo de e-Ciencia en el que los agentes apoyan los servicios Grid. 1. INTRODUCCI\u00d3N Las organizaciones virtuales -LRB- VOs -RRB- facilitan el intercambio coordinado de recursos y la resoluci\u00f3n de problemas que involucran a varias partes geogr\u00e1ficamente remotas -LSB- 9 -RSB-. Los VO definen y regulan las interacciones -LRB- facilitando as\u00ed la coordinaci\u00f3n -RRB- entre software y/o agentes humanos que se comunican para lograr objetivos individuales y globales -LSB- 16 -RSB-. Los VO se realizan como sistemas de m\u00faltiples agentes y una caracter\u00edstica m\u00e1s deseable de dichos sistemas es la apertura, mediante la cual los nuevos componentes dise\u00f1ados por otras partes se adaptan sin problemas. Las normas regulan el comportamiento observable de agentes de software heterog\u00e9neos e interesados, dise\u00f1ados por varias partes que pueden no confiar completamente entre s\u00ed -LSB- 3, 24 -RSB-. Sin embargo, las VO reguladas por normas pueden experimentar problemas cuando las normas asignadas a sus agentes est\u00e1n en conflicto -LRB- es decir, una acci\u00f3n est\u00e1 simult\u00e1neamente prohibida y permitida -RRB- o inconsistente -LRB- es decir, una acci\u00f3n est\u00e1 simult\u00e1neamente prohibida y obligada -RRB- . Proponemos un medio para detectar y resolver autom\u00e1ticamente conflictos e inconsistencias en VO reguladas por normas. Hacemos uso de la unificaci\u00f3n de t\u00e9rminos de primer orden -LSB- 8 -RSB- para descubrir si y c\u00f3mo las normas se superponen en su influencia -LRB- es decir,los agentes y valores de par\u00e1metros en las acciones de los agentes que las normas pueden afectar -RRB-. Esto permite una soluci\u00f3n detallada mediante la cual se restringe la influencia de normas conflictivas o inconsistentes para conjuntos de valores particulares. Por ejemplo, las normas `` al agente x se le permite enviar la oferta -LRB- ag1, 20 -RRB- '' y `` al agente ag2 se le proh\u00edbe enviar la oferta -LRB- y, z -RRB- '' -LRB- donde x, y, z son variables y ag1, ag2, 20 son constantes -RRB- est\u00e1n en conflicto porque sus agentes, acciones y t\u00e9rminos -LRB- dentro de las acciones -RRB- se unifican. Resolvemos el conflicto anotando normas con conjuntos de valores que sus variables no pueden tener, restringiendo as\u00ed su influencia. En nuestro ejemplo, el conflicto se evita si requerimos que la variable y no pueda ser ag1 y que z no pueda ser 20. En la siguiente secci\u00f3n proporcionamos una definici\u00f3n minimalista para VO regulados por normas. En la secci\u00f3n 3 definimos formalmente los conflictos normativos y explicamos c\u00f3mo se detectan y resuelven. En la secci\u00f3n 4 describimos c\u00f3mo se puede adaptar la maquinaria de la secci\u00f3n anterior para detectar y resolver inconsistencias normativas. En la secci\u00f3n 5 describimos c\u00f3mo se utilizan nuestras normas restringidas en sociedades de agentes conscientes de las normas. En la secci\u00f3n 6 explicamos c\u00f3mo se puede utilizar nuestra maquinaria para detectar y resolver conflictos/inconsistencias indirectas, es decir, aquellos causados \u200b\u200ba trav\u00e9s de relaciones entre acciones; ampliamos y adaptamos la maquinaria para dar cabida a la delegaci\u00f3n de normas. En la secci\u00f3n 7 ilustramos nuestro enfoque con un ejemplo de agentes de software regulados por normas que sirven al Grid.En la secci\u00f3n 7 ilustramos nuestro enfoque con un ejemplo de agentes de software regulados por normas que sirven al Grid.En la secci\u00f3n 7 ilustramos nuestro enfoque con un ejemplo de agentes de software regulados por normas que sirven al Grid.", "keyphrases": ["\u00f3rgano virtual", "sistema multiagente", "norma-regular vo", "agente", "conflicto de normas", "conflicto prohibir", "norma inconsistente", "agente externo", "agente gobernador"]}
{"file_name": "J-31", "text": "Calcular la estrategia \u00f3ptima a seguir \u2217 RESUMEN En los sistemas multiagente, los escenarios estrat\u00e9gicos a menudo se analizan bajo el supuesto de que los jugadores eligen sus estrategias simult\u00e1neamente. Sin embargo, este modelo no siempre es realista. En muchos escenarios, un jugador puede comprometerse con una estrategia antes de que el otro jugador tome una decisi\u00f3n. Estos modelos se denominan sin\u00f3nimos de liderazgo, compromiso o Stackelberg, y el juego \u00f3ptimo en dichos modelos suele ser significativamente diferente del juego \u00f3ptimo en el modelo en el que las estrategias se seleccionan simult\u00e1neamente. El reciente aumento del inter\u00e9s en soluciones inform\u00e1ticas de teor\u00eda de juegos ha ignorado hasta ahora los modelos de liderazgo -LRB- con la excepci\u00f3n del inter\u00e9s en el dise\u00f1o de mecanismos, donde el dise\u00f1ador est\u00e1 impl\u00edcitamente en una posici\u00f3n de liderazgo -RRB-. En este art\u00edculo, estudiamos c\u00f3mo calcular estrategias \u00f3ptimas para comprometerse tanto con estrategias puras como con estrategias mixtas, tanto en juegos de forma normal como bayesianos. Damos tanto resultados positivos -LRB- de algoritmos eficientes -RRB- como resultados negativos -LRB- resultados de dureza NP -RRB-. 1. INTRODUCCI\u00d3N En sistemas multiagente con agentes interesados \u200b\u200b-LRB-, incluyendo la mayor\u00eda de entornos econ\u00f3micos -RRB-, la acci\u00f3n \u00f3ptima que debe tomar un agente depende de las acciones que toman los otros agentes. Para analizar c\u00f3mo deber\u00eda comportarse un agente en tales entornos, es necesario aplicar las herramientas de la teor\u00eda de juegos. Normalmente, cuando se modela un escenario estrat\u00e9gico en el marco de la teor\u00eda de juegos, se supone que los jugadores eligen sus estrategias simult\u00e1neamente. Esto es especialmente cierto cuando el escenario se modela como un juego de forma normal, que s\u00f3lo especifica la utilidad de cada agente en funci\u00f3n del vector de estrategias que los agentes eligen, y no proporciona ninguna informaci\u00f3n sobre el orden en el que los agentes hacen sus acciones. sus decisiones y lo que los agentes observan sobre decisiones anteriores de otros agentes. Dado que el juego se modela en forma normal, normalmente se analiza utilizando el concepto de equilibrio de Nash. Un equilibrio de Nash especifica una estrategia para cada jugador, de modo que ning\u00fan jugador tiene un incentivo para desviarse individualmente de este perfil de estrategias. -LRB- Normalmente, se permite que las estrategias sean mixtas, es decir, distribuciones de probabilidad sobre las estrategias -LRB- puras -RRB- originales. -RRB- Se garantiza la existencia de un equilibrio de Nash -LRB- de estrategia mixta -RRB- en juegos finitos -LSB- 18 -RSB-, pero un problema es que puede haber m\u00faltiples equilibrios de Nash. Esto conduce al problema de selecci\u00f3n del equilibrio: c\u00f3mo un agente puede saber qu\u00e9 estrategia utilizar si no sabe qu\u00e9 equilibrio debe utilizar. Cuando el escenario se modela como un juego extensivo, es posible especificar que algunos jugadores reciban cierta informaci\u00f3n sobre las acciones realizadas por otros anteriormente en el juego antes de decidir sobre su acci\u00f3n. Sin embargo, en general, los jugadores no saben todo lo que pas\u00f3 al principio del juego. Debido a esto,Por lo general, estos juegos todav\u00eda se analizan utilizando un concepto de equilibrio, donde se especifica una estrategia mixta para cada jugador y se requiere que la estrategia de cada jugador sea la mejor respuesta a las estrategias de los dem\u00e1s. -LRB- Normalmente ahora se impone una restricci\u00f3n adicional a las estrategias para garantizar que los jugadores no jueguen de una manera irracional con respecto a la informaci\u00f3n que han recibido hasta ahora. Esto conduce a refinamientos del equilibrio de Nash, como el equilibrio perfecto en subjuegos y el equilibrio secuencial. -RRB- Sin embargo, en muchos entornos del mundo real, las estrategias no se seleccionan de manera tan simult\u00e1nea. Muchas veces, un jugador -LRB- el l\u00edder -RRB- es capaz de comprometerse con una estrategia antes que otro jugador -LRB- el seguidor -RRB-. Esto puede deberse a diversas razones. Por ejemplo, uno de los jugadores puede llegar al sitio en el que se va a jugar el juego antes que otro agente -LRB-. Por ejemplo, en escenarios econ\u00f3micos, un jugador puede ingresar a un mercado antes y comprometerse con una forma de hacer negocios -RRB. -. Ese poder de compromiso tiene un profundo impacto en c\u00f3mo se debe jugar el juego. Por ejemplo, al l\u00edder le puede convenir jugar una estrategia que est\u00e9 dominada en la representaci\u00f3n normal del juego. En general, si es posible comprometerse con estrategias mixtas, entonces -LRB- bajo supuestos menores -RRB- nunca est\u00e1 de m\u00e1s, y a menudo ayuda, comprometerse con una estrategia -LSB- 26 -RSB-. Verse obligado a comprometerse con una estrategia pura a veces ayuda y a veces perjudica -LRB- por ejemplo, comprometerse con una estrategia pura en piedra, papel o tijera antes de que la decisi\u00f3n del otro jugador naturalmente resulte en una p\u00e9rdida -RRB-. En este art\u00edculo asumiremos que el compromiso siempre es forzado; si no es as\u00ed, el jugador que tiene la opci\u00f3n de comprometerse puede simplemente comparar el resultado del compromiso con el resultado de no compromiso -LRB- movimiento simult\u00e1neo -RRB-. Los modelos de liderazgo son especialmente importantes en entornos con m\u00faltiples agentes de software interesados. Una vez que se finaliza el c\u00f3digo para un agente -LRB- o para un equipo de agentes -RRB- y se implementa el agente, el agente se compromete a ejecutar la estrategia -LRB- posiblemente aleatoria -RRB- que prescribe el c\u00f3digo. Finalmente, tambi\u00e9n existe una situaci\u00f3n de liderazgo impl\u00edcita en el campo del dise\u00f1o de mecanismos, en la que un jugador -LRB-, el dise\u00f1ador -RRB-, elige las reglas del juego que luego juegan los jugadores restantes. De hecho, el dise\u00f1ador del mecanismo puede beneficiarse al comprometerse con una elecci\u00f3n que, si las acciones de los agentes -LRB- restantes -RRB- fueran fijas, ser\u00eda sub\u00f3ptima. Sin embargo, se ha ignorado el c\u00e1lculo de la estrategia \u00f3ptima con la que comprometerse en una situaci\u00f3n de liderazgo. Te\u00f3ricamente, las situaciones de liderazgo pueden considerarse simplemente como un juego extensivo en el que un jugador elige primero una estrategia -LRB- para el juego original -RRB-. Sin embargo, el n\u00famero de estrategias en este juego extensivo puede ser extremadamente grande. Por ejemplo, si el l\u00edder puede comprometerse con una estrategia mixta en el juego original,entonces cada una de las estrategias mixtas -LRB- de -RRB- constituye una estrategia pura en la representaci\u00f3n en forma extensiva de la situaci\u00f3n de liderazgo. -LRB- Observamos que no es lo mismo un compromiso de reparto que un reparto sobre compromisos. -RRB- Adem\u00e1s, si el juego original es en s\u00ed mismo un juego en forma extensiva, el n\u00famero de estrategias en la representaci\u00f3n en forma extensiva de la situaci\u00f3n de liderazgo -LRB- que es un juego en forma extensiva diferente -RRB- se vuelve a\u00fan mayor. Debido a esto, normalmente no es factible desde el punto de vista computacional transformar simplemente el juego original en una representaci\u00f3n en forma extensiva de la situaci\u00f3n de liderazgo; en cambio, tenemos que analizar el juego en su representaci\u00f3n original. En este art\u00edculo, estudiamos c\u00f3mo calcular la estrategia \u00f3ptima a seguir, tanto en juegos de forma normal -LRB- Secci\u00f3n 2 -RRB- como en juegos bayesianos, que son un caso especial de juegos de forma extensiva -LRB- Secci\u00f3n 3 -RRB -. 4. CONCLUSIONES E INVESTIGACIONES FUTURAS En los sistemas multiagente, los escenarios estrat\u00e9gicos a menudo se analizan bajo el supuesto de que los jugadores eligen sus estrategias simult\u00e1neamente. Esto requiere alguna noci\u00f3n de equilibrio -LRB-, equilibrio de Nash y sus refinamientos -RRB-, y a menudo conduce al problema de selecci\u00f3n de equilibrio: no est\u00e1 claro para cada jugador individual seg\u00fan qu\u00e9 equilibrio debe jugar. Sin embargo, este modelo no siempre es realista. En muchos escenarios, un jugador puede comprometerse con una estrategia antes de que el otro jugador tome una decisi\u00f3n. Por ejemplo, un agente puede llegar al sitio -LRB- real o virtual -RRB- del juego antes que el otro, o, en el caso espec\u00edfico de los agentes de software, el c\u00f3digo de un agente puede completarse y confirmarse antes que el de otro. agente. Estos modelos se denominan sin\u00f3nimos de liderazgo, compromiso o Stackelberg, y el juego \u00f3ptimo en dichos modelos suele ser significativamente diferente del juego \u00f3ptimo en el modelo en el que las estrategias se seleccionan simult\u00e1neamente. Espec\u00edficamente, si el compromiso con estrategias mixtas es posible, entonces el compromiso -LRB- \u00f3ptimo -RRB- nunca perjudica al l\u00edder y, a menudo, ayuda. El reciente aumento del inter\u00e9s en soluciones inform\u00e1ticas de teor\u00eda de juegos ha ignorado hasta ahora los modelos de liderazgo -LRB- con la excepci\u00f3n del inter\u00e9s en el dise\u00f1o de mecanismos, donde el dise\u00f1ador est\u00e1 impl\u00edcitamente en una posici\u00f3n de liderazgo -RRB-. En este art\u00edculo, estudiamos c\u00f3mo calcular estrategias \u00f3ptimas para comprometerse tanto con estrategias puras como con estrategias mixtas, tanto en juegos de forma normal como bayesianos. Para juegos de forma normal, demostramos que la estrategia pura \u00f3ptima con la que comprometerse se puede encontrar de manera eficiente para cualquier n\u00famero de jugadores. Se puede encontrar una estrategia mixta \u00f3ptima para comprometerse en un juego de forma normal de manera eficiente para dos jugadores usando programaci\u00f3n lineal -LRB- y no m\u00e1s eficientemente que eso, en el sentido de que cualquier programa lineal con una restricci\u00f3n de probabilidad puede codificarse como tal. problema -RRB-.-LRB- Esta es una generalizaci\u00f3n de la computabilidad en tiempo polinomial de estrategias minimax en juegos de forma normal. -RRB- El problema se vuelve NP-dif\u00edcil para tres jugadores -LRB- o m\u00e1s -RRB-. En los juegos bayesianos, el problema de encontrar una estrategia pura \u00f3ptima con la que comprometerse es NP-dif\u00edcil incluso en juegos de dos jugadores en los que el seguidor tiene un solo tipo, aunque los juegos de dos jugadores en los que el l\u00edder tiene un solo tipo pueden resolverse de manera eficiente. El problema de encontrar una estrategia mixta \u00f3ptima con la que comprometerse en un juego bayesiano es NP-dif\u00edcil incluso en juegos de dos jugadores en los que el l\u00edder tiene un solo tipo, aunque los juegos de dos jugadores en los que el seguidor tiene un solo tipo pueden resolverse eficientemente utilizando una generalizaci\u00f3n del enfoque de programaci\u00f3n lineal para juegos de forma normal. Las dos tablas siguientes resumen estos resultados. Resultados por apuesta por estrategias mixtas. -LRB- Con m\u00e1s de 2 jugadores, el ``seguidor'' es el \u00faltimo jugador en comprometerse, el ``l\u00edder'' es el primero. -RRB- La investigaci\u00f3n futura puede tomar varias direcciones. Tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias \u00f3ptimas para comprometernos en otras1 representaciones concisas de juegos de forma normal, por ejemplo, en juegos gr\u00e1ficos -LSB- 10 -RSB- o juegos de gr\u00e1ficos de acci\u00f3n/efecto local -LSB- 14, 1. -RSB-. Para los casos en los que calcular una estrategia \u00f3ptima con la que comprometerse es NP-dif\u00edcil, tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias aproximadamente \u00f3ptimas con las que comprometerse. Tambi\u00e9n se pueden estudiar modelos en los que varios jugadores -LRB- pero no todos -RRB- se comprometen al mismo tiempo. Otra direcci\u00f3n interesante a seguir es ver si calcular estrategias mixtas \u00f3ptimas con las que comprometernos puede ayudarnos o arrojar luz sobre el c\u00e1lculo de los equilibrios de Nash. A menudo, las estrategias mixtas \u00f3ptimas a las que comprometerse son tambi\u00e9n estrategias de equilibrio de Nash -LRB- por ejemplo, en juegos de dos jugadores de suma cero esto siempre es cierto -RRB-, aunque no siempre es as\u00ed -LRB- por ejemplo, como Como ya hemos se\u00f1alado, a veces la estrategia \u00f3ptima con la que comprometerse es una estrategia estrictamente dominada, que nunca puede ser una estrategia de equilibrio de Nash -RRB-.el ``l\u00edder'' es el primero. -RRB- La investigaci\u00f3n futura puede tomar varias direcciones. Tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias \u00f3ptimas para comprometernos en otras1 representaciones concisas de juegos de forma normal, por ejemplo, en juegos gr\u00e1ficos -LSB- 10 -RSB- o juegos de gr\u00e1ficos de acci\u00f3n/efecto local -LSB- 14, 1. -RSB-. Para los casos en los que calcular una estrategia \u00f3ptima con la que comprometerse es NP-dif\u00edcil, tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias aproximadamente \u00f3ptimas con las que comprometerse. Tambi\u00e9n se pueden estudiar modelos en los que varios jugadores -LRB- pero no todos -RRB- se comprometen al mismo tiempo. Otra direcci\u00f3n interesante a seguir es ver si calcular estrategias mixtas \u00f3ptimas con las que comprometernos puede ayudarnos o arrojar luz sobre el c\u00e1lculo de los equilibrios de Nash. A menudo, las estrategias mixtas \u00f3ptimas a las que comprometerse son tambi\u00e9n estrategias de equilibrio de Nash -LRB- por ejemplo, en juegos de dos jugadores de suma cero esto siempre es cierto -RRB-, aunque no siempre es as\u00ed -LRB- por ejemplo, como Como ya hemos se\u00f1alado, a veces la estrategia \u00f3ptima con la que comprometerse es una estrategia estrictamente dominada, que nunca puede ser una estrategia de equilibrio de Nash -RRB-.el ``l\u00edder'' es el primero. -RRB- La investigaci\u00f3n futura puede tomar varias direcciones. Tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias \u00f3ptimas para comprometernos en otras1 representaciones concisas de juegos de forma normal, por ejemplo, en juegos gr\u00e1ficos -LSB- 10 -RSB- o juegos de gr\u00e1ficos de acci\u00f3n/efecto local -LSB- 14, 1. -RSB-. Para los casos en los que calcular una estrategia \u00f3ptima con la que comprometerse es NP-dif\u00edcil, tambi\u00e9n podemos estudiar el c\u00e1lculo de estrategias aproximadamente \u00f3ptimas con las que comprometerse. Tambi\u00e9n se pueden estudiar modelos en los que varios jugadores -LRB- pero no todos -RRB- se comprometen al mismo tiempo. Otra direcci\u00f3n interesante a seguir es ver si calcular estrategias mixtas \u00f3ptimas con las que comprometernos puede ayudarnos o arrojar luz sobre el c\u00e1lculo de los equilibrios de Nash. A menudo, las estrategias mixtas \u00f3ptimas a las que comprometerse son tambi\u00e9n estrategias de equilibrio de Nash -LRB- por ejemplo, en juegos de dos jugadores de suma cero esto siempre es cierto -RRB-, aunque no siempre es as\u00ed -LRB- por ejemplo, como Como ya hemos se\u00f1alado, a veces la estrategia \u00f3ptima con la que comprometerse es una estrategia estrictamente dominada, que nunca puede ser una estrategia de equilibrio de Nash -RRB-.", "keyphrases": ["estrategia optima", "sistema multiag", "manera simult\u00e1nea", "modelo stackelberg", "modelo de liderazgo", "pura estrategia", "mezclar estrategias", "juego en forma normal", "juego bayesiano", "equilibrio de Nash", "np-duro"]}
{"file_name": "H-13", "text": "La influencia de las caracter\u00edsticas de los subt\u00edtulos en los patrones de clics en la b\u00fasqueda web RESUMEN Los motores de b\u00fasqueda web presentan listas de subt\u00edtulos, que comprenden t\u00edtulo, fragmento y URL, para ayudar a los usuarios a decidir qu\u00e9 resultados de b\u00fasqueda visitar. Comprender la influencia de las caracter\u00edsticas de estos subt\u00edtulos en el comportamiento de b\u00fasqueda en la Web puede ayudar a validar algoritmos y pautas para su generaci\u00f3n mejorada. En este art\u00edculo desarrollamos una metodolog\u00eda para utilizar registros de clics de un motor de b\u00fasqueda comercial para estudiar el comportamiento del usuario al interactuar con los t\u00edtulos de los resultados de b\u00fasqueda. Los hallazgos de nuestro estudio sugieren que las caracter\u00edsticas de los subt\u00edtulos relativamente simples, como la presencia de todos los t\u00e9rminos de consulta, la legibilidad del fragmento y la longitud de la URL que se muestra en el t\u00edtulo, pueden influir significativamente en el comportamiento de b\u00fasqueda web de los usuarios. 1. INTRODUCCI\u00d3N Los principales motores de b\u00fasqueda web comerciales presentan sus resultados de forma muy parecida. Cada resultado de b\u00fasqueda se describe mediante un breve t\u00edtulo, que comprende la URL de la p\u00e1gina web asociada, un t\u00edtulo y un breve resumen -LRB- o ``snippet'' -RRB- que describe el contenido de la p\u00e1gina. A menudo, el fragmento se extrae de la propia p\u00e1gina web, pero tambi\u00e9n se puede tomar de fuentes externas, como los res\u00famenes generados por humanos que se encuentran en los directorios web. La Figura 1 muestra una b\u00fasqueda web t\u00edpica, con subt\u00edtulos para los tres resultados principales. Si bien los tres t\u00edtulos comparten lo mismo, el fragmento del tercer t\u00edtulo es casi el doble de largo que el del primero, mientras que el fragmento falta por completo en el segundo t\u00edtulo. El t\u00edtulo del tercer t\u00edtulo contiene todos los t\u00e9rminos de consulta en orden, mientras que los t\u00edtulos del primer y segundo t\u00edtulo contienen solo dos de los tres t\u00e9rminos. Uno de los t\u00e9rminos de consulta se repite en el primer t\u00edtulo. Todos los t\u00e9rminos de consulta aparecen en la URL del tercer t\u00edtulo, mientras que ninguno aparece en la URL del primer t\u00edtulo. Si bien estas diferencias pueden parecer menores, tambi\u00e9n pueden tener un impacto sustancial en el comportamiento del usuario. Una motivaci\u00f3n principal para proporcionar un t\u00edtulo es ayudar al usuario a determinar la relevancia de la p\u00e1gina asociada sin tener que hacer clic para acceder al resultado. En el caso de una consulta de navegaci\u00f3n, especialmente cuando el destino es bien conocido, la URL por s\u00ed sola puede ser suficiente para identificar la p\u00e1gina deseada. Pero en el caso de una consulta informativa, el t\u00edtulo y el fragmento pueden ser necesarios para guiar al usuario en la selecci\u00f3n de una p\u00e1gina para un estudio m\u00e1s profundo, y puede juzgar la relevancia de una p\u00e1gina bas\u00e1ndose \u00fanicamente en el t\u00edtulo. Cuando este juicio es correcto, puede acelerar el proceso de b\u00fasqueda al permitir al usuario evitar material no deseado. Cuando falla, el usuario puede perder el tiempo haciendo clic en un resultado inapropiado y escaneando una p\u00e1gina que contiene poco o nada de inter\u00e9s. Peor a\u00fan, el usuario puede ser enga\u00f1ado y saltarse una p\u00e1gina que contiene la informaci\u00f3n deseada. Los tres resultados de la figura 1 son relevantes, con algunas limitaciones. El primer resultado enlaza con el sitio principal de Yahoo Kids! p\u00e1gina principal,pero luego es necesario seguir un enlace en un men\u00fa para encontrar la p\u00e1gina principal de los juegos. A pesar de las apariencias, el segundo resultado enlaza con una colecci\u00f3n sorprendentemente grande de juegos en l\u00ednea, principalmente con temas ambientales. Desafortunadamente, estas caracter\u00edsticas de la p\u00e1gina no se reflejan completamente en los t\u00edtulos. En este art\u00edculo, examinamos la influencia de las caracter\u00edsticas de los subt\u00edtulos en el comportamiento de b\u00fasqueda web del usuario, utilizando clics extra\u00eddos de los registros de los motores de b\u00fasqueda como nuestra principal herramienta de investigaci\u00f3n. Comprender esta influencia puede ayudar a validar algoritmos y pautas para la generaci\u00f3n mejorada de Figura 1: Tres resultados principales de la consulta: juegos infantiles en l\u00ednea. los propios subt\u00edtulos. Adem\u00e1s, estas caracter\u00edsticas pueden desempe\u00f1ar un papel en el proceso de inferir juicios de relevancia a partir del comportamiento del usuario -LSB- 1 -RSB-. Al comprender mejor su influencia, pueden resultar mejores juicios. Diferentes algoritmos de generaci\u00f3n de subt\u00edtulos pueden seleccionar fragmentos de diferentes longitudes de diferentes \u00e1reas de una p\u00e1gina. Los fragmentos se pueden generar de forma independiente de la consulta, proporcionando un resumen de la p\u00e1gina en su conjunto, o de forma dependiente de la consulta, proporcionando un resumen de c\u00f3mo la p\u00e1gina se relaciona con los t\u00e9rminos de la consulta. La elecci\u00f3n correcta del fragmento puede depender de aspectos tanto de la consulta como de la p\u00e1gina de resultados. Para los enlaces que redireccionan, es posible mostrar URL alternativas. Adem\u00e1s, para las p\u00e1ginas que figuran en directorios web editados por humanos, como Open Directory Project, es posible mostrar t\u00edtulos alternativos y fragmentos derivados de estos listados. Cuando estos fragmentos, t\u00edtulos y URL alternativos est\u00e9n disponibles, la selecci\u00f3n de una combinaci\u00f3n adecuada para su visualizaci\u00f3n puede guiarse por sus caracter\u00edsticas. Un fragmento de un directorio web puede constar de oraciones completas y ser menos fragmentario que un fragmento extra\u00eddo. Un t\u00edtulo extra\u00eddo del cuerpo puede proporcionar una mayor cobertura de los t\u00e9rminos de la consulta. El trabajo presentado en este art\u00edculo se llev\u00f3 a cabo en el context del motor de b\u00fasqueda de Windows Live. Los experimentos informados en secciones posteriores se basan en registros de consultas de Windows Live, p\u00e1ginas de resultados y juicios de relevancia recopilados como parte de una investigaci\u00f3n en curso sobre el rendimiento del motor de b\u00fasqueda -LSB- 1, 2 -RSB-. No obstante, dada la similitud de los formatos de subt\u00edtulos en los principales motores de b\u00fasqueda web, creemos que los resultados son aplicables a estos otros motores. La consulta en ` www.dmoz.org figura 1 produce resultados con relevancia similar en los otros motores de b\u00fasqueda principales. Esta y otras consultas producen t\u00edtulos que presentan variaciones similares. Adem\u00e1s, creemos que nuestra metodolog\u00eda puede generalizarse a otras aplicaciones de b\u00fasqueda cuando haya suficientes datos de clics disponibles. 2. TRABAJO RELACIONADO Si bien los motores de b\u00fasqueda web comerciales han seguido enfoques similares para la visualizaci\u00f3n de subt\u00edtulos desde su g\u00e9nesis, se ha publicado relativamente poca investigaci\u00f3n sobre m\u00e9todos para generar estos subt\u00edtulos y evaluar su impacto en el comportamiento del usuario.La mayor\u00eda de las investigaciones sobre la visualizaci\u00f3n de resultados web han propuesto cambios sustanciales en la interfaz, en lugar de abordar los detalles de las interfaces existentes. 2.1 Visualizaci\u00f3n de resultados web Varadarajan y Hristidis -LSB- 16 -RSB- se encuentran entre los pocos que han intentado mejorar directamente los fragmentos generados por los sistemas de b\u00fasqueda comerciales, sin introducir cambios adicionales en la interfaz. Generaron fragmentos a partir de \u00e1rboles de gr\u00e1ficos de documentos y compararon experimentalmente estos fragmentos con los fragmentos generados para los mismos documentos por el sistema de b\u00fasqueda de escritorio de Google y el sistema de b\u00fasqueda de escritorio de MSN. Evaluaron su m\u00e9todo pidiendo a los usuarios que compararan fragmentos de diversas fuentes. 6. CONCLUSIONES Las inversiones de clic constituyen una herramienta adecuada para evaluar la influencia de las caracter\u00edsticas de los subt\u00edtulos. Utilizando inversiones de clic, hemos demostrado que las funciones de subt\u00edtulos relativamente simples pueden influir significativamente en el comportamiento del usuario. Hasta donde sabemos, esta es la primera metodolog\u00eda validada para evaluar la calidad de los subt\u00edtulos web a trav\u00e9s de retroalimentaci\u00f3n impl\u00edcita. Tambi\u00e9n esperamos abordar directamente el objetivo de predecir la relevancia a partir de los clics y otra informaci\u00f3n presente en los registros de los motores de b\u00fasqueda.", "keyphrases": ["patr\u00f3n de clics", "caracter\u00edstica de subt\u00edtulos", "comportamiento de b\u00fasqueda web", "Factor humano", "extraer resumen", "retazo", "registro de consultas", "queri reformul", "palabra significativa", "clic inverso", "coincidencia de t\u00e9rminos de consulta"]}
{"file_name": "I-5", "text": "Hacia una asignaci\u00f3n de recursos basada en agentes autoorganizada en un entorno multiservidor RESUMEN Las aplicaciones distribuidas requieren t\u00e9cnicas distribuidas para una asignaci\u00f3n eficiente de recursos. Estas t\u00e9cnicas deben tener en cuenta la heterogeneidad y la posible falta de fiabilidad de los recursos y de los consumidores de recursos en entornos distribuidos. En este art\u00edculo proponemos un algoritmo distribuido que resuelve el problema de asignaci\u00f3n de recursos en sistemas distribuidos multiagente. Nuestra soluci\u00f3n se basa en la autoorganizaci\u00f3n de los agentes, que no requiere ning\u00fan facilitador ni capa de gesti\u00f3n. La asignaci\u00f3n de recursos en el sistema es un efecto puramente emergente. Presentamos los resultados del mecanismo de asignaci\u00f3n de recursos propuesto en el entorno multiservidor est\u00e1tico y din\u00e1mico simulado. 1. INTRODUCCI\u00d3N En este sentido, cada agente es un consumidor de recursos que adquiere una determinada cantidad de recursos para la ejecuci\u00f3n de sus tareas. Es dif\u00edcil para un mecanismo central de asignaci\u00f3n de recursos recopilar y gestionar la informaci\u00f3n sobre todos los recursos compartidos y los consumidores de recursos para realizar de manera efectiva la asignaci\u00f3n de recursos. Por lo tanto, se requieren soluciones distribuidas al problema de asignaci\u00f3n de recursos. Los investigadores han reconocido estos requisitos -LSB- 10 -RSB- y han propuesto t\u00e9cnicas para la asignaci\u00f3n distribuida de recursos. Un tipo prometedor de enfoques distribuidos se basa en modelos de mercado econ\u00f3mico -LSB- 4 -RSB-, inspirados en principios de los mercados de valores reales. Incluso si esos enfoques se distribuyen, generalmente requieren un facilitador para fijar precios, descubrir recursos y enviar trabajos a los recursos -LSB- 5, 9 -RSB-. Otro problema mayormente no resuelto de estos enfoques es el ajuste de las restricciones presupuestarias de precio y tiempo para permitir una asignaci\u00f3n eficiente de recursos en sistemas grandes y din\u00e1micos -LSB- 22 -RSB-. En este art\u00edculo proponemos una soluci\u00f3n distribuida al problema de asignaci\u00f3n de recursos basada en la autoorganizaci\u00f3n de los consumidores de recursos en un sistema con recursos limitados. En nuestro enfoque, los agentes asignan tareas din\u00e1micamente a servidores que proporcionan una cantidad limitada de recursos. En nuestro enfoque, los agentes seleccionan de forma aut\u00f3noma la plataforma de ejecuci\u00f3n para la tarea en lugar de pedirle a un intermediario de recursos que haga la asignaci\u00f3n. Todo el control necesario para nuestro algoritmo se distribuye entre los agentes del sistema. Optimizan el proceso de asignaci\u00f3n de recursos continuamente a lo largo de su vida ante cambios en la disponibilidad de recursos compartidos aprendiendo de decisiones de asignaci\u00f3n pasadas. La \u00fanica informaci\u00f3n disponible para todos los agentes es la carga de recursos y la informaci\u00f3n de \u00e9xito de la asignaci\u00f3n de asignaciones de recursos anteriores. No se difunde informaci\u00f3n adicional sobre la carga de recursos sobre los servidores. El mecanismo propuesto no requiere una autoridad de control central, una capa de gesti\u00f3n de recursos ni introducir comunicaci\u00f3n adicional entre agentes para decidir qu\u00e9 tarea se asigna en qu\u00e9 servidor.Demostramos que este mecanismo funciona bien en sistemas din\u00e1micos con una gran cantidad de tareas y se puede adaptar f\u00e1cilmente a varios tama\u00f1os de sistemas. Adem\u00e1s, el rendimiento general del sistema no se ve afectado en caso de que los agentes o servidores fallen o dejen de estar disponibles. El enfoque propuesto proporciona una manera f\u00e1cil de implementar la asignaci\u00f3n distribuida de recursos y tiene en cuenta las tendencias de los sistemas multiagente hacia la autonom\u00eda, la heterogeneidad y la falta de confiabilidad de los recursos y agentes. Esta t\u00e9cnica propuesta puede complementarse f\u00e1cilmente con t\u00e9cnicas para poner en cola o rechazar solicitudes de asignaci\u00f3n de recursos de los agentes -LSB-11-RSB-. Estas capacidades de autogesti\u00f3n de los agentes de software permiten una asignaci\u00f3n confiable de recursos incluso en un entorno con proveedores de recursos poco confiables. Esto se puede lograr mediante las interacciones mutuas entre agentes aplicando t\u00e9cnicas de la teor\u00eda de sistemas complejos. La autoorganizaci\u00f3n de todos los agentes conduce a una autoorganizaci\u00f3n de los 2. TRABAJOS RELACIONADOS La asignaci\u00f3n de recursos es un problema importante en el \u00e1rea de la inform\u00e1tica. En t\u00e9rminos generales, la asignaci\u00f3n de recursos es un mecanismo o pol\u00edtica para la gesti\u00f3n eficiente y efectiva del acceso a un recurso o conjunto de recursos limitado por parte de sus consumidores. En el caso m\u00e1s simple, los consumidores de recursos solicitan a un intermediario o despachador central los recursos disponibles donde se asignar\u00e1 el consumidor de recursos. El corredor suele tener pleno conocimiento de todos los recursos del sistema. En esos enfoques, el consumidor de recursos no puede influir en el proceso de decisi\u00f3n de asignaci\u00f3n. El equilibrio de carga -LSB- 3 -RSB- es un caso especial del problema de asignaci\u00f3n de recursos que utiliza un intermediario que intenta ser justo con todos los recursos equilibrando la carga del sistema por igual entre todos los proveedores de recursos. Este mecanismo funciona mejor en un sistema homog\u00e9neo. Una t\u00e9cnica distribuida simple para la gesti\u00f3n de recursos es la planificaci\u00f3n de la capacidad rechazando o poniendo en cola a los agentes entrantes para evitar la sobrecarga de recursos -LSB- 11 -RSB-. Desde la perspectiva del propietario del recurso, esta t\u00e9cnica es importante para evitar la sobrecarga del recurso, pero no es suficiente para una asignaci\u00f3n eficaz de los recursos. Esta t\u00e9cnica s\u00f3lo puede proporcionar un buen complemento para los mecanismos de asignaci\u00f3n de recursos distribuidos. Estos coordinadores normalmente necesitan tener conocimiento global sobre el estado de todos los recursos del sistema. Un ejemplo de algoritmo de asignaci\u00f3n din\u00e1mica de recursos es el proyecto Cactus -LSB- 1 -RSB- para la asignaci\u00f3n de trabajos computacionales muy costosos. El valor de las soluciones distribuidas para el problema de asignaci\u00f3n de recursos ha sido reconocido por la investigaci\u00f3n -LSB-10-RSB-. Inspir\u00e1ndose en los principios de los mercados de valores, se han desarrollado modelos de mercado econ\u00f3mico para negociar recursos para regular la oferta y la demanda en la red. Los usuarios intentan comprar los recursos baratos necesarios para ejecutar el trabajo, mientras que los proveedores intentan obtener la mayor cantidad de ganancias posible y operar los recursos disponibles a plena capacidad.En Clearwater -LSB- 10 -RSB- se presenta una colecci\u00f3n de diferentes t\u00e9cnicas de asignaci\u00f3n de recursos distribuidos basadas en modelos de mercado. Buyya et al. desarroll\u00f3 un marco de asignaci\u00f3n de recursos basado en la regulaci\u00f3n de la oferta y la demanda -LSB- 4 -RSB- para Nimrod-G -LSB- 6 -RSB- con el enfoque principal en los plazos de trabajo y las restricciones presupuestarias. El modelo de asignaci\u00f3n de recursos basado en agentes -LRB- ARAM -RRB- para grids est\u00e1 dise\u00f1ado para programar trabajos computacionales costosos utilizando agentes. El inconveniente de este modelo es el uso extensivo del intercambio de mensajes entre agentes para el monitoreo peri\u00f3dico y el intercambio de informaci\u00f3n dentro de la estructura jer\u00e1rquica. Las subtareas de un trabajo migran a trav\u00e9s de la red hasta encontrar un recurso que cumpla con las restricciones de precio. El itinerario de migraci\u00f3n de los trabajos est\u00e1 determinado por los recursos para conectarlos en diferentes topolog\u00edas -LSB- 17 -RSB-. El mecanismo propuesto en este documento elimina la necesidad de intercambio peri\u00f3dico de informaci\u00f3n sobre cargas de recursos y no necesita una topolog\u00eda de conexi\u00f3n entre los recursos. En los \u00faltimos a\u00f1os se ha publicado un trabajo considerable sobre t\u00e9cnicas de asignaci\u00f3n descentralizada de recursos utilizando la teor\u00eda de juegos. Es un problema de decisi\u00f3n mal definido que supone y modela el razonamiento inductivo. En este juego de decisiones repetitivas, un n\u00famero impar de agentes tiene que elegir entre dos recursos bas\u00e1ndose en informaci\u00f3n de \u00e9xitos pasados, tratando de ubicarse en el recurso con la minor\u00eda. Galstyan et al. -LSB- 14 -RSB- estudi\u00f3 una variaci\u00f3n con m\u00e1s de dos recursos, cambiando las capacidades de recursos y la informaci\u00f3n de los agentes vecinos. Demostraron que los agentes pueden adaptarse eficazmente a las capacidades cambiantes en este entorno utilizando un conjunto de tablas de b\u00fasqueda simples -LRB- estrategias -RRB- por agente. Otra t\u00e9cnica distribuida que se emplea para resolver el problema de asignaci\u00f3n de recursos se basa en el aprendizaje por refuerzo -LSB- 18 -RSB-. De manera similar a nuestro enfoque, un conjunto de agentes compite por una cantidad limitada de recursos bas\u00e1ndose \u00fanicamente en la experiencia individual previa. En este documento, el objetivo del sistema es maximizar el rendimiento del sistema y al mismo tiempo garantizar la equidad con los recursos, medido como el tiempo promedio de procesamiento por unidad de trabajo. En -LSB-16-RSB- se presenta un enfoque de asignaci\u00f3n de recursos para redes de sensores basado en t\u00e9cnicas de autoorganizaci\u00f3n y aprendizaje por refuerzo, centr\u00e1ndose principalmente en la optimizaci\u00f3n del consumo de energ\u00eda de los nodos de la red. Nosotros, LSB-19-RSB, propusimos un enfoque de equilibrio de carga autoorganizado para un \u00fanico servidor centrado en optimizar los costos de comunicaci\u00f3n de los agentes m\u00f3viles. Un agente m\u00f3vil rechazar\u00e1 una migraci\u00f3n a un servidor de agente remoto si espera que el servidor de destino ya est\u00e9 sobrecargado por otros agentes o tareas del servidor. Los propios agentes toman sus decisiones bas\u00e1ndose en previsiones de utilizaci\u00f3n del servidor. En este art\u00edculo se presenta una soluci\u00f3n para un entorno multiservidor sin considerar los costos de comunicaci\u00f3n o migraci\u00f3n. 6.CONCLUSIONES Y TRABAJO FUTURO En este art\u00edculo se present\u00f3 una t\u00e9cnica de asignaci\u00f3n de recursos distribuida autoorganizada para sistemas multiagente. Permitimos que los agentes seleccionen ellos mismos la plataforma de ejecuci\u00f3n para sus tareas antes de cada ejecuci\u00f3n en tiempo de ejecuci\u00f3n. En nuestro enfoque, los agentes compiten por una asignaci\u00f3n en una de las Figura 5: Los resultados del experimento 2 en un entorno de servidor din\u00e1mico promediaron m\u00e1s de 100 repeticiones. recurso compartido disponible. Los agentes perciben el entorno de su servidor y adoptan sus acciones para competir de manera m\u00e1s eficiente en el nuevo entorno creado. Este proceso es adaptativo y tiene una fuerte retroalimentaci\u00f3n ya que las decisiones de asignaci\u00f3n influyen indirectamente en las decisiones de otros agentes. La asignaci\u00f3n de recursos es un efecto puramente emergente. Nuestro mecanismo demuestra que la asignaci\u00f3n de recursos puede realizarse mediante la competencia efectiva de agentes individuales y aut\u00f3nomos. Tampoco necesitan coordinaci\u00f3n ni informaci\u00f3n de una autoridad superior ni se requiere una comunicaci\u00f3n directa adicional entre agentes. Este mecanismo se inspir\u00f3 en el razonamiento inductivo y los principios de racionalidad limitada que permite a los agentes adaptar sus estrategias para competir eficazmente en un entorno din\u00e1mico. En el caso de que un servidor no est\u00e9 disponible, los agentes pueden adaptarse r\u00e1pidamente a esta nueva situaci\u00f3n explorando nuevos recursos o permanecer en el servidor local si no es posible una asignaci\u00f3n. Especialmente en entornos din\u00e1micos y escalables, como los sistemas grid, se requiere un mecanismo robusto y distribuido para la asignaci\u00f3n de recursos. Nuestro enfoque de asignaci\u00f3n de recursos autoorganizado se evalu\u00f3 con una serie de experimentos de simulaci\u00f3n en un entorno din\u00e1mico de agentes y recursos del servidor. Los resultados presentados para este nuevo enfoque para la optimizaci\u00f3n de la migraci\u00f3n estrat\u00e9gica son muy prometedores y justifican una mayor investigaci\u00f3n en un entorno real de sistema multiagente. Es una pol\u00edtica distribuida, escalable y f\u00e1cil de entender para la regulaci\u00f3n de la oferta y la demanda de recursos. Todo el control se implementa en los agentes. Un mecanismo de decisi\u00f3n simple basado en diferentes creencias del agente crea un comportamiento emergente que conduce a una asignaci\u00f3n efectiva de recursos. Este enfoque puede ampliarse o respaldarse f\u00e1cilmente mediante mecanismos de equilibrio/cola de recursos proporcionados por los recursos. Nuestro enfoque se adapta a los cambios del entorno pero no es evolutivo. No hay descubrimiento de nuevas estrategias por parte de los agentes. investigado en el futuro. En un futuro pr\u00f3ximo, investigaremos si es posible una adaptaci\u00f3n autom\u00e1tica de la tasa de decadencia de la informaci\u00f3n hist\u00f3rica de nuestro algoritmo y si puede mejorar el rendimiento de la asignaci\u00f3n de recursos. Una gran cantidad de recursos compartidos requiere informaci\u00f3n hist\u00f3rica m\u00e1s antigua para evitar una exploraci\u00f3n de recursos con demasiada frecuencia. Por el contrario, un entorno din\u00e1mico con capacidades variables requiere informaci\u00f3n m\u00e1s actualizada para hacer predicciones m\u00e1s confiables.Somos conscientes de la larga fase de aprendizaje en entornos con una gran cantidad de recursos compartidos conocidos por cada agente. En el caso de que los agentes soliciten m\u00e1s recursos que los recursos compartidos proporcionados por todos los servidores, todos los agentes explorar\u00e1n aleatoriamente todos los servidores conocidos. Este proceso de adquirir informaci\u00f3n de carga de recursos sobre todos los servidores puede llevar mucho tiempo en caso de que no se proporcionen suficientes recursos compartidos para todas las tareas. En esta situaci\u00f3n, es dif\u00edcil para un agente recopilar de manera eficiente informaci\u00f3n hist\u00f3rica sobre todos los servidores remotos. Este tema necesita m\u00e1s investigaci\u00f3n en el futuro.", "keyphrases": ["sistema multiagente", "agente", "asignaci\u00f3n de recursos", "algoritmo de distribuci\u00f3n", "tarea de asignaci\u00f3n din\u00e1mica", "red de servidor", "utilidades del servidor", "proceso de adaptaci\u00f3n", "competir", "vaticinador"]}
{"file_name": "J-14", "text": "Calcular buenos equilibrios de Nash en juegos gr\u00e1ficos * RESUMEN Este art\u00edculo aborda el problema de la selecci\u00f3n justa de equilibrios en juegos gr\u00e1ficos. Nuestro enfoque se basa en la estructura de datos denominada pol\u00edtica de mejor respuesta, propuesta por Kearns et al. -LSB- 13 -RSB- como forma de representar todos los equilibrios de Nash de un juego gr\u00e1fico. En -LSB-9-RSB-, se demostr\u00f3 que la pol\u00edtica de mejor respuesta tiene tama\u00f1o polin\u00f3mico siempre que el gr\u00e1fico subyacente sea una ruta. En este art\u00edculo, mostramos que si el gr\u00e1fico subyacente es un \u00e1rbol de grados acotados y la pol\u00edtica de mejor respuesta tiene tama\u00f1o polin\u00f3mico, entonces existe un algoritmo eficiente que construye un equilibrio de Nash que garantiza ciertos pagos a todos los participantes. Otro concepto de soluci\u00f3n atractivo es un equilibrio de Nash que maximiza el bienestar social. Mostramos que, si bien calcular exactamente este \u00faltimo es inviable -LRB-, demostramos que resolver este problema puede involucrar n\u00fameros algebraicos de un grado arbitrariamente alto -RRB-, existe un FPTAS para encontrar dicho equilibrio siempre que se haya implementado la mejor pol\u00edtica de respuesta. Tama\u00f1o del polinomio. Estos dos algoritmos se pueden combinar para producir equilibrios de Nash que satisfacen varios criterios de equidad. 1. INTRODUCCI\u00d3N Esta es la intuici\u00f3n detr\u00e1s de los juegos gr\u00e1ficos, que fueron introducidas por Kearns, Littman y Singh en -LSB- 13 -RSB- como un esquema de representaci\u00f3n compacto para juegos con muchos jugadores. En un juego gr\u00e1fico de n jugadores, cada jugador est\u00e1 asociado con un v\u00e9rtice de un gr\u00e1fico subyacente G, y los pagos de cada jugador dependen de su acci\u00f3n as\u00ed como de las acciones de sus vecinos en el gr\u00e1fico. Si el grado m\u00e1ximo de G es \u0394, y cada jugador tiene dos acciones disponibles, entonces el juego se puede representar usando n2\u0394 +1 n\u00fameros. Por el contrario, necesitamos n2n n\u00fameros para representar un juego general de n jugadores y 2 acciones, lo cual s\u00f3lo es pr\u00e1ctico para valores peque\u00f1os de n. Para juegos gr\u00e1ficos con \u0394 constante, el tama\u00f1o del juego es lineal en n. Uno de los problemas m\u00e1s naturales de un juego gr\u00e1fico es el de encontrar un equilibrio de Nash, cuya existencia se deriva del c\u00e9lebre teorema de Nash -LRB-, ya que los juegos gr\u00e1ficos son s\u00f3lo un caso especial de juegos de n jugadores -RRB-. El primer intento de abordar este problema se realiz\u00f3 en -LSB-13-RSB-, donde los autores consideran juegos gr\u00e1ficos con dos acciones por jugador en los que el gr\u00e1fico subyacente es un \u00e1rbol de grados acotados. Proponen un algoritmo gen\u00e9rico para encontrar equilibrios de Nash que puede especializarse de dos maneras: un algoritmo de tiempo exponencial para encontrar un equilibrio de Nash -LRB- exacto -RRB-, y un esquema de aproximaci\u00f3n de tiempo totalmente polinomial -LRB- FPTAS -RRB- para encontrar una aproximaci\u00f3n al equilibrio de Nash. Para cualquier e > 0, este algoritmo genera un equilibrio e-Nash, que es un perfil de estrategia en el que ning\u00fan jugador puede mejorar su pago en m\u00e1s de e cambiando unilateralmente su estrategia. Si bien los equilibrios de e-Nash suelen ser m\u00e1s f\u00e1ciles de calcular que los equilibrios de Nash exactos, este concepto de soluci\u00f3n tiene varios inconvenientes. Primero,Los jugadores pueden ser sensibles a una peque\u00f1a p\u00e9rdida en los pagos, por lo que el perfil de estrategia que es un equilibrio e-Nash no ser\u00e1 estable. En segundo lugar, los perfiles de estrategia que est\u00e1n cerca de ser equilibrios de Nash pueden ser mucho mejores con respecto a las propiedades bajo consideraci\u00f3n que los equilibrios de Nash exactos. Por lo tanto, la aproximaci\u00f3n -LRB- al valor -RRB- de la mejor soluci\u00f3n que corresponde a un equilibrio de e-Nash puede no ser indicativa de lo que se puede lograr bajo un equilibrio de Nash exacto. Esto es especialmente importante si el prop\u00f3sito de la soluci\u00f3n aproximada es proporcionar un buen punto de referencia para un sistema de agentes ego\u00edstas, ya que el punto de referencia impl\u00edcito en un equilibrio e-Nash puede ser poco realista. Por estas razones, en este art\u00edculo nos centramos en el problema de calcular los equilibrios exactos de Nash. Bas\u00e1ndose en las ideas de -LSB- 14 -RSB-, Elkind et al. -LSB- 9 -RSB- mostr\u00f3 c\u00f3mo encontrar un -LRB- exacto -RRB- equilibrio de Nash en tiempo polinomial cuando el equilibrio de Nash subyacente. Por el contrario, encontrar un equilibrio de Nash en un gr\u00e1fico general acotado en grados parece ser computacionalmente intratable: ha sido mostrado -LRB- ver -LSB- 5, 12, 7 -RSB- -RRB- para estar completo para la clase de complejidad PPAD. -LSB- 9 -RSB- extiende este resultado de dureza al caso en el que el gr\u00e1fico subyacente tiene un ancho de ruta limitado. Un juego gr\u00e1fico puede no tener un equilibrio de Nash \u00fanico; de hecho, puede tener muchos equilibrios exponenciales. Adem\u00e1s, algunos equilibrios de Nash son m\u00e1s deseables que otros. En lugar de tener un algoritmo que simplemente encuentre alg\u00fan equilibrio de Nash, nos gustar\u00eda tener algoritmos para encontrar equilibrios de Nash con varias propiedades socialmente deseables, como maximizar la rentabilidad general o distribuir las ganancias de manera justa. Una propiedad \u00fatil de la estructura de datos de -LSB- 13 -RSB- es que representa simult\u00e1neamente el conjunto de todos los equilibrios de Nash del juego subyacente. Si esta representaci\u00f3n tiene tama\u00f1o polin\u00f3mico -LRB- como es el caso de los caminos, como se muestra en -LSB- 9 -RSB- -RRB-, se puede esperar extraer de ella un equilibrio de Nash con las propiedades deseadas. De hecho, en -LSB- 13 -RSB- los autores mencionan que esto s\u00ed es posible si uno est\u00e1 interesado en encontrar un equilibrio -LRB- aproximado -RRB- a-Nash. El objetivo de este art\u00edculo es extender esto a los equilibrios exactos de Nash. 1.1 Nuestros resultados En este art\u00edculo, estudiamos juegos gr\u00e1ficos de 2 acciones para n jugadores en \u00e1rboles de grados acotados para los cuales la estructura de datos de -LSB- 13 -RSB- tiene un tama\u00f1o poli -LRB- n -RRB-. Nos centramos en el problema de encontrar equilibrios de Nash exactos con ciertas propiedades socialmente deseables. En particular, mostramos c\u00f3mo encontrar un equilibrio de Nash que -LRB- casi -RRB- maximice el bienestar social, es decir, la suma de los pagos de los jugadores, y mostramos c\u00f3mo encontrar un equilibrio de Nash que -LRB- casi -RRB - Satisface los l\u00edmites de pago prescritos para todos los jugadores. Los juegos gr\u00e1ficos sobre \u00e1rboles de grados acotados tienen una estructura algebraica simple. Una caracter\u00edstica atractiva, que se desprende de -LSB- 13 -RSB-,es que cada juego de este tipo tiene un equilibrio de Nash en el que la estrategia de cada jugador es un n\u00famero racional. La secci\u00f3n 3 estudia la estructura algebraica de aquellos equilibrios de Nash que maximizan el bienestar social. Mostramos -LRB- Teoremas 1 y 2 -RRB- que, sorprendentemente, el conjunto de equilibrios de Nash que maximizan el bienestar social es m\u00e1s complejo. Parece ser una caracter\u00edstica novedosa del escenario que consideramos aqu\u00ed, que un equilibrio de Nash \u00f3ptimo es dif\u00edcil de representar, en una situaci\u00f3n en la que es f\u00e1cil encontrar y representar un equilibrio de Nash. Como el equilibrio de Nash que maximiza el bienestar social puede ser dif\u00edcil de representar eficientemente, tenemos que conformarnos con una aproximaci\u00f3n. Sin embargo, la diferencia crucial entre nuestro enfoque y el de art\u00edculos anteriores -LSB- 13, 16, 19 -RSB- es que requerimos que nuestro algoritmo genere un equilibrio de Nash exacto, aunque no necesariamente el \u00f3ptimo con respecto a nuestros criterios. En la Secci\u00f3n 4, describimos un algoritmo que satisface este requisito. Es decir, proponemos un algoritmo que para cualquier e > 0 encuentra un equilibrio de Nash cuyo pago total est\u00e1 dentro del \u00f3ptimo. Datta -LSB- 8 -RSB- obtuvo un resultado m\u00e1s relacionado con pre1A en un context diferente, quien muestra que los juegos de n jugadores y 2 acciones son universales en el sentido de que cualquier variedad algebraica real puede representarse como el conjunto de Nash totalmente mixto. equilibrios de tales juegos. Mostramos -LRB- Secci\u00f3n 4.1 -RRB- que bajo algunas restricciones en las matrices de pagos, el algoritmo se puede transformar en un algoritmo de tiempo polinomial -LRB- verdaderamente -RRB- que genera un equilibrio de Nash cuyo pago total est\u00e1 dentro de un 1 \u2212 e factor del \u00f3ptimo. En la secci\u00f3n 5, consideramos el problema de encontrar un equilibrio de Nash en el que el pago esperado de cada jugador Vi exceda un umbral prescrito Ti. Usando la idea de la Secci\u00f3n 4 damos -LRB- Teorema 5 -RRB- un esquema de aproximaci\u00f3n temporal totalmente polinomial para este problema. El tiempo de ejecuci\u00f3n del algoritmo est\u00e1 limitado por un polinomio en n, Pmax y E. Si la instancia tiene un equilibrio de Nash que satisface los umbrales prescritos, entonces el algoritmo construye un equilibrio de Nash en el que el pago esperado de cada jugador Vi es al menos Ti. \u2212 E. En la Secci\u00f3n 6, introducimos otros criterios naturales para seleccionar un equilibrio de Nash \"bueno\" y mostramos que los algoritmos descritos en las dos secciones anteriores pueden usarse como bloques de construcci\u00f3n para encontrar equilibrios de Nash que satisfagan estos criterios. En particular, en la secci\u00f3n 6.1 mostramos c\u00f3mo encontrar un equilibrio de Nash que se aproxime al bienestar social m\u00e1ximo, garantizando al mismo tiempo que cada pago individual est\u00e9 cerca de un umbral prescrito. En la secci\u00f3n 6.2 mostramos c\u00f3mo encontrar un equilibrio de Nash que -LRB- casi -RRB- maximice el pago individual m\u00ednimo. Finalmente, en la secci\u00f3n 6.3 mostramos c\u00f3mo encontrar un equilibrio de Nash en el que los pagos individuales de los jugadores sean cercanos entre s\u00ed. 1.2 Trabajo relacionado Nuestro esquema de aproximaci\u00f3n -LRB- Teorema 3 y Teorema 4 -RRB- muestra un contraste entre los juegos que estudiamos y los juegos de n-acci\u00f3n para dos jugadores, para los cuales los problemas correspondientes suelen ser intratables. Para juegos de n acciones de dos jugadores, el problema de encontrar equilibrios de Nash con propiedades especiales suele ser NP-dif\u00edcil. En particular, este es el caso de los equilibrios de Nash que maximizan el bienestar social -LSB- 11, 6 -RSB-. Adem\u00e1s, es probable que sea dif\u00edcil incluso aproximarse a tales equilibrios. En particular, Chen, Deng y Teng -LSB- 4 -RSB- muestran que existe alg\u00fan e, polinomio inverso en n, para el cual calcular un equilibrio e-Nash en juegos de 2 jugadores con n acciones por jugador es PPAD-completo. Lipton y Markakis -LSB- 15 -RSB- estudian las propiedades algebraicas de los equilibrios de Nash y se\u00f1alan que se pueden utilizar algoritmos est\u00e1ndar de eliminaci\u00f3n de cuantificadores para resolverlos. Tenga en cuenta que estos algoritmos no son de tiempo polinomial en general. Los juegos que estudiamos en este art\u00edculo tienen equilibrios de Nash computables en tiempo polinomial en los que todas las estrategias mixtas son n\u00fameros racionales, pero un equilibrio de Nash \u00f3ptimo puede necesariamente incluir estrategias mixtas con un alto grado algebraico. Cualquier equilibrio de Nash es un CE, pero lo contrario no se cumple en general. A diferencia de los equilibrios de Nash, se pueden encontrar equilibrios correlacionados para juegos gr\u00e1ficos de bajo grado -LRB- as\u00ed como para otras clases de juegos multijugador representados de forma concisa -RRB- en tiempo polin\u00f3mico -LSB- 17 -RSB-. Pero, para los juegos gr\u00e1ficos es NP-dif\u00edcil encontrar un equilibrio correlacionado que maximice el pago total -LSB- 18 -RSB-. Sin embargo, los resultados de dureza NP se aplican a juegos m\u00e1s generales que el que consideramos aqu\u00ed; en particular, los gr\u00e1ficos no son \u00e1rboles. A partir de -LSB- 2 -RSB- tambi\u00e9n se sabe que existen juegos de 2 jugadores y 2 acciones para los cuales el pago total esperado del mejor equilibrio correlacionado es mayor que el mejor equilibrio de Nash, y analizamos este tema con m\u00e1s detalle en la Secci\u00f3n 7. 7. CONCLUSIONES Hemos estudiado el problema de la selecci\u00f3n del equilibrio en juegos gr\u00e1ficos sobre \u00e1rboles de grados acotados. Consideramos varios criterios para seleccionar un equilibrio de Nash, como maximizar el bienestar social, asegurar un l\u00edmite inferior en el pago esperado de cada jugador, etc. Primero, nos concentramos en la complejidad algebraica de un equilibrio de Nash que maximiza el bienestar social, y demostr\u00f3 fuertes resultados negativos para ese problema. Es decir, demostramos que incluso para juegos gr\u00e1ficos sobre trayectorias, cualquier n\u00famero algebraico \u03b1 E -LSB- 0, 1 -RSB- puede ser la \u00fanica estrategia disponible para alg\u00fan jugador en todos los equilibrios de Nash que maximizan el bienestar social. Esto contrasta marcadamente con el hecho de que los juegos gr\u00e1ficos sobre \u00e1rboles siempre poseen un equilibrio de Nash en el que las estrategias de todos los jugadores son n\u00fameros racionales. Luego proporcionamos algoritmos de aproximaci\u00f3n para seleccionar equilibrios de Nash con propiedades especiales. Si bien el problema de encontrar equilibrios de Nash aproximados para varias clases de juegos ha recibido mucha atenci\u00f3n en los \u00faltimos a\u00f1os,la mayor parte del trabajo existente tiene como objetivo encontrar equilibrios E-Nash que satisfagan -LRB- o que est\u00e9n E-cerca de satisfacer -RRB- ciertas propiedades. Nuestro enfoque es diferente en el sentido de que insistimos en generar un equilibrio de Nash exacto, que est\u00e9 E-cerca de satisfacer un requisito dado. Como se argument\u00f3 en la introducci\u00f3n, hay varias razones para preferir una soluci\u00f3n que constituya un equilibrio de Nash exacto. Si bien probamos nuestros resultados para juegos en un camino, se pueden generalizar a cualquier \u00e1rbol para el cual las pol\u00edticas de mejor respuesta tengan representaciones compactas como uniones de rect\u00e1ngulos. En la versi\u00f3n completa del art\u00edculo describimos nuestros algoritmos para el caso general. Un trabajo adicional en este sentido podr\u00eda incluir extensiones a los tipos de garant\u00edas buscadas para los equilibrios de Nash, como garantizar pagos totales para subconjuntos de jugadores, seleccionar equilibrios en los que algunos jugadores reciban pagos significativamente m\u00e1s altos que sus pares, etc. Sin embargo, por el momento , quiz\u00e1s sea m\u00e1s importante investigar si los equilibrios de Nash de los juegos gr\u00e1ficos pueden calcularse de manera descentralizada, en contraste con los algoritmos que hemos introducido aqu\u00ed. Es natural preguntarse si nuestros resultados o los de -LSB- 9 -RSB- pueden generalizarse a juegos de tres o m\u00e1s acciones. Sin embargo, parece que esto dificultar\u00e1 significativamente el an\u00e1lisis. En particular, cabe se\u00f1alar que se pueden ver los juegos con pagos acotados como un caso especial muy limitado de juegos con tres acciones por jugador. Es decir, dado un juego de dos acciones con l\u00edmites de pagos, considere un juego en el que cada jugador Vi tiene una tercera acci\u00f3n que le garantiza un pago de Ti sin importar lo que hagan los dem\u00e1s. Entonces, comprobar si existe un equilibrio de Nash en el que ninguno de los jugadores asigna una probabilidad distinta de cero a su tercera acci\u00f3n es equivalente a comprobar si existe un equilibrio de Nash que satisfaga los l\u00edmites de pago en el juego original, y la secci\u00f3n 5.1 muestra que encontrar un equilibrio de Nash exacto La soluci\u00f3n a este problema requiere nuevas ideas. Alternativamente, puede ser interesante buscar resultados similares en el context de equilibrios correlacionados -LRB- CE -RRB-, especialmente porque el mejor CE puede tener un valor mayor -LRB- de pago total esperado -RRB- que el mejor NE. Se sabe por -LSB- 1 -RSB- que el valor de mediaci\u00f3n de los juegos de 2 jugadores, 2 acciones con pagos no negativos es como m\u00e1ximo 43, y exhiben un juego de 3 jugadores para el cual es infinito.En la versi\u00f3n completa del art\u00edculo describimos nuestros algoritmos para el caso general. Un trabajo adicional en este sentido podr\u00eda incluir extensiones a los tipos de garant\u00edas buscadas para los equilibrios de Nash, como garantizar pagos totales para subconjuntos de jugadores, seleccionar equilibrios en los que algunos jugadores reciban pagos significativamente m\u00e1s altos que sus pares, etc. Sin embargo, por el momento , quiz\u00e1s sea m\u00e1s importante investigar si los equilibrios de Nash de los juegos gr\u00e1ficos pueden calcularse de manera descentralizada, en contraste con los algoritmos que hemos introducido aqu\u00ed. Es natural preguntarse si nuestros resultados o los de -LSB- 9 -RSB- pueden generalizarse a juegos de tres o m\u00e1s acciones. Sin embargo, parece que esto dificultar\u00e1 significativamente el an\u00e1lisis. En particular, cabe se\u00f1alar que se pueden ver los juegos con pagos acotados como un caso especial muy limitado de juegos con tres acciones por jugador. Es decir, dado un juego de dos acciones con l\u00edmites de pagos, considere un juego en el que cada jugador Vi tiene una tercera acci\u00f3n que le garantiza un pago de Ti sin importar lo que hagan los dem\u00e1s. Entonces, comprobar si existe un equilibrio de Nash en el que ninguno de los jugadores asigna una probabilidad distinta de cero a su tercera acci\u00f3n es equivalente a comprobar si existe un equilibrio de Nash que satisfaga los l\u00edmites de pago en el juego original, y la secci\u00f3n 5.1 muestra que encontrar un equilibrio de Nash exacto La soluci\u00f3n a este problema requiere nuevas ideas. Alternativamente, puede ser interesante buscar resultados similares en el context de equilibrios correlacionados -LRB- CE -RRB-, especialmente porque el mejor CE puede tener un valor mayor -LRB- de pago total esperado -RRB- que el mejor NE. Se sabe por -LSB- 1 -RSB- que el valor de mediaci\u00f3n de los juegos de 2 jugadores, 2 acciones con pagos no negativos es como m\u00e1ximo 43, y exhiben un juego de 3 jugadores para el cual es infinito.En la versi\u00f3n completa del art\u00edculo describimos nuestros algoritmos para el caso general. Un trabajo adicional en este sentido podr\u00eda incluir extensiones a los tipos de garant\u00edas buscadas para los equilibrios de Nash, como garantizar pagos totales para subconjuntos de jugadores, seleccionar equilibrios en los que algunos jugadores reciban pagos significativamente m\u00e1s altos que sus pares, etc. Sin embargo, por el momento , quiz\u00e1s sea m\u00e1s importante investigar si los equilibrios de Nash de los juegos gr\u00e1ficos pueden calcularse de manera descentralizada, en contraste con los algoritmos que hemos introducido aqu\u00ed. Es natural preguntarse si nuestros resultados o los de -LSB- 9 -RSB- pueden generalizarse a juegos de tres o m\u00e1s acciones. Sin embargo, parece que esto dificultar\u00e1 significativamente el an\u00e1lisis. En particular, cabe se\u00f1alar que se pueden ver los juegos con pagos acotados como un caso especial muy limitado de juegos con tres acciones por jugador. Es decir, dado un juego de dos acciones con l\u00edmites de pagos, considere un juego en el que cada jugador Vi tiene una tercera acci\u00f3n que le garantiza un pago de Ti sin importar lo que hagan los dem\u00e1s. Entonces, comprobar si existe un equilibrio de Nash en el que ninguno de los jugadores asigna una probabilidad distinta de cero a su tercera acci\u00f3n es equivalente a comprobar si existe un equilibrio de Nash que satisfaga los l\u00edmites de pago en el juego original, y la secci\u00f3n 5.1 muestra que encontrar un equilibrio de Nash exacto La soluci\u00f3n a este problema requiere nuevas ideas. Alternativamente, puede ser interesante buscar resultados similares en el context de equilibrios correlacionados -LRB- CE -RRB-, especialmente porque el mejor CE puede tener un valor mayor -LRB- de pago total esperado -RRB- que el mejor NE. Se sabe por -LSB- 1 -RSB- que el valor de mediaci\u00f3n de los juegos de 2 jugadores, 2 acciones con pagos no negativos es como m\u00e1ximo 43, y exhiben un juego de 3 jugadores para el cual es infinito.Entonces, comprobar si existe un equilibrio de Nash en el que ninguno de los jugadores asigna una probabilidad distinta de cero a su tercera acci\u00f3n es equivalente a comprobar si existe un equilibrio de Nash que satisfaga los l\u00edmites de pago en el juego original, y la secci\u00f3n 5.1 muestra que encontrar un equilibrio de Nash exacto La soluci\u00f3n a este problema requiere nuevas ideas. Alternativamente, puede ser interesante buscar resultados similares en el context de equilibrios correlacionados -LRB- CE -RRB-, especialmente porque el mejor CE puede tener un valor mayor -LRB- de pago total esperado -RRB- que el mejor NE. Se sabe por -LSB- 1 -RSB- que el valor de mediaci\u00f3n de los juegos de 2 jugadores, 2 acciones con pagos no negativos es como m\u00e1ximo 43, y exhiben un juego de 3 jugadores para el cual es infinito.Entonces, comprobar si existe un equilibrio de Nash en el que ninguno de los jugadores asigna una probabilidad distinta de cero a su tercera acci\u00f3n es equivalente a comprobar si existe un equilibrio de Nash que satisfaga los l\u00edmites de pago en el juego original, y la secci\u00f3n 5.1 muestra que encontrar un equilibrio de Nash exacto La soluci\u00f3n a este problema requiere nuevas ideas. Alternativamente, puede ser interesante buscar resultados similares en el context de equilibrios correlacionados -LRB- CE -RRB-, especialmente porque el mejor CE puede tener un valor mayor -LRB- de pago total esperado -RRB- que el mejor NE. Se sabe por -LSB- 1 -RSB- que el valor de mediaci\u00f3n de los juegos de 2 jugadores, 2 acciones con pagos no negativos es como m\u00e1ximo 43, y exhiben un juego de 3 jugadores para el cual es infinito.", "keyphrases": ["juego grafico", "equilibrio de Nash", "esquema aproximado", "algoritmo de tiempo exponencial", "aproximado", "varias propiedades socialmente deseadas", "recompensa general", "distribuir ganancias", "bienestar social", "juego gr\u00e1fico de pago integral g", "cortar el inconveniente", "perfil estrat\u00e9gico", "gr\u00e1fico de grados"]}
{"file_name": "I-22", "text": "Modelado de carga cognitiva realista para mejorar los modelos mentales compartidos en la colaboraci\u00f3n entre humanos y agentes RESUMEN Los miembros del equipo humano a menudo desarrollan expectativas compartidas para predecir las necesidades de los dem\u00e1s y coordinar sus comportamientos. En este art\u00edculo se propone el concepto ``Mapa de Creencias Compartidas'' como base para desarrollar expectativas compartidas realistas entre un equipo de Pares Humano-Agente -LRB-HAPs-RRB-. El establecimiento de mapas de creencias compartidas se basa en el intercambio de informaci\u00f3n entre agentes, cuya eficacia depende en gran medida de las cargas de procesamiento de los agentes y de las cargas cognitivas instant\u00e1neas de sus socios humanos. Investigamos modelos de carga cognitiva basados \u200b\u200ben HMM para facilitar que los miembros del equipo \"compartan la informaci\u00f3n correcta con la parte adecuada en el momento adecuado\". El concepto de mapa de creencias compartidas y los modelos de carga cognitiva/de procesamiento se han implementado en una arquitectura de agente cognitivo: SMMall. Se llevaron a cabo una serie de experimentos para evaluar el concepto, los modelos y sus impactos en la evoluci\u00f3n de modelos mentales compartidos de los equipos HAP. 1. INTRODUCCI\u00d3N El trabajo en equipo multiagente centrado en el ser humano ha atra\u00eddo cada vez m\u00e1s atenci\u00f3n en el campo de los sistemas multiagente -LSB- 2, 10, 4 -RSB-. Humanos y aut\u00f3nomos En resumen, los humanos y los agentes pueden unirse para lograr un mejor desempe\u00f1o, dado que podr\u00edan establecer cierta conciencia mutua para coordinar sus actividades de iniciativa mixta. Sin embargo, la base de la colaboraci\u00f3n entre humanos y agentes sigue siendo cuestionada debido a modelos poco realistas de conciencia mutua del estado de las cosas. En particular, pocos investigadores van m\u00e1s all\u00e1 para evaluar los principios del modelado de construcciones mentales compartidas entre un ser humano y su agente asistente. Adem\u00e1s, las relaciones entre humanos y agentes pueden ir m\u00e1s all\u00e1 de los socios y convertirse en equipos. Por lo tanto, existe una clara demanda de investigaciones que ampl\u00eden y profundicen nuestra comprensi\u00f3n de los principios del modelado mental compartido entre miembros de un equipo mixto humano-agente. Existen l\u00edneas de investigaci\u00f3n sobre el trabajo en equipo multiagente, tanto de forma te\u00f3rica como emp\u00edrica. Por ejemplo, Joint Intention -LSB- 3 -RSB- y SharedPlans -LSB- 5 -RSB- son dos marcos te\u00f3ricos para especificar colaboraciones de agentes. Uno de los inconvenientes es que, aunque ambos tienen una profunda ra\u00edz filos\u00f3fica y cognitiva, no se adaptan al modelado de los miembros del equipo humano. Los estudios cognitivos sugirieron que se espera que los equipos que han compartido modelos mentales tengan expectativas comunes de la tarea y del equipo, lo que les permite predecir el comportamiento y las necesidades de recursos de los miembros del equipo con mayor precisi\u00f3n -LSB- 14, 6 -RSB-. Cannon-Bowers et al. -LSB- 14 -RSB- argumentan expl\u00edcitamente que los miembros del equipo deben tener modelos compatibles que conduzcan a \"expectativas\" comunes. Estamos de acuerdo en esto y creemos que el establecimiento de expectativas compartidas entre los miembros del equipo humano y de agentes es un paso cr\u00edtico para avanzar en la investigaci\u00f3n del trabajo en equipo centrado en el ser humano.Cabe se\u00f1alar que el concepto de expectativa compartida puede incluir en t\u00e9rminos generales la asignaci\u00f3n de roles y su din\u00e1mica, esquemas y progresos del trabajo en equipo, patrones e intenciones de comunicaci\u00f3n, etc. Si bien el objetivo a largo plazo de nuestra investigaci\u00f3n es comprender c\u00f3mo las estructuras cognitivas compartidas pueden 6. CONCLUSI\u00d3N La atenci\u00f3n de la investigaci\u00f3n reciente sobre el trabajo en equipo centrado en el ser humano exige en gran medida el dise\u00f1o de sistemas de agentes como ayudas cognitivas que puedan modelar y explotar a los socios humanos. capacidades cognitivas para ofrecer ayuda de forma discreta. En este art\u00edculo, investigamos varios factores que rodean el desafiante problema de la evoluci\u00f3n de modelos mentales compartidos de equipos compuestos por pares humano-agente. La principal contribuci\u00f3n de esta investigaci\u00f3n incluye -LRB-1-RRB- Se propusieron modelos de carga basados \u200b\u200ben HMM para que un agente estime la carga cognitiva de su compa\u00f1ero humano y las cargas de procesamiento de otros compa\u00f1eros de equipo HAP; -LRB- 2 -RRB- Se introdujo e implement\u00f3 el concepto de mapa de creencias compartidas. Permite a los miembros del grupo representar y razonar eficazmente sobre modelos mentales compartidos; -LRB- 3 -RRB- Se realizaron experimentos para evaluar los modelos de carga cognitiva/de procesamiento basados \u200b\u200ben HMM y los impactos de la comunicaci\u00f3n multipartita en la evoluci\u00f3n de los SMM de equipo. Durante los experimentos tambi\u00e9n se demostr\u00f3 la utilidad de los mapas de creencias compartidas.", "keyphrases": ["compartir mapa de creencias", "trabajo en equipo multiag", "heurista", "raz\u00f3n", "resoluci\u00f3n de problemas", "colaborar", "trabajo en equipo", "esperar", "esquema de trabajo en equipo", "equipo humano-agente realiza", "teor\u00eda de la carga cognitiva", "actuaci\u00f3n humana", "asignaci\u00f3n de recursos", "tarea realizada", "compartir informaci\u00f3n", "comuna multipartidista"]}
{"file_name": "J-23", "text": "Ratios de frugalidad y mecanismos veraces mejorados para la cobertura de Vertex * En las subastas del sistema establecido, hay varios equipos de agentes superpuestos y una tarea que puede ser completada por cualquiera de estos equipos. El objetivo del subastador es contratar un equipo y pagar lo menos posible. Ejemplos de esta configuraci\u00f3n incluyen subastas de ruta m\u00e1s corta y subastas de cobertura de v\u00e9rtices. Recientemente, Karlin, Kempe y Tamir introdujeron una nueva definici\u00f3n de ratio de frugalidad para este problema. Informalmente, el `` ratio de frugalidad '' es el ratio entre el pago total de un mecanismo y un l\u00edmite de pago deseado. La relaci\u00f3n captura el grado en que el mecanismo paga de m\u00e1s, en relaci\u00f3n con el costo justo percibido en una subasta veraz. En este art\u00edculo, proponemos una nueva subasta veraz en tiempo polinomial para el problema de cobertura de v\u00e9rtices y limitamos su \u00edndice de frugalidad. Mostramos que la calidad de la soluci\u00f3n tiene un factor constante \u00f3ptimo y el \u00edndice de frugalidad est\u00e1 dentro de un factor constante del mejor l\u00edmite posible en el peor de los casos; Esta es la primera subasta de este problema que tiene estas propiedades. Adem\u00e1s, mostramos c\u00f3mo transformar cualquier subasta veraz en una frugal preservando al mismo tiempo el ratio de aproximaci\u00f3n. Adem\u00e1s, consideramos dos modificaciones naturales de la definici\u00f3n de Karlin et al., y analizamos las propiedades de los l\u00edmites de pago resultantes, como la monotonicidad, la dureza computacional y la robustez con respecto a la regla de resoluci\u00f3n de sorteo. Estudiamos las relaciones entre los diferentes l\u00edmites de pago, tanto para sistemas de conjuntos generales como para subastas de sistemas de conjuntos espec\u00edficos, como las subastas de ruta y las subastas de cobertura de v\u00e9rtices. Usamos estas nuevas definiciones en la prueba de nuestro resultado principal para subastas de cobertura de v\u00e9rtices mediante una t\u00e9cnica de arranque, que puede ser de inter\u00e9s independiente. 1. INTRODUCCI\u00d3N En una subasta de sistema fijo hay un \u00fanico comprador y muchos vendedores que pueden prestar diversos servicios. Se supone que los requisitos del comprador pueden ser satisfechos por varios subconjuntos de proveedores; estos subconjuntos se denominan conjuntos factibles. Suponemos que cada vendedor tiene un costo por brindar sus servicios, pero posiblemente presenta una oferta mayor al subastador. Con base en estas ofertas, el subastador selecciona un subconjunto factible de proveedores y realiza pagos a los proveedores de este subconjunto. Cada proveedor seleccionado disfruta de una ganancia del pago menos el costo. Los vendedores quieren maximizar las ganancias, mientras que el comprador quiere minimizar la cantidad que paga. Un objetivo natural en este context es dise\u00f1ar una subasta veraz, en la que los vendedores tengan un incentivo para ofertar su costo real. Esto se puede lograr pagando a cada proveedor seleccionado una prima por encima de su oferta, de tal manera que el proveedor no tenga incentivos para sobrepujar. Una cuesti\u00f3n interesante en el dise\u00f1o de mecanismos es cu\u00e1nto tendr\u00e1 que pagar de m\u00e1s el subastador para garantizar ofertas veraces. En el context de las subastas de rutas, este tema fue abordado por primera vez por Archer y Tardos -LSB- 1 -RSB-.Definen el ratio de frugalidad de un mecanismo como el ratio entre su pago total y el coste del camino m\u00e1s barato separado del camino seleccionado por el mecanismo. Muestran que, para una gran clase de mecanismos veraces para este problema, la relaci\u00f3n de frugalidad es tan grande como el n\u00famero de aristas en el camino m\u00e1s corto. Talwar -LSB- 21 -RSB- extiende esta definici\u00f3n de \u00edndice de frugalidad a sistemas de conjuntos generales y estudia el \u00edndice de frugalidad del mecanismo VCG cl\u00e1sico -LSB- 22, 4, 14 -RSB- para muchos sistemas de conjuntos espec\u00edficos, como la expansi\u00f3n m\u00ednima. Arboles y cobertores. Si bien la definici\u00f3n de ratio de frugalidad propuesta por -LSB- 1 -RSB- est\u00e1 bien motivada y ha sido fundamental para estudiar mecanismos veraces para sistemas de conjuntos, no es completamente satisfactoria. Considere, por ejemplo, el gr\u00e1fico de la Figura 1 con los costos CAB = CBC = Figura 1: El gr\u00e1fico de diamantes Este gr\u00e1fico tiene dos conexiones y el pago VCG al camino ganador ABCD est\u00e1 acotado. Sin embargo, el gr\u00e1fico no contiene ning\u00fan camino A - D que sea separado de ABCD y, por lo tanto, la relaci\u00f3n de frugalidad de VCG en este gr\u00e1fico permanece indefinida. Al mismo tiempo, no existe un monopolio, es decir, no existe un proveedor que aparezca en todos los conjuntos factibles. Para abordar este problema, Karlin et al. -LSB- 16 -RSB- sugieren un mejor punto de referencia, que se define para cualquier sistema de conjuntos libre de monopolios. Con base en esta nueva definici\u00f3n, los autores construyen nuevos mecanismos para el problema del camino m\u00e1s corto y muestran que el sobrepago de estos mecanismos est\u00e1 dentro de un factor \u00f3ptimo constante. 1.1 Nuestros resultados Subastas de cobertura de v\u00e9rtices Proponemos una subasta veraz en tiempo polinomial para la cobertura de v\u00e9rtices que genera una soluci\u00f3n cuyo costo est\u00e1 dentro de un factor de 2 del \u00f3ptimo, y cuyo \u00edndice de frugalidad es como m\u00e1ximo 2\u0394, donde \u0394 es el grado m\u00e1ximo del gr\u00e1fico -LRB- Teorema 4 -RRB-. Complementamos este resultado demostrando -LRB- Teorema 5 -RRB- que para cualquier \u0394 y n, existen gr\u00e1ficas de grado m\u00e1ximo \u0394 y tama\u00f1o \u0398 -LRB- n -RRB- para las cuales cualquier mecanismo veraz tiene una relaci\u00f3n de frugalidad al menos \u0394 / 2. Esto significa que la calidad de la soluci\u00f3n de nuestra subasta es \u00f3ptima con un factor de 2 y el \u00edndice de frugalidad est\u00e1 dentro de un factor de 4 del mejor l\u00edmite posible para las entradas del peor caso. Hasta donde sabemos, esta es la primera subasta de este problema que disfruta de estas propiedades. Adem\u00e1s, mostramos c\u00f3mo transformar cualquier mecanismo veraz para el problema de cobertura de v\u00e9rtices en uno frugal preservando al mismo tiempo la relaci\u00f3n de aproximaci\u00f3n. Razones de frugalidad Nuestros resultados de cobertura de v\u00e9rtices naturalmente sugieren dos modificaciones de la definici\u00f3n de \u03bd en -LSB- 16 -RSB-. Estas modificaciones se pueden realizar de forma independiente entre s\u00ed, lo que da como resultado cuatro l\u00edmites de pago diferentes TUmax, TUmin, NTUmax y NTUmin, donde NTUmin es igual al l\u00edmite de pago original \u03bd de en -LSB- 16 -RSB-. Si bien nuestro principal resultado sobre las subastas de cobertura de v\u00e9rtices -LRB- Teorema 4 -RRB- es con respecto a NTUmin = \u03bd, hacemos uso de las nuevas definiciones comparando primero el pago de nuestro mecanismo con un NTUmax l\u00edmite m\u00e1s d\u00e9bil,y luego arrancando desde este resultado para obtener el l\u00edmite deseado. Inspir\u00e1ndonos en esta aplicaci\u00f3n, nos embarcamos en un estudio m\u00e1s detallado de estos l\u00edmites de pago. Nuestros resultados aqu\u00ed son los siguientes: 1. Observamos -LRB- Proposici\u00f3n 1 -RRB- que los cuatro l\u00edmites de pago siempre obedecen a un orden particular que es independiente de la elecci\u00f3n del sistema establecido y del vector de costos, es decir, TUmin < NTUmin < NTUm\u00e1x < TUm\u00e1x. Proporcionamos ejemplos -LRB- Proposici\u00f3n 5 y Corolarios 1 y 2 -RRB- que muestran que para el problema de cobertura de v\u00e9rtices, dos l\u00edmites consecutivos cualesquiera pueden diferir en un factor de n \u2212 2, donde n es el n\u00famero de agentes. Luego mostramos -LRB- Teorema 2 -RRB- que esta separaci\u00f3n es casi mejor posible para sistemas de conjuntos generales demostrando que para cualquier sistema de conjuntos TUmax/TUmin < n. Por el contrario, demostramos -LRB- Teorema 3 -RRB- que para subastas de ruta TUmax/TUmin < 2. Proporcionamos ejemplos -LRB- Proposiciones 2, 3 y 4 -RRB- que muestran que este l\u00edmite es ajustado. Vemos esto como un argumento para el estudio de las subastas de cobertura de v\u00e9rtices, ya que parecen ser m\u00e1s representativas del problema general de selecci\u00f3n del equipo que las subastas de ruta ampliamente estudiadas. 2. Esta observaci\u00f3n sugiere que los cuatro l\u00edmites de pago deber\u00edan estudiarse en un marco unificado; adem\u00e1s, nos lleva a creer que la t\u00e9cnica de arranque del Teorema 4 puede tener otras aplicaciones. 3. Evaluamos los l\u00edmites de pago introducidos aqu\u00ed con respecto a una lista de verificaci\u00f3n de caracter\u00edsticas deseables. Esto puede verse como un argumento a favor del uso de l\u00edmites NTUmax y TUmax m\u00e1s d\u00e9biles pero computables de manera eficiente. Trabajo relacionado Las subastas Vertex-cover han sido estudiadas en el pasado por Talwar -LSB- 21 -RSB- y Calinescu -LSB- 5 -RSB-. Ambos art\u00edculos se basan en la definici\u00f3n de \u00edndice de frugalidad utilizada en -LSB- 1 -RSB-; Como se mencion\u00f3 anteriormente, esto significa que sus resultados solo se aplican a gr\u00e1ficos bipartitos. Talwar -LSB- 21 -RSB- muestra que el ratio de frugalidad de VCG es como m\u00e1ximo \u0394. Sin embargo, dado que encontrar la cobertura de v\u00e9rtices m\u00e1s barata es un problema NP-dif\u00edcil, el mecanismo VCG es computacionalmente inviable. El primer art\u00edculo -LRB- y, hasta donde sabemos, el \u00fanico -RRB- para investigar mecanismos veraces en tiempo polinomial para la cobertura de v\u00e9rtices es -LSB- 5 -RSB-. Este art\u00edculo estudia una subasta que se basa en el algoritmo de asignaci\u00f3n codiciosa, que tiene una relaci\u00f3n de aproximaci\u00f3n de log n. Si bien el enfoque principal de -LSB- 5 -RSB- es el problema m\u00e1s general de cobertura de conjuntos, los resultados de -LSB- 5 -RSB- implican una relaci\u00f3n de frugalidad de 2\u03942 para la cobertura de v\u00e9rtices. 2. PRELIMINARES En la mayor parte de este art\u00edculo, analizamos las subastas para sistemas de conjuntos. En las subastas del sistema de conjuntos, cada elemento e del conjunto de terreno es propiedad de un agente independiente y tiene asociado un coste ce no negativo. El objetivo del centro es seleccionar -LRB- comprar -RRB- un conjunto factible. Cada elemento e del conjunto seleccionado tiene un coste de ce. Los elementos que no sean seleccionados no suponen ning\u00fan coste. La subasta se desarrolla de la siguiente manera: todos los elementos del terreno hacen sus ofertas,el centro selecciona un conjunto factible bas\u00e1ndose en las ofertas y realiza pagos a los agentes. Formalmente, una subasta se define mediante una regla de asignaci\u00f3n A : R '' _, F y una regla de pago P : R '' _, R ''. La regla de asignaci\u00f3n toma como entrada un vector de ofertas y decide cu\u00e1l de los conjuntos en F debe seleccionarse. La regla de pago tambi\u00e9n toma como entrada un vector de ofertas y decide cu\u00e1nto pagar a cada agente. Los requisitos est\u00e1ndar son racionalidad individual, es decir, el pago a cada agente debe ser al menos tan alto como su costo incurrido -LRB- 0 para agentes que no est\u00e1n en el conjunto seleccionado y ce para agentes en el conjunto seleccionado -RRB- y compatibilidad de incentivos. o veracidad, es decir, la estrategia dominante de cada agente es ofrecer su coste real. Una regla de asignaci\u00f3n es mon\u00f3tona si un agente no puede aumentar sus posibilidades de ser seleccionado aumentando su oferta. Dada una regla de asignaci\u00f3n mon\u00f3tona A y un vector de oferta b, la oferta umbral te de un agente e EA -LRB- b -RRB- es la oferta m\u00e1s alta de este agente que a\u00fan gana la subasta, dado que las ofertas de otros participantes siguen siendo las m\u00e1s altas. mismo. Es bien sabido -LRB- ver, por ejemplo -LSB- 19, 13 -RSB- -RRB- que cualquier subasta que tenga una regla de asignaci\u00f3n mon\u00f3tona y pague a cada agente su oferta umbral es veraz; por el contrario, cualquier subasta veraz tiene una regla de asignaci\u00f3n mon\u00f3tona. El mecanismo VCG es un mecanismo veraz que maximiza el \"bienestar social\" y paga 0 a los agentes perdedores. Para las subastas de sistemas de conjuntos, esto simplemente significa elegir el conjunto factible m\u00e1s barato, pagar a cada agente del conjunto seleccionado su oferta umbral y pagar 0 a todos los dem\u00e1s agentes. Tenga en cuenta, sin embargo, que el mecanismo VCG puede ser dif\u00edcil de implementar, ya que encontrar un conjunto factible m\u00e1s barato puede resultar dif\u00edcil. Si U es un conjunto de agentes, c -LRB- U -RRB- denota Ew \u2208 U cw. De manera similar, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- que cualquier subasta que tenga una regla de asignaci\u00f3n mon\u00f3tona y pague a cada agente su oferta umbral es veraz; por el contrario, cualquier subasta veraz tiene una regla de asignaci\u00f3n mon\u00f3tona. El mecanismo VCG es un mecanismo veraz que maximiza el \"bienestar social\" y paga 0 a los agentes perdedores. Para las subastas de sistemas de conjuntos, esto simplemente significa elegir el conjunto factible m\u00e1s barato, pagar a cada agente del conjunto seleccionado su oferta umbral y pagar 0 a todos los dem\u00e1s agentes. Tenga en cuenta, sin embargo, que el mecanismo VCG puede ser dif\u00edcil de implementar, ya que encontrar un conjunto factible m\u00e1s barato puede resultar dif\u00edcil. Si U es un conjunto de agentes, c -LRB- U -RRB- denota Ew \u2208 U cw. De manera similar, b -LRB- U -RRB- denota Ew \u2208 U bw.13 -RSB- -RRB- que cualquier subasta que tenga una regla de asignaci\u00f3n mon\u00f3tona y pague a cada agente su oferta umbral es veraz; por el contrario, cualquier subasta veraz tiene una regla de asignaci\u00f3n mon\u00f3tona. El mecanismo VCG es un mecanismo veraz que maximiza el \"bienestar social\" y paga 0 a los agentes perdedores. Para las subastas de sistemas de conjuntos, esto simplemente significa elegir el conjunto factible m\u00e1s barato, pagar a cada agente del conjunto seleccionado su oferta umbral y pagar 0 a todos los dem\u00e1s agentes. Tenga en cuenta, sin embargo, que el mecanismo VCG puede ser dif\u00edcil de implementar, ya que encontrar un conjunto factible m\u00e1s barato puede resultar dif\u00edcil. Si U es un conjunto de agentes, c -LRB- U -RRB- denota Ew \u2208 U cw. De manera similar, b -LRB- U -RRB- denota Ew \u2208 U bw.", "keyphrases": ["relaci\u00f3n frugal", "t\u00e9cnica de arranque", "subasta de cobertura de v\u00e9rtices", "utilidad de transferencia", "pago consecutivo obligado", "regla de asignaci\u00f3n mon\u00f3tona", "cobre", "polinomi-tiempo", "no mon\u00f3tono"]}
{"file_name": "I-11", "text": "Caracterizaci\u00f3n y predicci\u00f3n de agentes en tiempo real RESUMEN Razonar sobre los agentes que observamos en el mundo es un desaf\u00edo. Nuestra informaci\u00f3n disponible a menudo se limita a observaciones del comportamiento externo del agente en el pasado y en el presente. Para comprender estas acciones es necesario deducir el estado interno del agente, que incluye no s\u00f3lo elementos racionales -LRB- como las intenciones y planes -RRB-, sino tambi\u00e9n emotivos -LRB- como el miedo -RRB-. Adem\u00e1s, a menudo queremos predecir las acciones futuras del agente, que est\u00e1n limitadas no s\u00f3lo por estas caracter\u00edsticas internas, sino tambi\u00e9n por la din\u00e1mica de la interacci\u00f3n del agente con su entorno. BEE -LRB- Evoluci\u00f3n y extrapolaci\u00f3n del comportamiento -RRB- utiliza un modelo del entorno basado en agentes m\u00e1s r\u00e1pido que en tiempo real para caracterizar el estado interno de los agentes mediante la evoluci\u00f3n frente al comportamiento observado y luego predecir su comportamiento futuro, teniendo en cuenta la din\u00e1mica de su interacci\u00f3n con el medio ambiente. 1. INTRODUCCI\u00d3N El razonamiento sobre los agentes que observamos en el mundo debe integrar dos niveles dispares. Nuestras observaciones a menudo se limitan al comportamiento externo del agente, que frecuentemente puede resumirse num\u00e9ricamente como una trayectoria en el espacio-tiempo -LRB- quiz\u00e1s puntuada por acciones de un vocabulario bastante limitado -RRB-. Sin embargo, este comportamiento est\u00e1 impulsado por el estado interno del agente, que -LRB- en el caso de un humano -RRB- puede involucrar conceptos psicol\u00f3gicos y cognitivos de alto nivel como intenciones y emociones. Un desaf\u00edo central en muchos dominios de aplicaciones es el razonamiento a partir de observaciones externas del comportamiento de los agentes hasta una estimaci\u00f3n de su estado interno. Este razonamiento est\u00e1 motivado por el deseo de predecir el comportamiento del agente. Este problema se ha abordado tradicionalmente bajo la r\u00fabrica de \"reconocimiento del plan\" o \"inferencia del plan\". ''Muchos problemas realistas se desv\u00edan de estas condiciones. \u2022 El aumento del n\u00famero de agentes conduce a una explosi\u00f3n combinatoria que puede hundir el an\u00e1lisis convencional. \u2022 La din\u00e1mica ambiental puede frustrar las intenciones de los agentes. \u2022 Los agentes muchas veces intentan ocultar sus intenciones -LRB- e incluso su presencia -RRB-, en lugar de compartir informaci\u00f3n intencionadamente. \u2022 El estado emocional de un agente puede ser al menos tan importante como su estado racional para determinar su comportamiento. BEE -LRB- Evoluci\u00f3n y extrapolaci\u00f3n del comportamiento -RRB- es un enfoque novedoso para reconocer el estado racional y emocional de m\u00faltiples agentes que interact\u00faan bas\u00e1ndose \u00fanicamente en su comportamiento, sin recurrir a comunicaciones intencionales por parte de ellos. Est\u00e1 inspirado en t\u00e9cnicas utilizadas para predecir el comportamiento de sistemas din\u00e1micos no lineales, en las que una representaci\u00f3n del sistema se ajusta continuamente a su comportamiento pasado reciente. Para sistemas din\u00e1micos no lineales, la representaci\u00f3n es una ecuaci\u00f3n matem\u00e1tica de forma cerrada. En BEE, es un conjunto de par\u00e1metros que gobiernan el comportamiento de los agentes de software que representan a los individuos que se analizan.La versi\u00f3n actual de BEE caracteriza y predice el comportamiento de los agentes que representan a los soldados en combate urbano -LSB- 8 -RSB-. La secci\u00f3n 2 revisa trabajos previos relevantes. La secci\u00f3n 3 describe la arquitectura de BEE. La secci\u00f3n 4 informa los resultados de los experimentos con el sistema. La secci\u00f3n 5 concluye. 2. TRABAJO ANTERIOR BEE admite comparaci\u00f3n con investigaciones previas en IA -LRB- reconocimiento de planos -RRB-, Modelos Ocultos de Markov y sistemas de din\u00e1mica no lineal -LRB- predicci\u00f3n de trayectorias -RRB-. 2.1 Reconocimiento de planes en IA La teor\u00eda del agente com\u00fanmente describe el estado cognitivo de un agente en t\u00e9rminos de sus creencias, deseos e intenciones -LRB- el llamado modelo ``BDI'' -LSB- 5, 20 -RSB- -RRB- . Las creencias de un agente son proposiciones sobre el estado del mundo que considera verdaderas, en funci\u00f3n de sus percepciones. Sus deseos son proposiciones sobre el mundo que le gustar\u00eda que fueran ciertas. Los deseos no son necesariamente consistentes entre s\u00ed: un agente podr\u00eda desear ser rico y no trabajar al mismo tiempo. Las intenciones u objetivos de un agente son un subconjunto de sus deseos que ha seleccionado, en funci\u00f3n de sus creencias, para guiar sus acciones futuras. A diferencia de los deseos, las metas deben ser consistentes entre s\u00ed -LRB- o al menos creer que son consistentes por el agente -RRB-. Los objetivos de un agente gu\u00edan sus acciones. As\u00ed, uno deber\u00eda poder aprender algo sobre los objetivos de un agente observando sus acciones pasadas, y el conocimiento de los objetivos del agente, a su vez, permite sacar conclusiones sobre lo que el agente puede hacer en el futuro. Este proceso de razonamiento desde las acciones de un agente hasta sus objetivos se conoce como \"reconocimiento del plan\" o \"inferencia del plan\". El reconocimiento del plan rara vez se busca por s\u00ed mismo. Por lo general, admite una funci\u00f3n de nivel superior. Por ejemplo, en interfaces hombre-computadora, reconocer el plan de un usuario puede permitir que el sistema proporcione informaci\u00f3n y opciones m\u00e1s apropiadas para la acci\u00f3n del usuario. En un sistema de tutor\u00eda, inferir el plan del estudiante es un primer paso para identificar planes con errores y proporcionar la soluci\u00f3n adecuada. En muchos casos, la funci\u00f3n de nivel superior predice probables acciones futuras de la entidad cuyo plan se infiere. Nos centramos en el reconocimiento de planes en apoyo de la predicci\u00f3n. El plan de un agente es un insumo necesario para predecir su comportamiento futuro, pero no es suficiente. Es necesario tener en cuenta al menos otras dos influencias, una interna y otra externa. La influencia externa es la din\u00e1mica del entorno, que puede incluir otros agentes. La din\u00e1mica del mundo real impone limitaciones importantes. \u2022 El entorno puede interferir con los deseos del agente -LSB- 4, 10 -RSB-. \u2022 La mayor\u00eda de las interacciones entre agentes, y entre los agentes y el mundo, no son lineales. Un an\u00e1lisis racional de los objetivos de un agente puede permitirnos predecir lo que intentar\u00e1, pero cualquier plan no trivial con varios pasos depender\u00e1 sensiblemente en cada paso de la reacci\u00f3n del entorno.y nuestra predicci\u00f3n debe tener en cuenta tambi\u00e9n esta reacci\u00f3n. La simulaci\u00f3n real de futuros es una forma -LRB- la \u00fanica que conocemos ahora -RRB- de abordar el impacto de la din\u00e1mica ambiental sobre las acciones de un agente. Los agentes humanos tambi\u00e9n est\u00e1n sujetos a una influencia interna. El estado emocional del agente puede modular su proceso de decisi\u00f3n y su foco de atenci\u00f3n -LRB- y por tanto su percepci\u00f3n del entorno -RRB-. En casos extremos, la emoci\u00f3n puede llevar a un agente a elegir acciones que desde el punto de vista de un an\u00e1lisis l\u00f3gico pueden parecer irracionales. El trabajo actual sobre el reconocimiento de planes para la predicci\u00f3n se centra en el plan racional y no tiene en cuenta ni las influencias ambientales externas ni los sesgos emocionales internos. BEE integra los tres elementos en sus predicciones. 2.2 Modelos ocultos de Markov BEE es superficialmente similar a los modelos ocultos de Markov -LRB- HMM -LSB- 19 -RSB- -RRB-. BEE ofrece dos beneficios importantes sobre HMM. En primer lugar, las variables ocultas de un \u00fanico agente no satisfacen la propiedad de Markov. Es decir, sus valores en t + 1 dependen no s\u00f3lo de sus valores en t, sino tambi\u00e9n de las variables ocultas de otros agentes. Se podr\u00eda evitar esta limitaci\u00f3n construyendo un \u00fanico HMM sobre el espacio de estados conjunto de todos los agentes, pero este enfoque es combinatoriamente prohibitivo. BEE combina la eficiencia de modelar de forma independiente agentes individuales con la realidad de tener en cuenta las interacciones entre ellos. En segundo lugar, los modelos de Markov suponen que las probabilidades de transici\u00f3n son estacionarias. El proceso evolutivo de BEE actualiza continuamente las personalidades de los agentes bas\u00e1ndose en observaciones reales y, por lo tanto, toma en cuenta autom\u00e1ticamente los cambios en las personalidades de los agentes. 2.3 Ajuste de sistemas no lineales en tiempo real Muchos sistemas de inter\u00e9s pueden describirse mediante un vector de n\u00fameros reales que cambia en funci\u00f3n del tiempo. Las dimensiones del vector definen el espacio de estados del sistema. La predicci\u00f3n a largo plazo de tal sistema es imposible. Sin embargo, a menudo resulta \u00fatil anticipar el comportamiento del sistema a una corta distancia en el futuro. Este proceso se repite constantemente, proporcionando al usuario una visi\u00f3n limitada del futuro. Este enfoque es s\u00f3lido y se aplica ampliamente, pero requiere sistemas que puedan describirse de manera eficiente con ecuaciones matem\u00e1ticas. BEE extiende este enfoque a los comportamientos de los agentes, que ajusta al comportamiento observado mediante un algoritmo gen\u00e9tico. 5. CONCLUSIONES En muchos dominios, es importante razonar a partir del comportamiento observado de una entidad hasta una estimaci\u00f3n de su estado interno, y luego extrapolar esa estimaci\u00f3n para predecir el comportamiento futuro de la entidad. BEE realiza esta tarea utilizando una simulaci\u00f3n de enjambres de agentes m\u00e1s r\u00e1pida que en tiempo real, coordinada a trav\u00e9s de feromonas digitales. Esta simulaci\u00f3n integra el conocimiento de las regiones amenazadas, un an\u00e1lisis cognitivo de las creencias, deseos e intenciones del agente, un modelo de la disposici\u00f3n y el estado emocional del agente,y la din\u00e1mica de las interacciones con el medio ambiente. Al hacer evolucionar a los agentes en este rico entorno, podemos adaptar su estado interno a su comportamiento observado. En los juegos de guerra realistas, el sistema detecta con \u00e9xito emociones deliberadamente jugadas y hace predicciones razonables sobre el comportamiento futuro de las entidades. BEE solo puede modelar variables de estado internas que impactan el comportamiento externo del agente. No se pueden ajustar variables que el agente no manifiesta externamente, ya que la base del ciclo evolutivo es una comparaci\u00f3n del comportamiento externo del agente simulado con el de la entidad real. Esta limitaci\u00f3n es grave si nuestro prop\u00f3sito es comprender el estado interno de la entidad por s\u00ed mismo. Si nuestro prop\u00f3sito al adaptar agentes es predecir su comportamiento posterior, la limitaci\u00f3n es mucho menos grave. Las variables de estado que no impactan el comportamiento, si bien son invisibles para un an\u00e1lisis basado en el comportamiento, son irrelevantes para una predicci\u00f3n conductual. \u2022 Nuestro limitado repertorio inicial de emociones es un peque\u00f1o subconjunto de aquellos que han sido distinguidos por los psic\u00f3logos y que podr\u00edan ser \u00fatiles para comprender y proyectar la conducta. Esperamos ampliar el conjunto de emociones y disposiciones de apoyo que BEE puede detectar. \u2022 El mapeo entre el estado psicol\u00f3gico -LRB- cognitivo y emocional -RRB- de un agente y su comportamiento externo no es uno a uno. Varios estados internos diferentes podr\u00edan ser consistentes con un determinado comportamiento observado bajo un conjunto de condiciones ambientales, pero podr\u00edan producir comportamientos distintos bajo otras condiciones. Si el entorno del pasado reciente confunde estados internos tan distintos, seremos incapaces de distinguirlos. Mientras el entorno permanezca en este estado, nuestras predicciones ser\u00e1n precisas, cualquiera que sea el estado interno que asignemos al agente. Si luego el entorno cambia a uno en el que los diferentes estados internos conducen a diferentes comportamientos, utilizar el estado interno previamente elegido producir\u00e1 predicciones inexactas. Una forma de abordar estas preocupaciones es sondear el mundo real, perturb\u00e1ndolo de maneras que estimulen comportamientos distintos de entidades cuyo estado psicol\u00f3gico es de otro modo indistinguible. Este tipo de sondeo es una importante t\u00e9cnica de inteligencia. La simulaci\u00f3n de BEE, m\u00e1s r\u00e1pida que en tiempo real, puede permitirnos identificar acciones de sondeo apropiadas, aumentando en gran medida la eficacia de los esfuerzos de inteligencia.ya que la base del ciclo evolutivo es una comparaci\u00f3n del comportamiento exterior del agente simulado con el de la entidad real. Esta limitaci\u00f3n es grave si nuestro prop\u00f3sito es comprender el estado interno de la entidad por s\u00ed mismo. Si nuestro prop\u00f3sito al adaptar agentes es predecir su comportamiento posterior, la limitaci\u00f3n es mucho menos grave. Las variables de estado que no impactan el comportamiento, si bien son invisibles para un an\u00e1lisis basado en el comportamiento, son irrelevantes para una predicci\u00f3n conductual. \u2022 Nuestro limitado repertorio inicial de emociones es un peque\u00f1o subconjunto de aquellos que han sido distinguidos por los psic\u00f3logos y que podr\u00edan ser \u00fatiles para comprender y proyectar la conducta. Esperamos ampliar el conjunto de emociones y disposiciones de apoyo que BEE puede detectar. \u2022 El mapeo entre el estado psicol\u00f3gico -LRB- cognitivo y emocional -RRB- de un agente y su comportamiento externo no es uno a uno. Varios estados internos diferentes podr\u00edan ser consistentes con un determinado comportamiento observado bajo un conjunto de condiciones ambientales, pero podr\u00edan producir comportamientos distintos bajo otras condiciones. Si el entorno del pasado reciente confunde estados internos tan distintos, seremos incapaces de distinguirlos. Mientras el entorno permanezca en este estado, nuestras predicciones ser\u00e1n precisas, cualquiera que sea el estado interno que asignemos al agente. Si luego el entorno cambia a uno en el que los diferentes estados internos conducen a diferentes comportamientos, utilizar el estado interno previamente elegido producir\u00e1 predicciones inexactas. Una forma de abordar estas preocupaciones es sondear el mundo real, perturb\u00e1ndolo de maneras que estimulen comportamientos distintos de entidades cuyo estado psicol\u00f3gico es de otro modo indistinguible. Este tipo de sondeo es una importante t\u00e9cnica de inteligencia. La simulaci\u00f3n de BEE, m\u00e1s r\u00e1pida que en tiempo real, puede permitirnos identificar acciones de sondeo apropiadas, aumentando en gran medida la eficacia de los esfuerzos de inteligencia.ya que la base del ciclo evolutivo es una comparaci\u00f3n del comportamiento exterior del agente simulado con el de la entidad real. Esta limitaci\u00f3n es grave si nuestro prop\u00f3sito es comprender el estado interno de la entidad por s\u00ed mismo. Si nuestro prop\u00f3sito al adaptar agentes es predecir su comportamiento posterior, la limitaci\u00f3n es mucho menos grave. Las variables de estado que no impactan el comportamiento, si bien son invisibles para un an\u00e1lisis basado en el comportamiento, son irrelevantes para una predicci\u00f3n conductual. \u2022 Nuestro limitado repertorio inicial de emociones es un peque\u00f1o subconjunto de aquellos que han sido distinguidos por los psic\u00f3logos y que podr\u00edan ser \u00fatiles para comprender y proyectar la conducta. Esperamos ampliar el conjunto de emociones y disposiciones de apoyo que BEE puede detectar. \u2022 El mapeo entre el estado psicol\u00f3gico -LRB- cognitivo y emocional -RRB- de un agente y su comportamiento externo no es uno a uno. Varios estados internos diferentes podr\u00edan ser consistentes con un determinado comportamiento observado bajo un conjunto de condiciones ambientales, pero podr\u00edan producir comportamientos distintos bajo otras condiciones. Si el entorno del pasado reciente confunde estados internos tan distintos, seremos incapaces de distinguirlos. Mientras el entorno permanezca en este estado, nuestras predicciones ser\u00e1n precisas, cualquiera que sea el estado interno que asignemos al agente. Si luego el entorno cambia a uno en el que los diferentes estados internos conducen a diferentes comportamientos, utilizar el estado interno previamente elegido producir\u00e1 predicciones inexactas. Una forma de abordar estas preocupaciones es sondear el mundo real, perturb\u00e1ndolo de maneras que estimulen comportamientos distintos de entidades cuyo estado psicol\u00f3gico es de otro modo indistinguible. Este tipo de sondeo es una importante t\u00e9cnica de inteligencia. La simulaci\u00f3n de BEE, m\u00e1s r\u00e1pida que en tiempo real, puede permitirnos identificar acciones de sondeo apropiadas, aumentando en gran medida la eficacia de los esfuerzos de inteligencia.Si el entorno del pasado reciente confunde estados internos tan distintos, seremos incapaces de distinguirlos. Mientras el entorno permanezca en este estado, nuestras predicciones ser\u00e1n precisas, cualquiera que sea el estado interno que asignemos al agente. Si luego el entorno cambia a uno en el que los diferentes estados internos conducen a diferentes comportamientos, utilizar el estado interno previamente elegido producir\u00e1 predicciones inexactas. Una forma de abordar estas preocupaciones es sondear el mundo real, perturb\u00e1ndolo de maneras que estimulen comportamientos distintos de entidades cuyo estado psicol\u00f3gico es de otro modo indistinguible. Este tipo de sondeo es una importante t\u00e9cnica de inteligencia. La simulaci\u00f3n de BEE, m\u00e1s r\u00e1pida que en tiempo real, puede permitirnos identificar acciones de sondeo apropiadas, aumentando en gran medida la eficacia de los esfuerzos de inteligencia.Si el entorno del pasado reciente confunde estados internos tan distintos, seremos incapaces de distinguirlos. Mientras el entorno permanezca en este estado, nuestras predicciones ser\u00e1n precisas, cualquiera que sea el estado interno que asignemos al agente. Si luego el entorno cambia a uno en el que los diferentes estados internos conducen a diferentes comportamientos, utilizar el estado interno previamente elegido producir\u00e1 predicciones inexactas. Una forma de abordar estas preocupaciones es sondear el mundo real, perturb\u00e1ndolo de maneras que estimulen comportamientos distintos de entidades cuyo estado psicol\u00f3gico es de otro modo indistinguible. Este tipo de sondeo es una importante t\u00e9cnica de inteligencia. La simulaci\u00f3n de BEE, m\u00e1s r\u00e1pida que en tiempo real, puede permitirnos identificar acciones de sondeo apropiadas, aumentando en gran medida la eficacia de los esfuerzos de inteligencia.", "keyphrases": ["raz\u00f3n del agente", "comportamiento externo", "estado interno", "predecir el comportamiento del agente", "comportamiento evolut y extrapol", "sistema din\u00e1mico no lineal", "objetivo del agente", "emocionado", "sabor a feromonas", "disponer", "comportamiento futuro"]}
{"file_name": "J-9", "text": "Computaci\u00f3n en un mercado de informaci\u00f3n distribuida \u2217 RESUMEN Seg\u00fan la teor\u00eda econ\u00f3mica, respaldada por evidencia emp\u00edrica y de laboratorio, el precio de equilibrio de un t\u00edtulo financiero refleja toda la informaci\u00f3n relativa al valor del t\u00edtulo. Investigamos el proceso computacional en el camino hacia el equilibrio, donde la informaci\u00f3n distribuida entre los comerciantes se revela paso a paso a lo largo del tiempo y se incorpora al precio de mercado. Desarrollamos un modelo simplificado de un mercado de informaci\u00f3n, junto con estrategias comerciales, para formalizar las propiedades computacionales del proceso. Mostramos que no se garantiza que los valores cuyos pagos no pueden expresarse como funciones de umbral ponderadas de bits de entrada distribuidos converjan al equilibrio adecuado predicho por la teor\u00eda econ\u00f3mica. Por otro lado, se garantiza que los valores cuyos pagos son funciones de umbral converger\u00e1n, para todas las distribuciones de probabilidad anteriores. Adem\u00e1s, estos valores umbral convergen como m\u00e1ximo en n rondas, donde n es el n\u00famero de bits de informaci\u00f3n distribuida. Tambi\u00e9n demostramos un l\u00edmite inferior, que muestra un tipo de umbral de seguridad que requiere al menos n/2 rondas para converger en el peor de los casos. \u2217 Este trabajo fue apoyado por la Iniciativa de Investigaci\u00f3n Universitaria del Departamento de Defensa -LRB- URI -RRB- administrada por la Oficina de Investigaci\u00f3n Naval bajo la subvenci\u00f3n N00014-01-1-0795. \u2020 Respaldado en parte por la subvenci\u00f3n ONR N00014-01-0795 y las subvenciones NSF CCR-0105337, CCR-TC-0208972, ANI-0207399 e ITR-0219018. \u2021 Este trabajo se realiz\u00f3 en NEC Laboratories America, Princeton, Nueva Jersey. 1. INTRODUCCI\u00d3N La forma fuerte de la hip\u00f3tesis de los mercados eficientes establece que los precios de mercado incorporan casi instant\u00e1neamente toda la informaci\u00f3n disponible para todos los comerciantes. Como resultado, los precios de mercado codifican los mejores pron\u00f3sticos de resultados futuros dada toda la informaci\u00f3n, incluso si esa informaci\u00f3n se distribuye entre muchas fuentes. El proceso de incorporaci\u00f3n de informaci\u00f3n es, en esencia, un c\u00f3mputo distribuido. Cada comerciante comienza con su propia informaci\u00f3n. A medida que se realizan transacciones, la informaci\u00f3n resumida se revela a trav\u00e9s de los precios de mercado. Los comerciantes aprenden o infieren qu\u00e9 informaci\u00f3n es probable que otros tengan al observar los precios y luego actualizan sus propias creencias en funci\u00f3n de sus observaciones. Con el tiempo, si el proceso funciona como se anuncia, toda la informaci\u00f3n se revela y todos los comerciantes convergen al mismo estado de informaci\u00f3n. En este punto, el mercado se encuentra en lo que se llama un equilibrio de expectativas racionales -LSB- 11, 16, 19 -RSB-. Toda la informaci\u00f3n disponible para todos los comerciantes ahora se refleja en los precios vigentes y no es deseable realizar m\u00e1s operaciones hasta que haya nueva informaci\u00f3n disponible. Si bien la mayor\u00eda de los mercados no est\u00e1n dise\u00f1ados con la agregaci\u00f3n de informaci\u00f3n como motivaci\u00f3n principal (por ejemplo, los derivados). En este art\u00edculo, investigamos la naturaleza del proceso computacional mediante el cual la informaci\u00f3n distribuida se revela y combina con el tiempo en los precios en los mercados de informaci\u00f3n. Para ello, en la Secci\u00f3n 3,Proponemos un modelo de mercado de informaci\u00f3n que es manejable para el an\u00e1lisis te\u00f3rico y, creemos, captura gran parte de la esencia importante de los mercados de informaci\u00f3n reales. Demostramos que s\u00f3lo los valores booleanos cuyos pagos pueden expresarse como funciones de umbral de los bits de informaci\u00f3n de entrada distribuidos tienen la garant\u00eda de converger como lo predice la teor\u00eda de las expectativas racionales. Es posible que los valores booleanos con pagos m\u00e1s complejos no converjan en algunas distribuciones anteriores. Tambi\u00e9n proporcionamos l\u00edmites superior e inferior sobre el tiempo de convergencia para estos valores umbral. Mostramos que, para todas las distribuciones anteriores, el precio de un valor umbral converge a su precio de equilibrio de expectativas racionales en como m\u00e1ximo n rondas, donde n es el n\u00famero de bits de informaci\u00f3n distribuida. Mostramos que este l\u00edmite del peor de los casos es ajustado dentro de un factor de dos ilustrando una situaci\u00f3n en la que un umbral de seguridad requiere n/2 rondas para converger.", "keyphrases": ["teor\u00eda econ\u00f3mica", "empir y laboratorios evid", "precio de equilibrio", "finanzas seguras", "valor de seguridad", "proceso computacional", "camino hacia el equilibrio", "comerciante", "precio de mercado", "modelo simplificado", "estrategia comercial", "propiedades de c\u00e1lculo del proceso", "asegurar", "saldar", "funci\u00f3n umbral", "distribuci\u00f3n probable", "redondo", "n\u00famero de bits", "distribuir informar", "l\u00edmite inferior", "peor de los casos", "informar al mercado"]}
{"file_name": "H-19", "text": "An\u00e1lisis de trayectorias de caracter\u00edsticas para la detecci\u00f3n de eventos RESUMEN Consideramos el problema de analizar trayectorias de palabras tanto en el dominio del tiempo como de la frecuencia, con el objetivo espec\u00edfico de identificar palabras peri\u00f3dicas y aperi\u00f3dicas importantes y menos informadas. Se puede agrupar un conjunto de palabras con tendencias id\u00e9nticas para reconstruir un evento sin ninguna supervisi\u00f3n. La frecuencia del documento de cada palabra a lo largo del tiempo se trata como una serie de tiempo, donde cada elemento es la puntuaci\u00f3n de la frecuencia del documento - frecuencia inversa del documento -LRB- DFIDF -RRB- en un momento dado. En este art\u00edculo, 1 -RRB- aplicamos por primera vez el an\u00e1lisis espectral para categorizar caracter\u00edsticas de diferentes eventos: importantes y menos reportados, peri\u00f3dicos y aperi\u00f3dicos; 2 -RRB- model\u00f3 caracter\u00edsticas aperi\u00f3dicas con densidad gaussiana y caracter\u00edsticas peri\u00f3dicas con densidades de mezcla gaussianas, y posteriormente detect\u00f3 el estallido de cada caracter\u00edstica mediante el enfoque gaussiano truncado; 3 -RRB- propuso un algoritmo de detecci\u00f3n de eventos codiciosos no supervisados \u200b\u200bpara detectar eventos peri\u00f3dicos y aperi\u00f3dicos. Todos los m\u00e9todos anteriores se pueden aplicar a datos de series de tiempo en general. Evaluamos exhaustivamente nuestros m\u00e9todos en el Reuters News Corpus -LSB- 3 -RSB- de 1 a\u00f1o y demostramos que pod\u00edan descubrir eventos peri\u00f3dicos y aperi\u00f3dicos significativos. 1. INTRODUCCI\u00d3N Hay m\u00e1s de 4.000 fuentes de noticias en l\u00ednea en el mundo. Monitorearlos manualmente para detectar eventos importantes se ha vuelto dif\u00edcil o pr\u00e1cticamente imposible. De hecho, la comunidad de detecci\u00f3n y seguimiento de temas -LRB-TDT-RRB- lleva muchos a\u00f1os intentando encontrar una soluci\u00f3n pr\u00e1ctica que ayude a las personas a seguir las noticias de forma eficaz. las soluciones propuestas para la detecci\u00f3n de eventos -LSB- 20, 5, 17, 4, 21, 7, 14, 10 -RSB- son demasiado simplistas -LRB- basadas en la similitud de cosenos -LSB- 5 -RSB- -RRB- o poco pr\u00e1cticas debido a la necesidad de sintonizar un gran n\u00famero de par\u00e1metros -LSB- 9 -RSB-. Por lo tanto, en este art\u00edculo analizamos las noticias y las tendencias destacadas desde la perspectiva del an\u00e1lisis de una se\u00f1al de palabra de una serie temporal. Trabajos anteriores como -LSB- 9 -RSB- han intentado reconstruir un evento con sus caracter\u00edsticas representativas. Sin embargo, en muchas tareas de detecci\u00f3n predictiva de eventos -LRB- es decir, detecci\u00f3n retrospectiva de eventos -RRB-, existe un vasto conjunto de caracter\u00edsticas potenciales s\u00f3lo para un conjunto fijo de observaciones -LRB- es decir, las r\u00e1fagas obvias -RRB-. De estas caracter\u00edsticas, a menudo se espera que s\u00f3lo una peque\u00f1a cantidad sean \u00fatiles. En particular, estudiamos el nuevo problema de analizar trayectorias de caracter\u00edsticas para la detecci\u00f3n de eventos, tomando prestada una t\u00e9cnica bien conocida del procesamiento de se\u00f1ales: identificar correlaciones distributivas entre todas las caracter\u00edsticas mediante an\u00e1lisis espectral. Para evaluar nuestro m\u00e9todo, posteriormente proponemos un algoritmo de detecci\u00f3n de eventos no supervisados \u200b\u200bpara flujos de noticias. Figura 1: Correlaci\u00f3n de caracter\u00edsticas -LRB- DFIDF: tiempo -RRB- entre a -RRB- Semana Santa y abril b -RRB- No auditado y finalizado. Como ejemplo ilustrativo, consideremos la correlaci\u00f3n entre las palabras Semana Santa y Abril del Corpus de Reuters.En la gr\u00e1fica de su DFIDF normalizado en la Figura 1 -LRB- a -RRB-, observamos la fuerte superposici\u00f3n entre las dos palabras alrededor del 04/1997, lo que significa que probablemente ambas pertenecen al mismo evento durante ese tiempo -LRB- Fiesta de Pascua. -RRB-. En este ejemplo, el evento oculto Fiesta de Pascua es un evento aperi\u00f3dico importante t\u00edpico en datos de 1 a\u00f1o. Otro ejemplo se muestra en la Figura 1 -LRB- b -RRB-, donde las palabras No auditado y Finalizado ` Reuters Corpus son el conjunto de datos predeterminado para todos los ejemplos. exhiben un comportamiento similar durante per\u00edodos de 3 meses. Estas dos palabras en realidad se originaron en el mismo evento peri\u00f3dico: los informes de p\u00e9rdidas de ingresos netos, que las empresas que cotizan en bolsa publican trimestralmente. Otras observaciones extra\u00eddas de la Figura 1 son: 1 -RRB- el per\u00edodo de r\u00e1fagas de abril es mucho m\u00e1s largo que Semana Santa, lo que sugiere que abril puede existir en otros eventos durante el mismo per\u00edodo; 2 -RRB- No auditado tiene un valor DFIDF promedio m\u00e1s alto que Finalizado, lo que indica que No auditado es m\u00e1s representativo del evento subyacente. Estos dos ejemplos no son m\u00e1s que la punta del iceberg entre todas las tendencias y correlaciones de palabras ocultas en una corriente de noticias como Reuters. Si se logra descubrir un gran n\u00famero de ellos, podr\u00eda ayudar significativamente a las tareas de la TDT. En particular, indica la importancia de extraer caracter\u00edsticas de correlaci\u00f3n para detectar eventos correspondientes. En resumen, postulamos que: 1 -RRB- Un evento se describe por sus caracter\u00edsticas representativas. Con base en estas observaciones, podemos extraer caracter\u00edsticas representativas de un evento dado o detectar un evento a partir de una lista de caracter\u00edsticas altamente correlacionadas. En este art\u00edculo, nos centramos en esto \u00faltimo, es decir, en c\u00f3mo se pueden descubrir caracter\u00edsticas correlacionadas para formar un evento de manera no supervisada. 1.1 Contribuciones Este art\u00edculo tiene tres contribuciones principales: 9 Hasta donde sabemos, nuestro enfoque es el primero en categorizar caracter\u00edsticas de palabras para eventos heterog\u00e9neos. 9 Proponemos un enfoque simple y eficaz basado en la densidad de la mezcla para modelar y detectar explosiones de caracter\u00edsticas. 9 Creamos un algoritmo de detecci\u00f3n de eventos no supervisados \u200b\u200bpara detectar eventos peri\u00f3dicos y aperi\u00f3dicos. Nuestro algoritmo ha sido evaluado en un flujo de noticias real para demostrar su eficacia. 2. TRABAJO RELACIONADO Adem\u00e1s, la mayor\u00eda de las investigaciones TDT hasta ahora se han centrado en agrupar/clasificar documentos en tipos de temas, identificar oraciones novedosas -LSB- 6 -RSB- para nuevos eventos, etc., sin prestar mucha atenci\u00f3n al an\u00e1lisis de la palabra trayectoria con respecto al tiempo. Swan y Allan -LSB- 18 -RSB- intentaron por primera vez utilizar t\u00e9rminos coexistentes para construir un evento. Sin embargo, s\u00f3lo consideraron entidades nombradas y pares de sintagmas nominales, sin considerar sus periodicidades. Por el contrario, nuestro art\u00edculo considera todo lo anterior. Recientemente, ha habido un gran inter\u00e9s en modelar un evento en flujos de text como una \"explosi\u00f3n de actividades\" mediante la incorporaci\u00f3n de informaci\u00f3n temporal. Sin embargo, ninguno de los trabajos existentes identific\u00f3 espec\u00edficamente caracter\u00edsticas de eventos, excepto Fung et al. -LSB- 9 -RSB-,quien agrup\u00f3 rasgos tetonas para identificar varios eventos explosivos. Nuestro trabajo se diferencia de -LSB- 9 -RSB- en varios aspectos: 1 -RRB- analizamos cada caracter\u00edstica, no solo las caracter\u00edsticas en r\u00e1fagas; 2 -RRB- clasificamos caracter\u00edsticas seg\u00fan dos dimensiones categ\u00f3ricas -LRB- periodicidad y potencia -RRB-, lo que produce en total cinco tipos de caracter\u00edsticas principales; 3 -RRB- no restringimos que cada caracter\u00edstica pertenezca exclusivamente a un solo evento. Vlachos et al. han utilizado previamente t\u00e9cnicas de an\u00e1lisis espectral. -LSB- 19 -RSB- para identificar periodicidades y r\u00e1fagas de registros de consultas. Su atenci\u00f3n se centr\u00f3 en detectar m\u00faltiples periodicidades a partir del gr\u00e1fico del espectro de potencia, que luego se utilizaron para indexar palabras para la b\u00fasqueda \"consulta por r\u00e1faga\". En este art\u00edculo, utilizamos el an\u00e1lisis espectral para clasificar caracter\u00edsticas de palabras seg\u00fan dos dimensiones, a saber, periodicidad y espectro de potencia, con el objetivo final de identificar eventos de r\u00e1fagas tanto peri\u00f3dicas como aperi\u00f3dicas. 8. CONCLUSIONES Este art\u00edculo adopt\u00f3 una perspectiva completamente nueva al analizar las trayectorias de caracter\u00edsticas como se\u00f1ales en el dominio del tiempo. Al considerar las frecuencias de los documentos de palabras tanto en el dominio del tiempo como de la frecuencia, pudimos derivar muchas caracter\u00edsticas nuevas sobre los flujos de noticias que antes se desconoc\u00edan, por ejemplo, las diferentes distribuciones de palabras vac\u00edas durante los d\u00edas laborables y los fines de semana. Por primera vez en el \u00e1rea de la TDT, aplicamos un enfoque sistem\u00e1tico para detectar autom\u00e1ticamente eventos peri\u00f3dicos y aperi\u00f3dicos importantes y menos reportados. La idea clave de nuestro trabajo radica en las observaciones de que -LRB- a -RRB- los eventos peri\u00f3dicos tienen -LRB- a -RRB- caracter\u00edsticas representativas peri\u00f3dicas y -LRB- un -RRB- los eventos importantes tienen -LRB- en -RRB- activo caracter\u00edsticas representativas, diferenciadas por sus espectros de potencia y periodos de tiempo. Para abordar el problema de detecci\u00f3n de eventos reales, se utiliz\u00f3 un enfoque simple y eficaz basado en la densidad de la mezcla para identificar r\u00e1fagas de caracter\u00edsticas y sus per\u00edodos de r\u00e1faga asociados. Tambi\u00e9n dise\u00f1amos un algoritmo codicioso no supervisado para detectar eventos peri\u00f3dicos y aperi\u00f3dicos, que logr\u00f3 detectar eventos reales como se muestra en la evaluaci\u00f3n de un flujo de noticias real. Aunque no hemos realizado ninguna comparaci\u00f3n comparativa con otro enfoque, simplemente porque no existe ning\u00fan trabajo previo en el problema abordado. Sin embargo, creemos que nuestro m\u00e9todo simple y eficaz ser\u00e1 \u00fatil para todos los profesionales de la TDT, y ser\u00e1 especialmente \u00fatil para el an\u00e1lisis exploratorio inicial de los flujos de noticias.Su atenci\u00f3n se centr\u00f3 en detectar m\u00faltiples periodicidades a partir del gr\u00e1fico del espectro de potencia, que luego se utilizaron para indexar palabras para la b\u00fasqueda \"consulta por r\u00e1faga\". En este art\u00edculo, utilizamos el an\u00e1lisis espectral para clasificar caracter\u00edsticas de palabras seg\u00fan dos dimensiones, a saber, periodicidad y espectro de potencia, con el objetivo final de identificar eventos de r\u00e1fagas tanto peri\u00f3dicas como aperi\u00f3dicas. 8. CONCLUSIONES Este art\u00edculo adopt\u00f3 una perspectiva completamente nueva al analizar las trayectorias de caracter\u00edsticas como se\u00f1ales en el dominio del tiempo. Al considerar las frecuencias de los documentos de palabras tanto en el dominio del tiempo como de la frecuencia, pudimos derivar muchas caracter\u00edsticas nuevas sobre los flujos de noticias que antes se desconoc\u00edan, por ejemplo, las diferentes distribuciones de palabras vac\u00edas durante los d\u00edas laborables y los fines de semana. Por primera vez en el \u00e1rea de la TDT, aplicamos un enfoque sistem\u00e1tico para detectar autom\u00e1ticamente eventos peri\u00f3dicos y aperi\u00f3dicos importantes y menos reportados. La idea clave de nuestro trabajo radica en las observaciones de que -LRB- a -RRB- los eventos peri\u00f3dicos tienen -LRB- a -RRB- caracter\u00edsticas representativas peri\u00f3dicas y -LRB- un -RRB- los eventos importantes tienen -LRB- en -RRB- activo caracter\u00edsticas representativas, diferenciadas por sus espectros de potencia y periodos de tiempo. Para abordar el problema de detecci\u00f3n de eventos reales, se utiliz\u00f3 un enfoque simple y eficaz basado en la densidad de la mezcla para identificar r\u00e1fagas de caracter\u00edsticas y sus per\u00edodos de r\u00e1faga asociados. Tambi\u00e9n dise\u00f1amos un algoritmo codicioso no supervisado para detectar eventos peri\u00f3dicos y aperi\u00f3dicos, que logr\u00f3 detectar eventos reales como se muestra en la evaluaci\u00f3n de un flujo de noticias real. Aunque no hemos realizado ninguna comparaci\u00f3n comparativa con otro enfoque, simplemente porque no existe ning\u00fan trabajo previo en el problema abordado. Sin embargo, creemos que nuestro m\u00e9todo simple y eficaz ser\u00e1 \u00fatil para todos los profesionales de la TDT, y ser\u00e1 especialmente \u00fatil para el an\u00e1lisis exploratorio inicial de los flujos de noticias.Su atenci\u00f3n se centr\u00f3 en detectar m\u00faltiples periodicidades a partir del gr\u00e1fico del espectro de potencia, que luego se utilizaron para indexar palabras para la b\u00fasqueda \"consulta por r\u00e1faga\". En este art\u00edculo, utilizamos el an\u00e1lisis espectral para clasificar caracter\u00edsticas de palabras seg\u00fan dos dimensiones, a saber, periodicidad y espectro de potencia, con el objetivo final de identificar eventos de r\u00e1fagas tanto peri\u00f3dicas como aperi\u00f3dicas. 8. CONCLUSIONES Este art\u00edculo adopt\u00f3 una perspectiva completamente nueva al analizar las trayectorias de caracter\u00edsticas como se\u00f1ales en el dominio del tiempo. Al considerar las frecuencias de los documentos de palabras tanto en el dominio del tiempo como de la frecuencia, pudimos derivar muchas caracter\u00edsticas nuevas sobre los flujos de noticias que antes se desconoc\u00edan, por ejemplo, las diferentes distribuciones de palabras vac\u00edas durante los d\u00edas laborables y los fines de semana. Por primera vez en el \u00e1rea de la TDT, aplicamos un enfoque sistem\u00e1tico para detectar autom\u00e1ticamente eventos peri\u00f3dicos y aperi\u00f3dicos importantes y menos reportados. La idea clave de nuestro trabajo radica en las observaciones de que -LRB- a -RRB- los eventos peri\u00f3dicos tienen -LRB- a -RRB- caracter\u00edsticas representativas peri\u00f3dicas y -LRB- un -RRB- los eventos importantes tienen -LRB- en -RRB- activo caracter\u00edsticas representativas, diferenciadas por sus espectros de potencia y periodos de tiempo. Para abordar el problema de detecci\u00f3n de eventos reales, se utiliz\u00f3 un enfoque simple y eficaz basado en la densidad de la mezcla para identificar r\u00e1fagas de caracter\u00edsticas y sus per\u00edodos de r\u00e1faga asociados. Tambi\u00e9n dise\u00f1amos un algoritmo codicioso no supervisado para detectar eventos peri\u00f3dicos y aperi\u00f3dicos, que logr\u00f3 detectar eventos reales como se muestra en la evaluaci\u00f3n de un flujo de noticias real. Aunque no hemos realizado ninguna comparaci\u00f3n comparativa con otro enfoque, simplemente porque no existe ning\u00fan trabajo previo en el problema abordado. Sin embargo, creemos que nuestro m\u00e9todo simple y eficaz ser\u00e1 \u00fatil para todos los profesionales de la TDT, y ser\u00e1 especialmente \u00fatil para el an\u00e1lisis exploratorio inicial de los flujos de noticias.Para abordar el problema de detecci\u00f3n de eventos reales, se utiliz\u00f3 un enfoque simple y eficaz basado en la densidad de la mezcla para identificar r\u00e1fagas de caracter\u00edsticas y sus per\u00edodos de r\u00e1faga asociados. Tambi\u00e9n dise\u00f1amos un algoritmo codicioso no supervisado para detectar eventos peri\u00f3dicos y aperi\u00f3dicos, que logr\u00f3 detectar eventos reales como se muestra en la evaluaci\u00f3n de un flujo de noticias real. Aunque no hemos realizado ninguna comparaci\u00f3n comparativa con otro enfoque, simplemente porque no existe ning\u00fan trabajo previo en el problema abordado. Sin embargo, creemos que nuestro m\u00e9todo simple y eficaz ser\u00e1 \u00fatil para todos los profesionales de la TDT, y ser\u00e1 especialmente \u00fatil para el an\u00e1lisis exploratorio inicial de los flujos de noticias.Para abordar el problema de detecci\u00f3n de eventos reales, se utiliz\u00f3 un enfoque simple y eficaz basado en la densidad de la mezcla para identificar r\u00e1fagas de caracter\u00edsticas y sus per\u00edodos de r\u00e1faga asociados. Tambi\u00e9n dise\u00f1amos un algoritmo codicioso no supervisado para detectar eventos peri\u00f3dicos y aperi\u00f3dicos, que logr\u00f3 detectar eventos reales como se muestra en la evaluaci\u00f3n de un flujo de noticias real. Aunque no hemos realizado ninguna comparaci\u00f3n comparativa con otro enfoque, simplemente porque no existe ning\u00fan trabajo previo en el problema abordado. Sin embargo, creemos que nuestro m\u00e9todo simple y eficaz ser\u00e1 \u00fatil para todos los profesionales de la TDT, y ser\u00e1 especialmente \u00fatil para el an\u00e1lisis exploratorio inicial de los flujos de noticias.", "keyphrases": ["detecci\u00f3n de eventos", "palabra trayectoria", "evento de periodo", "evento de periodo", "se\u00f1al de palabra", "an\u00e1lisis espectral", "detectar tema", "pista de tema", "flujo de text", "nueva corriente", "serie de tiempo"]}
{"file_name": "H-3", "text": "Uso de contexts de consulta en la recuperaci\u00f3n de informaci\u00f3n RESUMEN La consulta del usuario es un elemento que especifica una necesidad de informaci\u00f3n, pero no es el \u00fanico. Los estudios en la literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretaci\u00f3n de una consulta. Estudios recientes han intentado tener en cuenta los intereses del usuario mediante la creaci\u00f3n de un perfil de usuario. Sin embargo, un \u00fanico perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contexts espec\u00edficos de consulta en lugar de contexts centrados en el usuario, incluido el context alrededor de la consulta y el context dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de inter\u00e9s, mientras que el segundo se refiere a palabras de context dentro de la consulta, lo que resulta particularmente \u00fatil para la selecci\u00f3n de relaciones de t\u00e9rminos relevantes. En este art\u00edculo, ambos tipos de context se integran en un modelo de RI basado en el modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores contextuales aporta mejoras significativas en la efectividad de la recuperaci\u00f3n. 1. INTRODUCCI\u00d3N Las consultas, especialmente las consultas breves, no proporcionan una especificaci\u00f3n completa de la informaci\u00f3n necesaria. Muchos t\u00e9rminos relevantes pueden faltar en las consultas y los t\u00e9rminos incluidos pueden ser ambiguos. Estas cuestiones han sido abordadas en un gran n\u00famero de estudios previos. Sin embargo, en estos estudios se ha asumido generalmente que la consulta es el \u00fanico elemento disponible sobre las necesidades de informaci\u00f3n del usuario. En realidad, la consulta siempre se formula en un context de b\u00fasqueda. Estos factores incluyen, entre muchos otros, el dominio de inter\u00e9s, conocimientos, preferencias, etc. del usuario. Todos estos elementos especifican las 8. CONCLUSIONES Los enfoques tradicionales de RI suelen considerar la consulta como el \u00fanico elemento disponible para satisfacer las necesidades de informaci\u00f3n del usuario. Muchos estudios previos han investigado la integraci\u00f3n de algunos factores contextuales en los modelos de IR, normalmente incorporando un perfil de usuario. De manera similar a algunos estudios anteriores, proponemos modelar dominios tem\u00e1ticos en lugar del usuario. Investigaciones anteriores sobre el context se centraron en factores relacionados con la consulta. En este art\u00edculo mostramos que los factores dentro de la consulta tambi\u00e9n son importantes: ayudan a seleccionar las relaciones de t\u00e9rminos apropiadas para aplicar en la expansi\u00f3n de la consulta. Hemos integrado los factores contextuales anteriores, junto con el modelo de retroalimentaci\u00f3n, en un modelo de lenguaje \u00fanico. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contexts en RI. Este trabajo tambi\u00e9n muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo se puede mejorar a\u00fan m\u00e1s en varios aspectos, incluidos otros m\u00e9todos para extraer relaciones de t\u00e9rminos, integrar m\u00e1s palabras de context en condiciones e identificar dominios de consulta. Tambi\u00e9n ser\u00eda interesante probar el m\u00e9todo en la b\u00fasqueda web utilizando el historial de b\u00fasqueda del usuario.", "keyphrases": ["perfil de usuario", "context espec\u00edfico de consulta", "centrado en el usuario", "dominio de inter\u00e9s", "factor de context", "palabra sentido desambiguo", "informar necesidad", "context de b\u00fasqueda", "conocimiento del dominio", "utilidad del conocimiento genero", "problema del conocimiento ambiguo", "independiente del context", "context informar", "modelo de dominio", "soluci\u00f3n radical", "busqueda de persona en google"]}
{"file_name": "C-28", "text": "PackageBLAST: un servicio de cuadr\u00edcula adaptativo de pol\u00edticas m\u00faltiples para la comparaci\u00f3n de secuencias biol\u00f3gicas * RESUMEN En este art\u00edculo, proponemos un marco de asignaci\u00f3n de tareas adaptativo para realizar b\u00fasquedas BLAST en un entorno de cuadr\u00edcula contra segmentos de bases de datos de secuencias. El marco, llamado PackageBLAST, proporciona una infraestructura para elegir o incorporar estrategias de asignaci\u00f3n de tareas. Adem\u00e1s, proponemos un mecanismo para calcular el peso de ejecuci\u00f3n de los nodos de la red, adaptando la pol\u00edtica de asignaci\u00f3n elegida a la potencia computacional actual de los nodos. Nuestros resultados presentan muy buenas aceleraciones y tambi\u00e9n muestran que ninguna estrategia de asignaci\u00f3n es capaz de lograr los tiempos de ejecuci\u00f3n m\u00e1s bajos para todos los escenarios. 1. INTRODUCCI\u00d3N SW -LSB- 14 -RSB- es un algoritmo exacto que encuentra la mejor alineaci\u00f3n local entre dos secuencias de tama\u00f1o n en tiempo y espacio cuadr\u00e1ticos. Por esta raz\u00f3n, se propusieron heur\u00edsticas como BLAST -LSB- 3 -RSB- para reducir el tiempo de ejecuci\u00f3n. CLS. Apoyado por ACM. La programaci\u00f3n de recursos es uno de los componentes m\u00e1s importantes de un sistema de red. La elecci\u00f3n de los mejores recursos para una aplicaci\u00f3n particular se llama asignaci\u00f3n de tareas, que es un problema NP-Complete. Las aplicaciones Grid no suelen tener altas velocidades de comunicaci\u00f3n y muchas de ellas siguen el modelo maestro/esclavo -LSB- 13 -RSB-. Para programar aplicaciones maestro/esclavo se propusieron muchas pol\u00edticas de asignaci\u00f3n de tareas, como Self Scheduling -LSB- 15 -RSB- y FAC2 -LSB- 8 -RSB-. La elecci\u00f3n de la mejor pol\u00edtica de asignaci\u00f3n depende del patr\u00f3n de acceso a la aplicaci\u00f3n y del entorno en el que se ejecuta -LSB- 13 -RSB-. En este art\u00edculo, proponemos PackageBLAST, un servicio de grilla adaptativo de pol\u00edticas m\u00faltiples para ejecutar b\u00fasquedas BLAST en grillas compuestas por bases de datos gen\u00e9ticas segmentadas. PackageBLAST se ejecuta en Globus 3 -LSB- 4 -RSB- y, hasta ahora, proporciona cinco pol\u00edticas de asignaci\u00f3n. Adem\u00e1s, proponemos un mecanismo adaptativo para asignar pesos a los nodos de la red, teniendo en cuenta su carga de trabajo actual. Hasta donde sabemos, este es el primer servicio grid que ejecuta BLAST con pol\u00edticas de m\u00faltiples tareas con una base de datos segmentada en una plataforma heterog\u00e9nea no dedicada. Este art\u00edculo est\u00e1 organizado de la siguiente forma: La secci\u00f3n 2 presenta el problema de comparaci\u00f3n de secuencias y el algoritmo BLAST. La Secci\u00f3n 3 describe las pol\u00edticas de asignaci\u00f3n de redes. La secci\u00f3n 4 analiza el trabajo relacionado. La Secci\u00f3n 5 presenta el dise\u00f1o de PackageBLAST. Los resultados experimentales se analizan en la secci\u00f3n 6. La secci\u00f3n 7 concluye el art\u00edculo. 4. TRABAJOS RELACIONADOS En primer lugar, se segmenta la base de datos gen\u00e9tica. Luego, las consultas se distribuyen uniformemente entre los nodos. Si el nodo no tiene un fragmento de base de datos, se realiza una copia local. Se propone un m\u00e9todo que asocia fragmentos de datos a nodos, intentando minimizar el n\u00famero de copias. BLAST + + -LSB- 10 -RSB- agrupa m\u00faltiples secuencias para reducir el n\u00famero de accesos a la base de datos. Se utiliza un enfoque maestro/esclavo que asigna las consultas a los esclavos de acuerdo con la pol\u00edtica fija -LRB- secci\u00f3n 3.3 -RRB-. Cada trabajador ejecuta BLAST++ de forma independiente y, finalmente,los resultados son recopilados y combinados por el maestro. GridBlast -LSB- 9 -RSB- es una aplicaci\u00f3n de grilla maestra/esclava que utiliza Globus 2. Distribuye secuencias entre los nodos de la grilla usando dos pol\u00edticas de asignaci\u00f3n: FCFS y minmax. Sin embargo, para utilizar minmax, se debe conocer el tiempo total de ejecuci\u00f3n de cada tarea BLAST. Habiendo decidido qu\u00e9 secuencias comparar\u00e1 cada nodo, GridBlast env\u00eda las secuencias, los archivos ejecutables y toda la base de datos al nodo elegido. Cuando finaliza la b\u00fasqueda, los resultados se compactan y se env\u00edan al maestro. Grid Blast Toolkit -LRB- GBTK -RRB- -LSB- 12 -RSB- es un portal web para ejecutar b\u00fasquedas BLAST en Globus 3. Todas las bases de datos gen\u00e9ticas se colocan est\u00e1ticamente en los nodos del grid -LRB- sin replicaci\u00f3n -RRB-. GBTK es una aplicaci\u00f3n maestro/esclavo que recibe las secuencias y el nombre de la base de datos gen\u00e9tica. Luego verifica si el nodo que contiene la base de datos est\u00e1 disponible. Si el nodo no est\u00e1 disponible, se elige el nodo menos cargado y se copia en \u00e9l la base de datos. La base de datos se replica en los nodos, pero solo una parte se procesa en cada nodo. Figura 2: Mecanismo de segmentaci\u00f3n y distribuci\u00f3n de PackageBLAST. 7. CONCLUSI\u00d3N En este art\u00edculo, propusimos y evaluamos PackageBLAST, un servicio grid adaptable de pol\u00edticas m\u00faltiples para ejecutar b\u00fasquedas BLAST maestro/esclavo. PackageBLAST contiene un marco donde el usuario puede elegir o incorporar pol\u00edticas de asignaci\u00f3n. Tambi\u00e9n definimos una estrategia, PSS, que adapta la pol\u00edtica elegida a un entorno de red heterog\u00e9neo y no dedicado. Los resultados recopilados al ejecutar PackageBLAST con 5 pol\u00edticas de asignaci\u00f3n en un banco de pruebas de grid fueron muy buenos. Para comparar una secuencia de ADN real de 10KBP con la base de datos gen\u00e9tica nr, pudimos reducir el tiempo de ejecuci\u00f3n de 30,88 min a 2,11 min. Adem\u00e1s, demostramos que, en nuestro banco de pruebas, no existe una pol\u00edtica de asignaci\u00f3n que siempre logre el mejor rendimiento y eso hace evidente la importancia de proporcionar m\u00faltiples pol\u00edticas. Adem\u00e1s, demostramos que la introducci\u00f3n del PSS gener\u00f3 muy buenos avances en el desempe\u00f1o de algunas pol\u00edticas. Como trabajo futuro, pretendemos ejecutar PackageBLAST en una red geogr\u00e1ficamente dispersa, para evaluar el impacto de las altas latencias de la red en las pol\u00edticas de asignaci\u00f3n y en PSS. Adem\u00e1s, pretendemos brindar soporte para la sincronizaci\u00f3n de bases de datos gen\u00f3micas y operaciones din\u00e1micas de entrada/salida para esclavos.Todas las bases de datos gen\u00e9ticas se colocan est\u00e1ticamente en los nodos de la grilla -LRB- sin replicaci\u00f3n -RRB-. GBTK es una aplicaci\u00f3n maestro/esclavo que recibe las secuencias y el nombre de la base de datos gen\u00e9tica. Luego verifica si el nodo que contiene la base de datos est\u00e1 disponible. Si el nodo no est\u00e1 disponible, se elige el nodo menos cargado y se copia en \u00e9l la base de datos. La base de datos se replica en los nodos, pero solo una parte se procesa en cada nodo. Figura 2: Mecanismo de segmentaci\u00f3n y distribuci\u00f3n de PackageBLAST. 7. CONCLUSI\u00d3N En este art\u00edculo, propusimos y evaluamos PackageBLAST, un servicio grid adaptable de pol\u00edticas m\u00faltiples para ejecutar b\u00fasquedas BLAST maestro/esclavo. PackageBLAST contiene un marco donde el usuario puede elegir o incorporar pol\u00edticas de asignaci\u00f3n. Tambi\u00e9n definimos una estrategia, PSS, que adapta la pol\u00edtica elegida a un entorno de red heterog\u00e9neo y no dedicado. Los resultados recopilados al ejecutar PackageBLAST con 5 pol\u00edticas de asignaci\u00f3n en un banco de pruebas de grid fueron muy buenos. Para comparar una secuencia de ADN real de 10KBP con la base de datos gen\u00e9tica nr, pudimos reducir el tiempo de ejecuci\u00f3n de 30,88 min a 2,11 min. Adem\u00e1s, demostramos que, en nuestro banco de pruebas, no existe una pol\u00edtica de asignaci\u00f3n que siempre logre el mejor rendimiento y eso hace evidente la importancia de proporcionar m\u00faltiples pol\u00edticas. Adem\u00e1s, demostramos que la introducci\u00f3n del PSS gener\u00f3 muy buenos avances en el desempe\u00f1o de algunas pol\u00edticas. Como trabajo futuro, pretendemos ejecutar PackageBLAST en una red geogr\u00e1ficamente dispersa, para evaluar el impacto de las altas latencias de la red en las pol\u00edticas de asignaci\u00f3n y en PSS. Adem\u00e1s, pretendemos brindar soporte para la sincronizaci\u00f3n de bases de datos gen\u00f3micas y operaciones din\u00e1micas de entrada/salida para esclavos.Todas las bases de datos gen\u00e9ticas se colocan est\u00e1ticamente en los nodos de la grilla -LRB- sin replicaci\u00f3n -RRB-. GBTK es una aplicaci\u00f3n maestro/esclavo que recibe las secuencias y el nombre de la base de datos gen\u00e9tica. Luego verifica si el nodo que contiene la base de datos est\u00e1 disponible. Si el nodo no est\u00e1 disponible, se elige el nodo menos cargado y se copia en \u00e9l la base de datos. La base de datos se replica en los nodos, pero solo una parte se procesa en cada nodo. Figura 2: Mecanismo de segmentaci\u00f3n y distribuci\u00f3n de PackageBLAST. 7. CONCLUSI\u00d3N En este art\u00edculo, propusimos y evaluamos PackageBLAST, un servicio grid adaptable de pol\u00edticas m\u00faltiples para ejecutar b\u00fasquedas BLAST maestro/esclavo. PackageBLAST contiene un marco donde el usuario puede elegir o incorporar pol\u00edticas de asignaci\u00f3n. Tambi\u00e9n definimos una estrategia, PSS, que adapta la pol\u00edtica elegida a un entorno de red heterog\u00e9neo y no dedicado. Los resultados recopilados al ejecutar PackageBLAST con 5 pol\u00edticas de asignaci\u00f3n en un banco de pruebas de grid fueron muy buenos. Para comparar una secuencia de ADN real de 10KBP con la base de datos gen\u00e9tica nr, pudimos reducir el tiempo de ejecuci\u00f3n de 30,88 min a 2,11 min. Adem\u00e1s, demostramos que, en nuestro banco de pruebas, no existe una pol\u00edtica de asignaci\u00f3n que siempre logre el mejor rendimiento y eso hace evidente la importancia de proporcionar m\u00faltiples pol\u00edticas. Adem\u00e1s, demostramos que la introducci\u00f3n del PSS gener\u00f3 muy buenos avances en el desempe\u00f1o de algunas pol\u00edticas. Como trabajo futuro, pretendemos ejecutar PackageBLAST en una red geogr\u00e1ficamente dispersa, para evaluar el impacto de las altas latencias de la red en las pol\u00edticas de asignaci\u00f3n y en PSS. Adem\u00e1s, pretendemos brindar soporte para la sincronizaci\u00f3n de bases de datos gen\u00f3micas y operaciones din\u00e1micas de entrada/salida para esclavos.Demostramos que la introducci\u00f3n del PSS gener\u00f3 muy buenos avances en el desempe\u00f1o de algunas pol\u00edticas. Como trabajo futuro, pretendemos ejecutar PackageBLAST en una red geogr\u00e1ficamente dispersa, para evaluar el impacto de las altas latencias de la red en las pol\u00edticas de asignaci\u00f3n y en PSS. Adem\u00e1s, pretendemos brindar soporte para la sincronizaci\u00f3n de bases de datos gen\u00f3micas y operaciones din\u00e1micas de entrada/salida para esclavos.Demostramos que la introducci\u00f3n del PSS gener\u00f3 muy buenos avances en el desempe\u00f1o de algunas pol\u00edticas. Como trabajo futuro, pretendemos ejecutar PackageBLAST en una red geogr\u00e1ficamente dispersa, para evaluar el impacto de las altas latencias de la red en las pol\u00edticas de asignaci\u00f3n y en PSS. Adem\u00e1s, pretendemos brindar soporte para la sincronizaci\u00f3n de bases de datos gen\u00f3micas y operaciones din\u00e1micas de entrada/salida para esclavos.", "keyphrases": ["comparaci\u00f3n de secuencia biol\u00f3gica", "adaptar el servicio de red multipol\u00edtica", "asignaci\u00f3n de tareas", "b\u00fasqueda de explosi\u00f3n", "explosi\u00f3n de paquete", "bioinformaci\u00f3n", "computaci\u00f3n en red", "biolog\u00eda inform\u00e1tica", "proyecto genoma", "base de datos de gineta segmentada", "plataforma heterog\u00e9nea no dedicada", "entorno de red", "pss", "peso del paquete adaptar autoprogramaci\u00f3n"]}
{"file_name": "C-14", "text": "Estrategia de implementaci\u00f3n de sensores para la detecci\u00f3n de objetivos RESUMEN Para monitorear una regi\u00f3n para el cruce del tr\u00e1fico, se pueden implementar sensores para realizar una detecci\u00f3n colaborativa de objetivos. Una red de sensores de este tipo alcanza un cierto nivel de rendimiento de detecci\u00f3n con un coste de implementaci\u00f3n asociado. Este art\u00edculo aborda este problema proponiendo la exposici\u00f3n de la ruta como una medida de la bondad de una implementaci\u00f3n y presenta un enfoque para la implementaci\u00f3n secuencial en pasos. Ilustra que el costo de implementaci\u00f3n se puede minimizar para lograr el rendimiento de detecci\u00f3n deseado eligiendo adecuadamente la cantidad de sensores implementados en cada paso. 1. INTRODUCCI\u00d3N Una red de este tipo se puede utilizar para monitorear el entorno, detectar, clasificar y localizar eventos espec\u00edficos y rastrear objetivos en una regi\u00f3n espec\u00edfica. El despliegue de redes de sensores var\u00eda seg\u00fan la aplicaci\u00f3n considerada. Puede ser predeterminado cuando el entorno es suficientemente conocido y est\u00e1 bajo control, en cuyo caso los sensores pueden colocarse estrat\u00e9gicamente a mano. Este art\u00edculo investiga estrategias de implementaci\u00f3n para redes de sensores que realizan la detecci\u00f3n de objetivos en una regi\u00f3n de inter\u00e9s. Dado que las observaciones locales realizadas por los sensores dependen de su posici\u00f3n, el rendimiento del algoritmo de detecci\u00f3n es funci\u00f3n del despliegue. Una posible medida de la bondad del despliegue para la detecci\u00f3n de objetivos se denomina exposici\u00f3n de trayectoria. Es una medida de la probabilidad de detectar un objetivo que atraviesa la regi\u00f3n siguiendo un camino determinado. Cuanto mayor sea la exposici\u00f3n de la ruta, mejor ser\u00e1 el despliegue. El conjunto de caminos a considerar puede verse limitado por el entorno. Por ejemplo, si se espera que el objetivo siga una carretera, s\u00f3lo se deben considerar los caminos que consisten en las carreteras. En este estudio, se supone que el despliegue es aleatorio, lo que corresponde a muchas aplicaciones pr\u00e1cticas donde la regi\u00f3n a monitorear no es accesible para la colocaci\u00f3n precisa de sensores. El objetivo de este art\u00edculo es determinar la cantidad de sensores que se implementar\u00e1n para llevar a cabo la detecci\u00f3n de objetivos en una regi\u00f3n de inter\u00e9s. Las compensaciones se encuentran entre el rendimiento de la red, el costo de los sensores implementados y el costo de implementar los sensores. Este art\u00edculo est\u00e1 organizado de la siguiente forma: En la secci\u00f3n 2, se propone una definici\u00f3n de exposici\u00f3n del camino y se desarrolla un m\u00e9todo para evaluar la exposici\u00f3n de un camino determinado. En la secci\u00f3n 3, se formula el problema del despliegue aleatorio y se presentan varias soluciones. El art\u00edculo concluye con la secci\u00f3n 7. 7. CONCLUSI\u00d3N Este art\u00edculo aborda el problema del despliegue de sensores en una regi\u00f3n que debe ser monitoreada para detectar intrusiones en objetivos. Se propone y analiza un mecanismo de colaboraci\u00f3n de sensores para realizar la detecci\u00f3n de objetivos para evaluar la exposici\u00f3n de los caminos a trav\u00e9s de la regi\u00f3n. La exposici\u00f3n m\u00ednima se utiliza como medida de la bondad del despliegue, siendo el objetivo maximizar la exposici\u00f3n del camino menos expuesto en la regi\u00f3n. En el caso de que los sensores se coloquen aleatoriamente en una regi\u00f3n a monitorear,Se desarrolla un mecanismo para el despliegue secuencial en pasos. La estrategia consiste en desplegar un n\u00famero limitado de sensores a la vez hasta conseguir la exposici\u00f3n m\u00ednima deseada. La funci\u00f3n de costo utilizada en este estudio depende de la cantidad de sensores implementados en cada paso y del costo de cada implementaci\u00f3n. Mediante simulaci\u00f3n, se evalu\u00f3 la distribuci\u00f3n de la exposici\u00f3n m\u00ednima obtenida mediante el despliegue aleatorio para un n\u00famero variable de sensores desplegados. Estos resultados se utilizaron para evaluar el costo de implementaci\u00f3n de un n\u00famero variable de sensores implementados en cada paso. Descubrimos que la cantidad \u00f3ptima de sensores implementados en cada paso var\u00eda con el costo relativo asignado a la implementaci\u00f3n y los sensores. Los resultados de este estudio se pueden extender a regiones m\u00e1s grandes con diferentes par\u00e1metros objetivo. La soluci\u00f3n propuesta en este art\u00edculo tambi\u00e9n se puede mejorar considerando la implementaci\u00f3n de un n\u00famero variable de sensores en cada paso y este problema de m\u00faltiples variables requiere m\u00e1s investigaci\u00f3n.", "keyphrases": ["detectar objetivo", "red de sensores", "exposici\u00f3n del camino", "n\u00famero de sensores", "despliegue secuencial", "exposici\u00f3n m\u00ednima", "colocaci\u00f3n aleatoria del sensor", "campo sensor", "objetivo decai"]}
{"file_name": "C-6", "text": "Dise\u00f1o e implementaci\u00f3n de un sistema de gesti\u00f3n de contenido distribuido RESUMEN La convergencia de los avances en tecnolog\u00edas de almacenamiento, codificaci\u00f3n y redes nos ha llevado a un entorno donde se almacenan e intercambian rutinariamente enormes cantidades de contenido multimedia continuo entre dispositivos habilitados para la red. Realizar un seguimiento de -LRB- o gestionar -RRB- dicho contenido sigue siendo un desaf\u00edo debido al gran volumen de datos. El almacenamiento de medios continuos ``en vivo'' -LRB- tales como contenido de TV o radio -RRB- se suma a la complejidad en el sentido de que este contenido no tiene un comienzo o un final bien definido y, por lo tanto, es engorroso de manejar. El almacenamiento en red permite que el contenido que l\u00f3gicamente se considera parte de la misma colecci\u00f3n se distribuya a trav\u00e9s de una red, lo que hace que la tarea de gesti\u00f3n de contenidos sea casi imposible de realizar sin un sistema de gesti\u00f3n de contenidos. En este art\u00edculo presentamos el dise\u00f1o y la implementaci\u00f3n del sistema de gesti\u00f3n de contenidos Spectrum, que se ocupa del contenido multimedia enriquecido de forma eficaz en este entorno. Spectrum tiene una arquitectura modular que permite su aplicaci\u00f3n tanto en escenarios independientes como en varios escenarios en red. Un aspecto \u00fanico de Spectrum es que requiere que se apliquen una pol\u00edtica de retenci\u00f3n -LRB- o m\u00e1s -RRB- a cada contenido almacenado en el sistema. Esto significa que no existen pol\u00edticas de desalojo. El contenido al que ya no se le aplica una pol\u00edtica de retenci\u00f3n simplemente se elimina del sistema. Se pueden aplicar f\u00e1cilmente diferentes pol\u00edticas de retenci\u00f3n al mismo contenido, lo que facilita naturalmente el intercambio sin duplicaciones. Este enfoque tambi\u00e9n permite a Spectrum aplicar f\u00e1cilmente al contenido pol\u00edticas basadas en el tiempo, que son componentes b\u00e1sicos necesarios para lidiar con el almacenamiento de medios continuos en vivo. No solo describimos los detalles de la arquitectura Spectrum sino que tambi\u00e9n brindamos casos de uso t\u00edpicos. 1. INTRODUCCI\u00d3N La manipulaci\u00f3n y gesti\u00f3n de contenidos es y siempre ha sido una de las funciones principales de una computadora. Las aplicaciones inform\u00e1ticas iniciales incluyen formateadores de text y compiladores de programas. Inicialmente, el contenido se gestionaba mediante la interacci\u00f3n expl\u00edcita del usuario mediante el uso de archivos y sistemas de archivos. A medida que la tecnolog\u00eda ha avanzado, tanto los tipos de contenido como la forma en que las personas desean utilizarlo han cambiado enormemente. Nuevos tipos de contenido, como flujos multimedia continuos, se han vuelto comunes debido a la convergencia de avances en tecnolog\u00edas de almacenamiento, codificaci\u00f3n y redes. Otro ejemplo es la combinaci\u00f3n de codificaci\u00f3n y tecnolog\u00eda de redes de banda ancha. Esta combinaci\u00f3n ha permitido a los usuarios acceder y compartir contenido multimedia en redes de \u00e1rea local y remota, actuando la propia red como un enorme dep\u00f3sito de datos. La proliferaci\u00f3n de contenido de alta calidad habilitada por estos avances en tecnolog\u00eda de almacenamiento, codificaci\u00f3n y redes crea la necesidad de nuevas formas de manipular y administrar los datos.El objetivo de nuestro trabajo es el almacenamiento de contenido rico en medios y, en particular, el almacenamiento de contenido multimedia continuo, ya sea en formato preempaquetado o \"en vivo\". \u2022 Si bien esto es cierto para todo tipo de contenido, el almacenamiento de contenido multimedia continuo es especialmente problem\u00e1tico. En primer lugar, el contenido multimedia continuo sigue siendo muy exigente en t\u00e9rminos de recursos de almacenamiento, lo que significa que un enfoque sin pol\u00edticas para almacenarlo no funcionar\u00e1 para todos los sistemas excepto para los m\u00e1s peque\u00f1os. En segundo lugar, el almacenamiento de contenido \"en vivo\", como televisi\u00f3n o radio, es inherentemente problem\u00e1tico ya que estas se\u00f1ales son flujos continuos sin puntos finales. Esto significa que antes de que uno pueda siquiera pensar en gestionar dicho contenido, es necesario abstraerlo y convertirlo en algo que pueda manipularse y gestionarse. . Cuando se trata de medios continuos almacenados, existe la necesidad de gestionar dicho contenido tanto a nivel detallado como agregado. Por ejemplo, un usuario individual de PVR que desee conservar s\u00f3lo los aspectos m\u00e1s destacados de un evento deportivo en particular no deber\u00eda tener que almacenar el contenido perteneciente al evento completo. . Como se indic\u00f3 anteriormente, intentar realizar un seguimiento del contenido en un sistema independiente sin un sistema de gesti\u00f3n de contenidos es muy dif\u00edcil. Sin embargo, cuando los dispositivos de almacenamiento reales se distribuyen a trav\u00e9s de una red, la tarea de realizar un seguimiento del contenido es casi imposible. Este escenario es cada vez m\u00e1s com\u00fan en los sistemas de distribuci\u00f3n de contenidos basados \u200b\u200ben redes y es probable que tambi\u00e9n adquiera importancia en escenarios de redes dom\u00e9sticas. Parecer\u00eda claro entonces que se necesita un sistema de gesti\u00f3n de contenidos que pueda manejar eficientemente contenido rico en medios y al mismo tiempo explotar la capacidad de conexi\u00f3n en red de los dispositivos de almacenamiento. Este sistema deber\u00eda permitir el almacenamiento eficiente y el acceso al contenido a trav\u00e9s de dispositivos de almacenamiento en red heterog\u00e9neos seg\u00fan las preferencias del usuario. El sistema de gesti\u00f3n de contenidos debe traducir las preferencias del usuario en pol\u00edticas apropiadas de almacenamiento de bajo nivel y debe permitir que esas preferencias se expresen con un nivel fino de granularidad -LRB- sin requerirlo en general -RRB-. El sistema de gesti\u00f3n de contenidos debe permitir al usuario manipular y razonar sobre -LRB-, es decir, cambiar la pol\u00edtica de almacenamiento asociada con -RRB- el almacenamiento de -LRB- partes de -RRB- contenido multimedia continuo. Abordar este problema de gesti\u00f3n de contenido distribuido es dif\u00edcil debido a la cantidad de requisitos impuestos al sistema. Por ejemplo :. El sistema de gesti\u00f3n de contenidos debe operar en una gran cantidad de sistemas heterog\u00e9neos. En algunos casos, el sistema puede estar administrando contenido almacenado en un sistema de archivos local, mientras que en otros, el contenido puede estar almacenado en un dispositivo de almacenamiento de red independiente. El administrador de contenido puede ser responsable de implementar las pol\u00edticas que utiliza para hacer referencia al contenido o esa funci\u00f3n puede delegarse a una computadora separada. Se necesita una interfaz de programa de aplicaci\u00f3n -LRB- API -RRB- y protocolos de red asociados para que el sistema de gesti\u00f3n de contenidos proporcione una interfaz uniforme. .El sistema de gesti\u00f3n de contenidos debe ser flexible y poder manejar diferentes requisitos para las pol\u00edticas de gesti\u00f3n de contenidos. Estas pol\u00edticas reflejan qu\u00e9 contenido se debe obtener, cu\u00e1ndo se debe recuperar, cu\u00e1nto tiempo se debe conservar y bajo qu\u00e9 circunstancias se debe descartar. Esto significa que el sistema de gesti\u00f3n de contenidos deber\u00eda permitir que m\u00faltiples aplicaciones hagan referencia a contenidos con un amplio conjunto de pol\u00edticas y que todas deber\u00edan funcionar juntas sin problemas. . El sistema de gesti\u00f3n de contenidos debe poder monitorear las referencias de contenido y utilizar esa informaci\u00f3n para colocar el contenido en la ubicaci\u00f3n correcta de la red para un acceso eficiente a las aplicaciones. . El sistema de gesti\u00f3n de contenido debe manejar la interacci\u00f3n entre la poblaci\u00f3n de contenido impl\u00edcito y expl\u00edcito en el borde de la red. . El sistema de contenidos debe poder gestionar de manera eficiente grandes conjuntos de contenidos, incluidos flujos continuos. Debe poder empaquetar este contenido de tal manera que sea conveniente para los usuarios acceder a \u00e9l. Para abordar estos problemas, hemos dise\u00f1ado e implementado la arquitectura del sistema de gesti\u00f3n de contenidos Spectrum. Permite que m\u00faltiples aplicaciones hagan referencia a contenido utilizando diferentes pol\u00edticas. N\u00f3tese que la arquitectura Spectrum supone la existencia de una red de distribuci\u00f3n de contenidos -LRB- CDN -RRB- que puede facilitar la distribuci\u00f3n eficiente de contenidos -LRB- por ejemplo, la arquitectura PRISM CDN -LSB- 2 -RSB- -RRB-. La secci\u00f3n 2 describe la arquitectura de nuestro sistema de gesti\u00f3n de contenidos. En la Secci\u00f3n 3 describimos nuestra implementaci\u00f3n de la arquitectura Spectrum y ejemplos de su uso. 4. TRABAJOS RELACIONADOS Varios autores han abordado el problema de la gesti\u00f3n de contenidos en redes distribuidas. Gran parte del trabajo se centra en el aspecto de gesti\u00f3n de pol\u00edticas. Por ejemplo, en -LSB- 5 -RSB-, se considera el problema de servir contenido multimedia a trav\u00e9s de servidores distribuidos. El contenido se distribuye entre los recursos del servidor en proporci\u00f3n a la demanda de los usuarios mediante un protocolo de difusi\u00f3n de demanda. El rendimiento del esquema se compara mediante simulaci\u00f3n. En -LSB- 1 -RSB- el contenido se distribuye entre subcach\u00e9s. La base de conocimientos de cach\u00e9 permite emplear pol\u00edticas sofisticadas. La simulaci\u00f3n se utiliza para comparar el esquema propuesto con algoritmos de reemplazo conocidos. Nuestro trabajo se diferencia en que estamos considerando m\u00e1s que los aspectos de gesti\u00f3n de pol\u00edticas del problema. Despu\u00e9s de considerar cuidadosamente la funcionalidad requerida para implementar la gesti\u00f3n de contenido en el entorno de red, hemos dividido el sistema en tres funciones simples, a saber, administrador de contenido, administrador de pol\u00edticas y administrador de almacenamiento. Esto nos ha permitido implementar y experimentar f\u00e1cilmente con un sistema prototipo. Otro trabajo relacionado implica los llamados sistemas de recomendaci\u00f3n de TV que se utilizan en PVR para seleccionar autom\u00e1ticamente contenido para los usuarios, por ejemplo -LSB-6-RSB-. Finalmente, en el entorno CDN comercial los proveedores -LRB-, por ejemploCisco y Netapp -RRB- han desarrollado e implementado productos y herramientas de gesti\u00f3n de contenidos. 5. CONCLUSI\u00d3N Y TRABAJO FUTURO En este art\u00edculo presentamos el dise\u00f1o e implementaci\u00f3n de la arquitectura de gesti\u00f3n de contenidos de Spectrum. Spectrum permite aplicar pol\u00edticas de almacenamiento a grandes vol\u00famenes de contenido para facilitar un almacenamiento eficiente. En concreto, el sistema permite aplicar diferentes pol\u00edticas al mismo contenido sin replicaci\u00f3n. Spectrum tambi\u00e9n puede aplicar pol\u00edticas que tengan en cuenta el tiempo y que se ocupen eficazmente del almacenamiento de contenido multimedia continuo. Finalmente, el dise\u00f1o modular de la arquitectura Spectrum permite realizaciones tanto independientes como distribuidas para que el sistema pueda implementarse en una variedad de aplicaciones. Hay una serie de cuestiones abiertas que requerir\u00e1n trabajo futuro. Algunos de estos problemas incluyen: \u2022 Prevemos que Spectrum pueda administrar contenido en sistemas que van desde grandes CDN hasta dispositivos m\u00e1s peque\u00f1os como TiVO -LSB- 8 -RSB-. Para que estos sistemas m\u00e1s peque\u00f1os sean compatibles con Spectrum, necesitar\u00e1n redes y una API externa. Cuando esa API est\u00e9 disponible, tendremos que descubrir c\u00f3mo encajarla en la arquitectura de Spectrum. \u2022 Spectrum nombra el contenido por URL, pero intencionalmente no hemos definido el formato de las URL de Spectrum, c\u00f3mo se relacionan con el nombre real del contenido o c\u00f3mo se deben presentar los nombres y las URL al usuario. \u2022 En este art\u00edculo nos hemos centrado en la gesti\u00f3n de contenidos para objetos de medios continuos. \u2022 Cualquier proyecto que ayude a permitir que el contenido multimedia se comparta f\u00e1cilmente a trav\u00e9s de Internet tendr\u00e1 que superar obst\u00e1culos legales antes de que pueda lograr una aceptaci\u00f3n generalizada. Adaptar Spectrum para cumplir con los requisitos legales probablemente requerir\u00e1 m\u00e1s trabajo t\u00e9cnico.c\u00f3mo se relacionan con el nombre real del contenido, o c\u00f3mo se deben presentar los nombres y las URL al usuario. \u2022 En este art\u00edculo nos hemos centrado en la gesti\u00f3n de contenidos para objetos de medios continuos. \u2022 Cualquier proyecto que ayude a permitir que el contenido multimedia se comparta f\u00e1cilmente a trav\u00e9s de Internet tendr\u00e1 que superar obst\u00e1culos legales antes de que pueda lograr una aceptaci\u00f3n generalizada. Adaptar Spectrum para cumplir con los requisitos legales probablemente requerir\u00e1 m\u00e1s trabajo t\u00e9cnico.c\u00f3mo se relacionan con el nombre real del contenido, o c\u00f3mo se deben presentar los nombres y las URL al usuario. \u2022 En este art\u00edculo nos hemos centrado en la gesti\u00f3n de contenidos para objetos de medios continuos. \u2022 Cualquier proyecto que ayude a permitir que el contenido multimedia se comparta f\u00e1cilmente a trav\u00e9s de Internet tendr\u00e1 que superar obst\u00e1culos legales antes de que pueda lograr una aceptaci\u00f3n generalizada. Adaptar Spectrum para cumplir con los requisitos legales probablemente requerir\u00e1 m\u00e1s trabajo t\u00e9cnico.", "keyphrases": ["sistema de gesti\u00f3n de contenido del espectro", "almacenamiento continuo de medios", "escenario de red dom\u00e9stica", "interfaz de programa de aplicaci\u00f3n", "red de distribuci\u00f3n de contenidos", "ubicaci\u00f3n uniforme de recursos", "gesti\u00f3n pol\u00edtica", "dvr de habilitaci\u00f3n de red", "sistema de bases de datos de alto rendimiento", "gesti\u00f3n del espectro de nivel de operador"]}
{"file_name": "H-11", "text": "Dise\u00f1o \u00f3ptimo laplaciano para la recuperaci\u00f3n de im\u00e1genes RESUMEN La retroalimentaci\u00f3n de relevancia es una t\u00e9cnica poderosa para mejorar el rendimiento de la recuperaci\u00f3n de im\u00e1genes basada en contenido -LRB- CBIR -RRB-. Solicita al usuario juicios de relevancia sobre las im\u00e1genes recuperadas devueltas por los sistemas CBIR. Luego, el etiquetado del usuario se utiliza para aprender un clasificador para distinguir entre im\u00e1genes relevantes e irrelevantes. Sin embargo, es posible que las im\u00e1genes m\u00e1s encontradas no sean las m\u00e1s informativas. Por lo tanto, el desaf\u00edo es determinar qu\u00e9 im\u00e1genes sin etiquetar ser\u00edan las m\u00e1s informativas -LRB- es decir, mejorar\u00edan m\u00e1s el clasificador -RRB- si fueran etiquetadas y utilizadas como muestras de entrenamiento. En este art\u00edculo, proponemos un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano -LRB-LOD-RRB-, para la recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia. Nuestro algoritmo se basa en un modelo de regresi\u00f3n que minimiza el error de m\u00ednimos cuadrados en las im\u00e1genes medidas -LRB- o etiquetadas -RRB- y simult\u00e1neamente preserva la estructura geom\u00e9trica local del espacio de la imagen. Espec\u00edficamente, asumimos que si dos im\u00e1genes est\u00e1n lo suficientemente cerca entre s\u00ed, entonces sus medidas -LRB- o sus etiquetas -RRB- tambi\u00e9n lo estar\u00e1n. Al construir un gr\u00e1fico vecino m\u00e1s cercano, la estructura geom\u00e9trica del espacio de la imagen se puede describir mediante el gr\u00e1fico laplaciano. Discutimos c\u00f3mo los resultados del campo del dise\u00f1o experimental \u00f3ptimo pueden usarse para guiar nuestra selecci\u00f3n de un subconjunto de im\u00e1genes que nos brinde la mayor cantidad de informaci\u00f3n. Los resultados experimentales en la base de datos Corel sugieren que el enfoque propuesto logra una mayor precisi\u00f3n en la recuperaci\u00f3n de im\u00e1genes de retroalimentaci\u00f3n de relevancia. 1. INTRODUCCI\u00d3N En muchas tareas de aprendizaje autom\u00e1tico y recuperaci\u00f3n de informaci\u00f3n, no faltan datos sin etiquetar, pero las etiquetas son caras. Por lo tanto, el desaf\u00edo es determinar qu\u00e9 muestras sin etiquetar ser\u00edan las m\u00e1s informativas -LRB- es decir, mejorar\u00edan m\u00e1s el clasificador -RRB- si fueran etiquetadas y utilizadas como muestras de entrenamiento. Este problema suele denominarse aprendizaje activo -LSB- 4 -RSB-. Muchas aplicaciones del mundo real se pueden integrar en un marco de aprendizaje activo. En particular, consideramos el problema de la recuperaci\u00f3n de im\u00e1genes basada en contenido impulsada por retroalimentaci\u00f3n de relevancia -LRB- CBIR -RRB- -LSB- 13 -RSB-. La recuperaci\u00f3n de im\u00e1genes basada en contenido ha atra\u00eddo importantes intereses en la \u00faltima d\u00e9cada -LSB- 13 -RSB-. Est\u00e1 motivado por el r\u00e1pido crecimiento de las bases de datos de im\u00e1genes digitales que, a su vez, requieren esquemas de b\u00fasqueda eficientes. En lugar de describir una imagen utilizando text, en estos sistemas una consulta de imagen se describe utilizando una o m\u00e1s im\u00e1genes de ejemplo. Las caracter\u00edsticas visuales de bajo nivel -LRB- color, textura, forma, etc. -RRB- se extraen autom\u00e1ticamente para representar las im\u00e1genes. Para reducir la brecha sem\u00e1ntica, se introduce retroalimentaci\u00f3n de relevancia en CBIR -LSB- 12 -RSB-. En muchos de los sistemas CBIR actuales basados \u200b\u200ben retroalimentaci\u00f3n de relevancia, se requiere que el usuario proporcione sus juicios de relevancia sobre las im\u00e1genes principales devueltas por el sistema.Luego, las im\u00e1genes etiquetadas se utilizan para entrenar un clasificador para separar las im\u00e1genes que coinciden con el concepto de consulta de aquellas que no. Sin embargo, en general, las im\u00e1genes m\u00e1s devueltas pueden no ser las m\u00e1s informativas. En el peor de los casos, todas las im\u00e1genes principales etiquetadas por el usuario pueden ser positivas y, por lo tanto, las t\u00e9cnicas de clasificaci\u00f3n est\u00e1ndar no se pueden aplicar debido a la falta de ejemplos negativos. A diferencia de los problemas de clasificaci\u00f3n est\u00e1ndar en los que las muestras etiquetadas est\u00e1n predeterminadas, en la recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia el sistema puede seleccionar activamente las im\u00e1genes para etiquetar. De este modo, el aprendizaje activo puede introducirse de forma natural en la recuperaci\u00f3n de im\u00e1genes. A pesar de muchas t\u00e9cnicas de aprendizaje activo existentes, Support Vector Machine -LRB- SVM -RRB- aprendizaje activo -LSB- 14 -RSB- y aprendizaje activo basado en regresi\u00f3n -LSB- 1 -RSB- han recibido el mayor inter\u00e9s. La principal desventaja del aprendizaje activo SVM es que el l\u00edmite estimado puede no ser lo suficientemente preciso. Adem\u00e1s, no podr\u00e1 aplicarse al inicio de la recuperaci\u00f3n cuando no haya im\u00e1genes etiquetadas. Algunos otros algoritmos de aprendizaje activo basados \u200b\u200ben SVM se pueden encontrar en -LSB- 7 -RSB-, -LSB- 9 -RSB-. En estad\u00edstica, el problema de seleccionar muestras para etiquetar se suele denominar dise\u00f1o experimental. La muestra x se denomina experimento y su etiqueta y se denomina medici\u00f3n. El estudio del dise\u00f1o experimental \u00f3ptimo -LRB- OED -RRB- -LSB- 1 -RSB- se ocupa del dise\u00f1o de experimentos que se espera minimicen las varianzas de un modelo parametrizado. La intenci\u00f3n del dise\u00f1o experimental \u00f3ptimo suele ser maximizar la confianza en un modelo determinado, minimizar las variaciones de los par\u00e1metros para la identificaci\u00f3n del sistema o minimizar la variaci\u00f3n de salida del modelo. Los enfoques de dise\u00f1o experimental cl\u00e1sicos incluyen el dise\u00f1o \u00f3ptimo A, el dise\u00f1o \u00f3ptimo D y el dise\u00f1o \u00f3ptimo E. Todos estos enfoques se basan en un modelo de regresi\u00f3n de m\u00ednimos cuadrados. En comparaci\u00f3n con los algoritmos de aprendizaje activo basados \u200b\u200ben SVM, los enfoques de dise\u00f1o experimental son mucho m\u00e1s eficientes en el c\u00e1lculo. Sin embargo, este tipo de enfoques solo toma en cuenta los datos medidos -LRB- o etiquetados -RRB- en su funci\u00f3n objetivo, mientras que los datos no medidos -LRB- o no etiquetados -RRB- se ignoran. Benefici\u00e1ndose de los avances recientes en el dise\u00f1o experimental \u00f3ptimo y el aprendizaje semisupervisado, en este art\u00edculo proponemos un novedoso algoritmo de aprendizaje activo para la recuperaci\u00f3n de im\u00e1genes, llamado Dise\u00f1o \u00d3ptimo Laplaciano -LRB-LOD-RRB-. A diferencia de los m\u00e9todos de dise\u00f1o experimental tradicionales cuyas funciones de p\u00e9rdida solo se definen en los puntos medidos, la funci\u00f3n de p\u00e9rdida de nuestro algoritmo LOD propuesto se define tanto en los puntos medidos como en los no medidos. Espec\u00edficamente, introducimos un regularizador que preserva la localidad en la funci\u00f3n de p\u00e9rdida est\u00e1ndar basada en el error de m\u00ednimos cuadrados. La nueva funci\u00f3n de p\u00e9rdida tiene como objetivo encontrar un clasificador que sea localmente lo m\u00e1s fluido posible. En otras palabras, si dos puntos est\u00e1n suficientemente cerca entre s\u00ed en el espacio de entrada, se espera que compartan la misma etiqueta.Una vez definida la funci\u00f3n de p\u00e9rdida, podemos seleccionar los puntos de datos m\u00e1s informativos que se presentan al usuario para su etiquetado. Ser\u00eda importante tener en cuenta que las im\u00e1genes m\u00e1s informativas pueden no ser las im\u00e1genes m\u00e1s devueltas. El resto del documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2, proporcionamos una breve descripci\u00f3n del trabajo relacionado. Nuestro algoritmo de dise\u00f1o \u00f3ptimo laplaciano propuesto se presenta en la Secci\u00f3n 3. En la Secci\u00f3n 4, comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Podemos seleccionar los puntos de datos m\u00e1s informativos que se presentan al usuario para su etiquetado. Ser\u00eda importante tener en cuenta que las im\u00e1genes m\u00e1s informativas pueden no ser las im\u00e1genes m\u00e1s devueltas. El resto del documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2, proporcionamos una breve descripci\u00f3n del trabajo relacionado. Nuestro algoritmo de dise\u00f1o \u00f3ptimo laplaciano propuesto se presenta en la Secci\u00f3n 3. En la Secci\u00f3n 4, comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Podemos seleccionar los puntos de datos m\u00e1s informativos que se presentan al usuario para su etiquetado. Ser\u00eda importante tener en cuenta que las im\u00e1genes m\u00e1s informativas pueden no ser las im\u00e1genes m\u00e1s devueltas. El resto del documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2, proporcionamos una breve descripci\u00f3n del trabajo relacionado. Nuestro algoritmo de dise\u00f1o \u00f3ptimo laplaciano propuesto se presenta en la Secci\u00f3n 3. En la Secci\u00f3n 4, comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.El resto del documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2, proporcionamos una breve descripci\u00f3n del trabajo relacionado. Nuestro algoritmo de dise\u00f1o \u00f3ptimo laplaciano propuesto se presenta en la Secci\u00f3n 3. En la Secci\u00f3n 4, comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.El resto del documento est\u00e1 organizado de la siguiente manera. En la Secci\u00f3n 2, proporcionamos una breve descripci\u00f3n del trabajo relacionado. Nuestro algoritmo de dise\u00f1o \u00f3ptimo laplaciano propuesto se presenta en la Secci\u00f3n 3. En la Secci\u00f3n 4, comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Comparamos nuestro algoritmo con los algoritmos m\u00e1s modernos y presentamos los resultados experimentales sobre la recuperaci\u00f3n de im\u00e1genes. Finalmente, proporcionamos algunos comentarios finales y sugerencias para trabajos futuros en la Secci\u00f3n 5. 2. TRABAJO RELACIONADO Dado que nuestro algoritmo propuesto se basa en un marco de regresi\u00f3n. El trabajo m\u00e1s relacionado es el dise\u00f1o experimental \u00f3ptimo -LSB- 1 -RSB-, incluyendo A-Optimal Design, D-Optimal Design y EOptimal Design. En esta secci\u00f3n, damos una breve descripci\u00f3n de estos enfoques. 2.1 El problema del aprendizaje activo El problema gen\u00e9rico del aprendizaje activo es el siguiente. En otras palabras, los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.los puntos zi -LRB- i = 1, \u00b7 \u00b7 \u00b7, k -RRB- pueden mejorar m\u00e1s el clasificador si se etiquetan y utilizan como puntos de entrenamiento. 2.2 Dise\u00f1o Experimental \u00d3ptimo Consideramos un modelo de regresi\u00f3n lineal. Diferentes observaciones tienen errores que son independientes, pero con varianzas iguales \u03c32. Por lo tanto, la estimaci\u00f3n de m\u00e1xima verosimilitud para el vector de peso, \u02c6w, es la que minimiza el error de suma al cuadrado. Las tres medidas escalares m\u00e1s comunes del tama\u00f1o de la matriz de covarianza de par\u00e1metros en el dise\u00f1o experimental \u00f3ptimo son: \u2022 Dise\u00f1o D-\u00f3ptimo: determinante de Hsse . \u2022 Dise\u00f1o A-\u00f3ptimo: traza de Hsse. \u2022 Dise\u00f1o E-\u00f3ptimo: valor propio m\u00e1ximo de Hsse. Dado que el c\u00e1lculo del determinante y los valores propios de una matriz es mucho m\u00e1s costoso que el c\u00e1lculo de la traza de la matriz, el dise\u00f1o A-\u00f3ptimo es m\u00e1s eficiente que los otros dos. Algunos trabajos recientes sobre dise\u00f1o experimental se pueden encontrar en -LSB- 6 -RSB-, -LSB- 16 -RSB-. 7. CONCLUSIONES Y TRABAJO FUTURO Este art\u00edculo describe un novedoso algoritmo de aprendizaje activo, llamado Dise\u00f1o \u00d3ptimo Laplaciano, para permitir una recuperaci\u00f3n de im\u00e1genes con retroalimentaci\u00f3n de relevancia m\u00e1s efectiva. Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.Nuestro algoritmo se basa en una funci\u00f3n objetivo que simult\u00e1neamente minimiza el error emp\u00edrico y preserva la estructura geom\u00e9trica local del espacio de datos. Utilizando t\u00e9cnicas de dise\u00f1o experimental, nuestro algoritmo encuentra las im\u00e1genes m\u00e1s informativas para etiquetar. Estas im\u00e1genes etiquetadas y las im\u00e1genes sin etiquetar en la base de datos se utilizan para aprender un clasificador. Los resultados experimentales en la base de datos Corel muestran que tanto el aprendizaje activo como el aprendizaje semisupervisado pueden mejorar significativamente el rendimiento de la recuperaci\u00f3n. En este art\u00edculo, consideramos el problema de recuperaci\u00f3n de im\u00e1genes en datos de im\u00e1genes peque\u00f1as, est\u00e1ticas y de dominio cerrado. Para la b\u00fasqueda de im\u00e1genes web, es posible recopilar una gran cantidad de informaci\u00f3n sobre los clics del usuario. Esta informaci\u00f3n se puede utilizar de forma natural para construir el gr\u00e1fico de afinidad en nuestro algoritmo.", "keyphrases": ["retroalimentaci\u00f3n relevante", "imagen representa", "recuperaci\u00f3n de im\u00e1genes de contentbas", "aprendizaje activo", "modelo de regresi\u00f3n de m\u00ednimos cuadrados", "dise\u00f1o de experimento \u00f3ptimo", "imagen de retorno superior", "tasa precisa", "estructura geom\u00e9trica intr\u00ednseca", "reconocer patten", "etiqueta"]}
{"file_name": "J-27", "text": "Aprendiendo de la preferencia revelada RESUMEN Una secuencia de precios y demandas es racionalizable si existe una funci\u00f3n de utilidad c\u00f3ncava, continua y mon\u00f3tona tal que las demandas sean los maximizadores de la funci\u00f3n de utilidad sobre el conjunto presupuestario correspondiente al precio. Afriat -LSB- 1 -RSB- present\u00f3 condiciones necesarias y suficientes para que una secuencia finita fuera racionalizable. Varian -LSB- 20 -RSB- y posteriormente Blundell et al. -LSB- 3, 4 -RSB- continu\u00f3 esta l\u00ednea de trabajo estudiando m\u00e9todos no param\u00e9tricos para pronosticar la demanda. Sus resultados caracterizan esencialmente la capacidad de aprendizaje de clases degeneradas de funciones de demanda y, por lo tanto, no llegan a dar un grado general de confianza en el pron\u00f3stico. El presente art\u00edculo complementa esta l\u00ednea de investigaci\u00f3n al introducir un modelo estad\u00edstico y una medida de complejidad a trav\u00e9s de la cual podemos estudiar la capacidad de aprendizaje de clases de funciones de demanda y derivar un grado de confianza en los pron\u00f3sticos. Nuestros resultados muestran que la clase de todas las funciones de demanda tiene una complejidad ilimitada y, por lo tanto, no se puede aprender, pero que existen clases interesantes y potencialmente \u00fatiles que se pueden aprender a partir de muestras finitas. Tambi\u00e9n presentamos un algoritmo de aprendizaje que es una adaptaci\u00f3n de una nueva demostraci\u00f3n del teorema de Afriat debida a Teo y Vohra -LSB- 17 -RSB-. 1. INTRODUCCI\u00d3N La relaci\u00f3n de preferencia es, por tanto, el factor clave para comprender el comportamiento del consumidor. Uno de los supuestos comunes en esta teor\u00eda es que la relaci\u00f3n de preferencia est\u00e1 representada por una funci\u00f3n de utilidad y que los agentes se esfuerzan por maximizar su utilidad dada una restricci\u00f3n presupuestaria. Este patr\u00f3n de comportamiento es la esencia de la oferta y la demanda, los equilibrios generales y otros aspectos de la teor\u00eda del consumidor. Adem\u00e1s, como explicamos en la secci\u00f3n 2, las observaciones b\u00e1sicas sobre el comportamiento de la demanda del mercado sugieren que las funciones de utilidad son mon\u00f3tonas y c\u00f3ncavas. Esto nos lleva a la pregunta, planteada por primera vez por Samuelson -LSB- 18 -RSB-, \u00bfhasta qu\u00e9 punto es refutable esta teor\u00eda? Dadas las observaciones de precio y demanda, \u00bfbajo qu\u00e9 circunstancias podemos concluir que los datos son consistentes con el comportamiento de un agente maximizador de utilidad equipado con una funci\u00f3n de utilidad c\u00f3ncava mon\u00f3tona y sujeto a una restricci\u00f3n presupuestaria? Samuelson dio una condici\u00f3n necesaria pero insuficiente sobre la preferencia subyacente conocida como el axioma d\u00e9bil de la preferencia revelada. Uzawa -LSB- 16 -RSB- y Mas-Colell -LSB- 10, 11 -RSB- introdujeron una noci\u00f3n de ingreso-Lipschitz y demostraron que las funciones de demanda con esta propiedad son racionalizables. Estas propiedades no requieren ning\u00fan supuesto param\u00e9trico y son t\u00e9cnicamente refutables, pero suponen el conocimiento de toda la funci\u00f3n de demanda y dependen en gran medida de las propiedades diferenciales de las funciones de demanda. Por tanto, se necesita una cantidad infinita de informaci\u00f3n para refutar la teor\u00eda. A menudo ocurre que, adem\u00e1s de las observaciones de la demanda, hay informaci\u00f3n adicional sobre el sistema y es sensato hacer suposiciones param\u00e9tricas, a saber,estipular alguna forma funcional de utilidad. La coherencia con la maximizaci\u00f3n de la utilidad depender\u00eda entonces de fijar los par\u00e1metros de la funci\u00f3n de utilidad para que sean coherentes con las observaciones y con un conjunto de ecuaciones llamadas ecuaciones de Slutski. Si tales par\u00e1metros existen, concluimos que la forma de utilidad estipulada es consistente con las observaciones. Este enfoque es \u00fatil cuando hay razones para hacer estas estipulaciones; proporciona una funci\u00f3n de utilidad expl\u00edcita que puede usarse para hacer pron\u00f3sticos precisos sobre la demanda de precios no observados. La desventaja de este enfoque es que los datos de la vida real a menudo son inconsistentes con las formas funcionales convenientes. Adem\u00e1s, si las observaciones son inconsistentes, no est\u00e1 claro si se trata de una refutaci\u00f3n de la forma funcional estipulada o de una maximizaci\u00f3n de la utilidad. Pregunta cu\u00e1ndo se puede determinar que un conjunto finito de observaciones es consistente con la maximizaci\u00f3n de la utilidad sin hacer suposiciones param\u00e9tricas. Muestra que la racionalizabilidad de un conjunto finito de observaciones es equivalente al axioma fuerte de la preferencia revelada. Richter -LSB- 15 -RSB- muestra que el axioma fuerte de preferencia revelada es equivalente a la racionalizabilidad mediante una funci\u00f3n de utilidad mon\u00f3tona estrictamente c\u00f3ncava. Afriat -LSB- 1 -RSB- proporciona otro conjunto de condiciones de racionalizaci\u00f3n que las observaciones deben satisfacer. Varian -LSB- 20 -RSB- introduce el axioma generalizado de preferencia revelada -LRB- GARP -RRB-, una forma equivalente de la condici\u00f3n de consistencia de Afriat que es m\u00e1s f\u00e1cil de verificar computacionalmente. Afriat -LSB- 1 -RSB- demostr\u00f3 su teorema mediante la construcci\u00f3n expl\u00edcita de una funci\u00f3n de utilidad que atestigua la coherencia. Varian -LSB- 20 -RSB- dio un paso m\u00e1s y pas\u00f3 de la coherencia a la previsi\u00f3n. El algoritmo de pron\u00f3stico de Varian b\u00e1sicamente descarta los paquetes que se revelan inferiores a los paquetes observados y encuentra un paquete del conjunto restante que, junto con las observaciones, es consistente con GARP. Adem\u00e1s, introduce la \"m\u00e9trica monetaria\" de Samuelson como una funci\u00f3n de utilidad can\u00f3nica y proporciona funciones de utilidad envolvente superior e inferior para la m\u00e9trica monetaria. Knoblauch -LSB- 9 -RSB- muestra que estas envolventes se pueden calcular de manera eficiente. Blundell et al. presentan un enfoque diferente. -LSB- 3, 4 -RSB-. Estos art\u00edculos presentan un modelo en el que un agente observa los precios y las curvas de Engel para estos precios. Esto mejora los l\u00edmites originales de Varian, aunque la idea b\u00e1sica sigue siendo descartar las demandas que se revelan inferiores. Este modelo es en cierto sentido un h\u00edbrido entre los enfoques de Mas-Colell y Afriat. El primero requiere informaci\u00f3n completa para todos los precios, el segundo para un n\u00famero finito de precios. Por otro lado, el enfoque adoptado por Blundell et al. requiere informaci\u00f3n completa s\u00f3lo sobre un n\u00famero finito de trayectorias de precios. Diferentes segmentos de la poblaci\u00f3n enfrentan los mismos precios con diferentes presupuestos, y por mucho que los datos agregados puedan atestiguar sobre las preferencias individuales,Muestre c\u00f3mo var\u00eda la demanda con el presupuesto. Aplicando m\u00e9todos estad\u00edsticos no param\u00e9tricos, reconstruyen una trayectoria a partir de las demandas observadas de diferentes segmentos y la utilizan para obtener l\u00edmites m\u00e1s estrictos. Lo m\u00e1s probable es que ambos m\u00e9todos proporcionen un buen pron\u00f3stico para una funci\u00f3n de demanda fija despu\u00e9s de un n\u00famero suficiente de observaciones, suponiendo que estuvieran distribuidas de manera razonable. Sin embargo, estos m\u00e9todos no consideran la complejidad de las funciones de demanda y no utilizan ning\u00fan modelo probabil\u00edstico de las observaciones. Por lo tanto, no pueden proporcionar ninguna estimaci\u00f3n del n\u00famero de observaciones que ser\u00edan suficientes para un buen pron\u00f3stico o del grado de confianza en dicho pron\u00f3stico. En este art\u00edculo examinamos la viabilidad de pronosticar la demanda con un alto grado de confianza utilizando las condiciones de Afriat. Formulamos la pregunta en t\u00e9rminos de si la clase de funciones de demanda derivadas de utilidades c\u00f3ncavas mon\u00f3tonas se puede aprender de manera eficiente mediante PAC. Nuestro primer resultado es negativo. Demostramos, al calcular la dimensi\u00f3n de destrucci\u00f3n de grasa, que sin ning\u00fan supuesto previo, el conjunto de todas las funciones de demanda inducidas por funciones de utilidad c\u00f3ncavas mon\u00f3tonas es demasiado rico para poder aprenderlo de manera eficiente mediante PAC. Sin embargo, bajo algunos supuestos previos sobre el conjunto de funciones de demanda, mostramos que la dimensi\u00f3n de destrucci\u00f3n de grasa es finita y, por lo tanto, los conjuntos correspondientes se pueden aprender mediante PAC. En la secci\u00f3n 2 analizamos brevemente los supuestos b\u00e1sicos de la teor\u00eda de la demanda y sus implicaciones. En la secci\u00f3n 3 presentamos una nueva prueba del teorema de Afriat que incorpora un algoritmo para generar eficientemente una funci\u00f3n de pron\u00f3stico debido a Teo y Vohra -LSB- 17 -RSB-. Mostramos que este algoritmo es computacionalmente eficiente y puede usarse como algoritmo de aprendizaje. En la secci\u00f3n 4 damos una breve introducci\u00f3n al aprendizaje de PAC que incluye varias modificaciones para aprender funciones con valores vectoriales reales. Tambi\u00e9n dibujamos resultados en los l\u00edmites superiores. En la secci\u00f3n 5 estudiamos la capacidad de aprendizaje de las funciones de demanda y calculamos directamente la dimensi\u00f3n destructiva de la clase de todas las funciones de demanda y una clase de funciones de demanda de Lipschitz de ingreso con una constante de Lipschitz de ingreso global acotada.Formulamos la pregunta en t\u00e9rminos de si la clase de funciones de demanda derivadas de utilidades c\u00f3ncavas mon\u00f3tonas se puede aprender de manera eficiente mediante PAC. Nuestro primer resultado es negativo. Demostramos, al calcular la dimensi\u00f3n de destrucci\u00f3n de grasa, que sin ning\u00fan supuesto previo, el conjunto de todas las funciones de demanda inducidas por funciones de utilidad c\u00f3ncavas mon\u00f3tonas es demasiado rico para poder aprenderlo de manera eficiente mediante PAC. Sin embargo, bajo algunos supuestos previos sobre el conjunto de funciones de demanda, mostramos que la dimensi\u00f3n de destrucci\u00f3n de grasa es finita y, por lo tanto, los conjuntos correspondientes se pueden aprender mediante PAC. En la secci\u00f3n 2 analizamos brevemente los supuestos b\u00e1sicos de la teor\u00eda de la demanda y sus implicaciones. En la secci\u00f3n 3 presentamos una nueva prueba del teorema de Afriat que incorpora un algoritmo para generar eficientemente una funci\u00f3n de pron\u00f3stico debido a Teo y Vohra -LSB- 17 -RSB-. Mostramos que este algoritmo es computacionalmente eficiente y puede usarse como algoritmo de aprendizaje. En la secci\u00f3n 4 damos una breve introducci\u00f3n al aprendizaje de PAC que incluye varias modificaciones para aprender funciones con valores vectoriales reales. Tambi\u00e9n dibujamos resultados en l\u00edmites superiores. En la secci\u00f3n 5 estudiamos la capacidad de aprendizaje de las funciones de demanda y calculamos directamente la dimensi\u00f3n destructiva de la clase de todas las funciones de demanda y una clase de funciones de demanda de Lipschitz de ingreso con una constante de Lipschitz de ingreso global acotada.Formulamos la pregunta en t\u00e9rminos de si la clase de funciones de demanda derivadas de utilidades c\u00f3ncavas mon\u00f3tonas se puede aprender de manera eficiente mediante PAC. Nuestro primer resultado es negativo. Demostramos, al calcular la dimensi\u00f3n de destrucci\u00f3n de grasa, que sin ning\u00fan supuesto previo, el conjunto de todas las funciones de demanda inducidas por funciones de utilidad c\u00f3ncavas mon\u00f3tonas es demasiado rico para poder aprenderlo de manera eficiente mediante PAC. Sin embargo, bajo algunos supuestos previos sobre el conjunto de funciones de demanda, mostramos que la dimensi\u00f3n de destrucci\u00f3n de grasa es finita y, por lo tanto, los conjuntos correspondientes se pueden aprender mediante PAC. En la secci\u00f3n 2 analizamos brevemente los supuestos b\u00e1sicos de la teor\u00eda de la demanda y sus implicaciones. En la secci\u00f3n 3 presentamos una nueva prueba del teorema de Afriat que incorpora un algoritmo para generar eficientemente una funci\u00f3n de pron\u00f3stico debido a Teo y Vohra -LSB- 17 -RSB-. Mostramos que este algoritmo es computacionalmente eficiente y puede usarse como algoritmo de aprendizaje. En la secci\u00f3n 4 damos una breve introducci\u00f3n al aprendizaje de PAC que incluye varias modificaciones para aprender funciones con valores vectoriales reales. Tambi\u00e9n dibujamos resultados en l\u00edmites superiores. En la secci\u00f3n 5 estudiamos la capacidad de aprendizaje de las funciones de demanda y calculamos directamente la dimensi\u00f3n destructiva de la clase de todas las funciones de demanda y una clase de funciones de demanda de Lipschitz de ingreso con una constante de Lipschitz de ingreso global acotada.", "keyphrases": ["aprender de revelar preferir", "problema complejo", "pron\u00f3stico", "probablemente aproximadamente correcto", "funci\u00f3n \u00fatil c\u00f3ncava mon\u00f3tona", "funci\u00f3n de demanda", "racionalizar", "conjunto finito de observaciones", "incom-lipschitz", "dimensiones de grasa destrozada"]}
{"file_name": "C-18", "text": "Un an\u00e1lisis inicial y una presentaci\u00f3n de malware que exhibe un comportamiento similar al de un enjambre RESUMEN Se observ\u00f3 que Slammer, que actualmente es el gusano inform\u00e1tico m\u00e1s r\u00e1pido registrado en la historia, infecta el 90 por ciento de todos los hosts vulnerables de Internet en 10 minutos. Aunque la acci\u00f3n principal que realiza el gusano Slammer es una replicaci\u00f3n relativamente sencilla de s\u00ed mismo, a\u00fan as\u00ed se propaga tan r\u00e1pidamente que la respuesta humana fue ineficaz. La mayor\u00eda de las estrategias de contramedidas propuestas se basan principalmente en algoritmos de detecci\u00f3n y limitaci\u00f3n de tasas. Sin embargo, se est\u00e1n dise\u00f1ando y desarrollando estrategias de este tipo para contener eficazmente gusanos cuyo comportamiento es similar al de Slammer. En nuestro trabajo, planteamos la hip\u00f3tesis de que los gusanos de la pr\u00f3xima generaci\u00f3n ser\u00e1n radicalmente diferentes y que, potencialmente, dichas t\u00e9cnicas resultar\u00e1n ineficaces. Espec\u00edficamente, proponemos estudiar una nueva generaci\u00f3n de gusanos llamados ''Swarm Worms'', cuyo comportamiento se basa en el concepto de ''inteligencia emergente''. La Inteligencia Emergente es el comportamiento de sistemas, muy parecido a los sistemas biol\u00f3gicos como las hormigas o las abejas, donde interacciones locales simples de miembros aut\u00f3nomos, con acciones primitivas simples, dan lugar a un comportamiento global complejo e inteligente. En este manuscrito presentaremos los principios b\u00e1sicos detr\u00e1s de la idea de '' Gusanos de enjambre '', as\u00ed como la estructura b\u00e1sica requerida para ser considerado un '' Gusano de enjambre ''. Adem\u00e1s, presentaremos resultados preliminares sobre las velocidades de propagaci\u00f3n de uno de esos gusanos enjambre, llamado gusano ZachiK. Demostraremos que ZachiK es capaz de propagarse a un ritmo 2 \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que gusanos similares sin capacidad de enjambre. 1. INTRODUCCI\u00d3N Y TRABAJOS ANTERIORES En las primeras horas de la ma\u00f1ana -LRB- 05:30 GMT -RRB- del 25 de enero de 2003 el gusano inform\u00e1tico m\u00e1s r\u00e1pido de la historia comenz\u00f3 a propagarse por Internet. Desde Slammer, los investigadores han explorado el comportamiento de los gusanos que se propagan r\u00e1pidamente y han dise\u00f1ado estrategias de contramedida basadas principalmente en algoritmos de limitaci\u00f3n y detecci\u00f3n de velocidad. Por ejemplo, Zou, et al., -LSB- 2 -RSB-, propusieron un esquema en el que se utiliza un filtro de Kalman para detectar la propagaci\u00f3n temprana de un gusano. Es decir, se est\u00e1n dise\u00f1ando y desarrollando sistemas para contener eficazmente gusanos cuyo comportamiento es similar al de Slammer. En el trabajo descrito aqu\u00ed, planteamos la hip\u00f3tesis de que los gusanos de la pr\u00f3xima generaci\u00f3n ser\u00e1n diferentes y, por lo tanto, dichas t\u00e9cnicas pueden tener algunas limitaciones importantes. Espec\u00edficamente, proponemos estudiar una nueva generaci\u00f3n de gusanos llamados ''Swarm Worms'', cuyo comportamiento se basa en el concepto de ''inteligencia emergente''. El concepto de inteligencia emergente se estudi\u00f3 por primera vez en asociaci\u00f3n con los sistemas biol\u00f3gicos. En tales estudios, los primeros investigadores descubrieron una variedad de comportamientos interesantes de insectos o animales en la naturaleza. Una bandada de p\u00e1jaros surca el cielo. En general,Este tipo de movimiento agregado se ha denominado \"comportamiento de enjambre\". '' Los bi\u00f3logos e inform\u00e1ticos en el campo de la inteligencia artificial han estudiado estos enjambres biol\u00f3gicos e intentaron crear modelos que expliquen c\u00f3mo los elementos de un enjambre interact\u00faan, logran objetivos y evolucionan. Los conceptos b\u00e1sicos que se han desarrollado durante la \u00faltima d\u00e9cada para explicar los \"enjambres y el comportamiento de los enjambres\" incluyen cuatro componentes b\u00e1sicos. Estos son: 1. Simplicidad de l\u00f3gica y acciones: un enjambre est\u00e1 compuesto por N agentes cuya inteligencia es limitada. Los agentes del enjambre utilizan reglas locales simples para gobernar sus acciones. Algunos modelos llamaron a esto acciones o comportamientos primitivos; 2. Mecanismos de comunicaci\u00f3n local: los agentes interact\u00faan con otros miembros del enjambre a trav\u00e9s de mecanismos de comunicaci\u00f3n \"locales\" simples. Por ejemplo, un p\u00e1jaro en una bandada detecta la posici\u00f3n del p\u00e1jaro adyacente y aplica una regla simple de evitar y seguir. 3. 4. ''Inteligencia emergente'': El comportamiento agregado de agentes aut\u00f3nomos da como resultado comportamientos ''inteligentes'' complejos; incluida la autoorganizaci\u00f3n ''. Para comprender completamente el comportamiento de tales enjambres es necesario construir un modelo que explique el comportamiento de lo que llamaremos gusanos gen\u00e9ricos. Este modelo, que ampl\u00eda el trabajo de Weaver -LSB- 5 -RSB-, se presenta aqu\u00ed en el apartado 2. Adem\u00e1s, pretendemos ampliar dicho modelo de tal forma que explique claramente los comportamientos de esta nueva clase de gusanos potencialmente peligrosos. llamados gusanos de enjambre. Los gusanos de enjambre se comportan de manera muy parecida a los enjambres biol\u00f3gicos y exhiben un alto grado de aprendizaje, comunicaci\u00f3n e inteligencia distribuida. Estos gusanos de enjambre son potencialmente m\u00e1s da\u00f1inos que sus hom\u00f3logos gen\u00e9ricos similares. Espec\u00edficamente, se cre\u00f3 la primera instancia, hasta donde sabemos, de un gusano de aprendizaje de este tipo, llamado ZachiK. ZachiK es un gusano enjambre sencillo para descifrar contrase\u00f1as que incorpora diferentes estrategias de aprendizaje e intercambio de informaci\u00f3n. Un gusano de enjambre de este tipo se implement\u00f3 en una red de \u00e1rea local de treinta hosts -LRB- 30 -RRB- y se simul\u00f3 en una topolog\u00eda de 10.000 nodos. Los resultados preliminares mostraron que estos gusanos son capaces de comprometer a sus hu\u00e9spedes a un ritmo hasta dos \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que su contraparte gen\u00e9rica. El resto de este manuscrito est\u00e1 estructurado de la siguiente manera. En la secci\u00f3n 2 se presenta un modelo abstracto tanto de gusanos gen\u00e9ricos como de gusanos de enjambre. Este modelo se utiliza en la secci\u00f3n 2.6 para describir el primer caso de un gusano enjambre, ZachiK. En la secci\u00f3n 4 se presentan los resultados preliminares tanto de mediciones emp\u00edricas como de simulaci\u00f3n. Finalmente, en la secci\u00f3n 5 se presentan nuestras conclusiones y reflexiones sobre el trabajo futuro. 5. RESUMEN Y TRABAJO FUTURO En este manuscrito, hemos presentado un modelo abstracto, similar en algunos aspectos al de Weaver -LSB- 5 -RSB-, que ayuda a explicar la naturaleza gen\u00e9rica de los gusanos.El modelo presentado en la secci\u00f3n 2 se ampli\u00f3 para incorporar una nueva clase de gusanos potencialmente peligrosos llamados Swarm Worms. Los gusanos de enjambre se comportan de manera muy parecida a los enjambres biol\u00f3gicos y exhiben un alto grado de aprendizaje, comunicaci\u00f3n e inteligencia distribuida. Estos gusanos de enjambre son potencialmente m\u00e1s da\u00f1inos que sus hom\u00f3logos gen\u00e9ricos. Adem\u00e1s, hasta donde sabemos, se cre\u00f3 la primera instancia de un gusano de aprendizaje de este tipo, llamado ZachiK. ZachiK es un gusano enjambre sencillo para descifrar contrase\u00f1as que incorpora diferentes estrategias de aprendizaje e intercambio de informaci\u00f3n. Un gusano de enjambre de este tipo se implement\u00f3 en una red de \u00e1rea local de treinta hosts -LRB- 30 -RRB- y se simul\u00f3 en una topolog\u00eda de 10.000 nodos. Los resultados preliminares mostraron que estos gusanos son capaces de comprometer los hosts a un ritmo hasta 2 \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que su contraparte gen\u00e9rica, manteniendo al mismo tiempo sus capacidades de sigilo. Este trabajo abre una nueva \u00e1rea de problemas interesantes. Algunos de los problemas m\u00e1s interesantes y apremiantes a considerar son los siguientes: \u2022 \u00bfEs posible aplicar algunos de los conceptos de aprendizaje desarrollados durante los \u00faltimos diez a\u00f1os en las \u00e1reas de inteligencia de enjambre, sistemas de agentes y control distribuido al dise\u00f1o de sistemas de enjambre sofisticados? gusanos de tal manera que se produzca un verdadero comportamiento emergente? \u2022 \u00bfLas t\u00e9cnicas actuales que se est\u00e1n desarrollando en el dise\u00f1o de sistemas de detecci\u00f3n y contramedidas de intrusiones y sistemas de supervivencia son efectivas contra esta nueva clase de gusanos? ; y \u2022 \u00bfQu\u00e9 t\u00e9cnicas, si es que hay alguna, pueden desarrollarse para crear defensas contra los gusanos de enjambre?\u00bfSe puede desarrollar para crear defensas contra los gusanos de enjambre?\u00bfSe puede desarrollar para crear defensas contra los gusanos de enjambre?", "keyphrases": ["malwar", "gusano enjambre", "inteligencia emergente", "gusano portazo", "mec\u00e1nico comunal local", "zachik", "m\u00e9todo prng", "lista de objetivos pregenerados", "distribuir inteligencia", "detecci\u00f3n de intrusos", "sistema de contramedidas"]}
{"file_name": "J-26", "text": "Agencia combinatoria RESUMEN Muchas investigaciones recientes se refieren a sistemas, como Internet, cuyos componentes pertenecen y son operados por diferentes partes, cada una con su propio objetivo \"ego\u00edsta\". El campo del Dise\u00f1o de Mecanismos Algor\u00edtmicos maneja la cuesti\u00f3n de la informaci\u00f3n privada en poder de las diferentes partes en dichos entornos computacionales. Este art\u00edculo aborda un problema complementario en tales contexts: el manejo de las ``acciones ocultas'' que realizan las diferentes partes. Nuestro modelo es una variante combinatoria del problema cl\u00e1sico del agente principal de la teor\u00eda econ\u00f3mica. En nuestro entorno, un director debe motivar a un equipo de agentes estrat\u00e9gicos para que realicen esfuerzos costosos en su nombre, pero sus acciones le son ocultas. Nos centramos en casos en los que combinaciones complejas de los esfuerzos de los agentes influyen en el resultado. El principal motiva a los agentes ofreci\u00e9ndoles un conjunto de contratos, que en conjunto colocan a los agentes en un punto de equilibrio del juego inducido. Presentamos modelos formales para este escenario, sugerimos y nos embarcamos en un an\u00e1lisis de algunas cuestiones b\u00e1sicas, pero dejamos muchas preguntas abiertas. 1. INTRODUCCI\u00d3N 1.1 Antecedentes Una de las caracter\u00edsticas m\u00e1s sorprendentes de las redes inform\u00e1ticas modernas (en particular Internet) es que diferentes partes de ellas pertenecen a diferentes individuos, empresas y organizaciones y son operadas por ellos. Por lo tanto, el an\u00e1lisis y dise\u00f1o de protocolos para este entorno naturalmente necesita tener en cuenta los diferentes intereses econ\u00f3micos \"ego\u00edstas\" de los diferentes participantes. En particular, el campo del dise\u00f1o de mecanismos algor\u00edtmicos -LSB-6-RSB- utiliza incentivos apropiados para ``extraer'' la informaci\u00f3n privada de los participantes. Este art\u00edculo aborda el desconocimiento complementario, el de las acciones ocultas. En muchos casos, los comportamientos reales (acciones) de los diferentes participantes est\u00e1n \"ocultos\" para los dem\u00e1s y s\u00f3lo influyen indirectamente en el resultado final. \u00bfC\u00f3mo podemos garantizar que los diferentes servidores realicen realmente la combinaci\u00f3n correcta de asignaciones? Una clase relacionada de ejemplos se refiere a cuestiones de seguridad: cada `` enlace '' en un sistema complejo puede ejercer diferentes niveles de esfuerzo para proteger alguna propiedad de seguridad deseada del sistema. \u00bfC\u00f3mo podemos asegurar que se alcance el nivel deseado de 5. ASPECTOS ALGOR\u00cdTMICOS Nuestro an\u00e1lisis a lo largo del art\u00edculo arroja algo de luz sobre los aspectos algor\u00edtmicos del c\u00e1lculo del mejor contrato. En esta secci\u00f3n exponemos estas implicaciones -LRB- para las pruebas ver -LSB- 2 -RSB- -RRB-. Primero consideramos el modelo general donde la funci\u00f3n tecnol\u00f3gica est\u00e1 dada por una funci\u00f3n mon\u00f3tona arbitraria t -LRB- con valores racionales -RRB-, y luego consideramos el caso de tecnolog\u00edas estructuradas dadas por una representaci\u00f3n de red de la funci\u00f3n booleana subyacente. 5.1 Tecnolog\u00edas de acci\u00f3n binaria de resultado binario Aqu\u00ed asumimos que se nos da una tecnolog\u00eda y un valor v como entrada, y nuestra salida debe ser el contrato \u00f3ptimo, es decirel conjunto S* de agentes a contratar y el contrato pi para cada i ES*. En el caso general, la funci\u00f3n de \u00e9xito t es de tama\u00f1o exponencial en n, el n\u00famero de agentes, y tendremos que ocuparnos de eso. En el caso especial de tecnolog\u00edas an\u00f3nimas, la descripci\u00f3n de t es s\u00f3lo los n +1 n\u00fameros t0,..., tn, y en este caso nuestro an\u00e1lisis en la secci\u00f3n 3 es completamente suficiente para calcular el contrato \u00f3ptimo. \u2022 La \u00f3rbita de la tecnolog\u00eda tanto en el caso de agencia como en los casos no estrat\u00e9gicos. \u2022 Un contrato \u00f3ptimo para cualquier valor v dado, tanto para el caso de agencia como para el caso no estrat\u00e9gico. \u2022 El precio de la irresponsabilidad POU -LRB- t,~c -RRB-. PRUEBA. Probamos las afirmaciones para el caso no an\u00f3nimo, la prueba para el caso an\u00f3nimo es similar. Primero mostramos c\u00f3mo construir la \u00f3rbita de la tecnolog\u00eda -LRB-, se aplica el mismo procedimiento en ambos casos -RRB-. Para construir la \u00f3rbita encontramos todos los puntos de transici\u00f3n y los conjuntos que est\u00e1n en la \u00f3rbita. El contrato vac\u00edo siempre es \u00f3ptimo para v = 0. Supongamos que hemos calculado los contratos \u00f3ptimos y los puntos de transici\u00f3n hasta alg\u00fan punto de transici\u00f3n v para el cual S es un contrato \u00f3ptimo con la mayor probabilidad de \u00e9xito. Mostramos c\u00f3mo calcular el siguiente punto de transici\u00f3n y el siguiente contrato \u00f3ptimo. Seg\u00fan el Lema 3, el siguiente contrato en la \u00f3rbita -LRB- para valores m\u00e1s altos -RRB- tiene una mayor probabilidad de \u00e9xito -LRB-, no hay dos conjuntos con la misma probabilidad de \u00e9xito en la \u00f3rbita -RRB-. Calculamos el siguiente contrato \u00f3ptimo mediante el siguiente procedimiento. Repasamos todos los conjuntos T tales que t -LRB- T -RRB- > t -LRB- S -RRB-, y calculamos el valor para el cual el principal es indiferente entre contratar con T y contratar con S. El valor m\u00ednimo de indiferencia es el siguiente punto de transici\u00f3n y el contrato que tiene el valor m\u00ednimo de indiferencia es el siguiente contrato \u00f3ptimo. La linealidad de la utilidad en el valor y la monotonicidad de la probabilidad de \u00e9xito de los contratos \u00f3ptimos aseguran que lo anterior funcione. Claramente, el c\u00e1lculo anterior es polin\u00f3mico en el tama\u00f1o de entrada. Una vez que tenemos la \u00f3rbita, est\u00e1 claro que se puede calcular un contrato \u00f3ptimo para cualquier valor v dado. Encontramos el punto de transici\u00f3n m\u00e1s grande que no es mayor que el valor v, y el contrato \u00f3ptimo en v es el conjunto con mayor probabilidad de \u00e9xito en este punto de transici\u00f3n. Finalmente, como podemos calcular la \u00f3rbita de la tecnolog\u00eda tanto en el caso de agencia como en el no estrat\u00e9gico en tiempo polin\u00f3mico, podemos encontrar el precio de la falta de rendici\u00f3n de cuentas en tiempo polin\u00f3mico. Seg\u00fan el Lema 1, el precio de la falta de rendici\u00f3n de cuentas POU -LRB- t -RRB- se obtiene en alg\u00fan punto de transici\u00f3n, por lo que s\u00f3lo necesitamos repasar todos los puntos de transici\u00f3n y encontrar aquel que tenga el \u00edndice de bienestar social m\u00e1ximo. Una pregunta m\u00e1s interesante es si, dada la funci\u00f3n t como una caja negra, podemos calcular el contrato \u00f3ptimo en el tiempo que sea polin\u00f3mico en n. Podemos demostrar que, en general, este no es el caso: TEOREMA 5.Dado como entrada una caja negra para una funci\u00f3n de \u00e9xito t -LRB- cuando los costos son id\u00e9nticos -RRB-, y un valor v, el n\u00famero de consultas que se necesitan, en el peor de los casos, para encontrar el contrato \u00f3ptimo es exponencial en n . PRUEBA. Considere la siguiente familia de tecnolog\u00edas. Para algunos peque\u00f1os e > 0 y k = -LSB- n/2 -RSB- definimos la probabilidad de \u00e9xito para un conjunto T dado de la siguiente manera. Si el algoritmo consulta como m\u00e1ximo -LRB- n -RRB- -- 2 conjuntos fin/2 -RSB- de tama\u00f1o k, entonces no siempre puede determinar el contrato \u00f3ptimo -LRB- como cualquiera de los conjuntos que no ha consultado. podr\u00eda ser el \u00f3ptimo -RRB-. Concluimos que -LRB- n -RRB- -- 1 consultas fin/2 -RSB- son necesarias para determinar el contrato \u00f3ptimo, y esto es exponencial en n. 5.2 Tecnolog\u00edas estructuradas En esta secci\u00f3n consideraremos la representaci\u00f3n natural de redes de lectura \u00fanica para la funci\u00f3n booleana subyacente. Por lo tanto, el problema que abordaremos ser\u00e1: El problema del contrato \u00f3ptimo para redes de lectura \u00fanica: Entrada: Una red de lectura \u00fanica G = -LRB- V, E -RRB-, con dos v\u00e9rtices espec\u00edficos s, t; valores racionales - ye, \u03b4e para cada jugador e \u2208 E -LRB- y ce = 1 -RRB-, y un valor racional v. Salida: Un conjunto S de agentes que deber\u00edan contratarse en un contrato \u00f3ptimo. Sea t -LRB- E -RRB- la probabilidad de \u00e9xito cuando cada arista tiene \u00e9xito con probabilidad \u03b4e. Primero notamos que incluso calcular el valor t -LRB- E -RRB- es un problema dif\u00edcil: se llama problema de confiabilidad de la red y se sabe que es #P \u2212 dif\u00edcil -LSB- 8 -RSB-. Un peque\u00f1o esfuerzo revelar\u00e1 que nuestro problema no es m\u00e1s f\u00e1cil: TEOREMA 6. El problema del contrato \u00f3ptimo para redes de lectura \u00fanica es #P - dif\u00edcil -LRB- bajo reducciones de Turing -RRB-. PRUEBA. Demostraremos que se puede utilizar un algoritmo para este problema para resolver el problema de confiabilidad de la red. Dada una instancia de un problema de confiabilidad de red < G, -LCB- -LRB- e -RCB- eEE > -LRB- donde -LRB- e denota la probabilidad de \u00e9xito de e -RRB-, definimos una instancia del contrato \u00f3ptimo problema de la siguiente manera: primero defina un nuevo gr\u00e1fico G ' que se obtiene '' Y '' haciendo G con un nuevo jugador x, con - yx muy cerca de 21 y \u03b4x = 1 \u2212 - yx. Una vez que encontramos dicho valor, elegimos - yx st c 1 -- 2\u03b3x es mayor que ese valor -RRB-. Denotemos \u03b2x = 1 \u2212 2-yx. El valor cr\u00edtico de v donde el jugador x ingresa el contrato \u00f3ptimo de G', se puede encontrar usando b\u00fasqueda binaria sobre el algoritmo que supuestamente encuentra el contrato \u00f3ptimo para cualquier red y cualquier valor. Tenga en cuenta que en este valor cr\u00edtico v, el principal es indiferente entre el conjunto E y E \u222a -LCB- x -RCB-. por tanto, si siempre podemos encontrar el contrato \u00f3ptimo, tambi\u00e9n podremos calcular el valor de t -LRB- E -RRB-. En conclusi\u00f3n, calcular el contrato \u00f3ptimo en general es dif\u00edcil. Estos resultados sugieren dos direcciones naturales de investigaci\u00f3n. La primera v\u00eda es estudiar familias de tecnolog\u00edas cuyos contratos \u00f3ptimos puedan calcularse en tiempo polinomial.La segunda v\u00eda es explorar algoritmos de aproximaci\u00f3n para el problema del contrato \u00f3ptimo. Un posible candidato para la primera direcci\u00f3n es la familia de redes serie-paralelas, para las cuales el problema de confiabilidad de la red -LRB- que calcula el valor de t -RRB- es polin\u00f3mico.", "keyphrases": ["conjunto \u00f3ptimo de contrato", "cl\u00e1sico agente-principio", "calidad de servicio", "agencia combinatoria", "equilibrio de Nash", "acci\u00f3n contractual", "\u00f3rbita k", "tecnolog\u00eda an\u00f3nima", "red seri-paralela", "precio de no cuenta"]}
{"file_name": "J-11", "text": "Redes comerciales con agentes que fijan precios RESUMEN En una amplia gama de mercados, los compradores y vendedores individuales a menudo comercian a trav\u00e9s de intermediarios, quienes determinan los precios mediante consideraciones estrat\u00e9gicas. Normalmente, no todos los compradores y vendedores tienen acceso a los mismos intermediarios y comercian a precios correspondientemente diferentes que reflejan sus cantidades relativas de poder en el mercado. Modelamos este fen\u00f3meno utilizando un juego en el que compradores, vendedores y comerciantes realizan transacciones comerciales en un gr\u00e1fico que representa el acceso que cada comprador y vendedor tiene a los comerciantes. En este modelo, los comerciantes fijan los precios estrat\u00e9gicamente y luego los compradores y vendedores reaccionan a los precios que se les ofrecen. Mostramos que el juego resultante siempre tiene un equilibrio de Nash perfecto en subjuegos, y que todos los equilibrios conducen a una asignaci\u00f3n de bienes eficiente -LRB-, es decir, socialmente \u00f3ptima -RRB-. Extendemos estos resultados a un tipo m\u00e1s general de mercado de emparejamiento, como el que se encuentra en el emparejamiento de solicitantes de empleo y empleadores. Finalmente, consideramos c\u00f3mo las ganancias obtenidas por los comerciantes dependen del gr\u00e1fico subyacente: aproximadamente, un comerciante puede obtener una ganancia positiva si y s\u00f3lo si tiene una conexi\u00f3n \"esencial\" en la estructura de la red, proporcionando as\u00ed un gr\u00e1fico. base te\u00f3rica para cuantificar la cantidad de competencia entre comerciantes. Nuestro trabajo difiere de estudios recientes sobre c\u00f3mo el precio se ve afectado por la estructura de la red a trav\u00e9s de nuestro modelado de la fijaci\u00f3n de precios como una actividad estrat\u00e9gica llevada a cabo por un subconjunto de agentes en el sistema, en lugar de estudiar los precios fijados a trav\u00e9s del equilibrio competitivo o mediante un mecanismo veraz. 1. INTRODUCCI\u00d3N En una variedad de entornos donde los mercados median las interacciones de compradores y vendedores, se observan varias propiedades recurrentes: los compradores y vendedores individuales a menudo comercian a trav\u00e9s de intermediarios, no todos los compradores y vendedores tienen acceso a los mismos intermediarios, y no todos los compradores y los vendedores comercian al mismo precio. Un ejemplo de este escenario es el comercio de productos agr\u00edcolas en los pa\u00edses en desarrollo. Dadas las redes de transporte inadecuadas y el acceso limitado de los agricultores pobres al capital, muchos agricultores no tienen otra alternativa que comerciar con intermediarios en mercados locales ineficientes. Un pa\u00eds en desarrollo puede tener muchos de estos mercados parcialmente superpuestos junto con mercados modernos y eficientes -LSB- 2 -RSB-. Los mercados financieros ofrecen un ejemplo diferente de un entorno con estas caracter\u00edsticas generales. En estos mercados, gran parte del comercio entre compradores y vendedores est\u00e1 intermediado por una variedad de agentes que van desde corredores hasta creadores de mercado y sistemas de comercio electr\u00f3nico. Para muchos activos no existe un mercado \u00fanico; El comercio de un solo activo puede ocurrir simult\u00e1neamente en el piso de una bolsa, en redes cruzadas, en bolsas electr\u00f3nicas y en mercados de otros pa\u00edses. Algunos compradores y vendedores tienen acceso a muchos o todos estos centros de negociaci\u00f3n; otros tienen acceso s\u00f3lo a uno o algunos de ellos. El precio al que se negocia el activo puede diferir entre estos centros de negociaci\u00f3n.De hecho, no existe un \"precio\", ya que diferentes comerciantes pagan o reciben precios diferentes. En muchos entornos tambi\u00e9n existe una brecha entre el precio que un comprador paga por un activo, el precio de venta, y el precio que un vendedor recibe por el activo, el precio de oferta. Los diferenciales, definidos como la diferencia entre los precios de oferta y demanda, difieren significativamente entre estos mercados, aunque se negocie el mismo activo en los dos mercados. En este art\u00edculo, desarrollamos un marco en el que tales fen\u00f3menos emergen de un modelo de comercio basado en la teor\u00eda de juegos, en el que compradores, vendedores y comerciantes interact\u00faan en una red. Los bordes de la red conectan a los comerciantes con los compradores y vendedores y, por lo tanto, representan el acceso que los diferentes participantes del mercado tienen entre s\u00ed. Los comerciantes act\u00faan como intermediarios en un juego comercial de dos etapas: eligen estrat\u00e9gicamente los precios de oferta y demanda para ofrecer a los vendedores y compradores con los que est\u00e1n conectados; Luego, los vendedores y compradores reaccionan a los precios que enfrentan. As\u00ed, la red codifica el poder relativo en las posiciones estructurales de los participantes del mercado, incluidos los niveles impl\u00edcitos de competencia entre los comerciantes. Mostramos que este juego siempre tiene un equilibrio de Nash perfecto en subjuegos, y que todos los equilibrios conducen a una asignaci\u00f3n de bienes eficiente -LRB-, es decir, socialmente \u00f3ptima -RRB-. Tambi\u00e9n analizamos c\u00f3mo las ganancias de los comerciantes dependen de la estructura de la red, caracterizando esencialmente en t\u00e9rminos de teor\u00eda de gr\u00e1ficos c\u00f3mo la rentabilidad de un comerciante est\u00e1 determinada por la cantidad de competencia que experimenta con otros comerciantes. Al desarrollar un modelo de red que incluya expl\u00edcitamente a los comerciantes como agentes que fijan los precios, en un sistema junto con compradores y vendedores, podemos capturar la formaci\u00f3n de precios en un entorno de red como un proceso estrat\u00e9gico llevado a cabo por intermediarios, en lugar de como el resultado de un mecanismo controlado centralmente o ex\u00f3geno. El modelo b\u00e1sico: bienes indistinguibles. Nuestro objetivo al formular el modelo es expresar el proceso de fijaci\u00f3n de precios en mercados como los analizados anteriormente, donde no todos los participantes tienen acceso uniforme entre s\u00ed. Tenemos un conjunto B de compradores, un conjunto S de vendedores y un conjunto T de comerciantes. Hay un gr\u00e1fico no dirigido G que indica qui\u00e9n puede comerciar con qui\u00e9n. Esto refleja las limitaciones de que todas las transacciones comprador-vendedor pasen a trav\u00e9s de comerciantes como intermediarios. En la versi\u00f3n m\u00e1s b\u00e1sica del modelo, consideramos productos id\u00e9nticos, de los cuales inicialmente cada vendedor posee una copia. Tanto los compradores como los vendedores tienen cada uno un valor por una copia del bien, y suponemos que estos valores son de conocimiento com\u00fan. Posteriormente generalizaremos esto a un entorno en el que los bienes son distinguibles, los compradores pueden valorar diferentes bienes de manera diferente y, potencialmente, los vendedores tambi\u00e9n pueden valorar las transacciones con diferentes compradores de manera diferente. Tener diferentes valoraciones de los compradores refleja situaciones como la compra de una vivienda; agregar diferentes valoraciones de vendedores tambi\u00e9n captura mercados coincidentes, por ejemplo,vendedores como solicitantes de empleo y compradores como empleadores, preocup\u00e1ndose ambos por qui\u00e9n termina con qu\u00e9 \"bien\" -LRB- y con los comerciantes actuando como servicios que intermedian en la b\u00fasqueda de empleo -RRB-. As\u00ed, para empezar con el modelo b\u00e1sico, existe un solo tipo de bien; el bien viene en unidades indivisibles; y cada vendedor posee inicialmente una unidad del bien. Los tres tipos de agentes valoran el dinero al mismo ritmo; y cada i EBUS valora adicionalmente una copia del bien en \u03b8i unidades de dinero. Ning\u00fan agente quiere m\u00e1s de una copia del bien, por lo que las copias adicionales se valoran en 0. Cada agente tiene una dotaci\u00f3n inicial de dinero mayor que cualquier valoraci\u00f3n individual \u03b8i; el efecto de esto es garantizar que cualquier comprador que termine sin una copia del bien haya sido excluido del mercado debido a su valoraci\u00f3n y posici\u00f3n en la red, no a una falta de fondos. Imaginamos que cada bien que se vende fluye a lo largo de una secuencia de dos aristas: de un vendedor a un comerciante, y luego del comerciante a un comprador. La forma particular en que fluyen las mercanc\u00edas est\u00e1 determinada por el siguiente juego. En primer lugar, cada comerciante ofrece un precio de oferta a cada vendedor con el que est\u00e1 conectado y un precio de venta a cada comprador con el que est\u00e1 conectado. Luego, los vendedores y compradores eligen entre las ofertas que les presentan los comerciantes. Si varios comerciantes proponen el mismo precio a un vendedor o comprador, entonces no existe una mejor respuesta estricta para el vendedor o comprador. Finalmente, cada comerciante compra una copia del bien a cada vendedor que acepta su oferta y vende una copia del bien a cada comprador que acepta su oferta. Si un comerciante en particular descubre que m\u00e1s compradores que vendedores aceptan sus ofertas, entonces se ha comprometido a proporcionar m\u00e1s copias del bien de las que ha recibido, y diremos que esto resulta en una gran penalizaci\u00f3n para el comerciante por incumplimiento; El efecto de esto es que, en equilibrio, ning\u00fan operador elegir\u00e1 precios de oferta y demanda que resulten en un incumplimiento. M\u00e1s precisamente, una estrategia para cada comerciante t es una especificaci\u00f3n de un precio de oferta 3ti para cada vendedor i al que t est\u00e1 conectado, y un precio de venta \u03b1tj para cada comprador j al que t est\u00e1 conectado. -LRB- Tambi\u00e9n podemos manejar un modelo en el que un comerciante puede optar por no hacer una oferta a ciertos de sus vendedores o compradores adyacentes. -RRB- Cada vendedor o comprador elige entonces como m\u00e1ximo una arista de incidencia, indicando el comerciante con el que realizar\u00e1 la transacci\u00f3n, al precio indicado. -LRB- La elecci\u00f3n de una \u00fanica ventaja refleja el hecho de que los vendedores -LRB- a -RRB- inicialmente tienen cada uno solo una copia del bien, y los compradores -LRB- b -RRB- cada uno solo quiere una copia del bien. -RRB- Los pagos son los siguientes: Para cada vendedor i, el pago por seleccionar el comerciante t es 3ti, mientras que el beneficio por no seleccionar ning\u00fan comerciante es \u03b8i. -LRB- En el primer caso, el vendedor recibe 3ti unidades de dinero, mientras que en el segundo conserva su copia del bien, que valora en \u03b8i. -RRB- Para cada comprador j, el beneficio de seleccionar el comerciante t es \u03b8j -- \u03b1tj, mientras que el beneficio de no seleccionar ning\u00fan comerciante es 0.-LRB- En el primer caso, el comprador recibe el bien pero renuncia a \u03b1tj unidades de dinero. -RRB- Para cada comerciante t, con ofertas aceptadas de los vendedores i1,..., is y de los compradores j1,..., jb, el pago es Pr \u03b1tjr -- Pr 3tir, menos una penalizaci\u00f3n \u03c0 si b > s. La penalizaci\u00f3n se elige para que sea lo suficientemente grande como para que un operador nunca incurra en ella en equilibrio y, por lo tanto, generalmente no nos preocuparemos por la penalizaci\u00f3n. Esto define los elementos b\u00e1sicos del juego. El concepto de equilibrio que utilizamos es el equilibrio de Nash perfecto en subjuegos. Algunos ejemplos. Para ayudar a pensar en el modelo, ahora describimos tres ejemplos ilustrativos, representados en la Figura 1. Todos los vendedores en los ejemplos tendr\u00e1n valoraciones del bien iguales a 0; la valoraci\u00f3n de cada comprador se dibuja dentro de su c\u00edrculo; y el precio de oferta o demanda en cada borde se dibuja encima del borde. En la Figura 1 -LRB- a -RRB-, mostramos c\u00f3mo una subasta est\u00e1ndar de segundo precio surge naturalmente de nuestro modelo. Supongamos que las valoraciones de los compradores de arriba a abajo son w > x > y > z. Los precios de oferta y demanda mostrados son consistentes con un equilibrio en el que i1 y j1 aceptan las ofertas del comerciante t1, y ning\u00fan otro comprador acepta la oferta de su comerciante adyacente: por lo tanto, el comerciante t1 recibe el bien con un precio de oferta de x, y genera w -- x vendiendo el bien al comprador j1 por w. De esta manera, podemos considerar este caso particular como una subasta de un solo bien en la que los comerciantes act\u00faan como \"representantes\" de sus compradores adyacentes. El comprador con la valoraci\u00f3n m\u00e1s alta del bien termina con \u00e9l y el excedente se divide entre el vendedor y el comerciante asociado. Tenga en cuenta que se puede construir una subasta de k unidades con f > k compradores con la misma facilidad, construyendo un gr\u00e1fico bipartito completo sobre k vendedores y f comerciantes, y luego vinculando cada comerciante a un \u00fanico comprador distinto. En la Figura 1 -LRB- b -RRB-, mostramos c\u00f3mo los nodos con diferentes posiciones en la topolog\u00eda de la red pueden lograr diferentes beneficios, incluso cuando todos Figura 1 : -LRB- a -RRB- Una subasta, mediada por comerciantes, en la que el El comprador con la valoraci\u00f3n m\u00e1s alta del bien termina con \u00e9l. -LRB- b -RRB- Una red en la que el vendedor y el comprador intermedios se benefician de la competencia perfecta entre los comerciantes, mientras que los otros vendedores y compradores no tienen poder debido a su posici\u00f3n en la red. -LRB- c -RRB- Una forma de competencia perfecta impl\u00edcita: todos los diferenciales de oferta y demanda ser\u00e1n cero en equilibrio, incluso aunque ning\u00fan operador ``compita'' directamente con ning\u00fan otro operador por el mismo par comprador-vendedor. Las valoraciones de los compradores son num\u00e9ricamente las mismas. Espec\u00edficamente, el vendedor i2 y el comprador j2 ocupan posiciones poderosas, porque los dos comerciantes compiten por su negocio; Por otro lado, los dem\u00e1s vendedores y compradores se encuentran en posiciones d\u00e9biles, porque cada uno s\u00f3lo tiene una opci\u00f3n. Y, de hecho, en todo equilibrio existe un n\u00famero real x E -LSB- 0, 1 -RSB- tal que ambos operadores ofrecen precios de compra y venta de x a i2 y j2 respectivamente,mientras ofrecen ofertas de 0 y solicitudes de 1 a los dem\u00e1s vendedores y compradores. Por lo tanto, este ejemplo ilustra algunos ingredientes cruciales que identificaremos a un nivel m\u00e1s general en breve. Espec\u00edficamente, i2 y j2 experimentan los beneficios de la competencia perfecta, en el sentido de que los dos operadores reducen los diferenciales entre oferta y demanda a 0 al competir por su negocio. Por otro lado, los otros vendedores y compradores experimentan las desventajas del monopolio: no reciben ning\u00fan pago ya que solo tienen una \u00fanica opci\u00f3n para comerciar, y el comerciante correspondiente obtiene todas las ganancias. Obs\u00e9rvese adem\u00e1s c\u00f3mo este comportamiento natural surge del hecho de que los comerciantes son capaces de ofrecer diferentes precios a diferentes agentes, capturando el hecho de que no hay un \"precio\" fijo en los tipos de mercados que motivan el modelo, sino diferentes precios. Los precios reflejan el poder relativo de los diferentes agentes involucrados. El ejemplo anterior muestra quiz\u00e1s la forma m\u00e1s natural en la que el beneficio de un operador en una transacci\u00f3n particular puede caer a 0: cuando hay otro operador que puede replicar su funci\u00f3n con precisi\u00f3n. -LRB- En ese ejemplo, dos comerciantes ten\u00edan cada uno la capacidad de mover una copia del bien de i2 a j2. -RRB- Pero, como lo mostrar\u00e1n nuestros resultados posteriores, los operadores generalmente no obtienen ganancias debido a razones globales y de teor\u00eda de gr\u00e1ficos. El ejemplo de la Figura 1 -LRB- c -RRB- da una indicaci\u00f3n inicial de esto: se puede demostrar que para cada equilibrio, existe un ay E -LSB- 0, 1 -RSB- tal que cada precio de oferta y cada precio de venta son iguales. juguete. En otras palabras, todos los comerciantes obtienen cero ganancias, independientemente de que una copia del bien pase a trav\u00e9s de ellos o no, y, sin embargo, no hay dos comerciantes que tengan rutas entre vendedor y comprador en com\u00fan. Los diferenciales de precios han sido llevados a cero por una restricci\u00f3n global impuesta por el ciclo largo a trav\u00e9s de todos los agentes; Este es un ejemplo de competencia perfecta impl\u00edcita determinada por la topolog\u00eda de la red. Ampliaci\u00f3n del modelo a bienes distinguibles. Ampliamos el modelo b\u00e1sico a un entorno con bienes distinguibles, como sigue. Una estrategia para un comerciante ahora consiste en ofrecer una oferta a cada vendedor que especifica tanto un precio como un comprador, y ofrecer una demanda a cada comprador que especifica tanto un precio como un vendedor. -LRB- Tambi\u00e9n podemos manejar un modelo en el que un comerciante ofrece ofertas -LRB- respectivamente, pide -RRB- en forma de vectores, especificando esencialmente un `` men\u00fa '' con un precio adjunto a cada comprador -LRB- resp. vendedor -RRB-. -RRB- Cada comprador y vendedor selecciona una oferta de un comerciante adyacente y los pagos a todos los agentes se determinan como antes. Aqu\u00ed los vendedores son solicitantes de empleo, los compradores son empleadores y los comerciantes son los agentes que median en el mercado laboral. Por supuesto, si se especifican valoraciones por pares para los compradores pero solo valoraciones individuales para los vendedores, modelamos un entorno en el que los compradores pueden distinguir entre los bienes, pero a los vendedores no les importa a qui\u00e9n venden; esto -LRB- aproximadamente -RRB- captura entornos como los mercados inmobiliarios. Nuestros resultados. Para hacerlos precisos,Nosotros introducimos la siguiente notaci\u00f3n. -LRB- Los vendedores que no aparecen en ning\u00fan triple conservan su copia del bien. -RRB- Decimos que el valor de la asignaci\u00f3n es igual a Pe \u2208 M \u03b8jeie -- \u03b8ieje. Sea \u03b8 \u2217 el valor m\u00e1ximo de cualquier asignaci\u00f3n M que sea factible dada la red. Demostramos que cada instancia de nuestro juego tiene un equilibrio, y que en cada uno de esos equilibrios, la asignaci\u00f3n tiene un valor \u03b8 \u2217; en otras palabras, logra el mejor valor posible. Por lo tanto, los equilibrios en este modelo son siempre eficientes, en el sentido de que el mercado permite que el conjunto \"correcto\" de personas obtenga el bien, sujeto a las restricciones de la red. Establecemos la existencia y eficiencia de equilibrios construyendo un programa lineal para capturar el flujo de bienes a trav\u00e9s de la red; el dual de este programa lineal contiene suficiente informaci\u00f3n para extraer precios de equilibrio. Seg\u00fan la definici\u00f3n del juego, el valor de la asignaci\u00f3n de equilibrio se divide en pagos a los agentes, y es interesante preguntarse c\u00f3mo se distribuye este valor; en particular, cu\u00e1nta ganancia puede obtener un operador en funci\u00f3n de su posici\u00f3n. en la red. Encontramos que, aunque todos los equilibrios tienen el mismo valor, el pago de un operador determinado puede variar entre diferentes equilibrios. Tambi\u00e9n obtenemos resultados para la suma de todas las ganancias de los comerciantes. Trabajo relacionado. El enfoque de referencia est\u00e1ndar para analizar la interacci\u00f3n de compradores y vendedores es el modelo walrasiano en el que compradores y vendedores an\u00f3nimos intercambian un bien a un precio \u00fanico de equilibrio de mercado. Esta forma reducida de comercio, construida sobre la idealizaci\u00f3n de un precio de mercado, es un modelo poderoso que ha llevado a muchas ideas. Pero no es un buen modelo para examinar de d\u00f3nde provienen los precios o exactamente c\u00f3mo comercian entre s\u00ed los compradores y los vendedores. La dificultad es que en el modelo walrasiano no hay ning\u00fan agente que fije el precio y los agentes en realidad no comercian entre s\u00ed. De hecho, no hay mercado, en el sentido cotidiano de la palabra, en el modelo walrasiano. Es decir, no existe un lugar f\u00edsico o virtual donde compradores y vendedores interact\u00faen para comerciar y fijar precios. As\u00ed, en este modelo simple, todos los compradores y vendedores son uniformes y comercian al mismo precio, y tampoco hay papel para los intermediarios. Hay varias publicaciones en econom\u00eda y finanzas que examinan c\u00f3mo se fijan los precios en lugar de limitarse a determinar los precios de equilibrio. La literatura sobre competencia imperfecta es quiz\u00e1s la m\u00e1s antigua de ellas. Aqu\u00ed un monopolista, o un grupo de oliogopolistas, eligen precios para maximizar sus ganancias -LRB- ver -LSB- 14 -RSB- para el tratamiento est\u00e1ndar de estos mercados en los libros de text -RRB-. Un monopolista utiliza su conocimiento de la demanda del mercado para elegir un precio, o un conjunto de precios, si discrimina. Los oligopolistas juegan un juego en el que sus beneficios dependen de la demanda del mercado y de las acciones de sus competidores. En esta literatura hay agentes que fijan los precios, pero se mantiene la ficci\u00f3n de un mercado \u00fanico. En la literatura de b\u00fasqueda de equilibrio,las empresas fijan los precios y los consumidores los buscan -LRB- ver -LSB- 3 -RSB- -RRB-. Los consumidores terminan pagando precios diferentes, pero todos los consumidores tienen acceso a todas las empresas y no hay intermediarios. En la literatura sobre equilibrio general ha habido varios intentos de introducir la determinaci\u00f3n de precios. Una t\u00e9cnica de prueba est\u00e1ndar de la existencia de equilibrio competitivo implica un mecanismo de ajuste de precios en el que los precios responden al exceso de demanda. Se han introducido procesos m\u00e1s sofisticados para estudiar la estabilidad de los precios de equilibrio o la informaci\u00f3n necesaria para calcularlos. Pero, una vez m\u00e1s, aqu\u00ed no hay agentes que fijen los precios. En la literatura financiera, el trabajo sobre la microestructura del mercado tiene agentes que fijan precios -LRB- especialistas -RRB-, partes del mismo determinan precios de oferta y demanda por separado, y diferentes agentes reciben diferentes precios por el mismo activo -LRB- ver -LSB - 12 -RSB- para un tratamiento de la teor\u00eda de la microestructura -RRB-. El trabajo en econom\u00eda de la informaci\u00f3n ha identificado fen\u00f3menos similares -LRB- ver, por ejemplo, -LSB- 7 -RSB- -RRB-. Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.pero todos los consumidores tienen acceso a todas las empresas y no hay intermediarios. En la literatura sobre equilibrio general ha habido varios intentos de introducir la determinaci\u00f3n de precios. Una t\u00e9cnica de prueba est\u00e1ndar de la existencia de equilibrio competitivo implica un mecanismo de ajuste de precios en el que los precios responden al exceso de demanda. Se han introducido procesos m\u00e1s sofisticados para estudiar la estabilidad de los precios de equilibrio o la informaci\u00f3n necesaria para calcularlos. Pero, una vez m\u00e1s, aqu\u00ed no hay agentes que fijen los precios. En la literatura financiera, el trabajo sobre la microestructura del mercado tiene agentes que fijan precios -LRB- especialistas -RRB-, partes del mismo determinan precios de oferta y demanda por separado, y diferentes agentes reciben diferentes precios por el mismo activo -LRB- ver -LSB - 12 -RSB- para un tratamiento de la teor\u00eda de la microestructura -RRB-. El trabajo en econom\u00eda de la informaci\u00f3n ha identificado fen\u00f3menos similares -LRB- ver, por ejemplo, -LSB- 7 -RSB- -RRB-. Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.pero todos los consumidores tienen acceso a todas las empresas y no hay intermediarios. En la literatura sobre equilibrio general ha habido varios intentos de introducir la determinaci\u00f3n de precios. Una t\u00e9cnica de prueba est\u00e1ndar de la existencia de equilibrio competitivo implica un mecanismo de ajuste de precios en el que los precios responden al exceso de demanda. Se han introducido procesos m\u00e1s sofisticados para estudiar la estabilidad de los precios de equilibrio o la informaci\u00f3n necesaria para calcularlos. Pero, una vez m\u00e1s, aqu\u00ed no hay agentes que fijen los precios. En la literatura financiera, el trabajo sobre la microestructura del mercado tiene agentes que fijan precios -LRB- especialistas -RRB-, partes del mismo determinan precios de oferta y demanda por separado, y diferentes agentes reciben diferentes precios por el mismo activo -LRB- ver -LSB - 12 -RSB- para un tratamiento de la teor\u00eda de la microestructura -RRB-. El trabajo en econom\u00eda de la informaci\u00f3n ha identificado fen\u00f3menos similares -LRB- ver, por ejemplo, -LSB- 7 -RSB- -RRB-. Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Se han introducido procesos m\u00e1s sofisticados para estudiar la estabilidad de los precios de equilibrio o la informaci\u00f3n necesaria para calcularlos. Pero, una vez m\u00e1s, aqu\u00ed no hay agentes que fijen los precios. En la literatura financiera, el trabajo sobre la microestructura del mercado tiene agentes que fijan precios -LRB- especialistas -RRB-, partes del mismo determinan precios de oferta y demanda por separado, y diferentes agentes reciben diferentes precios por el mismo activo -LRB- ver -LSB - 12 -RSB- para un tratamiento de la teor\u00eda de la microestructura -RRB-. El trabajo en econom\u00eda de la informaci\u00f3n ha identificado fen\u00f3menos similares -LRB- ver, por ejemplo, -LSB- 7 -RSB- -RRB-. Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Se han introducido procesos m\u00e1s sofisticados para estudiar la estabilidad de los precios de equilibrio o la informaci\u00f3n necesaria para calcularlos. Pero, una vez m\u00e1s, aqu\u00ed no hay agentes que fijen los precios. En la literatura financiera, el trabajo sobre la microestructura del mercado tiene agentes que fijan precios -LRB- especialistas -RRB-, partes del mismo determinan precios de oferta y demanda por separado, y diferentes agentes reciben diferentes precios por el mismo activo -LRB- ver -LSB - 12 -RSB- para un tratamiento de la teor\u00eda de la microestructura -RRB-. El trabajo en econom\u00eda de la informaci\u00f3n ha identificado fen\u00f3menos similares -LRB- ver, por ejemplo, -LSB- 7 -RSB- -RRB-. Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Pero hay poca investigaci\u00f3n en estas publicaciones que examinen el efecto de las restricciones sobre qui\u00e9n puede comerciar con qui\u00e9n. Ha habido varios enfoques para estudiar c\u00f3mo la estructura de la red determina los precios. Estos han postulado la determinaci\u00f3n de los precios a trav\u00e9s de definiciones basadas en el equilibrio competitivo o el n\u00facleo, o mediante el uso de mecanismos veraces. Al revisar brevemente este trabajo, notaremos el contraste con nuestro enfoque, en el sentido de que modelamos los precios como si surgieran del comportamiento estrat\u00e9gico de los agentes en el sistema. En un trabajo reciente, Kakade et al. -LSB- 8 -RSB- han estudiado la distribuci\u00f3n de precios en equilibrio competitivo en un gr\u00e1fico bipartito sobre compradores y vendedores, generado mediante un modelo probabil\u00edstico capaz de producir distribuciones de grados de cola pesada -LSB- 11 -RSB-. Incluso-Dar et al. -LSB- 6 -RSB- se basa en esto para considerar los aspectos estrat\u00e9gicos de la formaci\u00f3n de redes cuando los precios surgen del equilibrio competitivo. Leonard estudia los precios de VCG en este context; Babaioff et al. y Chu y Shen proporcionan adem\u00e1s un mecanismo de equilibrio presupuestario. Por el contrario, nuestro modelo tiene valoraciones y precios conocidos que surgen del comportamiento estrat\u00e9gico de los comerciantes. Demange, Gale y Sotomayor -LSB- 5 -RSB-, y Kranton y Minehart -LSB- 9 -RSB-, analizan los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Analizar los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.Analizar los precios a los que se produce el comercio en una red, trabajando en el marco del dise\u00f1o de mecanismos. Kranton y Minehart utilizan un gr\u00e1fico bipartito con v\u00ednculos directos entre compradores y vendedores, y luego utilizan un mecanismo de subasta ascendente, en lugar de intermediarios estrat\u00e9gicos, para determinar los precios. Su subasta tiene propiedades de equilibrio deseables, pero, como se\u00f1alan Kranton y Minehart, es una abstracci\u00f3n de c\u00f3mo se asignan los bienes y se determinan los precios que es similar en esp\u00edritu a la abstracci\u00f3n del subastador walrasiano.", "keyphrases": ["teor\u00eda de juegos de algoritmos", "mercado", "red comercial", "interacci\u00f3n del comprador y el vendedor", "dotaci\u00f3n inicial de dinero", "precio de oferta", "competencia perfecta", "beneficio", "importe m\u00e1ximo y m\u00ednimo", "econom\u00eda y finanzas", "comportamiento estrat\u00e9gico del comerciante", "holgura complementaria", "monopolio"]}
{"file_name": "I-15", "text": "B\u00fasqueda e intercambio de informaci\u00f3n en redes din\u00e1micas a gran escala RESUMEN Encontrar los agentes adecuados en una red grande y din\u00e1mica para proporcionar los recursos necesarios de manera oportuna es un problema de larga data. Este art\u00edculo presenta un m\u00e9todo para buscar e compartir informaci\u00f3n que combina \u00edndices de enrutamiento con m\u00e9todos basados \u200b\u200ben tokens. El m\u00e9todo propuesto permite a los agentes realizar b\u00fasquedas de forma eficaz adquiriendo los intereses de sus vecinos, publicitando sus capacidades de suministro de informaci\u00f3n y manteniendo \u00edndices para enrutar consultas, de forma integrada. Espec\u00edficamente, el art\u00edculo demuestra a trav\u00e9s de experimentos de rendimiento c\u00f3mo las redes est\u00e1ticas y din\u00e1micas de agentes pueden \"sintonizarse\" para responder consultas de manera efectiva mientras re\u00fanen evidencia de los intereses y las capacidades de suministro de informaci\u00f3n de otros, sin alterar la topolog\u00eda ni imponer una estructura superpuesta a la red. de conocidos. 1. INTRODUCCI\u00d3N redes de agentes asociados. Por otro lado, existe mucha investigaci\u00f3n sobre redes de b\u00fasqueda sem\u00e1ntica peer to peer y redes sociales -LSB- 1,5,6,8,9,10,16,18,19 -RSB- muchas de las cuales tratan sobre tuning una red de pares para buscar e intercambiar informaci\u00f3n de forma eficaz. Lo hacen principalmente imponiendo estructuras superpuestas l\u00f3gicas y sem\u00e1nticas. Sin embargo, hasta donde sabemos, no existe ning\u00fan trabajo que demuestre la efectividad de un proceso de ajuste gradual en redes din\u00e1micas a gran escala que estudie el impacto de la informaci\u00f3n recopilada por los agentes a medida que se emiten y atienden m\u00e1s y m\u00e1s consultas en sesiones concurrentes en el sistema. red. La cuesti\u00f3n principal de este art\u00edculo se refiere a \"ajustar\" una red de agentes, cada uno con una experiencia espec\u00edfica, para buscar e intercambiar informaci\u00f3n de manera eficiente y eficaz, sin alterar la topolog\u00eda ni imponer una estructura superpuesta mediante agrupaciones, introducci\u00f3n de \u00edndices abreviados o re- alambrado. `Tuning' es la tarea de compartir y recopilar el conocimiento necesario para que los agentes propaguen solicitudes a los conocidos adecuados, minimizando el esfuerzo de b\u00fasqueda, aumentando la eficiencia y el beneficio del sistema. Espec\u00edficamente, este art\u00edculo propone un m\u00e9todo para buscar e intercambiar informaci\u00f3n en redes din\u00e1micas y de gran escala, que combina \u00edndices de enrutamiento con m\u00e9todos basados \u200b\u200ben tokens para compartir informaci\u00f3n en sistemas multiagente de gran escala. Este art\u00edculo est\u00e1 estructurado de la siguiente manera: la Secci\u00f3n 2 presenta el trabajo relacionado y motiva el m\u00e9todo propuesto. La secci\u00f3n 3 establece el problema y la secci\u00f3n 4 presenta en detalle las t\u00e9cnicas individuales y el m\u00e9todo general propuesto. La secci\u00f3n 5 presenta la configuraci\u00f3n experimental y los resultados, y la secci\u00f3n 6 concluye el art\u00edculo, esbozando el trabajo futuro. 2. TRABAJO RELACIONADO La provisi\u00f3n y el intercambio de informaci\u00f3n pueden considerarse como un proceso de decisi\u00f3n de Markov descentralizado y parcialmente observable -LSB- 3,4,11,14 -RSB-. En el caso general, el control descentralizado de sistemas din\u00e1micos a gran escala de agentes cooperativos es un problema dif\u00edcil. Las soluciones \u00f3ptimas s\u00f3lo pueden aproximarse mediante heur\u00edsticas,por relajaciones del problema original o por soluciones centralizadas. Sin embargo, en un sistema din\u00e1mico a gran escala con control descentralizado es muy dif\u00edcil para los agentes poseer vistas parciales precisas del entorno, y es a\u00fan m\u00e1s dif\u00edcil para los agentes poseer una visi\u00f3n global del entorno. Adem\u00e1s, las observaciones de los agentes no pueden asumirse como independientes, ya que las acciones de un agente pueden afectar las observaciones de otros: por ejemplo, cuando un agente se une o abandona el sistema, esto puede afectar la evaluaci\u00f3n de otros agentes de las capacidades de suministro de informaci\u00f3n de los vecinos. . Considerando actividades y observaciones independientes, los autores de -LSB- 4 -RSB- proponen una soluci\u00f3n te\u00f3rica de decisi\u00f3n que trata la acci\u00f3n est\u00e1ndar y el intercambio de informaci\u00f3n como elecciones expl\u00edcitas que el tomador de decisiones debe tomar. Se aproximan a la soluci\u00f3n utilizando un algoritmo miope. Su trabajo difiere del reportado aqu\u00ed en los siguientes aspectos: primero, apunta a optimizar la comunicaci\u00f3n, mientras que el objetivo aqu\u00ed es sintonizar la red para compartir informaci\u00f3n de manera efectiva, reducir la comunicaci\u00f3n y aumentar los beneficios del sistema. En tercer lugar, consideran que las transiciones y observaciones realizadas por los agentes son independientes, lo que, como ya se ha comentado, no es cierto en el caso general. Por \u00faltimo, a diferencia de su enfoque en el que los agentes transmiten mensajes, aqu\u00ed los agentes deciden no s\u00f3lo cu\u00e1ndo comunicarse, sino tambi\u00e9n a qui\u00e9n enviar un mensaje. Los enfoques basados \u200b\u200ben tokens son prometedores para ampliar la coordinaci\u00f3n y, por lo tanto, el suministro e intercambio de informaci\u00f3n a sistemas a gran escala de manera efectiva. En -LSB-11-RSB- los autores proporcionan un marco matem\u00e1tico para enrutar tokens, proporcionando tambi\u00e9n una aproximaci\u00f3n para resolver el problema original en el caso de actividades de agentes independientes. El m\u00e9todo propuesto requiere un gran volumen de c\u00e1lculos que los autores pretenden reducir restringiendo su aplicaci\u00f3n a equipos l\u00f3gicos est\u00e1ticos de agentes asociados. De acuerdo con este enfoque, en -LSB- 12,13,14 -RSB-, el intercambio de informaci\u00f3n se considera solo para redes est\u00e1ticas y no se demuestra el autoajuste de las redes. Como se mostrar\u00e1 en la secci\u00f3n 5, nuestros experimentos muestran que aunque estos enfoques pueden manejar el intercambio de informaci\u00f3n en redes din\u00e1micas, requieren una mayor cantidad de mensajes en comparaci\u00f3n con el enfoque propuesto aqu\u00ed y no pueden ajustar la red para un intercambio de informaci\u00f3n eficiente. La comunicaci\u00f3n proactiva se ha propuesto en -LSB- 17 -RSB- como resultado de una determinaci\u00f3n te\u00f3rica de decisiones din\u00e1micas de las estrategias de comunicaci\u00f3n. Este enfoque se basa en la especificaci\u00f3n de los agentes como \"proveedores\" y \"necesarios\": esto se hace mediante un c\u00e1lculo previo basado en un plan de las necesidades de informaci\u00f3n y las capacidades de provisi\u00f3n de los agentes. Sin embargo, este enfoque no puede ampliarse a redes grandes y din\u00e1micas, ya que ser\u00eda muy ineficiente para cada agente calcular y determinar sus necesidades potenciales y capacidades de suministro de informaci\u00f3n dada su interacci\u00f3n potencial con cientos de otros agentes.Al considerar la recuperaci\u00f3n de informaci\u00f3n en sistemas peer-to-peer desde una perspectiva de sistema multiagente, el enfoque propuesto en -LSB-18-RSB- se basa en un modelo de lenguaje de recopilaci\u00f3n de documentos de agentes. Al explotar los modelos de otros agentes en la red, los agentes construyen su visi\u00f3n de la red que se utiliza para tomar decisiones de enrutamiento. Inicialmente, los agentes construyen sus puntos de vista utilizando los modelos de sus vecinos. Luego, el sistema se reorganiza formando grupos de agentes con contenidos similares. Los cl\u00fasteres se explotan durante la recuperaci\u00f3n de informaci\u00f3n utilizando un enfoque kNN y un esquema de b\u00fasqueda de gradiente. Si bien este trabajo apunta a sintonizar una red para el suministro eficiente de informaci\u00f3n -LRB- a trav\u00e9s de su reorganizaci\u00f3n -RRB-, no demuestra la efectividad del enfoque con respecto a este tema. Adem\u00e1s, aunque durante la reorganizaci\u00f3n y la recuperaci\u00f3n miden la similitud de contenido entre agentes, se necesita un enfoque m\u00e1s detallado que permita a los agentes medir similitudes de elementos de informaci\u00f3n o subcolecciones de elementos de informaci\u00f3n. Bas\u00e1ndose en su trabajo en sistemas peer-to-peer, H.Zhand y V.Lesser en -LSB-19-RSB- estudian sesiones de b\u00fasqueda simult\u00e1neas. Teniendo en cuenta la investigaci\u00f3n en sistemas sem\u00e1nticos peer-to-peer1, la mayor\u00eda de los enfoques explotan lo que puede denominarse vagamente un \"\u00edndice de enrutamiento\". Una cuesti\u00f3n importante relativa a la b\u00fasqueda de informaci\u00f3n es \"qu\u00e9 informaci\u00f3n debe compartirse entre pares, cu\u00e1ndo y qu\u00e9 ajustes deben realizarse para que las consultas se dirijan a fuentes de informaci\u00f3n confiables de la manera m\u00e1s efectiva y eficiente\". RECORDATORIO -LSB- 10 -RSB- los pares recopilan informaci\u00f3n sobre las consultas que han sido respondidas exitosamente por otros pares, para posteriormente seleccionar pares a los que reenviar solicitudes: Este es un enfoque de aprendizaje lento que no implica publicidad del suministro de informaci\u00f3n entre pares. habilidades. Esto da como resultado un proceso de ajuste en el que la recuperaci\u00f3n general aumenta con el tiempo, mientras que la cantidad de mensajes por consulta permanece aproximadamente igual. Aqu\u00ed, los agentes anuncian activamente sus capacidades de suministro de informaci\u00f3n en funci\u00f3n de los intereses evaluados de sus pares: esto da como resultado una cantidad mucho menor de mensajes por consulta que los reportados en REMINDIN '. En -LSB- 5,6 -RSB- los pares, utilizando una ontolog\u00eda com\u00fan, anuncian su experiencia, que se explota para la formaci\u00f3n de una red sem\u00e1ntica superpuesta: las consultas se propagan en esta red dependiendo de su similitud con la experiencia de los pares. Seg\u00fan nuestro enfoque, los agentes anuncian selectivamente sus capacidades de suministro de informaci\u00f3n sobre temas espec\u00edficos a sus vecinos con intereses informativos similares -LRB- y s\u00f3lo a estos -RRB-. Sin embargo, esto se hace a medida que pasa el tiempo y mientras los agentes reciben solicitudes de sus pares. Generan una sobrecarga sustancial en entornos altamente din\u00e1micos, donde los nodos se unen o abandonan el sistema. 248 La Sexta Internacional. Conf. Conjunta.Los agentes anuncian sus capacidades de suministro de informaci\u00f3n teniendo en cuenta los intereses de sus vecinos. Dado el \u00e9xito de este m\u00e9todo, estudiaremos c\u00f3mo la adici\u00f3n de rutas l\u00f3gicas y la evoluci\u00f3n gradual de la topolog\u00eda de la red pueden aumentar a\u00fan m\u00e1s la efectividad del m\u00e9todo propuesto. 6. CONCLUSIONES Este art\u00edculo presenta un m\u00e9todo para el procesamiento de consultas sem\u00e1nticas en grandes redes de agentes que combina \u00edndices de enrutamiento con m\u00e9todos de intercambio de informaci\u00f3n. El m\u00e9todo presentado permite a los agentes mantener registros de los intereses de sus conocidos, anunciar sus capacidades de suministro de informaci\u00f3n a aquellos que tienen un gran inter\u00e9s en ellos y mantener \u00edndices para enrutar consultas a aquellos agentes que tienen las capacidades de suministro de informaci\u00f3n solicitadas. Espec\u00edficamente, el art\u00edculo demuestra a trav\u00e9s de extensos experimentos de rendimiento: -LRB- a -RRB- C\u00f3mo se pueden \"sintonizar\" las redes de agentes para proporcionar la informaci\u00f3n solicitada de manera efectiva, aumentando el beneficio y la eficiencia del sistema. -LRB- b -RRB- C\u00f3mo los diferentes tipos de conocimiento local -LRB- n\u00famero, repositorios de informaci\u00f3n local, porcentaje, intereses y capacidades de provisi\u00f3n de informaci\u00f3n de los conocidos -RRB- pueden guiar a los agentes para responder consultas de manera efectiva, equilibrando eficiencia y eficacia. -LRB- c -RRB- Que la tarea de ``tuning'' propuesta logre incrementar la eficiencia en la b\u00fasqueda e intercambio de informaci\u00f3n en redes de gran tama\u00f1o y alta din\u00e1mica. -LRB- d -RRB- Que la informaci\u00f3n recopilada y mantenida por los agentes respalde la b\u00fasqueda e intercambio de informaci\u00f3n eficiente y eficaz: la informaci\u00f3n inicial sobre las capacidades de provisi\u00f3n de informaci\u00f3n de los conocidos no es necesaria y un peque\u00f1o porcentaje de conocidos es suficiente. El trabajo adicional se refiere a experimentar con datos y ontolog\u00edas reales, diferencias en ontolog\u00edas entre agentes, cambios en la experiencia y la construcci\u00f3n paralela de estructuras superpuestas.Los intereses y las capacidades de suministro de informaci\u00f3n de los conocidos (RRB) pueden guiar a los agentes a responder consultas de manera efectiva, equilibrando la eficiencia y la eficacia. -LRB- c -RRB- Que la tarea de ``tuning'' propuesta logre incrementar la eficiencia en la b\u00fasqueda e intercambio de informaci\u00f3n en redes de gran tama\u00f1o y alta din\u00e1mica. -LRB- d -RRB- Que la informaci\u00f3n recopilada y mantenida por los agentes respalde la b\u00fasqueda e intercambio de informaci\u00f3n eficiente y eficaz: la informaci\u00f3n inicial sobre las capacidades de provisi\u00f3n de informaci\u00f3n de los conocidos no es necesaria y un peque\u00f1o porcentaje de conocidos es suficiente. El trabajo adicional se refiere a experimentar con datos y ontolog\u00edas reales, diferencias en ontolog\u00edas entre agentes, cambios en la experiencia y la construcci\u00f3n paralela de estructuras superpuestas.Los intereses y las capacidades de suministro de informaci\u00f3n de los conocidos (RRB) pueden guiar a los agentes a responder consultas de manera efectiva, equilibrando la eficiencia y la eficacia. -LRB- c -RRB- Que la tarea de ``tuning'' propuesta logre incrementar la eficiencia en la b\u00fasqueda e intercambio de informaci\u00f3n en redes de gran tama\u00f1o y alta din\u00e1mica. -LRB- d -RRB- Que la informaci\u00f3n recopilada y mantenida por los agentes respalde la b\u00fasqueda e intercambio de informaci\u00f3n eficiente y eficaz: la informaci\u00f3n inicial sobre las capacidades de provisi\u00f3n de informaci\u00f3n de los conocidos no es necesaria y un peque\u00f1o porcentaje de conocidos es suficiente. El trabajo adicional se refiere a experimentar con datos y ontolog\u00edas reales, diferencias en ontolog\u00edas entre agentes, cambios en la experiencia y la construcci\u00f3n paralela de estructuras superpuestas.", "keyphrases": ["informar buscar y compartir", "red social", "agente de cobre", "red de b\u00fasqueda de igual a igual", "sistema de igual a igual", "Red din\u00e1mica y de gran escala.", "proceso de decis de observaci\u00f3n parcial decente de markov", "control descentrador", "algoritmo miope", "enfoque knn", "esquema de b\u00fasqueda de gradiente"]}
{"file_name": "J-1", "text": "Mecanismos generalizados de reducci\u00f3n del comercio RESUMEN Al dise\u00f1ar un mecanismo hay varias propiedades deseables que mantener, como la compatibilidad de incentivos -LRB- IC -RRB-, la racionalidad individual -LRB- IR -RRB- y el equilibrio presupuestario -LRB- BB -RRB-. Es bien sabido -LSB- 15 -RSB- que es imposible que un mecanismo maximice el bienestar social siendo al mismo tiempo IR, IC y BB. Ha habido varios intentos de eludir -LSB- 15 -RSB- intercambiando bienestar por BB, por ejemplo, en dominios como subastas bilaterales -LSB- 13 -RSB-, mercados distribuidos -LSB- 3 -RSB- y cadena de suministro. problemas -LSB- 2, 4 -RSB-. En este art\u00edculo proporcionamos un procedimiento llamado Reducci\u00f3n Comercial Generalizada -LRB- GTR -RRB- para jugadores de un solo valor, que dado un mecanismo de IR e IC, genera un mecanismo que es IR, IC y BB con una p\u00e9rdida de bienestar. Limitamos el bienestar logrado mediante nuestro procedimiento a una amplia gama de \u00e1mbitos. En particular, nuestros resultados mejoran las soluciones existentes para problemas como mercados bilaterales con bienes homog\u00e9neos, mercados distribuidos y varios tipos de cadenas de suministro. Adem\u00e1s, nuestra soluci\u00f3n proporciona mecanismos de equilibrio presupuestario para varios problemas abiertos, como subastas combinatorias de doble cara y mercados distribuidos con ventajas de transporte estrat\u00e9gicas. 1. INTRODUCCI\u00d3N Al dise\u00f1ar un mecanismo, hay varias propiedades clave que es deseable mantener. En muchos de los mecanismos, la funci\u00f3n objetivo que el dise\u00f1ador de un mecanismo intenta maximizar es el bienestar social: el beneficio total para la sociedad. Sin embargo, es bien sabido por -LSB-15-RSB- que cualquier mecanismo que maximice el bienestar social manteniendo al mismo tiempo la racionalidad individual y la compatibilidad de incentivos genera necesariamente un d\u00e9ficit, es decir, no est\u00e1 presupuestariamente equilibrado. Para mantener la propiedad BB en un mecanismo IR e IC es necesario comprometerse con la optimizaci\u00f3n del bienestar social. 1.1 Trabajo relacionado y soluciones espec\u00edficas Ha habido varios intentos de dise\u00f1ar mecanismos de equilibrio presupuestario para dominios particulares2. En el problema de los mercados distribuidos -LRB- y en problemas estrechamente relacionados -RRB- los bienes se transportan entre ubicaciones geogr\u00e1ficas incurriendo en alg\u00fan costo constante por el transporte. -LSB- 16, 9, 3 -RSB- presentan mecanismos que se aproximan al bienestar social logrando un mecanismo de IR, IC y BB. Para problemas de la cadena de suministro -LSB- 2, 4 -RSB- limita la p\u00e9rdida de bienestar social que es necesario infligir al mecanismo para lograr la combinaci\u00f3n deseada de IR, IC y BB. A pesar de los trabajos discutidos anteriormente, la cuesti\u00f3n de c\u00f3mo dise\u00f1ar un mecanismo general que logre IR, IC y BB independientemente del dominio del problema permanece abierta. Adem\u00e1s, hay varios \u00e1mbitos en los que la cuesti\u00f3n de c\u00f3mo dise\u00f1ar un mecanismo de IR, CI y BB que se aproxime al bienestar social sigue siendo un problema abierto. Por ejemplo,En el importante \u00e1mbito de las subastas combinatorias de doble cara no se conoce ning\u00fan resultado que limite la p\u00e9rdida de bienestar social necesaria para lograr el equilibrio presupuestario. Otro ejemplo interesante es la pregunta abierta que dej\u00f3 -LSB- 3 -RSB-: \u00bfC\u00f3mo se puede limitar la p\u00e9rdida de bienestar social que se necesita para lograr el equilibrio presupuestario en un mercado distribuido de IR e IC donde las ventajas del transporte son estrat\u00e9gicas? Naturalmente, una respuesta al mercado distribuido de BB con ventajas estrat\u00e9gicas tiene enormes implicaciones pr\u00e1cticas, por ejemplo para las redes de transporte. 1.2 Nuestra Contribuci\u00f3n En este art\u00edculo unificamos todos los problemas discutidos anteriormente -LRB- tanto los resueltos como los abiertos -RRB- en un solo procedimiento de concepto de soluci\u00f3n. El procedimiento de soluci\u00f3n se denomin\u00f3 Reducci\u00f3n Comercial Generalizada -LRB-GTR-RRB-. GTR acepta un mecanismo IR e IC para jugadores de un solo valor y genera un mecanismo IR, IC y BB. El mecanismo de producci\u00f3n puede sufrir cierta p\u00e9rdida de bienestar como compensaci\u00f3n por lograr BB. Hay casos problem\u00e1ticos en los que no es necesaria una p\u00e9rdida de bienestar, pero por -LSB- 15 -RSB- hay casos problem\u00e1ticos en los que s\u00ed hay p\u00e9rdida de bienestar. Sin embargo, para una amplia clase de problemas podemos limitar la p\u00e9rdida de bienestar. Un caso particularmente interesante es aquel en el que el mecanismo de entrada es una asignaci\u00f3n eficiente. Adem\u00e1s de unificar muchos de los problemas de BB bajo un concepto de soluci\u00f3n \u00fanica, el procedimiento GTR mejora los resultados existentes y resuelve varios problemas abiertos en la literatura. Las soluciones existentes que mejora nuestro procedimiento GTR son subastas homog\u00e9neas de doble cara, mercados distribuidos -LSB- 3 -RSB- y cadena de suministro -LSB- 2, 4 -RSB-. Para las subastas homog\u00e9neas de doble cara, el procedimiento de soluci\u00f3n GTR mejora la conocida soluci\u00f3n -LSB-13-RSB- al permitir algunos casos en los que no se reduce el comercio en absoluto. Para los mercados distribuidos -LSB- 3 -RSB- y la cadena de suministro -LSB- 2, 4 -RSB- el procedimiento de soluci\u00f3n GTR mejora el l\u00edmite de p\u00e9rdidas de bienestar, es decir, permite lograr un mecanismo IR, IC y BB con menor p\u00e9rdida para el bienestar social. Recientemente tambi\u00e9n supimos que el procedimiento GTR permite convertir el modelo reci\u00e9n presentado -LSB- 6 -RSB- en un mecanismo BB. Adem\u00e1s de la contribuci\u00f3n principal descrita anteriormente, este art\u00edculo tambi\u00e9n define una clasificaci\u00f3n importante de dominios de problemas. Definimos dominios basados \u200b\u200ben clases y dominios basados \u200b\u200ben clases de adquisici\u00f3n. Las definiciones anteriores se basan en los diferentes \"poderes\" de competencia de los jugadores en mecanismos llamados competencia interna y externa.2 Nuestra Contribuci\u00f3n En este art\u00edculo unificamos todos los problemas discutidos anteriormente -LRB- tanto los resueltos como los abiertos -RRB- en un solo procedimiento de concepto de soluci\u00f3n. El procedimiento de soluci\u00f3n se denomin\u00f3 Reducci\u00f3n Comercial Generalizada -LRB-GTR-RRB-. GTR acepta un mecanismo IR e IC para jugadores de un solo valor y genera un mecanismo IR, IC y BB. El mecanismo de producci\u00f3n puede sufrir cierta p\u00e9rdida de bienestar como compensaci\u00f3n por lograr BB. Hay casos problem\u00e1ticos en los que no es necesaria una p\u00e9rdida de bienestar, pero por -LSB- 15 -RSB- hay casos problem\u00e1ticos en los que s\u00ed hay p\u00e9rdida de bienestar. Sin embargo, para una amplia clase de problemas podemos limitar la p\u00e9rdida de bienestar. Un caso particularmente interesante es aquel en el que el mecanismo de entrada es una asignaci\u00f3n eficiente. Adem\u00e1s de unificar muchos de los problemas de BB bajo un concepto de soluci\u00f3n \u00fanica, el procedimiento GTR mejora los resultados existentes y resuelve varios problemas abiertos en la literatura. Las soluciones existentes que mejora nuestro procedimiento GTR son subastas homog\u00e9neas de doble cara, mercados distribuidos -LSB- 3 -RSB- y cadena de suministro -LSB- 2, 4 -RSB-. Para las subastas homog\u00e9neas de doble cara, el procedimiento de soluci\u00f3n GTR mejora la conocida soluci\u00f3n -LSB-13-RSB- al permitir algunos casos en los que no se reduce el comercio en absoluto. Para los mercados distribuidos -LSB- 3 -RSB- y la cadena de suministro -LSB- 2, 4 -RSB- el procedimiento de soluci\u00f3n GTR mejora el l\u00edmite de p\u00e9rdidas de bienestar, es decir, permite lograr un mecanismo IR, IC y BB con menor p\u00e9rdida para el bienestar social. Recientemente tambi\u00e9n supimos que el procedimiento GTR permite convertir el modelo reci\u00e9n presentado -LSB- 6 -RSB- en un mecanismo BB. Adem\u00e1s de la contribuci\u00f3n principal descrita anteriormente, este art\u00edculo tambi\u00e9n define una clasificaci\u00f3n importante de dominios de problemas. Definimos dominios basados \u200b\u200ben clases y dominios basados \u200b\u200ben clases de adquisici\u00f3n. Las definiciones anteriores se basan en los diferentes \"poderes\" de competencia de los jugadores en mecanismos llamados competencia interna y externa.2 Nuestra Contribuci\u00f3n En este art\u00edculo unificamos todos los problemas discutidos anteriormente -LRB- tanto los resueltos como los abiertos -RRB- en un solo procedimiento de concepto de soluci\u00f3n. El procedimiento de soluci\u00f3n se denomin\u00f3 Reducci\u00f3n Comercial Generalizada -LRB-GTR-RRB-. GTR acepta un mecanismo IR e IC para jugadores de un solo valor y genera un mecanismo IR, IC y BB. El mecanismo de producci\u00f3n puede sufrir cierta p\u00e9rdida de bienestar como compensaci\u00f3n por lograr BB. Hay casos problem\u00e1ticos en los que no es necesaria una p\u00e9rdida de bienestar, pero por -LSB- 15 -RSB- hay casos problem\u00e1ticos en los que s\u00ed hay p\u00e9rdida de bienestar. Sin embargo, para una amplia clase de problemas podemos limitar la p\u00e9rdida de bienestar. Un caso particularmente interesante es aquel en el que el mecanismo de entrada es una asignaci\u00f3n eficiente. Adem\u00e1s de unificar muchos de los problemas de BB bajo un concepto de soluci\u00f3n \u00fanica, el procedimiento GTR mejora los resultados existentes y resuelve varios problemas abiertos en la literatura. Las soluciones existentes que mejora nuestro procedimiento GTR son subastas homog\u00e9neas de doble cara, mercados distribuidos -LSB- 3 -RSB- y cadena de suministro -LSB- 2, 4 -RSB-. Para las subastas homog\u00e9neas de doble cara, el procedimiento de soluci\u00f3n GTR mejora la conocida soluci\u00f3n -LSB-13-RSB- al permitir algunos casos en los que no se reduce el comercio en absoluto. Para los mercados distribuidos -LSB- 3 -RSB- y la cadena de suministro -LSB- 2, 4 -RSB- el procedimiento de soluci\u00f3n GTR mejora el l\u00edmite de p\u00e9rdidas de bienestar, es decir, permite lograr un mecanismo IR, IC y BB con menor p\u00e9rdida para el bienestar social. Recientemente tambi\u00e9n supimos que el procedimiento GTR permite convertir el modelo reci\u00e9n presentado -LSB- 6 -RSB- en un mecanismo BB. Adem\u00e1s de la contribuci\u00f3n principal descrita anteriormente, este art\u00edculo tambi\u00e9n define una clasificaci\u00f3n importante de dominios de problemas. Definimos dominios basados \u200b\u200ben clases y dominios basados \u200b\u200ben clases de adquisici\u00f3n. Las definiciones anteriores se basan en los diferentes \"poderes\" de competencia de los jugadores en mecanismos llamados competencia interna y externa.Para las subastas homog\u00e9neas de doble cara, el procedimiento de soluci\u00f3n GTR mejora la conocida soluci\u00f3n -LSB-13-RSB- al permitir algunos casos en los que no se reduce el comercio en absoluto. Para los mercados distribuidos -LSB- 3 -RSB- y la cadena de suministro -LSB- 2, 4 -RSB- el procedimiento de soluci\u00f3n GTR mejora el l\u00edmite de p\u00e9rdidas de bienestar, es decir, permite lograr un mecanismo IR, IC y BB con menor p\u00e9rdida para el bienestar social. Recientemente tambi\u00e9n supimos que el procedimiento GTR permite convertir el modelo reci\u00e9n presentado -LSB- 6 -RSB- en un mecanismo BB. Adem\u00e1s de la contribuci\u00f3n principal descrita anteriormente, este art\u00edculo tambi\u00e9n define una clasificaci\u00f3n importante de dominios de problemas. Definimos dominios basados \u200b\u200ben clases y dominios basados \u200b\u200ben clases de adquisici\u00f3n. Las definiciones anteriores se basan en los diferentes \"poderes\" de competencia de los jugadores en mecanismos llamados competencia interna y externa.Para las subastas homog\u00e9neas de doble cara, el procedimiento de soluci\u00f3n GTR mejora la conocida soluci\u00f3n -LSB-13-RSB- al permitir algunos casos en los que no se reduce el comercio en absoluto. Para los mercados distribuidos -LSB- 3 -RSB- y la cadena de suministro -LSB- 2, 4 -RSB- el procedimiento de soluci\u00f3n GTR mejora el l\u00edmite de p\u00e9rdidas de bienestar, es decir, permite lograr un mecanismo IR, IC y BB con menor p\u00e9rdida para el bienestar social. Recientemente tambi\u00e9n supimos que el procedimiento GTR permite convertir el modelo reci\u00e9n presentado -LSB- 6 -RSB- en un mecanismo BB. Adem\u00e1s de la contribuci\u00f3n principal descrita anteriormente, este art\u00edculo tambi\u00e9n define una clasificaci\u00f3n importante de dominios de problemas. Definimos dominios basados \u200b\u200ben clases y dominios basados \u200b\u200ben clases de adquisici\u00f3n. Las definiciones anteriores se basan en los diferentes \"poderes\" de competencia de los jugadores en mecanismos llamados competencia interna y externa.", "keyphrases": ["reducci\u00f3n comercial", "saldo presupuestario", "competencia interna", "competencia externa", "eficiente", "poder del jugador", "reducci\u00f3n del comercio de g\u00e9nero", "gtr", "optimo", "desigualdad en bienestar", "jugador multimente", "mecanismo de equilibrio presupuestario", "homog\u00e9neo bueno", "mercado de distribuci\u00f3n espacial"]}
{"file_name": "H-24", "text": "Investigaci\u00f3n del comportamiento de consulta y navegaci\u00f3n de los usuarios de motores de b\u00fasqueda avanzados RESUMEN Una forma de ayudar a todos los usuarios de motores de b\u00fasqueda web comerciales a tener m\u00e1s \u00e9xito en sus b\u00fasquedas es comprender mejor lo que hacen los usuarios con mayor experiencia en b\u00fasquedas y utilizar este conocimiento para beneficiar a todos. . En este art\u00edculo estudiamos los registros de interacci\u00f3n de los usuarios de motores de b\u00fasqueda avanzados -LRB- y de aquellos no tan avanzados -RRB- para comprender mejor c\u00f3mo realizan b\u00fasquedas estos grupos de usuarios. Los resultados muestran que existen marcadas diferencias en las consultas, los clics en los resultados, la navegaci\u00f3n posterior a la consulta y el \u00e9xito de la b\u00fasqueda de los usuarios que clasificamos como avanzados -LRB- en funci\u00f3n de su uso de operadores de consulta -RRB-, en relaci\u00f3n con los clasificados como no avanzado. Nuestros hallazgos tienen implicaciones sobre c\u00f3mo se debe apoyar a los usuarios avanzados durante sus b\u00fasquedas y c\u00f3mo sus interacciones podr\u00edan usarse para ayudar a los buscadores de todos los niveles de experiencia a encontrar informaci\u00f3n m\u00e1s relevante y aprender estrategias de b\u00fasqueda mejoradas. 1. INTRODUCCI\u00d3N La formulaci\u00f3n de declaraciones de consulta que capturen los aspectos m\u00e1s destacados de las necesidades de informaci\u00f3n y que sean significativas para los sistemas de recuperaci\u00f3n de informaci\u00f3n -LRB- IR -RRB- plantea un desaf\u00edo para muchos buscadores -LSB- 3 -RSB-. Estas t\u00e9cnicas pueden ser \u00fatiles para mejorar la precisi\u00f3n de los resultados, pero, adem\u00e1s de mediante an\u00e1lisis de registros -LRB-, por ejemplo, -LSB- 15 -RSB- -LSB- 27 -RSB- -RRB-, la comunidad investigadora generalmente las ha pasado por alto en sus intentos. para mejorar la calidad de los resultados de b\u00fasqueda. La investigaci\u00f3n de RI generalmente se ha centrado en formas alternativas para que los usuarios especifiquen sus necesidades en lugar de aumentar la adopci\u00f3n de sintaxis avanzada. En los \u00faltimos a\u00f1os se ha intensificado la investigaci\u00f3n sobre t\u00e9cnicas pr\u00e1cticas para complementar la tecnolog\u00eda de b\u00fasqueda existente y ayudar a los usuarios -LRB- por ejemplo -LSB- 18 -RSB- -LSB- 34 -RSB- -RRB-. Sin embargo, implementar estas t\u00e9cnicas a gran escala con latencias tolerables resulta complicado. Las consultas t\u00edpicas enviadas a los motores de b\u00fasqueda web toman la forma de una serie de tokens separados por espacios. Generalmente hay un operador booleano AND impl\u00edcito entre tokens que restringe los resultados de b\u00fasqueda a documentos que contienen todos los t\u00e9rminos de consulta. De Lima y Pedersen -LSB- 7 -RSB- investigaron el efecto del an\u00e1lisis, el reconocimiento de frases y la expansi\u00f3n en las consultas de b\u00fasqueda web. Demostraron que el reconocimiento autom\u00e1tico de frases en consultas puede mejorar la precisi\u00f3n de los resultados en la b\u00fasqueda web. Sin embargo, el valor de la sintaxis avanzada para los buscadores t\u00edpicos generalmente ha sido limitado, ya que la mayor\u00eda de los usuarios no conocen la sintaxis avanzada o no entienden c\u00f3mo usarla -LSB- 15 -RSB-. En este art\u00edculo exploramos el uso de operadores de consulta con m\u00e1s detalle y proponemos aplicaciones alternativas que no requieren que todos los usuarios utilicen expl\u00edcitamente sintaxis avanzada. Nuestra hip\u00f3tesis es que los buscadores que utilizan sintaxis de consulta avanzada demuestran un grado de experiencia en b\u00fasquedas que la mayor\u00eda de la poblaci\u00f3n de usuarios no tiene; afirmaci\u00f3n respaldada por investigaciones previas -LSB- 13 -RSB-.Estudiar el comportamiento de estos usuarios de motores de b\u00fasqueda avanzados puede arrojar informaci\u00f3n importante sobre la b\u00fasqueda y la exploraci\u00f3n de resultados de la que otros pueden beneficiarse. Utilizando registros recopilados de una gran cantidad de usuarios que dieron su consentimiento, investigamos las diferencias entre el comportamiento de b\u00fasqueda de aquellos que usan sintaxis avanzada y los que no, y las diferencias en la informaci\u00f3n a la que apuntan esos usuarios. Nos interesa responder tres preguntas de investigaci\u00f3n: -LRB- i -RRB- \u00bfExiste una relaci\u00f3n entre el uso de sintaxis avanzada y otras caracter\u00edsticas de una b\u00fasqueda? -LRB- ii -RRB- \u00bfExiste una relaci\u00f3n entre el uso de sintaxis avanzada y los comportamientos de navegaci\u00f3n posteriores a la consulta? -LRB- iii -RRB- \u00bfExiste una relaci\u00f3n entre el uso de sintaxis avanzada y las medidas de \u00e9xito de la b\u00fasqueda? A trav\u00e9s de un estudio y an\u00e1lisis experimental, ofrecemos posibles respuestas para cada una de estas preguntas. Una relaci\u00f3n entre el uso de sintaxis avanzada y cualquiera de estas caracter\u00edsticas podr\u00eda respaldar el dise\u00f1o de sistemas adaptados a los usuarios de motores de b\u00fasqueda avanzados, o utilizar las interacciones de los usuarios avanzados para ayudar a los usuarios no avanzados a tener m\u00e1s \u00e9xito en sus b\u00fasquedas. Describimos el trabajo relacionado en la Secci\u00f3n 2, los datos que utilizamos en este estudio basado en registros en la Secci\u00f3n 3, las caracter\u00edsticas de b\u00fasqueda en las que centramos nuestro an\u00e1lisis en la Secci\u00f3n 4 y los hallazgos de este an\u00e1lisis en la Secci\u00f3n 5. 2. Factores de TRABAJO RELACIONADO tales como la falta de conocimiento del dominio, la mala comprensi\u00f3n de la colecci\u00f3n de documentos que se busca y una necesidad de informaci\u00f3n poco desarrollada pueden influir en la calidad de las consultas que los usuarios env\u00edan a los sistemas IR -LRB- -LSB- 24 -RSB-, -LSB- 28 -RSB- -RRB-. Se han realizado diversas investigaciones sobre diferentes formas de ayudar a los usuarios a especificar sus necesidades de informaci\u00f3n de manera m\u00e1s efectiva. Belkin et al. -LSB- 4 -RSB- experiment\u00f3 proporcionando espacio adicional para que los usuarios escriban una descripci\u00f3n m\u00e1s detallada de sus necesidades de informaci\u00f3n. Kelly et al. intentaron un enfoque similar. -LSB- 18 -RSB-, quienes utilizaron formularios de aclaraci\u00f3n para obtener informaci\u00f3n adicional sobre el context de b\u00fasqueda de los usuarios. Se ha demostrado que estos enfoques son eficaces en los sistemas de recuperaci\u00f3n de mejores coincidencias, donde las consultas m\u00e1s largas generalmente conducen a resultados de b\u00fasqueda m\u00e1s relevantes -LSB- 4 -RSB-. Sin embargo, en la b\u00fasqueda web, donde muchos de los sistemas se basan en un modelo de recuperaci\u00f3n booleano extendido, las consultas m\u00e1s largas pueden en realidad afectar el rendimiento de la recuperaci\u00f3n, lo que lleva a que se recupere una peque\u00f1a cantidad de resultados potencialmente irrelevantes. No basta simplemente con solicitar m\u00e1s informaci\u00f3n a los usuarios; esta informaci\u00f3n debe ser de mejor calidad. La retroalimentaci\u00f3n de relevancia -LRB- RF -RRB- -LSB- 22 -RSB- y la expansi\u00f3n de consultas interactivas -LSB- 9 -RSB- son t\u00e9cnicas populares que se han utilizado para mejorar la calidad de la informaci\u00f3n que los usuarios brindan a los sistemas de IR con respecto a sus necesidades de informaci\u00f3n. . En el caso de RF, el usuario presenta al sistema ejemplos de informaci\u00f3n relevante que luego se utilizan para formular una consulta mejorada o recuperar un nuevo conjunto de documentos.Ha resultado dif\u00edcil lograr que los usuarios utilicen RF en el dominio web debido a la dificultad para transmitir el significado y el beneficio de RF a los usuarios t\u00edpicos -LSB- 17 -RSB-. Las sugerencias de consultas que se ofrecen en funci\u00f3n de los registros de consultas tienen el potencial de mejorar el rendimiento de la recuperaci\u00f3n con una carga limitada para el usuario. Este enfoque se limita a volver a ejecutar consultas populares y los buscadores a menudo ignoran las sugerencias que se les presentan -LSB- 1 -RSB-. Adem\u00e1s, ambas t\u00e9cnicas no ayudan a los usuarios a aprender a generar consultas m\u00e1s efectivas. La mayor\u00eda de los motores de b\u00fasqueda comerciales proporcionan una sintaxis de consulta avanzada que permite a los usuarios especificar sus necesidades de informaci\u00f3n con m\u00e1s detalle. Los operadores booleanos -LRB- AND, OR y NOT -RRB- pueden unir t\u00e9rminos y frases, y se pueden usar modificadores como ``sitio:'' y ``enlace:'' para restringir el espacio de b\u00fasqueda. Las consultas creadas con estas t\u00e9cnicas pueden ser poderosas. El an\u00e1lisis basado en registros de las interacciones de los usuarios con los motores de b\u00fasqueda Excite y AltaVista ha demostrado que s\u00f3lo entre el 10 y el 20 % de las consultas conten\u00edan alguna sintaxis avanzada -LSB- 14 -RSB- -LSB- 25 -RSB-. Este an\u00e1lisis puede ser una forma \u00fatil de capturar las caracter\u00edsticas de los usuarios que interact\u00faan con los sistemas IR. La investigaci\u00f3n sobre modelado de usuarios -LSB- 6 -RSB- y personalizaci\u00f3n -LSB- 30 -RSB- ha demostrado que recopilar m\u00e1s informaci\u00f3n sobre los usuarios puede mejorar la efectividad de las b\u00fasquedas, pero requiere m\u00e1s informaci\u00f3n sobre los usuarios de la que normalmente est\u00e1 disponible \u00fanicamente en los registros de interacci\u00f3n. A menos que se combine con una t\u00e9cnica cualitativa, como un cuestionario posterior a la sesi\u00f3n -LSB- 23 -RSB-, puede resultar dif\u00edcil asociar las interacciones con las caracter\u00edsticas del usuario. En nuestro estudio conjeturamos que, dada la dificultad para ubicar funciones de b\u00fasqueda avanzada dentro de la interfaz de b\u00fasqueda t\u00edpica y los problemas potenciales para comprender la sintaxis, aquellos usuarios que usan sintaxis avanzada regularmente representan una clase distinta de buscadores que exhibir\u00e1n otros comportamientos de b\u00fasqueda comunes. . Otros estudios sobre los comportamientos de b\u00fasqueda de los buscadores avanzados han intentado comprender mejor el conocimiento estrat\u00e9gico que han adquirido. No obstante, pueden brindar informaci\u00f3n valiosa sobre los comportamientos de los usuarios con experiencia en dominios, sistemas o b\u00fasquedas que excede la del usuario promedio. El comportamiento de consulta en particular se ha estudiado ampliamente para comprender mejor a los usuarios -LSB- 31 -RSB- y ayudar a otros usuarios -LSB- 16 -RSB-. En este art\u00edculo estudiamos otras caracter\u00edsticas de b\u00fasqueda de los usuarios de sintaxis avanzada en un intento de determinar si hay algo diferente en c\u00f3mo buscan estos usuarios de motores de b\u00fasqueda, y si sus b\u00fasquedas pueden usarse para beneficiar a aquellos que no hacen uso de las funciones avanzadas. de los motores de b\u00fasqueda. Para ello utilizamos registros de interacci\u00f3n recopilados de un gran conjunto de usuarios que dan su consentimiento durante un per\u00edodo prolongado. En la siguiente secci\u00f3n describimos los datos que utilizamos para estudiar el comportamiento de los usuarios que utilizan sintaxis avanzada, en relaci\u00f3n con aquellos que no utilizan esta sintaxis. Sesi\u00f3n 11 de actas de SIGIR 2007:Comportamiento de interacci\u00f3n, como consultas, clics en resultados, navegaci\u00f3n posterior a la consulta y \u00e9xito de la b\u00fasqueda. La clasificaci\u00f3n cruda de los usuarios basada en una sola caracter\u00edstica que se puede extraer f\u00e1cilmente del flujo de consultas arroja resultados notables sobre el comportamiento de interacci\u00f3n de los usuarios que no usan la sintaxis y los que s\u00ed la usan. Como hemos sugerido, los sistemas de b\u00fasqueda pueden aprovechar las interacciones de estos usuarios para mejorar la clasificaci\u00f3n de documentos, la recomendaci\u00f3n de p\u00e1ginas o incluso la capacitaci\u00f3n de los usuarios.", "keyphrases": ["motor de b\u00fasqueda", "pregunta", "informar relevante", "estrategia de b\u00fasqueda", "tolerante al latencia", "sintaxis avanzada", "comportamiento de navegaci\u00f3n", "comportamiento de b\u00fasqueda", "\u00e9xito de b\u00fasqueda", "retroalimentaci\u00f3n relevante", "relevante"]}
{"file_name": "J-8", "text": "Fuerte equilibrio en juegos de conexi\u00f3n con costos compartidos * RESUMEN En este trabajo estudiamos los juegos de conexi\u00f3n con costos compartidos, donde cada jugador tiene una fuente y un sumidero que le gustar\u00eda conectar, y el costo de los bordes se comparte equitativamente -LRB- juegos de conexi\u00f3n justa - RRB- o de forma arbitraria -LRB- juegos generales de conexi\u00f3n -RRB-. Se estudian las topolog\u00edas de grafos que garantizan la existencia de un equilibrio fuerte -LRB- donde ninguna coalici\u00f3n puede mejorar el coste de cada uno de sus miembros -RRB- independientemente de los costes espec\u00edficos en los bordes. Nuestros principales resultados de existencia son los siguientes: -LRB- 1 -RRB- Para una \u00fanica fuente y sumidero mostramos que siempre hay un equilibrio fuerte -LRB- tanto para juegos de conexi\u00f3n justos como generales -RRB-. -LRB- 2 -RRB- Para una sola fuente y m\u00faltiples sumideros mostramos que para un gr\u00e1fico paralelo en serie siempre existe un equilibrio fuerte -LRB- tanto para juegos de conexi\u00f3n justos como para juegos de conexi\u00f3n general -RRB-. -LRB- 3 -RRB- Para fuentes m\u00faltiples y sumidero mostramos que un gr\u00e1fico paralelo de extensi\u00f3n siempre admite un equilibrio fuerte en juegos de conexi\u00f3n justa. En cuanto a la calidad del equilibrio fuerte, mostramos que en cualquier juego de conexi\u00f3n justa el costo de un equilibrio fuerte es \u0398 -LRB- log n -RRB- de la soluci\u00f3n \u00f3ptima, donde n es el n\u00famero de jugadores. -LRB- Esto debe contrastarse con el precio \u03a9 -LRB- n -RRB- de la anarqu\u00eda para el mismo escenario. -RRB- Para juegos de conexi\u00f3n general de fuente \u00fanica y juegos de conexi\u00f3n justa de fuente \u00fanica, demostramos que un equilibrio fuerte es siempre una soluci\u00f3n \u00f3ptima. * Investigaci\u00f3n financiada en parte por una subvenci\u00f3n de la Fundaci\u00f3n de Ciencias de Israel, la Fundaci\u00f3n Binacional de Ciencias -LRB- BSF -RRB-, la Fundaci\u00f3n Alemana-Israel\u00ed -LRB- GIF -RRB-, la Beca Lady Davis, un premio de la facultad de IBM y el Programa IST de la Comunidad Europea, bajo la Red de Excelencia PASCAL, IST-2002-506778. Esta publicaci\u00f3n s\u00f3lo refleja las opiniones de los autores. 1. INTRODUCCI\u00d3N La teor\u00eda de juegos computacional ha introducido la cuesti\u00f3n de los incentivos en muchos de los problemas cl\u00e1sicos de optimizaci\u00f3n combinatoria. Considere los problemas cl\u00e1sicos de enrutamiento y transporte, como los problemas de multidifusi\u00f3n o de m\u00faltiples productos, que muchas veces se ven de la siguiente manera. Se nos proporciona un gr\u00e1fico con los costos de borde y las demandas de conectividad entre nodos, y nuestro objetivo es encontrar una soluci\u00f3n de costo m\u00ednimo. El punto de vista de la teor\u00eda de juegos asumir\u00eda que cada demanda individual est\u00e1 controlada por un jugador que optimiza su propia utilidad, y el resultado resultante podr\u00eda estar lejos de la soluci\u00f3n \u00f3ptima. Al considerar los incentivos individuales es necesario discutir el concepto de soluci\u00f3n apropiado. Gran parte de la investigaci\u00f3n en teor\u00eda de juegos computacional se ha centrado en el equilibrio cl\u00e1sico de Nash como concepto de soluci\u00f3n primaria. De hecho el equilibrio de Nash tiene muchos beneficios, y lo m\u00e1s importante es que siempre existe -LRB- en estrategias mixtas -RRB-. Sin embargo, el concepto de soluci\u00f3n del equilibrio de Nash s\u00f3lo es resistente a desviaciones unilaterales, mientras que en realidad los jugadores pueden coordinar sus acciones.Un equilibrio fuerte -LSB- 4 -RSB- es un estado del cual ninguna coalici\u00f3n -LRB- de cualquier tama\u00f1o -RRB- puede desviarse y mejorar la utilidad de cada miembro de la coalici\u00f3n -LRB- al tiempo que posiblemente reduce la utilidad de los jugadores fuera de la coalici\u00f3n. coalici\u00f3n -RRB-. Esta resiliencia a las desviaciones de las coaliciones de los jugadores es muy atractiva, y uno puede esperar que una vez que se alcance un equilibrio fuerte, sea muy probable que se mantenga. Desde el punto de vista de la teor\u00eda de juegos computacional, un beneficio adicional de un equilibrio fuerte es que tiene el potencial de reducir la distancia entre la soluci\u00f3n \u00f3ptima y la soluci\u00f3n obtenida como resultado de un comportamiento ego\u00edsta. El precio fuerte de la anarqu\u00eda -LRB- SPoA -RRB-, introducido en -LSB- 1 -RSB-, es la relaci\u00f3n entre el coste del peor equilibrio fuerte y el coste de una soluci\u00f3n \u00f3ptima. Obviamente, el SPoA s\u00f3lo tiene sentido en aquellos casos en los que existe un equilibrio fuerte. Una desventaja importante del equilibrio fuerte es que la mayor\u00eda de los juegos no admiten ning\u00fan equilibrio fuerte. Incluso los juegos cl\u00e1sicos simples como el dilema del prisionero no poseen ning\u00fan equilibrio fuerte -LRB-, lo que tambi\u00e9n es un ejemplo de un juego de congesti\u00f3n que no posee equilibrios fuertes -RRB-. Este desafortunado hecho ha reducido la concentraci\u00f3n en equilibrio fuerte, a pesar de sus propiedades altamente atractivas. En este trabajo nos concentramos en los juegos de conexi\u00f3n de costos compartidos, introducidos por -LSB- 3, 2 -RSB-. En un juego de este tipo, hay un gr\u00e1fico dirigido subyacente con costos de borde, y los usuarios individuales tienen demandas de conectividad -LRB- entre una fuente y un sumidero -RRB-. Consideramos dos modelos. El modelo de conexi\u00f3n de coste justo -LSB- 2 -RSB- permite a cada jugador seleccionar un camino desde la fuente hasta el sumidero2. En este juego, el costo de una ventaja se comparte equitativamente entre todos los jugadores que seleccionaron la ventaja, y el costo del jugador es la suma de sus costos en las ventajas que seleccion\u00f3. El juego de conexi\u00f3n general -LSB- 3 -RSB- permite a cada jugador ofrecer precios por las ventajas. En este juego se compra una ventaja si la suma de las ofertas cubre al menos su coste, y el coste del jugador es la suma de sus ofertas en las ventajas compradas -LRB- en ambos juegos asumimos que el jugador tiene que garantizar la ventaja. conectividad entre su fuente y su sumidero -RRB-. En este trabajo nos centramos en dos cuestiones importantes. El primero es identificar bajo qu\u00e9 condiciones se garantiza la existencia de un equilibrio fuerte, y el segundo es la calidad de los equilibrios fuertes. Para la parte de existencia, identificamos familias de topolog\u00edas de gr\u00e1ficos que poseen un equilibrio fuerte para cualquier asignaci\u00f3n de costos de borde. Se puede ver esta separaci\u00f3n entre la topolog\u00eda del gr\u00e1fico y los costos de los bordes, como una separaci\u00f3n entre la infraestructura subyacente y los costos que los jugadores observan para comprar bordes. Si bien se espera que la infraestructura sea estable durante largos per\u00edodos de tiempo, los costos que observan los jugadores pueden modificarse f\u00e1cilmente en per\u00edodos cortos. Nuestros resultados son los siguientes.Para el caso de un solo bien -LRB- todos los jugadores tienen la misma fuente y sumidero -RRB-, existe un fuerte equilibrio en cualquier gr\u00e1fico -LRB- tanto para juegos de conexi\u00f3n justos como generales -RRB-. Adem\u00e1s, el equilibrio fuerte tambi\u00e9n lo es, mientras que se sabe que cualquier juego de congesti\u00f3n admite al menos un equilibrio de Nash en estrategias puras -LSB- 16 -RSB-. 2El esquema justo de reparto de costos tambi\u00e9n es atractivo desde el punto de vista del dise\u00f1o del mecanismo, ya que es un mecanismo de reparto de costos a prueba de estrategias -LSB- 14 -RSB-. la soluci\u00f3n \u00f3ptima -LRB- es decir, los jugadores comparten un camino m\u00e1s corto desde la fuente com\u00fan hasta el sumidero com\u00fan -RRB-. Para el caso de una \u00fanica fuente y m\u00faltiples sumideros -LRB-, por ejemplo, en un \u00e1rbol de multidifusi\u00f3n -RRB-, mostramos que en un juego de conexi\u00f3n justa existe un equilibrio fuerte si el gr\u00e1fico subyacente es un gr\u00e1fico en serie paralela, y mostramos un ejemplo de una gr\u00e1fica paralela que no es de serie y que no tiene un equilibrio fuerte. Para el caso de m\u00faltiples productos -LRB-, m\u00faltiples fuentes y sumideros -RRB-, mostramos que en un juego de conexi\u00f3n justa si el gr\u00e1fico es un gr\u00e1fico paralelo de extensi\u00f3n entonces siempre hay un equilibrio fuerte, y mostramos un ejemplo de una serie. gr\u00e1fica paralela que no tiene un equilibrio fuerte. Hasta donde sabemos, somos los primeros en proporcionar una caracterizaci\u00f3n topol\u00f3gica de la existencia de equilibrio en juegos en red de m\u00faltiples productos y de una sola fuente. Para cualquier juego de conexi\u00f3n justo mostramos que si existe un equilibrio fuerte, es como m\u00e1ximo un factor de \u0398 -LRB- log n -RRB- de la soluci\u00f3n \u00f3ptima, donde n es el n\u00famero de jugadores. Esto debe contrastarse con el l\u00edmite \u0398 -LRB- n -RRB- que existe para el precio de la anarqu\u00eda -LSB- 2 -RSB-. Para juegos de conexi\u00f3n general de fuente \u00fanica, mostramos que cualquier gr\u00e1fico paralelo en serie posee un equilibrio fuerte y mostramos un ejemplo de un gr\u00e1fico que no tiene un equilibrio fuerte. En este caso tambi\u00e9n demostramos que cualquier equilibrio fuerte es \u00f3ptimo. Trabajo relacionado Recientemente se han proporcionado caracterizaciones topol\u00f3gicas para juegos en red de un solo producto para varias propiedades de equilibrio, incluida la existencia de equilibrio -LSB- 12, 7, 8 -RSB-, la unicidad del equilibrio -LSB- 10 -RSB- y la eficiencia del equilibrio -LSB- 17. , 11 -RSB-. En -LSB-12-RSB- se estudi\u00f3 la existencia de un equilibrio de Nash puro en juegos de congesti\u00f3n de redes de un solo producto con costos o ponderaciones espec\u00edficos para cada jugador. La existencia de un equilibrio fuerte se estudi\u00f3 tanto en juegos de congesti\u00f3n de utilidad decreciente -LRB-, por ejemplo, enrutamiento -RRB- como de utilidad creciente -LRB-, por ejemplo, reparto justo de costos -RRB-. -LSB- 7, 8 -RSB- han proporcionado una caracterizaci\u00f3n topol\u00f3gica completa para la existencia de un SE en juegos de congesti\u00f3n decrecientes de utilidad de un solo bien, y han demostrado que un SE siempre existe si y s\u00f3lo si el gr\u00e1fico subyacente es paralelo a la extensi\u00f3n. -LSB- 19 -RSB- han demostrado que en los juegos de congesti\u00f3n que aumentan la utilidad de un solo bien, la caracterizaci\u00f3n topol\u00f3gica es esencialmente equivalente a enlaces paralelos. Adem\u00e1s,han demostrado que estos resultados tambi\u00e9n son v\u00e1lidos para equilibrios fuertes correlacionados -LRB- en contraste con el entorno decreciente, donde los equilibrios fuertes correlacionados podr\u00edan no existir en absoluto -RRB-. Si bien los juegos de costo compartido justo que estudiamos son juegos de utilidad que aumentan la congesti\u00f3n de la red, derivamos una caracterizaci\u00f3n diferente a -LSB- 19 -RSB- debido a las diferentes suposiciones con respecto a las acciones de los jugadores.3 4. JUEGOS DE CONEXI\u00d3N GENERAL En esta secci\u00f3n, derivar nuestros resultados para juegos de conexi\u00f3n generales. 4.1 Existencia de un equilibrio fuerte Comenzamos con una caracterizaci\u00f3n de la existencia de un equilibrio fuerte en juegos de conexi\u00f3n general sim\u00e9tricos. Similar al Teorema 3.1 -LRB- usando una demostraci\u00f3n similar -RRB- establecemos, TEOREMA 4.1. En todo juego sim\u00e9trico de conexi\u00f3n justa existe un equilibrio fuerte. Si bien cada juego de conexi\u00f3n general de una sola fuente posee un equilibrio de Nash puro -LSB- 3 -RSB-, no necesariamente admite alg\u00fan equilibrio fuerte.11 el juego de conexi\u00f3n justa inspir\u00f3 este ejemplo. TEOREMA 4.2. Existe un juego de conexi\u00f3n general de fuente \u00fanica que no admite ning\u00fan equilibrio fuerte. PRUEBA. Considere un juego de conexi\u00f3n general de fuente \u00fanica con 3 jugadores en el gr\u00e1fico que se muestra en la Figura 4. Mostramos que ninguno de los NE es SE y, por lo tanto, el juego no posee ning\u00fan SE. A continuaci\u00f3n mostramos que para la clase de gr\u00e1ficos paralelos en serie, siempre hay un equilibrio fuerte en el caso de una sola fuente. PRUEBA. Sea \u039b un juego de conexi\u00f3n general de fuente \u00fanica en un SPG G = -LRB- V, E -RRB- con fuente sy sumidero t. Primero consideramos el siguiente orden parcial entre los jugadores. Para los jugadores i y j, tenemos que i \u2192 j si hay un camino dirigido de ti a tj. El algoritmo COMPUTE-SE, considera a los jugadores en orden creciente, comenzando con el jugador 1. Cada jugador i comprar\u00e1 completamente un subconjunto de los bordes, y cualquier jugador j > i considerar\u00e1 el costo de aquellos -LRB- comprados -RRB- bordes como cero. Cuando COMPUTE-SE considera al jugador j, el costo de las aristas que los jugadores 1 a j \u2212 1 han comprado se establece en cero, y el jugador j compra por completo un camino m\u00e1s corto Qj de s a tj. Es decir, para cada arista e G Qj \\ Ui < jQi tenemos pj -LRB- e -RRB- = ce y en caso contrario pj -LRB- e -RRB- = 0. A continuaci\u00f3n mostramos que el algoritmo COMPUTESE calcula un SE. Supongamos a modo de contradicci\u00f3n que el perfil p no es un SE. Entonces, existe una coalici\u00f3n que puede mejorar los costos de todos sus jugadores mediante una desviaci\u00f3n. Sea \u0393 una coalici\u00f3n de tama\u00f1o m\u00ednimo y sea el jugador i = max -LCB- j G \u0393 -RCB-. Para un jugador j G \u0393, sean \u00af Qj y \u00af pj el camino y el pago del jugador j despu\u00e9s de la desviaci\u00f3n, respectivamente. Sea Q ' un camino desde el sumidero del jugador i, es decir, ti, hasta el sumidero de G, es decir, entonces Q = \u00af Qi UQ ' es un camino desde la fuente s hasta el sumidero t. Para cualquier jugador j < i, sea yj el v\u00e9rtice de intersecci\u00f3n de Q y tj -LRB- seg\u00fan el Lema 2.1, se garantiza que existe -RRB-. Sea y el v\u00e9rtice m\u00e1s alejado del camino Q tal que y = yj para alg\u00fan j < i.El camino desde la fuente s al nodo y fue pagado en su totalidad por los jugadores j < i en p -LRB- antes de la desviaci\u00f3n -RRB-. Hay dos casos que consideramos. caso a: Despu\u00e9s de la desviaci\u00f3n, el jugador i no paga por las aristas en U j \u2208 \u0393 \\ -LCB- i -RCB- \u00af Qj. Antes de la desviaci\u00f3n de la coalici\u00f3n \u0393, los jugadores j < i pagaron \u00edntegramente un camino de s a y. A continuaci\u00f3n mostramos que ning\u00fan jugador k > i paga por ninguna ventaja en ning\u00fan camino desde sa ti. Considere un jugador k > i y sea Q0k = Qk U Q00k, donde Q00k es un camino que conecta tk con t. Sea yk el v\u00e9rtice de intersecci\u00f3n de Q0k y ti. Dado que existe un camino de s a yk que fue pagado en su totalidad por los jugadores j < k antes de la desviaci\u00f3n, en particular el camino Qis, yk, el jugador k no pagar\u00e1 por ninguna ventaja en ning\u00fan camino que conecte s e yk. Por lo tanto, el jugador i paga completamente por todas las aristas del camino \u00af Qiy, ti, es decir, \u00af pi -LRB- e -RRB- = ce para todas las aristas e E \u00af Qiy, ti. Ahora considere el algoritmo COMPUTAR en el paso en el que el jugador i selecciona el camino m\u00e1s corto desde la fuente s hasta su sumidero ti y determina su pago pi. En este punto, el jugador i podr\u00eda comprar el camino \u00af Qiy, ti, ya que los jugadores j < i ya pagaron un camino de s a y. Por lo tanto, ci -LRB- \u00af p -RRB- > ci -LRB- p -RRB-. Esto contradice el hecho de que el jugador i mejor\u00f3 su costo y por lo tanto no todos los jugadores de \u0393 reducen su costo. Esto implica que p es un equilibrio fuerte. 4.2 El alto precio de la anarqu\u00eda Si bien para cada juego de conexi\u00f3n general de fuente \u00fanica se cumple que PoS = 1 -LSB- 3 -RSB-, el precio de la anarqu\u00eda puede ser tan grande como n, incluso para dos bordes paralelos. Aqu\u00ed mostramos que cualquier equilibrio fuerte en juegos de conexi\u00f3n general de fuente \u00fanica produce el costo \u00f3ptimo. PRUEBA. Sea p = -LRB- p1,..., pn -RRB- un equilibrio fuerte, y sea T \u2217 el \u00e1rbol de Steiner de costo m\u00ednimo para todos los jugadores, arraigado en las fuentes -LRB- \u00fanicas -RRB-. Sea Te \u2217 el sub\u00e1rbol de T \u2217 desconectado de s cuando se elimina el borde e. Sea \u0393 -LRB- Te -RRB- el conjunto de jugadores que tienen sumideros en Te. Para un conjunto de aristas E, sea c -LRB- E -RRB- = Ee \u2208 E ce. Supongamos a modo de contradicci\u00f3n que c -LRB- p -RRB- > c -LRB- T \u2217 -RRB-. Demostraremos que existe un sub\u00e1rbol T0 de T \u2217, que conecta un subconjunto de jugadores \u0393 C _ N, y un nuevo conjunto de pagos \u00af p, tal que para cada i E \u0393, ci -LRB- \u00af p - RRB- < ci -LRB- p -RRB-. Esto contradice el supuesto de que p es un equilibrio fuerte. Primero mostramos c\u00f3mo encontrar un sub\u00e1rbol T0 de T \u2217, tal que para cualquier arista e, los pagos de los jugadores con sumideros en Te \u2217 sean mayores que el costo de Te \u2217 U -LCB- e -RCB-. Para construir T0, defina una ventaja e como mala si el costo de Te \u2217 U -LCB- e -RCB- es al menos los pagos de los jugadores con sumideros en Te \u2217, es decir, c -LRB- Te \u2217 U -LCB - e -RCB- -RRB- > P -LRB- Te \u2217 -RRB-. Sea B el conjunto de aristas malas. Por lo tanto, en T0 para cada arista e, tenemos que c -LRB- Te0 U -LCB- e -RCB- -RRB- < P -LRB- T0e -RRB-.Lo que queda es encontrar pagos p \u00af para los jugadores en \u0393 -LRB- T0 -RRB- de modo que comprar\u00e1n el \u00e1rbol T0 y cada jugador en \u0393 -LRB- T0 -RRB- reducir\u00e1 su costo, es decir, ci -LRB- p -RRB- > ci -LRB- \u00af p -RRB- para i E \u0393 -LRB- T0 -RRB-. -LRB- Recordemos que los pagos tienen la restricci\u00f3n de que el jugador i solo puede pagar por las aristas en el camino de s a ti. -RRB- Ahora definiremos los pagos de la coalici\u00f3n \u00af p. Sean ci -LRB- \u00af p, T0 e \u2208 Te \u00af pi -LRB- e -RRB- los pagos del jugador i por el sub\u00e1rbol T0e. Considere el siguiente proceso ascendente que define \u00af p. Asignamos los pagos del borde e en T0, luego asignamos pagos a todos los bordes en T0e. Por lo tanto, podemos actualizar los pagos p \u00af de los jugadores i E \u0393 -LRB- T0e -RRB-, estableciendo d\u00f3nde usamos el hecho de que E e -RRB-.", "keyphrases": ["juego de conexi\u00f3n de costos compartidos", "numero de jugador", "fuente \u00fanica y fregadero", "fregadero m\u00faltiple de fuente \u00fanica", "fuente m\u00faltiple y fregadero", "costo del edg", "juego de conexi\u00f3n justa", "juego de conexi\u00f3n de g\u00e9nero", "topolog\u00eda gr\u00e1fica", "equilibrio fuerte", "carbonita", "costo espec\u00edfico", "gr\u00e1fico paralelo extendido", "soluci\u00f3n \u00f3ptima"]}
{"file_name": "C-3", "text": "Aplicaciones autoadaptativas en la cuadr\u00edcula Resumen Las cuadr\u00edculas son inherentemente heterog\u00e9neas y din\u00e1micas. Un problema importante en la computaci\u00f3n grid es la selecci\u00f3n de recursos, es decir, encontrar un conjunto de recursos apropiado para la aplicaci\u00f3n. Otro problema es la adaptaci\u00f3n a las caracter\u00edsticas cambiantes del entorno de la red. Las soluciones existentes a estos dos problemas requieren que se conozca un modelo de rendimiento para una aplicaci\u00f3n. Sin embargo, construir tales modelos es una tarea compleja. En este art\u00edculo, investigamos un enfoque que no requiere modelos de desempe\u00f1o. Iniciamos una aplicaci\u00f3n sobre cualquier conjunto de recursos. Durante la ejecuci\u00f3n de la aplicaci\u00f3n, recopilamos peri\u00f3dicamente estad\u00edsticas sobre la ejecuci\u00f3n de la aplicaci\u00f3n y deducimos los requisitos de la aplicaci\u00f3n a partir de estas estad\u00edsticas. Luego, ajustamos el conjunto de recursos para que se ajuste mejor a las necesidades de la aplicaci\u00f3n. Este enfoque nos permite evitar cuellos de botella en el rendimiento, como enlaces WAN sobrecargados o procesadores muy lentos y, por lo tanto, puede generar mejoras de rendimiento significativas. Evaluamos nuestro enfoque en una serie de escenarios t\u00edpicos del Grid. 1. Introducci\u00f3n En los \u00faltimos a\u00f1os, la computaci\u00f3n grid se ha convertido en una alternativa real a la computaci\u00f3n paralela tradicional. Una red proporciona mucha potencia computacional y, por lo tanto, ofrece la posibilidad de resolver problemas muy grandes, especialmente si las aplicaciones pueden ejecutarse en m\u00faltiples sitios al mismo tiempo -LRB- 7; 15 ; 20 -RRB-. Sin embargo, la complejidad de los entornos Grid tambi\u00e9n es muchas veces mayor que la de las m\u00e1quinas paralelas tradicionales como clusters y supercomputadoras. Un problema importante es la selecci\u00f3n de recursos: seleccionar un conjunto de nodos inform\u00e1ticos de modo que la aplicaci\u00f3n alcance un buen rendimiento. En un entorno de red, este problema es a\u00fan m\u00e1s dif\u00edcil debido a la heterogeneidad de los recursos: los nodos de computaci\u00f3n tienen varios Otro problema importante es que el rendimiento y la disponibilidad de los recursos de la red var\u00edan con el tiempo: los enlaces de red o los nodos de computaci\u00f3n pueden sobrecargarse, o Los nodos de computaci\u00f3n pueden dejar de estar disponibles debido a fallas o porque han sido reclamados por una aplicaci\u00f3n de mayor prioridad. Adem\u00e1s, es posible que est\u00e9n disponibles nuevos y mejores recursos. Por lo tanto, para mantener un nivel de rendimiento razonable, la aplicaci\u00f3n debe adaptarse a las condiciones cambiantes. El problema de adaptaci\u00f3n se puede reducir al problema de selecci\u00f3n de recursos: la fase de selecci\u00f3n de recursos se puede repetir durante la ejecuci\u00f3n de la aplicaci\u00f3n, ya sea a intervalos regulares, o cuando se detecta un problema de rendimiento, o cuando hay nuevos recursos disponibles. Este enfoque ha sido adoptado por varios sistemas -LRB- 5 ; 14; 18 -RRB-. Para la selecci\u00f3n de recursos, se estima el tiempo de ejecuci\u00f3n de la aplicaci\u00f3n para algunos conjuntos de recursos y se selecciona para su ejecuci\u00f3n el conjunto que produce el tiempo de ejecuci\u00f3n m\u00e1s corto. Sin embargo, predecir el tiempo de ejecuci\u00f3n de la aplicaci\u00f3n en un conjunto determinado de recursos requiere conocimiento sobre la aplicaci\u00f3n. Normalmente, se utiliza un modelo de rendimiento anal\u00edtico,pero construir un modelo de este tipo es intr\u00ednsecamente dif\u00edcil y requiere una experiencia que los programadores de aplicaciones tal vez no tengan. En este art\u00edculo, presentamos y evaluamos un enfoque alternativo para la adaptaci\u00f3n de aplicaciones y la selecci\u00f3n de recursos que no necesita un modelo de rendimiento. Iniciamos una aplicaci\u00f3n sobre cualquier conjunto de recursos. Durante la ejecuci\u00f3n de la aplicaci\u00f3n, recopilamos peri\u00f3dicamente informaci\u00f3n sobre los tiempos de comunicaci\u00f3n y los tiempos de inactividad de los procesadores. Utilizamos estas estad\u00edsticas para estimar autom\u00e1ticamente los requisitos de recursos de la aplicaci\u00f3n. A continuaci\u00f3n, ajustamos el conjunto de recursos en el que se ejecuta la aplicaci\u00f3n agregando o eliminando nodos inform\u00e1ticos o incluso cl\u00fasteres completos. Se agregan o eliminan procesadores para permanecer entre los umbrales, adapt\u00e1ndose as\u00ed autom\u00e1ticamente al entorno cambiante. Una ventaja importante de nuestro enfoque es que mejora el rendimiento de las aplicaciones en muchas situaciones diferentes que son t\u00edpicas de la computaci\u00f3n grid. Maneja todos los casos siguientes: Nuestro trabajo supone que la aplicaci\u00f3n es maleable y puede ejecutarse -LRB- de manera eficiente -RRB- en m\u00faltiples sitios de una red -LRB- es decir, utilizando la asignaci\u00f3n conjunta -LRB- 15 -RRB- -RRB- . latencias de \u00e1rea. Hemos aplicado nuestras ideas a aplicaciones de divide y vencer\u00e1s que satisfacen estos requisitos. Se ha demostrado que divide y vencer\u00e1s es un paradigma atractivo para la programaci\u00f3n de aplicaciones grid -LRB- 4; 20 -RRB-. Creemos que nuestro enfoque puede extenderse a otras clases de aplicaciones con los supuestos dados. Implementamos nuestra estrategia en Satin, que es un marco centrado en Java para escribir aplicaciones de divide y vencer\u00e1s habilitadas para grid -LRB- 20 -RRB-. El resto del art\u00edculo se estructura de la siguiente manera. En la Secci\u00f3n 2, explicamos qu\u00e9 suposiciones hacemos sobre las aplicaciones y los recursos de la red. En la Secci\u00f3n 3, presentamos nuestra estrategia de selecci\u00f3n y adaptaci\u00f3n de recursos. En la Secci\u00f3n 4, describimos su implementaci\u00f3n en el marco Satin. En la Secci\u00f3n 5, evaluamos nuestro enfoque en varios escenarios de red. En la Secci\u00f3n 6, comparamos nuestro enfoque con el trabajo relacionado. Finalmente, en la Secci\u00f3n 7, concluimos y describimos el trabajo futuro. 2. Antecedentes y suposiciones En esta secci\u00f3n, describimos nuestras suposiciones sobre las aplicaciones y sus recursos. Asumimos el siguiente modelo de recursos. Las aplicaciones se ejecutan en varios sitios al mismo tiempo, donde los sitios son cl\u00fasteres o supercomputadoras. Los procesadores que pertenecen a un sitio est\u00e1n conectados mediante una LAN r\u00e1pida con baja latencia y gran ancho de banda. Los diferentes sitios est\u00e1n conectados por una WAN. La comunicaci\u00f3n entre sitios sufre de altas latencias. Estudiamos el problema de la adaptaci\u00f3n en el context de aplicaciones de divide y vencer\u00e1s. Sin embargo, creemos que nuestra metodolog\u00eda tambi\u00e9n se puede utilizar para otros tipos de aplicaciones. En esta secci\u00f3n resumimos los supuestos sobre las aplicaciones que son importantes para nuestro enfoque. La primera suposici\u00f3n que hacemos es que la aplicaci\u00f3n es maleable, es decir,es capaz de manejar procesadores que se unen y salen del c\u00e1lculo en curso. En -LRB-23-RRB-, mostramos c\u00f3mo las aplicaciones de divide y vencer\u00e1s pueden hacerse tolerantes a fallas y maleables. Los procesadores se pueden agregar o quitar en cualquier punto del c\u00e1lculo con poca sobrecarga. La segunda suposici\u00f3n es que la aplicaci\u00f3n puede ejecutarse de manera eficiente en procesadores con diferentes velocidades. Esto se puede lograr mediante el uso de una estrategia de equilibrio de carga din\u00e1mica, como el robo de trabajo utilizado por las aplicaciones de divide y vencer\u00e1s -LRB- 19 -RRB-. Adem\u00e1s, las aplicaciones master-worker suelen utilizar estrategias din\u00e1micas de equilibrio de carga -LRB-, por ejemplo, MW, un marco para escribir aplicaciones master-worker habilitadas para grid -LRB- 12 -RRB- -RRB-. Consideramos que es una suposici\u00f3n razonable para una aplicaci\u00f3n grid, ya que las aplicaciones para las cuales el procesador m\u00e1s lento se convierte en un cuello de botella no podr\u00e1n utilizar eficientemente los recursos de la grid. Finalmente, la aplicaci\u00f3n debe ser insensible a las latencias de \u00e1rea amplia, para que pueda ejecutarse eficientemente en una grilla de \u00e1rea amplia -LRB-16; 17 -RRB-. 6. Trabajo relacionado Varios proyectos Grid abordan la cuesti\u00f3n de la selecci\u00f3n y adaptaci\u00f3n de recursos. En GrADS -LRB- 18 -RRB- y ASSIST -LRB- 1 -RRB-, la selecci\u00f3n y adaptaci\u00f3n de recursos requiere un modelo de rendimiento que permita predecir los tiempos de ejecuci\u00f3n de las aplicaciones. En la fase de selecci\u00f3n de recursos, se examina una cantidad de conjuntos de recursos posibles y se selecciona el conjunto de recursos con el tiempo de ejecuci\u00f3n previsto m\u00e1s corto. Si se detecta una degradaci\u00f3n del rendimiento durante el c\u00e1lculo, se repite la fase de selecci\u00f3n de recursos. GrADS utiliza la relaci\u00f3n entre los tiempos de ejecuci\u00f3n previstos -LRB- de determinadas fases de la aplicaci\u00f3n -RRB- y los tiempos de ejecuci\u00f3n reales como indicador del rendimiento de la aplicaci\u00f3n. ASSIST utiliza el n\u00famero de iteraciones por unidad de tiempo -LRB- para aplicaciones iterativas -RRB- o el n\u00famero de tareas por unidad de tiempo -LRB- para aplicaciones regulares de maestro-trabajador -RRB- como indicador de rendimiento. La principal diferencia entre estos enfoques y nuestro enfoque es el uso de modelos de desempe\u00f1o. La principal ventaja es que una vez conocido el modelo de rendimiento, el sistema puede tomar decisiones de migraci\u00f3n m\u00e1s precisas que con nuestro enfoque. Sin embargo, incluso si el rendimiento no se adapta con la adaptaci\u00f3n, se conoce el problema de encontrar un conjunto de recursos \u00f3ptimo -LRB-, es decir, el conjunto de recursos con el tiempo de ejecuci\u00f3n m\u00ednimo. RRB- es NP-completo. A medida que aumenta el n\u00famero de recursos de red disponibles, la precisi\u00f3n de este enfoque disminuye, ya que el subconjunto de posibles conjuntos de recursos que pueden examinarse en un tiempo razonable se vuelve m\u00e1s peque\u00f1o. Otra desventaja de estos sistemas es que la detecci\u00f3n de degradaci\u00f3n del rendimiento es adecuada s\u00f3lo para aplicaciones iterativas o regulares. Cactus -LRB- 2 -RRB- y GridWay -LRB- 14 -RRB- no utilizan modelos de rendimiento. Sin embargo, estos marcos solo son adecuados para aplicaciones secuenciales -LRB- GridWay -RRB- o de un solo sitio -LRB- Cactus -RRB-.En ese caso, el problema de selecci\u00f3n de recursos se reduce a seleccionar la m\u00e1quina o el cl\u00faster m\u00e1s r\u00e1pido. La velocidad del reloj del procesador, la carga promedio y la cantidad de procesadores en un cl\u00faster -LRB- Cactus -RRB- se utilizan para clasificar los recursos y se selecciona el recurso con el rango m\u00e1s alto. La aplicaci\u00f3n se migra si se detecta una degradaci\u00f3n del rendimiento o se descubren mejores recursos. Tanto Cactus como GridWay utilizan el n\u00famero de iteraciones por unidad de tiempo como indicador de rendimiento. La principal limitaci\u00f3n de esta metodolog\u00eda es que es adecuada s\u00f3lo para aplicaciones secuenciales o de un solo sitio. Adem\u00e1s, la selecci\u00f3n de recursos basada en la velocidad del reloj no siempre es precisa. Finalmente, la detecci\u00f3n de degradaci\u00f3n del rendimiento es adecuada s\u00f3lo para aplicaciones iterativas y no puede usarse para c\u00e1lculos irregulares como problemas de b\u00fasqueda y optimizaci\u00f3n. El problema de la selecci\u00f3n de recursos tambi\u00e9n fue estudiado por el proyecto AppLeS -LRB- 5 -RRB-. En el context de este proyecto, se estudiaron varias aplicaciones y se crearon modelos de rendimiento para estas aplicaciones. Sobre la base de dicho modelo, se construye un agente de programaci\u00f3n que utiliza el modelo de rendimiento para seleccionar el mejor conjunto de recursos y la mejor programaci\u00f3n de aplicaciones en este conjunto. Los agentes de programaci\u00f3n de AppleS se escriben caso por caso y no se pueden reutilizar para otra aplicaci\u00f3n. Tambi\u00e9n se desarrollaron dos plantillas reutilizables para clases espec\u00edficas de aplicaciones, a saber, aplicaciones master-worker -LRB- plantilla AMWAT -RRB- y barrido de par\u00e1metros -LRB- plantilla APST -RRB-. 2 de cada 9 cl\u00fasteres comenzaron a fallar agregando nodos. Se alcanzaron 96 nodos. En -LRB- 13 -RRB-, se estudia el problema de programar aplicaciones maestro-trabajador. Por tanto, el problema se reduce a encontrar el n\u00famero adecuado de trabajadores. El enfoque aqu\u00ed es similar al nuestro en el sentido de que no se utiliza ning\u00fan modelo de desempe\u00f1o. En cambio, el sistema intenta deducir los requisitos de la aplicaci\u00f3n en tiempo de ejecuci\u00f3n y ajusta la cantidad de trabajadores para acercarse al n\u00famero ideal. 7. Conclusiones y trabajo futuro En este art\u00edculo, investigamos el problema de la selecci\u00f3n y adaptaci\u00f3n de recursos en entornos de red. Los enfoques existentes para estos problemas normalmente suponen la existencia de un modelo de rendimiento que permite predecir tiempos de ejecuci\u00f3n de aplicaciones en varios conjuntos de recursos. Sin embargo, crear modelos de rendimiento es intr\u00ednsecamente dif\u00edcil y requiere conocimientos sobre la aplicaci\u00f3n. Proponemos un enfoque que no requiere un conocimiento profundo sobre la aplicaci\u00f3n. Iniciamos la aplicaci\u00f3n en un conjunto arbitrario de recursos y monitoreamos su rendimiento. La supervisi\u00f3n del rendimiento nos permite conocer ciertos requisitos de la aplicaci\u00f3n, como la cantidad de procesadores que necesita la aplicaci\u00f3n o los requisitos de ancho de banda de la aplicaci\u00f3n. Usamos este conocimiento para refinar gradualmente el conjunto de recursos eliminando nodos inadecuados o agregando nuevos nodos si es necesario. Este enfoque no da como resultado el conjunto de recursos \u00f3ptimo, sino un conjunto de recursos razonable, es decirun conjunto libre de diversos cuellos de botella de rendimiento, como conexiones de red lentas o procesadores sobrecargados. Nuestro enfoque tambi\u00e9n permite que la aplicaci\u00f3n se adapte a las condiciones cambiantes de la red. Si la eficiencia promedio ponderada cae por debajo de cierto nivel, el coordinador de adaptaci\u00f3n comienza a eliminar los nodos \"peores\". Si la eficiencia promedio ponderada supera un cierto nivel, se agregan nuevos nodos. La aplicaci\u00f3n se adapta de forma totalmente autom\u00e1tica a las condiciones cambiantes. El trabajo futuro implicar\u00e1 ampliar nuestra estrategia de adaptaci\u00f3n para apoyar la migraci\u00f3n oportunista. Sin embargo, esto requiere programadores grid con una funcionalidad m\u00e1s sofisticada que la que existe actualmente. Tambi\u00e9n se necesita m\u00e1s investigaci\u00f3n para reducir los gastos generales de evaluaci\u00f3n comparativa. Otra l\u00ednea de investigaci\u00f3n que deseamos investigar es el uso del control de retroalimentaci\u00f3n para refinar la estrategia de adaptaci\u00f3n durante la ejecuci\u00f3n de la aplicaci\u00f3n. Finalmente, la implementaci\u00f3n centralizada del coordinador de adaptaci\u00f3n podr\u00eda convertirse en un cuello de botella para aplicaciones que se ejecutan en un n\u00famero muy grande de nodos -LRB-, cientos o miles -RRB-.", "keyphrases": ["computaci\u00f3n en red", "selecci\u00f3n de recursos", "entorno de red", "computaci\u00f3n paralela", "entorno paralelo homog\u00e9neo", "heterogeno de recursos", "red de \u00e1rea local de gran ancho de banda", "red de \u00e1rea amplia de menor ancho de banda", "enlace de red", "tiempo comun", "tiempo de inactividad del procesador", "grado de paralelo", "sobrecarga de recursos", "divide y vencer\u00e1s"]}
{"file_name": "I-19", "text": "Oferta \u00f3ptima en subastas simult\u00e1neas de segundo precio de bienes perfectamente sustituibles RESUMEN Derivamos estrategias de oferta \u00f3ptimas para un agente de oferta global que participa en m\u00faltiples subastas simult\u00e1neas de segundo precio con sustitutos perfectos. Primero consideramos un modelo en el que todos los dem\u00e1s postores son locales y participan en una \u00fanica subasta. Para este caso, demostramos que, asumiendo la libre disposici\u00f3n, el postor global siempre debe realizar ofertas distintas de cero en todas las subastas disponibles, independientemente de la distribuci\u00f3n de valoraci\u00f3n de los postores locales. Adem\u00e1s, para distribuciones de valoraci\u00f3n no decrecientes, demostramos que el problema de encontrar las ofertas \u00f3ptimas se reduce a dos dimensiones. Estos resultados son v\u00e1lidos tanto en el caso en que se conoce el n\u00famero de postores locales como cuando este n\u00famero est\u00e1 determinado por una distribuci\u00f3n de Poisson. Este an\u00e1lisis se extiende a los mercados en l\u00ednea donde, normalmente, las subastas ocurren de forma simult\u00e1nea y secuencial. Adem\u00e1s, al combinar resultados anal\u00edticos y de simulaci\u00f3n, demostramos que se obtienen resultados similares en el caso de varios postores globales, siempre que el mercado est\u00e9 formado por postores globales y locales. Finalmente, abordamos la eficiencia del mercado en general y mostramos que la informaci\u00f3n sobre el n\u00famero de postores locales es un determinante importante de la forma en que un postor global afecta la eficiencia. 1. INTRODUCCI\u00d3N El reciente aumento del inter\u00e9s en las subastas en l\u00ednea ha resultado en un n\u00famero cada vez mayor de subastas que ofrecen art\u00edculos muy similares o. En eBay, por ejemplo, a menudo hay cientos o incluso miles de subastas simult\u00e1neas en todo el mundo que venden art\u00edculos sustituibles1. En este context, es esencial desarrollar estrategias de licitaci\u00f3n que los agentes aut\u00f3nomos puedan utilizar para operar de manera efectiva en una amplia cantidad de subastas. Sin embargo, como mostraremos, este an\u00e1lisis tambi\u00e9n es relevante en un context m\u00e1s amplio en el que las subastas se realizan de forma secuencial y simult\u00e1nea. Por el contrario, aqu\u00ed consideramos estrategias de oferta para mercados con m\u00faltiples subastas simult\u00e1neas y sustitutos perfectos. En particular, nos centramos en Vickrey o subastas de oferta sellada de segundo precio. Sin embargo, nuestros resultados se generalizan a entornos con subastas en ingl\u00e9s, ya que \u00e9stas son estrat\u00e9gicamente equivalentes a las subastas de segundo precio. Dentro de este entorno, podemos caracterizar, por primera vez, la estrategia de maximizaci\u00f3n de la utilidad de un postor para ofertar simult\u00e1neamente en cualquier n\u00famero de subastas y para cualquier tipo de distribuci\u00f3n de valoraci\u00f3n del postor. Con m\u00e1s detalle, primero consideramos un mercado en el que un \u00fanico postor, llamado postor global, puede ofertar en cualquier n\u00famero de subastas, mientras que se supone que los otros postores, llamados postores locales, ofertan s\u00f3lo en una \u00fanica subasta. Para este caso, encontramos los siguientes resultados: \u2022 Mientras que en el caso de una subasta \u00fanica de segundo precio la mejor estrategia de un postor es ofertar su valor real, la mejor estrategia para un postor global es ofertar por debajo de \u00e9l. \u2022 Podemos demostrar que,Incluso si un postor global requiere solo un art\u00edculo, la utilidad esperada se maximiza al participar en todas las subastas que venden el art\u00edculo deseado. \u2022 Encontrar la oferta \u00f3ptima para cada subasta puede ser una tarea ardua si se consideran todas las combinaciones posibles. 2. TRABAJOS RELACIONADOS La investigaci\u00f3n en el \u00e1rea de subastas simult\u00e1neas se puede segmentar seg\u00fan dos l\u00edneas generales. Estos an\u00e1lisis se suelen utilizar cuando el formato de subasta empleado en las subastas concurrentes es el mismo -LRB-, por ejemplo, hay M subastas Vickrey o M subastas de primer precio -RRB-. Este art\u00edculo adopta el primer enfoque al estudiar un mercado de M subastas Vickrey simult\u00e1neas, ya que este enfoque produce estrategias de oferta demostrablemente \u00f3ptimas. Su trabajo analiza un mercado formado por parejas con valoraciones iguales que quieren pujar por una c\u00f3moda. Por lo tanto, el espacio de oferta de la pareja puede contener como m\u00e1ximo dos ofertas, ya que el marido y la mujer pueden estar como m\u00e1ximo en dos subastas distribuidas geogr\u00e1ficamente simult\u00e1neamente. Deducen un equilibrio de Nash de estrategia mixta para el caso especial en el que el n\u00famero de compradores es grande. Nuestro an\u00e1lisis se diferencia del de ellos en que estudiamos subastas concurrentes en las que los postores tienen valoraciones diferentes y el postor global puede pujar en todas las subastas al mismo tiempo -LRB- lo cual es totalmente posible dados agentes aut\u00f3nomos -RRB-. A continuaci\u00f3n, -LSB- 7 -RSB- estudi\u00f3 el caso de subastas simult\u00e1neas con bienes complementarios. Analizan el caso de los postores locales y globales y caracterizan las ofertas de los compradores y la eficiencia del mercado resultante. La configuraci\u00f3n proporcionada en -LSB- 7 -RSB- se extiende a\u00fan m\u00e1s al caso de valores comunes en -LSB- 9 -RSB-. Sin embargo, ninguno de estos trabajos se extiende f\u00e1cilmente al caso de bienes sustituibles que consideramos. Para este caso especial se deriva el espacio de estrategias de equilibrio mixto sim\u00e9trico, pero nuevamente nuestro resultado es m\u00e1s general. Finalmente, -LSB- 11 -RSB- considera el caso de subastas inglesas concurrentes, en las que desarrolla algoritmos de puja para compradores con diferentes actitudes de riesgo. Sin embargo, obliga a que las ofertas sean las mismas en todas las subastas, lo que mostramos en este documento no siempre es \u00f3ptimo. 7. CONCLUSIONES En este art\u00edculo, derivamos estrategias de maximizaci\u00f3n de la utilidad para ofertar en m\u00faltiples subastas simult\u00e1neas de segundo precio. Primero analizamos el caso en el que un \u00fanico postor global puja en todas las subastas, mientras que todos los dem\u00e1s postores son locales y pujan en una \u00fanica subasta. Para esta configuraci\u00f3n, encontramos el resultado contrario a la intuici\u00f3n de que es \u00f3ptimo realizar ofertas distintas de cero en todas las subastas que venden el art\u00edculo deseado, incluso cuando un postor solo requiere un art\u00edculo y no obtiene ning\u00fan beneficio adicional por tener m\u00e1s. Por tanto, un comprador potencial puede lograr un beneficio considerable participando en m\u00faltiples subastas y empleando una estrategia de oferta \u00f3ptima. Para una serie de distribuciones de valoraci\u00f3n comunes, mostramos anal\u00edticamente que el problema de encontrar ofertas \u00f3ptimas se reduce a dos dimensiones.Esto simplifica considerablemente el problema de optimizaci\u00f3n original y, por tanto, puede utilizarse en la pr\u00e1ctica para calcular las ofertas \u00f3ptimas para cualquier n\u00famero de subastas. Adem\u00e1s, investigamos un entorno con m\u00faltiples postores globales combinando soluciones anal\u00edticas con un enfoque de simulaci\u00f3n. Encontramos que la estrategia de un postor global no se estabiliza cuando s\u00f3lo hay postores globales presentes en el mercado, sino que s\u00f3lo converge cuando tambi\u00e9n hay postores locales. Sostenemos, sin embargo, que es probable que los mercados del mundo real contengan postores tanto locales como globales. Los resultados convergentes son entonces muy similares al escenario con un \u00fanico postor global, y encontramos que un postor se beneficia al ofertar de manera \u00f3ptima en m\u00faltiples subastas. Para entornos m\u00e1s complejos con m\u00faltiples postores globales, la simulaci\u00f3n se puede utilizar para encontrar estas ofertas para casos espec\u00edficos. Finalmente, comparamos la eficiencia de un mercado con m\u00faltiples subastas simult\u00e1neas con y sin un postor global. Mostramos que, si el postor puede predecir con precisi\u00f3n el n\u00famero de postores locales en cada subasta, la eficiencia aumenta ligeramente. Por el contrario, si hay mucha incertidumbre, la eficiencia disminuye significativamente a medida que aumenta el n\u00famero de subastas debido a la mayor probabilidad de que un postor global gane m\u00e1s de dos art\u00edculos. Estos resultados muestran que la forma en que un postor global afecta la eficiencia y, por tanto, el bienestar social, depende de la informaci\u00f3n de que dispone ese postor global. En trabajos futuros, pretendemos extender los resultados a sustitutos imperfectos -LRB-, es decir, cuando un postor global gana al ganar art\u00edculos adicionales -RRB-, y a entornos donde las subastas ya no son id\u00e9nticas. Esto \u00faltimo surge, por ejemplo, cuando el n\u00famero de postores locales -LRB- promedio -RRB- difiere por subasta o las subastas tienen diferentes configuraciones para par\u00e1metros como el precio de reserva.la eficiencia disminuye significativamente a medida que aumenta el n\u00famero de subastas debido a la mayor probabilidad de que un postor global gane m\u00e1s de dos art\u00edculos. Estos resultados muestran que la forma en que un postor global afecta la eficiencia y, por tanto, el bienestar social, depende de la informaci\u00f3n de que dispone ese postor global. En trabajos futuros, pretendemos extender los resultados a sustitutos imperfectos -LRB-, es decir, cuando un postor global gana al ganar art\u00edculos adicionales -RRB-, y a entornos donde las subastas ya no son id\u00e9nticas. Esto \u00faltimo surge, por ejemplo, cuando el n\u00famero de postores locales -LRB- promedio -RRB- difiere por subasta o las subastas tienen diferentes configuraciones para par\u00e1metros como el precio de reserva.la eficiencia disminuye significativamente a medida que aumenta el n\u00famero de subastas debido a la mayor probabilidad de que un postor global gane m\u00e1s de dos art\u00edculos. Estos resultados muestran que la forma en que un postor global afecta la eficiencia y, por tanto, el bienestar social, depende de la informaci\u00f3n de que dispone ese postor global. En trabajos futuros, pretendemos extender los resultados a sustitutos imperfectos -LRB-, es decir, cuando un postor global gana al ganar art\u00edculos adicionales -RRB-, y a entornos donde las subastas ya no son id\u00e9nticas. Esto \u00faltimo surge, por ejemplo, cuando el n\u00famero de postores locales -LRB- promedio -RRB- difiere por subasta o las subastas tienen diferentes configuraciones para par\u00e1metros como el precio de reserva.", "keyphrases": ["estrategia de oferta \u00f3ptima", "agente de ofertas globales", "subasta simult\u00e1nea de segundo precio", "distribuci\u00f3n de valor no decreciente", "mercado en l\u00ednea", "sistema multiag", "eficiencia del mercado", "sustituto perfecto", "subasta vickrei", "ciencias sociales y del comportamiento", "utilitariomaximis strategi"]}
{"file_name": "J-2", "text": "Redistribuci\u00f3n \u00f3ptima de los pagos de VCG en el peor de los casos en subastas de art\u00edculos heterog\u00e9neos con demanda unitaria RESUMEN Muchos problemas importantes en los sistemas multiagente implican la asignaci\u00f3n de m\u00faltiples recursos entre los agentes. Para los problemas de asignaci\u00f3n de recursos, el conocido mecanismo VCG satisface una lista de propiedades deseadas, que incluyen eficiencia, idoneidad estrat\u00e9gica, racionalidad individual y propiedad de no d\u00e9ficit. Sin embargo, el VCG generalmente no tiene un equilibrio presupuestario. En el caso del VCG, los agentes pagan los pagos del VCG, lo que reduce el bienestar social. Para compensar la p\u00e9rdida de bienestar social debido a los pagos del VCG, se introdujeron mecanismos de redistribuci\u00f3n del VCG. Estos mecanismos tienen como objetivo redistribuir la mayor cantidad posible de pagos de VCG a los agentes, manteniendo al mismo tiempo las propiedades deseadas antes mencionadas del mecanismo de VCG. Continuamos la b\u00fasqueda de mecanismos \u00f3ptimos de redistribuci\u00f3n de VCG en el peor de los casos: mecanismos que maximicen la fracci\u00f3n del pago total de VCG redistribuido en el peor de los casos. Anteriormente, un mecanismo de redistribuci\u00f3n de VCG \u00f3ptimo en el peor de los casos -LRB- denotado por la OMA -RRB- se caracterizaba para subastas de unidades m\u00faltiples con valores marginales no crecientes -LSB- 7 -RSB-. Posteriormente, WCO se generaliz\u00f3 a entornos que involucraban \u00edtems heterog\u00e9neos -LSB- 4 -RSB-, dando como resultado el mecanismo HETERO. -LSB- 4 -RSB- conjetur\u00f3 que HETERO es factible y \u00f3ptimo en el peor de los casos para subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. En este art\u00edculo, proponemos una forma m\u00e1s natural de generalizar el mecanismo de la OMA. Probamos que nuestro mecanismo generalizado, aunque representado de manera diferente, en realidad coincide con HETERO. Con base en esta nueva representaci\u00f3n de HETERO, demostramos que HETERO es efectivamente factible y \u00f3ptimo en el peor de los casos en subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. Finalmente, conjeturamos que HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en el entorno a\u00fan m\u00e1s general de subastas combinatorias con sustitutos brutos. 1. INTRODUCCI\u00d3N 1.1 Mecanismos de redistribuci\u00f3n de VCG Muchos problemas importantes en sistemas multiagente implican la asignaci\u00f3n de m\u00faltiples recursos entre los agentes. Para problemas de asignaci\u00f3n de recursos, el conocido mecanismo VCG satisface la siguiente lista de propiedades deseadas: \u2022 Eficiencia: la asignaci\u00f3n maximiza la valoraci\u00f3n total de los agentes -LRB- sin considerar los pagos -RRB-. \u2022 Resistencia a la estrategia: para cualquier agente, informar con veracidad es una estrategia dominante, independientemente del tipo de los dem\u00e1s agentes. \u2022 -LRB- Ex post -RRB- racionalidad individual: La utilidad final de cada agente -LRB- despu\u00e9s de deducir su pago -RRB- siempre es no negativa. \u2022 No deficitario: el pago total de los agentes no es negativo. Sin embargo, el VCG generalmente no tiene un equilibrio presupuestario. En el caso del VCG, los agentes pagan los pagos del VCG, lo que reduce el bienestar social. Para compensar la p\u00e9rdida de bienestar social debido a los pagos del VCG, se introdujeron mecanismos de redistribuci\u00f3n del VCG. Estos mecanismos a\u00fan asignan los recursos utilizando VCG. Adem\u00e1s de VCG,Estos mecanismos intentan redistribuir la mayor cantidad posible de pagos de VCG a los agentes. Requerimos que la redistribuci\u00f3n de un agente sea independiente de su propio tipo. Esto es suficiente para mantener la estrategia y la eficiencia -LRB- un agente no tiene control sobre su propia redistribuci\u00f3n -RRB-. Para dominios conectados sin problemas -LRB- que incluyen subastas de unidades m\u00faltiples con valores marginales no crecientes y subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -RRB-, el requisito anterior tambi\u00e9n es necesario para mantener la eficacia y la eficacia de la estrategia -LSB- 8 -RSB-. Un mecanismo de redistribuci\u00f3n VCG es factible si mantiene todas las propiedades deseadas del mecanismo VCG. Es decir, tambi\u00e9n requerimos que el proceso de redistribuci\u00f3n mantenga la racionalidad individual y la propiedad no deficitaria. Sea n el n\u00famero de agentes. Dado que todos los mecanismos de redistribuci\u00f3n de VCG comienzan asignando de acuerdo con el mecanismo de VCG, un mecanismo de redistribuci\u00f3n de VCG se caracteriza por su esquema de redistribuci\u00f3n r ~ = -LRB- r1, r2,..., rn -RRB-. Seg\u00fan el mecanismo de redistribuci\u00f3n VCG ~ r, la redistribuci\u00f3n del agente i es igual a ri -LRB- 01,..., 0i \u2212 1, 0i +1,..., 0n -RRB-, donde 0j es el tipo del agente j. -LRB- No tenemos que diferenciar entre el tipo verdadero de un agente y su tipo informado, ya que todos los mecanismos de redistribuci\u00f3n de VCG son a prueba de estrategias. -RRB- Un mecanismo de redistribuci\u00f3n de VCG an\u00f3nimo se caracteriza por una \u00fanica funci\u00f3n r. Bajo el mecanismo de redistribuci\u00f3n -LRB- an\u00f3nimo -RRB- VCG r, la redistribuci\u00f3n del agente i es igual a r -LRB- 0 \u2212 i -RRB-, donde 0 \u2212 i es el conjunto m\u00faltiple de los tipos de agentes distintos de i. Usamos \u03b8 ~ para indicar el perfil de tipo. Sea V CG -LRB- ~ \u03b8 -RRB- el total. Organizamos los resultados existentes seg\u00fan su configuraci\u00f3n. Pago VCG para este tipo de perfil. Un mecanismo de redistribuci\u00f3n de VCG r satisface la propiedad no deficitaria si la redistribuci\u00f3n total nunca excede el pago total de VCG. Un mecanismo de redistribuci\u00f3n VCG r es -LRB- ex post -RRB- individualmente racional si la utilidad final de cada agente es siempre no negativa. Despu\u00e9s de la redistribuci\u00f3n, la utilidad del agente i es exactamente su redistribuci\u00f3n r -LRB- \u03b8 \u2212 i -RRB-. Queremos encontrar mecanismos de redistribuci\u00f3n de VCG que maximicen la fracci\u00f3n del pago total de VCG redistribuido en el peor de los casos. Este problema de dise\u00f1o de mecanismo es equivalente al siguiente modelo de optimizaci\u00f3n funcional: En este art\u00edculo, caracterizaremos anal\u00edticamente un mecanismo de redistribuci\u00f3n de VCG \u00f3ptimo en el peor de los casos para subastas de art\u00edculos heterog\u00e9neos con demanda unitaria.1 Concluimos esta subsecci\u00f3n con un ejemplo de mecanismo de redistribuci\u00f3n de VCG en la configuraci\u00f3n m\u00e1s sencilla de subastas de un solo art\u00edculo. En una subasta de un solo art\u00edculo, el tipo de agente es un n\u00famero real no negativo que representa su utilidad para ganar el art\u00edculo. En las subastas de un solo art\u00edculo, el mecanismo de redistribuci\u00f3n Bailey-Cavallo VCG -LSB- 2, 3 -RSB- funciona de la siguiente manera: \u2022 Asigna el art\u00edculo seg\u00fan VCG: el agente 1 gana el art\u00edculo y paga \u03b82. Los dem\u00e1s agentes no ganan nada y no pagan.\u2022 Cada agente recibe una redistribuci\u00f3n que es igual a n1 veces el segundo tipo m\u00e1s alto: los agentes 1 y 2 reciben cada uno n1 \u03b83. Los dem\u00e1s agentes reciben cada uno n1\u03b82. El mecanismo anterior obviamente mantiene la eficacia y la eficacia de la estrategia -LRB-. La redistribuci\u00f3n de un agente no depende de su propio tipo -RRB-. Tambi\u00e9n mantiene la racionalidad individual porque todas las redistribuciones no son negativas. La redistribuci\u00f3n total es igual a 2n \u03b83 + el mecanismo anterior mantiene la propiedad no deficitaria. Finalmente, la redistribuci\u00f3n total de 2 n subastas de art\u00edculos, la fracci\u00f3n de redistribuci\u00f3n del peor caso de este mecanismo de ejemplo es n \u2212 2 n 1.2 Investigaciones previas sobre mecanismos de redistribuci\u00f3n de VCG \u00f3ptimos en el peor de los casos En esta subsecci\u00f3n, revisamos los resultados existentes sobre el VCG \u00f3ptimo en el peor de los casos. mecanismos de redistribuci\u00f3n. Redistribuci\u00f3n \u00f3ptima en el peor de los casos en subastas de unidades m\u00faltiples con demanda unitaria -LSB- 7, 12 -RSB-: En subastas de unidades m\u00faltiples con demanda unitaria, los art\u00edculos a la venta son id\u00e9nticos. Cada agente quiere como m\u00e1ximo una copia del art\u00edculo. -LRB- Las subastas de un solo art\u00edculo son casos especiales de subastas de unidades m\u00faltiples con demanda unitaria. -RRB- Sea m el n\u00famero de elementos. A lo largo de este art\u00edculo, s\u00f3lo consideramos casos en los que m \u2264 n \u2212 2.2 Aqu\u00ed, el tipo de un agente es un n\u00famero real no negativo que representa su valoraci\u00f3n por ganar una copia del art\u00edculo. -LSB- 7 -RSB- tambi\u00e9n caracteriz\u00f3 un mecanismo de redistribuci\u00f3n de VCG para subastas multiunitarias con demanda unitaria, llamado mecanismo de la OMA.3 La fracci\u00f3n de redistribuci\u00f3n del peor caso de la OMA es exactamente \u03b1 \u2217. Es decir, es el peor de los casos \u00f3ptimo. La OMA se obtuvo optimizando dentro de la familia de mecanismos de redistribuci\u00f3n lineal de VCG. Un mecanismo de redistribuci\u00f3n lineal de VCG r toma la siguiente forma: Aqu\u00ed, los ci son constantes. -LRB- S\u00f3lo consideramos los ci que corresponden a mecanismos de redistribuci\u00f3n de VCG factibles. -RRB- -LSB- \u03b8 \u2212 i -RSB- j es el j-\u00e9simo tipo m\u00e1s alto entre \u03b8 \u2212 i. El mecanismo lineal r se caracteriza por los valores de ci. Los valores \u00f3ptimos de ci son los siguientes: La caracterizaci\u00f3n de OMA es la siguiente: Redistribuci\u00f3n \u00f3ptima en el peor de los casos en subastas de unidades m\u00faltiples con valores marginales no crecientes -LSB- 7 -RSB-: Subastas de unidades m\u00faltiples con valores no 2 -LSB- 7 -RSB - demostr\u00f3 que para subastas de unidades m\u00faltiples con demanda unitaria, cuando m = n \u2212 1, la fracci\u00f3n de redistribuci\u00f3n del peor caso -LRB- de cualquier mecanismo de redistribuci\u00f3n de VCG factible -RRB- es como m\u00e1ximo 0. Dado que el escenario estudiado en este art\u00edculo es En subastas m\u00e1s generales -LRB- de art\u00edculos heterog\u00e9neos con demanda unitaria -RRB-, tambi\u00e9n tenemos que la fracci\u00f3n de redistribuci\u00f3n en el peor de los casos es como m\u00e1ximo 0 cuando m = n \u2212 1. Dado que las subastas de art\u00edculos heterog\u00e9neos con x unidades son casos especiales de subastas heterog\u00e9neas -subastas de art\u00edculos con x + 1 unidades, tenemos que para nuestra configuraci\u00f3n la fracci\u00f3n de redistribuci\u00f3n en el peor de los casos es como m\u00e1ximo 0 cuando m \u2265 n \u2212 1. Es decir, no redistribuir nada es \u00f3ptimo en el peor de los casos cuando m \u2265 n \u2212 1. Adem\u00e1s, para el objetivo de -LSB- 12 -RSB-, el mecanismo \u00f3ptimo coincide con la OMA s\u00f3lo cuando se aplica la restricci\u00f3n de racionalidad individual.Los valores marginales crecientes son m\u00e1s generales que las subastas de unidades m\u00faltiples con demanda unitaria. En este entorno m\u00e1s general, los art\u00edculos siguen siendo id\u00e9nticos, pero un agente puede exigir m\u00e1s de una copia del art\u00edculo. La valoraci\u00f3n de un agente por ganar la primera copia del art\u00edculo se denomina valor marginal inicial/primer. De manera similar, la valoraci\u00f3n adicional de un agente por ganar la i-\u00e9sima copia del art\u00edculo se llama su i-\u00e9simo valor marginal. El tipo de agente contiene m n\u00fameros reales no negativos -LRB- i-\u00e9simo valor marginal para i = 1,..., m -RRB-. En este context, se supone adem\u00e1s que los valores marginales no aumentan. Como se analiz\u00f3 anteriormente, en este entorno m\u00e1s general, la fracci\u00f3n de redistribuci\u00f3n del peor caso de cualquier mecanismo de redistribuci\u00f3n VCG todav\u00eda est\u00e1 limitada por arriba por \u03b1 *. -LSB- 7 -RSB- generaliz\u00f3 la OMA a este escenario y demostr\u00f3 que su fracci\u00f3n de redistribuci\u00f3n en el peor de los casos sigue siendo la misma. Por lo tanto, la OMA -LRB- despu\u00e9s de la generalizaci\u00f3n -RRB- tambi\u00e9n es \u00f3ptima en el peor de los casos para subastas de unidades m\u00faltiples con valores marginales no crecientes. La definici\u00f3n original de OMA no se generaliza directamente a subastas de unidades m\u00faltiples con valores marginales no crecientes. Cuando se trata de subastas de unidades m\u00faltiples con valores marginales no crecientes, el tipo de agente ya no es un valor \u00fanico, lo que significa que no existe \"el j-\u00e9simo tipo m\u00e1s alto entre \u03b8_i\". Abusamos de la notaci\u00f3n al no diferenciar los agentes y sus tipos. Por ejemplo, \u03b8_i es equivalente al conjunto de agentes distintos de i. Sea S un conjunto de agentes. yo \u2212 1 -RRB-. Aqu\u00ed, U -LRB- S, j -RRB- es el nuevo conjunto de agentes, despu\u00e9s de eliminar de S el agente con el j-\u00e9simo valor marginal inicial m\u00e1s alto en S. La forma general de OMA es la siguiente: Peor caso \u00f3ptimo Redistribuci\u00f3n en Subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LSB- 4 -RSB-: En las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria, los art\u00edculos a la venta son diferentes. Cada agente exige como m\u00e1ximo un art\u00edculo. Aqu\u00ed, el tipo de agente consta de m n\u00fameros reales no negativos -LRB- y su valoraci\u00f3n por el art\u00edculo ganador i para i = 1,..., m -RRB-. El foco principal de este art\u00edculo son las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. Dado que las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria son m\u00e1s generales que las subastas de unidades m\u00faltiples con demanda unitaria, \u03b1 * sigue siendo un l\u00edmite superior en la fracci\u00f3n de redistribuci\u00f3n del peor caso. -LSB- 4 -RSB- propuso el mecanismo HETERO, generalizando la OMA. Los autores conjeturaron que HETERO es factible y tiene una fracci\u00f3n de redistribuci\u00f3n en el peor de los casos igual a \u03b1 *. Es decir, los autores conjeturaron que HETERO es el peor de los casos \u00f3ptimo en este entorno. La principal contribuci\u00f3n de este art\u00edculo es una prueba de esta conjetura. Redistribuci\u00f3n en Subastas Combinatorias con Sustitutos Brutos -LSB- 6 -RSB-: La condici\u00f3n de sustitutos brutos fue propuesta por primera vez en -LSB- 9 -RSB-. Al igual que la demanda unitaria, la condici\u00f3n de sustitutos brutos es una condici\u00f3n sobre el tipo de agente -LRB- que no depende del mecanismo en discusi\u00f3n -RRB-. En palabras,El tipo de agente satisface la condici\u00f3n de sustitutos brutos si su demanda de un art\u00edculo no disminuye cuando los precios de los otros art\u00edculos aumentan. Tanto las subastas multiunitarias con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria son casos especiales de subastas combinatorias con sustitutos brutos -LSB- 5, 9 -RSB-. Los autores no encontraron un mecanismo \u00f3ptimo en el peor de los casos para esta configuraci\u00f3n. Al final de este art\u00edculo, conjeturamos que HETERO es \u00f3ptimo para subastas combinatorias con sustitutos brutos. Finalmente, Naroditskiy et al. -LSB- 13 -RSB- propuso una t\u00e9cnica num\u00e9rica para dise\u00f1ar mecanismos de redistribuci\u00f3n \u00f3ptimos en el peor de los casos. La t\u00e9cnica propuesta s\u00f3lo funciona para dominios de un solo par\u00e1metro. No se aplica a nuestra configuraci\u00f3n -LRB- dominio multiparam\u00e9trico -RRB-. 1.3 Nuestra contribuci\u00f3n Generalizamos la OMA a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. Probamos que el mecanismo generalizado, aunque representado de manera diferente, coincide con el mecanismo HETERO propuesto en -LSB- 4 -RSB-. Es decir, lo que propusimos no es un nuevo mecanismo, sino una nueva representaci\u00f3n de un mecanismo existente. Con base en nuestra nueva representaci\u00f3n de HETERO, demostramos que HETERO es efectivamente factible y \u00f3ptimo en el peor de los casos cuando se aplica a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria, confirmando as\u00ed la conjetura planteada en -LSB- 4 -RSB-. Concluimos con una nueva conjetura de que HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en el entorno a\u00fan m\u00e1s general de subastas combinatorias con sustitutos brutos. 4. CONCLUSI\u00d3N Concluimos nuestro art\u00edculo con la siguiente conjetura: CONJECTURA 1. Los sustitutos brutos implican monoton\u00eda de redistribuci\u00f3n. Es decir, HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en subastas combinatorias con sustitutos brutos. La idea es que tanto las subastas de unidades m\u00faltiples con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria satisfacen la monotonicidad de la redistribuci\u00f3n. Una conjetura natural es que la \"uni\u00f3n m\u00e1s restrictiva\" de estos dos entornos tambi\u00e9n satisface la monotonicidad de la redistribuci\u00f3n. Hay muchos entornos de subasta bien estudiados que contienen tanto subastas de unidades m\u00faltiples con valores marginales no crecientes como subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LRB-, cuya lista se puede encontrar en -LSB- 10 -RSB- -RRB-. Entre estos escenarios bien estudiados, las subastas combinatorias con sustitutos brutos son las m\u00e1s restrictivas.-LSB- 13 -RSB- propuso una t\u00e9cnica num\u00e9rica para dise\u00f1ar mecanismos de redistribuci\u00f3n \u00f3ptimos en el peor de los casos. La t\u00e9cnica propuesta s\u00f3lo funciona para dominios de un solo par\u00e1metro. No se aplica a nuestra configuraci\u00f3n -LRB- dominio multiparam\u00e9trico -RRB-. 1.3 Nuestra contribuci\u00f3n Generalizamos la OMA a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. Probamos que el mecanismo generalizado, aunque representado de manera diferente, coincide con el mecanismo HETERO propuesto en -LSB- 4 -RSB-. Es decir, lo que propusimos no es un nuevo mecanismo, sino una nueva representaci\u00f3n de un mecanismo existente. Con base en nuestra nueva representaci\u00f3n de HETERO, demostramos que HETERO es efectivamente factible y \u00f3ptimo en el peor de los casos cuando se aplica a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria, confirmando as\u00ed la conjetura planteada en -LSB- 4 -RSB-. Concluimos con una nueva conjetura de que HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en el entorno a\u00fan m\u00e1s general de subastas combinatorias con sustitutos brutos. 4. CONCLUSI\u00d3N Concluimos nuestro art\u00edculo con la siguiente conjetura: CONJECTURA 1. Los sustitutos brutos implican monoton\u00eda de redistribuci\u00f3n. Es decir, HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en subastas combinatorias con sustitutos brutos. La idea es que tanto las subastas de unidades m\u00faltiples con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria satisfacen la monotonicidad de la redistribuci\u00f3n. Una conjetura natural es que la \"uni\u00f3n m\u00e1s restrictiva\" de estos dos entornos tambi\u00e9n satisface la monotonicidad de la redistribuci\u00f3n. Hay muchos entornos de subasta bien estudiados que contienen tanto subastas de unidades m\u00faltiples con valores marginales no crecientes como subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LRB-, cuya lista se puede encontrar en -LSB- 10 -RSB- -RRB-. Entre estos escenarios bien estudiados, las subastas combinatorias con sustitutos brutos son las m\u00e1s restrictivas.-LSB- 13 -RSB- propuso una t\u00e9cnica num\u00e9rica para dise\u00f1ar mecanismos de redistribuci\u00f3n \u00f3ptimos en el peor de los casos. La t\u00e9cnica propuesta s\u00f3lo funciona para dominios de un solo par\u00e1metro. No se aplica a nuestra configuraci\u00f3n -LRB- dominio multiparam\u00e9trico -RRB-. 1.3 Nuestra contribuci\u00f3n Generalizamos la OMA a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria. Probamos que el mecanismo generalizado, aunque representado de manera diferente, coincide con el mecanismo HETERO propuesto en -LSB- 4 -RSB-. Es decir, lo que propusimos no es un nuevo mecanismo, sino una nueva representaci\u00f3n de un mecanismo existente. Con base en nuestra nueva representaci\u00f3n de HETERO, demostramos que HETERO es efectivamente factible y \u00f3ptimo en el peor de los casos cuando se aplica a subastas de art\u00edculos heterog\u00e9neos con demanda unitaria, confirmando as\u00ed la conjetura planteada en -LSB- 4 -RSB-. Concluimos con una nueva conjetura de que HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en el entorno a\u00fan m\u00e1s general de subastas combinatorias con sustitutos brutos. 4. CONCLUSI\u00d3N Concluimos nuestro art\u00edculo con la siguiente conjetura: CONJECTURA 1. Los sustitutos brutos implican monoton\u00eda de redistribuci\u00f3n. Es decir, HETERO sigue siendo factible y \u00f3ptimo en el peor de los casos en subastas combinatorias con sustitutos brutos. La idea es que tanto las subastas de unidades m\u00faltiples con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria satisfacen la monotonicidad de la redistribuci\u00f3n. Una conjetura natural es que la \"uni\u00f3n m\u00e1s restrictiva\" de estos dos entornos tambi\u00e9n satisface la monotonicidad de la redistribuci\u00f3n. Hay muchos entornos de subasta bien estudiados que contienen tanto subastas de unidades m\u00faltiples con valores marginales no crecientes como subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LRB-, cuya lista se puede encontrar en -LSB- 10 -RSB- -RRB-. Entre estos escenarios bien estudiados, las subastas combinatorias con sustitutos brutos son las m\u00e1s restrictivas.La idea es que tanto las subastas de unidades m\u00faltiples con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria satisfacen la monotonicidad de la redistribuci\u00f3n. Una conjetura natural es que la \"uni\u00f3n m\u00e1s restrictiva\" de estos dos entornos tambi\u00e9n satisface la monotonicidad de la redistribuci\u00f3n. Hay muchos entornos de subasta bien estudiados que contienen tanto subastas de unidades m\u00faltiples con valores marginales no crecientes como subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LRB-, cuya lista se puede encontrar en -LSB- 10 -RSB- -RRB-. Entre estos escenarios bien estudiados, las subastas combinatorias con sustitutos brutos son las m\u00e1s restrictivas.La idea es que tanto las subastas de unidades m\u00faltiples con valores marginales no crecientes como las subastas de art\u00edculos heterog\u00e9neos con demanda unitaria satisfacen la monotonicidad de la redistribuci\u00f3n. Una conjetura natural es que la \"uni\u00f3n m\u00e1s restrictiva\" de estos dos entornos tambi\u00e9n satisface la monotonicidad de la redistribuci\u00f3n. Hay muchos entornos de subasta bien estudiados que contienen tanto subastas de unidades m\u00faltiples con valores marginales no crecientes como subastas de art\u00edculos heterog\u00e9neos con demanda unitaria -LRB-, cuya lista se puede encontrar en -LSB- 10 -RSB- -RRB-. Entre estos escenarios bien estudiados, las subastas combinatorias con sustitutos brutos son las m\u00e1s restrictivas.", "keyphrases": ["dise\u00f1o mec\u00e1nico", "vickrei-clark-grove", "pago de redistribuci\u00f3n", "Mecanismo eficiente", "a prueba de estrategias", "mecanismo de individuaci\u00f3n", "mec\u00e1nico", "mecanismo de redistribuci\u00f3n lineal vcg", "transformar a programa lineal", "personaje analista", "mecanismo optim en el peor de los casos"]}
{"file_name": "H-4", "text": "Hacia evaluaciones de gesti\u00f3n de informaci\u00f3n personal basadas en tareas RESUMEN La gesti\u00f3n de informaci\u00f3n personal -LRB- PIM -RRB- es un \u00e1rea de investigaci\u00f3n de r\u00e1pido crecimiento que se ocupa de c\u00f3mo las personas almacenan, gestionan y reencuentran informaci\u00f3n. Una caracter\u00edstica de la investigaci\u00f3n PIM es que muchos sistemas han sido dise\u00f1ados para ayudar a los usuarios a gestionar y reencontrar informaci\u00f3n, pero muy pocos han sido evaluados. Esto ha sido observado por varios estudiosos y explicado por las dificultades que implica realizar evaluaciones PIM. Las dificultades incluyen que las personas vuelven a encontrar informaci\u00f3n dentro de colecciones personales \u00fanicas; los investigadores saben poco sobre las tareas que hacen que las personas vuelvan a encontrar informaci\u00f3n; y numerosos problemas de privacidad relacionados con la informaci\u00f3n personal. En este documento pretendemos facilitar las evaluaciones de PIM abordando cada una de estas dificultades. En la primera parte, presentamos un estudio diario de tareas de reencuentro de informaci\u00f3n. El estudio examina el tipo de tareas que requieren que los usuarios vuelvan a encontrar informaci\u00f3n y produce una taxonom\u00eda de tareas de b\u00fasqueda para mensajes de correo electr\u00f3nico y p\u00e1ginas web. En la segunda parte, proponemos una metodolog\u00eda de evaluaci\u00f3n basada en tareas basada en nuestros hallazgos y examinamos la viabilidad del enfoque utilizando dos m\u00e9todos diferentes de creaci\u00f3n de tareas. 1. INTRODUCCI\u00d3N La Gesti\u00f3n de Informaci\u00f3n Personal -LRB- PIM -RRB- es un \u00e1rea de investigaci\u00f3n en r\u00e1pido crecimiento que se ocupa de c\u00f3mo las personas almacenan, gestionan y reencuentran informaci\u00f3n. Los sistemas PIM (los m\u00e9todos y procedimientos mediante los cuales las personas manejan, categorizan y recuperan informaci\u00f3n en el d\u00eda a d\u00eda -LSB-18-RSB-) se est\u00e1n volviendo cada vez m\u00e1s populares. Sin embargo, la evaluaci\u00f3n de estos sistemas PIM es problem\u00e1tica. Una de las principales dificultades proviene del car\u00e1cter personal de PIM. Las personas recopilan informaci\u00f3n como consecuencia natural de realizar otras tareas. Esto significa que las colecciones que las personas generan son exclusivas de ellas y la informaci\u00f3n dentro de una colecci\u00f3n est\u00e1 intr\u00ednsecamente vinculada con las experiencias personales del propietario. Como las colecciones personales son \u00fanicas, no podemos crear tareas de evaluaci\u00f3n que sean aplicables a todos los participantes en una evaluaci\u00f3n. En segundo lugar, las colecciones personales pueden contener informaci\u00f3n que los participantes no se sienten c\u00f3modos compartiendo dentro de una evaluaci\u00f3n. La naturaleza precisa de esta informaci\u00f3n (qu\u00e9 informaci\u00f3n los individuos preferir\u00edan mantener privada) var\u00eda entre individuos, lo que dificulta basar las tareas de b\u00fasqueda en el contenido de colecciones individuales. Por lo tanto, los experimentadores enfrentan una serie de desaf\u00edos para realizar evaluaciones PIM realistas pero controladas. Sin embargo, recientemente los investigadores han comenzado a centrarse en formas de abordar el problema de la evaluaci\u00f3n PIM. Capra -LSB- 6 -RSB- tambi\u00e9n identifica la necesidad de evaluaciones controladas de laboratorio PIM para complementar otras t\u00e9cnicas de evaluaci\u00f3n, poniendo especial \u00e9nfasis en la necesidad de comprender el comportamiento de PIM a nivel de tarea. En este art\u00edculo, intentamos abordar las dificultades involucradas para facilitar evaluaciones PIM de laboratorio controladas.En la primera parte de este art\u00edculo presentamos un estudio diario de tareas de reencuentro de informaci\u00f3n. El estudio examina el tipo de tareas que requieren que los usuarios vuelvan a encontrar informaci\u00f3n y produce una taxonom\u00eda de tareas de b\u00fasqueda para mensajes de correo electr\u00f3nico y p\u00e1ginas web. Tambi\u00e9n analizamos las caracter\u00edsticas de las tareas que dificultan la reencontraci\u00f3n. En la segunda parte, proponemos una metodolog\u00eda de evaluaci\u00f3n basada en tareas basada en nuestros hallazgos y examinamos la viabilidad del enfoque utilizando diferentes m\u00e9todos de creaci\u00f3n de tareas. Por lo tanto, este art\u00edculo ofrece dos contribuciones al campo: una mayor comprensi\u00f3n del comportamiento de PIM a nivel de tarea y un m\u00e9todo de evaluaci\u00f3n que facilitar\u00e1 futuras investigaciones. 2. TRABAJO RELACIONADO Hay una variedad de enfoques disponibles para estudiar PIM. Los enfoques naturalistas estudian a los participantes actuando de forma natural, completando sus propias tareas a medida que ocurren, dentro de entornos familiares. Estos enfoques permiten a los investigadores superar muchas de las dificultades causadas por la naturaleza personal de PIM. Como las tareas realizadas son \"reales\" y no simuladas, los participantes pueden utilizar sus propias experiencias, conocimientos previos y recopilaci\u00f3n de informaci\u00f3n para completar las tareas. Tanto los m\u00e9todos etnogr\u00e1ficos como los de trabajo de campo requieren la presencia de un experimentador para evaluar c\u00f3mo se realiza el PIM, lo que plantea una serie de cuestiones. En primer lugar, la evaluaci\u00f3n de este modo es costosa; tomar largos per\u00edodos de tiempo para estudiar un peque\u00f1o n\u00famero de participantes y estas peque\u00f1as muestras pueden no ser representativas del comportamiento de poblaciones m\u00e1s grandes. En segundo lugar, como los participantes no pueden ser observados continuamente, los experimentadores deben elegir cu\u00e1ndo observar y esto puede afectar los resultados. Una estrategia alternativa a la realizaci\u00f3n de evaluaciones naturalistas es utilizar el an\u00e1lisis de archivos de registro. Este enfoque utiliza software de registro que captura una amplia muestra de las actividades del usuario en el context del uso natural de un sistema. Esto revela la necesidad de complementar los estudios naturalistas con experimentos controlados donde el experimentador pueda relacionar el comportamiento de los participantes del estudio con objetivos asociados con tareas de b\u00fasqueda conocidas. Una dificultad para realizar este tipo de evaluaci\u00f3n es la obtenci\u00f3n de colecciones para evaluar. Kelly -LSB- 16 -RSB- propone la introducci\u00f3n de una colecci\u00f3n de pruebas compartida que proporcionar\u00eda conjuntos de datos, tareas y m\u00e9tricas reutilizables y compartibles para aquellos interesados \u200b\u200ben realizar investigaciones PIM. Sin embargo, una colecci\u00f3n compartida no ser\u00eda adecuada para estudios de usuarios porque no ser\u00eda posible incorporar los aspectos personales de PIM mientras se utiliza una colecci\u00f3n com\u00fan y desconocida. Un enfoque alternativo es pedir a los usuarios que proporcionen sus propias colecciones de informaci\u00f3n para simular entornos familiares dentro del laboratorio. Este enfoque se ha aplicado para estudiar la reencuentro de fotograf\u00edas personales -LSB- 11 -RSB-, mensajes de correo electr\u00f3nico -LSB- 20 -RSB- y marcadores web -LSB- 21 -RSB-. La utilidad de este enfoque depende de lo f\u00e1cil que sea transferir la colecci\u00f3n u obtener acceso remoto.Otra soluci\u00f3n es utilizar toda la web como una colecci\u00f3n al estudiar la reencontrada de p\u00e1ginas web -LSB- 4 -RSB-. Esto puede ser apropiado para estudiar la re-b\u00fasqueda de p\u00e1ginas web porque estudios previos han demostrado que las personas suelen utilizar motores de b\u00fasqueda web para este prop\u00f3sito -LSB- 5 -RSB-. Una segunda dificultad al realizar estudios de laboratorio PIM es crear tareas para que las realicen los participantes y que puedan resolverse buscando en una colecci\u00f3n compartida o personal. Las tareas se relacionan con la actividad que resulta en una necesidad de informaci\u00f3n -LSB- 14 -RSB- y se reconoce que son importantes para determinar el comportamiento del usuario -LSB- 26 -RSB-. Se ha llevado a cabo una gran cantidad de trabajo para comprender la naturaleza de las tareas y c\u00f3mo el tipo de tarea influye en el comportamiento de b\u00fasqueda de informaci\u00f3n del usuario. Por ejemplo, las tareas se han categorizado en t\u00e9rminos de complejidad creciente -LSB- 3 -RSB- y se ha sugerido que la complejidad de las tareas afecta c\u00f3mo los buscadores perciben sus necesidades de informaci\u00f3n -LSB- 25 -RSB- y c\u00f3mo intentan encontrar informaci\u00f3n -LSB- 3 -RSB-. Otros trabajos previos han proporcionado metodolog\u00edas que permiten la simulaci\u00f3n de tareas al estudiar el comportamiento de b\u00fasqueda de informaci\u00f3n -LSB- 2 -RSB-. Sin embargo, se sabe poco sobre los tipos de tareas que hacen que las personas busquen en sus tiendas personales o vuelvan a encontrar informaci\u00f3n que han visto antes. En consecuencia, es dif\u00edcil idear situaciones de tareas laborales simuladas para PIM. La excepci\u00f3n es el estudio de la gesti\u00f3n de fotograf\u00edas personales, donde el trabajo de Rodden sobre la categorizaci\u00f3n de las tareas de b\u00fasqueda de fotograf\u00edas personales ha facilitado la creaci\u00f3n de situaciones de tareas laborales simuladas -LSB- 22 -RSB-. Ha habido otras sugerencias sobre c\u00f3mo clasificar las tareas PIM. Si bien estas son propiedades interesantes que pueden afectar la forma en que se realizar\u00e1 una tarea, no brindan a los experimentadores suficiente margen para idear tareas. Las colecciones personales son una de las razones por las que la creaci\u00f3n de tareas es tan dif\u00edcil. La taxonom\u00eda de tareas fotogr\u00e1ficas de Rodden proporciona una soluci\u00f3n en este caso porque permite clasificar las tareas adaptadas a colecciones privadas. Luego, los sistemas se pueden comparar entre tipos de tareas para diferentes usuarios -LSB- 11 -RSB-. Desafortunadamente, no existe una taxonom\u00eda equivalente para otros tipos de objetos de informaci\u00f3n. Adem\u00e1s, otros tipos de objetos son m\u00e1s sensibles a la privacidad que las fotograf\u00edas; Es poco probable que los participantes se contenten con permitir que los investigadores exploren sus colecciones de correo electr\u00f3nico para crear tareas como lo hicieron con las fotograf\u00edas en -LSB- 11 -RSB-. Esto presenta un problema grave: \u00bfc\u00f3mo pueden los investigadores idear tareas que correspondan a colecciones privadas sin comprender los tipos de tareas que realizan las personas o sin poner en peligro la privacidad de los participantes del estudio? Se han propuesto algunos m\u00e9todos. Por ejemplo, -LSB-20-RSB- estudi\u00f3 la b\u00fasqueda de correo electr\u00f3nico pidiendo a los participantes que volvieran a encontrar los correos electr\u00f3nicos que se hab\u00edan enviado a todos los miembros de un departamento; permitiendo que se utilicen las mismas tareas para todos los participantes del estudio.Este enfoque asegur\u00f3 que se evitaran problemas de privacidad y que los participantes pudieran usar cosas que recordaran para completar las tareas. Sin embargo, los sistemas s\u00f3lo se probaron utilizando un tipo de tarea: se pidi\u00f3 a los participantes que encontraran correos electr\u00f3nicos individuales, cada uno de los cuales compart\u00eda propiedades comunes. En la secci\u00f3n 4 mostramos que las personas realizan una gama m\u00e1s amplia de tareas de b\u00fasqueda de correo electr\u00f3nico que esta. En -LSB- 4 -RSB-, las tareas de b\u00fasqueda gen\u00e9ricas se crearon artificialmente ejecutando evaluaciones en dos sesiones. En la primera sesi\u00f3n, se pidi\u00f3 a los participantes que completaran tareas de trabajo que implicaban encontrar informaci\u00f3n desconocida. En la segunda sesi\u00f3n, los participantes volvieron a completar las mismas tareas, lo que naturalmente implic\u00f3 alg\u00fan comportamiento de reencuentro. Las limitaciones de esta t\u00e9cnica son que no permite a los participantes explotar ninguna conexi\u00f3n personal con la informaci\u00f3n porque la informaci\u00f3n que buscan puede no corresponder a ning\u00fan otro aspecto de sus vidas. Nuestra revisi\u00f3n de los enfoques de evaluaci\u00f3n motiva la necesidad de realizar experimentos de laboratorio controlados que permitan probar aspectos estrechamente definidos de los sistemas o interfaces. Desafortunadamente, tambi\u00e9n se ha demostrado que existen dificultades al realizar este tipo de evaluaci\u00f3n: es dif\u00edcil encontrar colecciones e idear tareas que correspondan a colecciones privadas y, al mismo tiempo, proteger la privacidad de los participantes en el estudio. En la siguiente secci\u00f3n presentamos un estudio diario de tareas de b\u00fasqueda de correo electr\u00f3nico y p\u00e1ginas web. El resultado es una clasificaci\u00f3n de tareas similar a la ideada por Rodden para fotograf\u00edas personales -LSB- 22 -RSB-. En la secci\u00f3n 5 nos basamos en este trabajo examinando m\u00e9todos para crear tareas que no comprometan la privacidad de los participantes y discutimos c\u00f3mo nuestro trabajo puede facilitar las evaluaciones de usuarios de PIM basadas en tareas. Mostramos que al recopilar tareas utilizando diarios electr\u00f3nicos, no solo podemos aprender sobre las tareas que hacen que las personas vuelvan a encontrar informaci\u00f3n personal, sino que tambi\u00e9n podemos aprender sobre el contenido de colecciones privadas sin comprometer la privacidad de los participantes. Este conocimiento se puede utilizar luego para construir tareas que se utilizar\u00e1n en evaluaciones PIM. 6. CONCLUSIONES Este art\u00edculo se ha centrado en superar las dificultades que implica la realizaci\u00f3n de evaluaciones de PIM. La naturaleza personal de PIM significa que es dif\u00edcil construir experimentos equilibrados porque cada participante tiene sus propias colecciones \u00fanicas que se autogeneran al completar otras tareas. Sugerimos que para incorporar los aspectos personales de PIM en las evaluaciones, se deber\u00eda examinar el rendimiento de los sistemas o de los usuarios cuando los usuarios completan tareas en sus propias colecciones. Este enfoque en s\u00ed tiene problemas porque la creaci\u00f3n de tareas para colecciones personales es dif\u00edcil: los investigadores no saben mucho sobre los tipos de tareas de reencuentro que realizan las personas y no saben qu\u00e9 informaci\u00f3n se encuentra dentro de las colecciones personales individuales.En este art\u00edculo describimos formas de superar estos desaf\u00edos para facilitar las evaluaciones de usuarios de PIM basadas en tareas. En la primera parte del art\u00edculo realizamos un estudio diario que examinaba las tareas que hac\u00edan que las personas volvieran a encontrar mensajes de correo electr\u00f3nico y p\u00e1ginas web. Los datos recopilados incluyeron una amplia gama de tareas laborales y no laborales y, en base a los datos, creamos una taxonom\u00eda de tareas de reencuentro web y de correo electr\u00f3nico. Descubrimos que las personas realizan tres tipos principales de tareas de reencuentro: tareas que requieren informaci\u00f3n espec\u00edfica de un \u00fanico recurso, tareas que requieren un \u00fanico recurso completo y tareas que requieren que se recupere informaci\u00f3n de m\u00faltiples recursos. En la segunda parte del art\u00edculo, discutimos la importancia de la taxonom\u00eda con respecto a la evaluaci\u00f3n de PIM. Demostramos que se pueden realizar experimentos equilibrados comparando el rendimiento del sistema o del usuario en las categor\u00edas de tareas dentro de la taxonom\u00eda. Tambi\u00e9n sugerimos dos m\u00e9todos para crear tareas que se pueden completar en colecciones personales. Estos m\u00e9todos no comprometen la privacidad de los participantes del estudio. Examinamos las t\u00e9cnicas sugeridas, en primer lugar simulando una situaci\u00f3n experimental: se pidi\u00f3 a los participantes que volvieran a realizar sus propias tareas mientras las registraban y, en segundo lugar, en el context de una evaluaci\u00f3n completa. Realizar evaluaciones de esta manera permitir\u00e1 probar los sistemas que se han propuesto para mejorar la capacidad de los usuarios para administrar y volver a encontrar su informaci\u00f3n, de modo que podamos conocer las necesidades y deseos de los usuarios. Por lo tanto, este art\u00edculo ha ofrecido dos contribuciones al campo: una mayor comprensi\u00f3n del comportamiento de PIM a nivel de tarea y un m\u00e9todo de evaluaci\u00f3n que facilitar\u00e1 futuras investigaciones.Realizar evaluaciones de esta manera permitir\u00e1 probar los sistemas que se han propuesto para mejorar la capacidad de los usuarios para administrar y volver a encontrar su informaci\u00f3n, de modo que podamos conocer las necesidades y deseos de los usuarios. Por lo tanto, este art\u00edculo ha ofrecido dos contribuciones al campo: una mayor comprensi\u00f3n del comportamiento de PIM a nivel de tarea y un m\u00e9todo de evaluaci\u00f3n que facilitar\u00e1 futuras investigaciones.Realizar evaluaciones de esta manera permitir\u00e1 probar los sistemas que se han propuesto para mejorar la capacidad de los usuarios para administrar y volver a encontrar su informaci\u00f3n, de modo que podamos conocer las necesidades y deseos de los usuarios. Por lo tanto, este art\u00edculo ha ofrecido dos contribuciones al campo: una mayor comprensi\u00f3n del comportamiento de PIM a nivel de tarea y un m\u00e9todo de evaluaci\u00f3n que facilitar\u00e1 futuras investigaciones.", "keyphrases": ["persona informar a la gerencia", "medir", "experimento", "Factor humano", "volver a encontrar informar", "emisi\u00f3n privada", "taxonomi", "recogida individual", "mensaje de correo electr\u00f3nico", "enfoque naturalista", "estudio base de laboratorio"]}
{"file_name": "I-12", "text": "Compartir experiencias para aprender las caracter\u00edsticas del usuario en entornos din\u00e1micos con datos dispersos RESUMEN Este art\u00edculo investiga el problema de estimar el valor de los par\u00e1metros probabil\u00edsticos necesarios para la toma de decisiones en entornos en los que un agente, operando dentro de un sistema multiagente, no tiene informaci\u00f3n a priori sobre la estructura de la distribuci\u00f3n de los valores de los par\u00e1metros. El agente debe poder producir estimaciones incluso cuando haya realizado s\u00f3lo un peque\u00f1o n\u00famero de observaciones directas y, por tanto, debe poder operar con datos escasos. El art\u00edculo describe un mecanismo que permite al agente mejorar significativamente su estimaci\u00f3n al aumentar sus observaciones directas con las obtenidas por otros agentes con los que se est\u00e1 coordinando. Para evitar sesgos no deseados en entornos relativamente heterog\u00e9neos y al mismo tiempo utilizar datos relevantes para mejorar sus estimaciones, el mecanismo sopesa las contribuciones de las observaciones de otros agentes bas\u00e1ndose en una estimaci\u00f3n en tiempo real del nivel de similitud entre cada uno de estos agentes y \u00e9l mismo. El m\u00f3dulo de \"autonom\u00eda de coordinaci\u00f3n\" de un sistema de gesti\u00f3n de coordinaci\u00f3n proporcion\u00f3 un entorno emp\u00edrico para la evaluaci\u00f3n. Las evaluaciones basadas en simulaci\u00f3n demostraron que el mecanismo propuesto supera las estimaciones basadas exclusivamente en las propias observaciones de un agente, as\u00ed como las estimaciones basadas en un agregado no ponderado de las observaciones de todos los dem\u00e1s agentes. 1. INTRODUCCI\u00d3N En muchos escenarios del mundo real, los agentes aut\u00f3nomos necesitan operar en entornos din\u00e1micos e inciertos en los que s\u00f3lo tienen informaci\u00f3n incompleta sobre los resultados de sus acciones y las caracter\u00edsticas de otros agentes o personas con quienes necesitan cooperar o colaborar. En tales entornos, los agentes pueden beneficiarse al compartir la informaci\u00f3n que recopilan, agrupando sus experiencias individuales para mejorar sus estimaciones de par\u00e1metros desconocidos necesarios para razonar sobre acciones en condiciones de incertidumbre. Este art\u00edculo aborda el problema de aprender la distribuci\u00f3n de los valores de un par\u00e1metro probabil\u00edstico que representa una caracter\u00edstica de una persona que interact\u00faa con un agente inform\u00e1tico. La caracter\u00edstica a aprender es -LRB- o est\u00e1 claramente relacionada con -RRB-, un factor importante en la toma de decisiones del agente.1 El entorno b\u00e1sico que consideramos es aquel en el que un agente acumula observaciones sobre una caracter\u00edstica espec\u00edfica del usuario y las utiliza. producir una estimaci\u00f3n oportuna de alguna medida que depende de la distribuci\u00f3n de esa caracter\u00edstica. Normalmente, los agentes deben tomar decisiones en tiempo real, simult\u00e1neamente con la ejecuci\u00f3n de la tarea y en medio de una gran incertidumbre. En lo que resta de este art\u00edculo utilizaremos el t\u00e9rmino \"ritmo r\u00e1pido\" para referirnos a dichos entornos. En entornos acelerados, la recopilaci\u00f3n de informaci\u00f3n puede ser limitada y no es posible aprender fuera de l\u00ednea o esperar hasta que se recopilen grandes cantidades de datos antes de tomar decisiones. De este modo,El objetivo de los m\u00e9todos de estimaci\u00f3n presentados en este art\u00edculo es minimizar el error promedio a lo largo del tiempo, en lugar de determinar un valor exacto al final de un largo per\u00edodo de interacci\u00f3n. Es decir, se espera que el agente trabaje con el usuario durante un tiempo limitado e intenta minimizar el error general en sus estimaciones. En tales entornos, los datos adquiridos individualmente por un agente -LRB- y sus propias observaciones -RRB- son demasiado escasos para que pueda obtener buenas estimaciones en el marco de tiempo requerido. Dada la ausencia de restricciones estructurales en el entorno, los enfoques que dependen de distribuciones estructuradas pueden dar como resultado un sesgo de estimaci\u00f3n significativamente alto. Consideramos este problema en el context de un sistema distribuido de m\u00faltiples agentes en el que los agentes inform\u00e1ticos apoyan a las personas que realizan tareas complejas en un entorno din\u00e1mico. El hecho de que los agentes formen parte de un entorno de m\u00faltiples agentes, en el que otros agentes tambi\u00e9n pueden estar recopilando datos para estimar una caracter\u00edstica similar de sus usuarios, ofrece la posibilidad de que un agente aumente sus propias observaciones con las de otros agentes, mejorando as\u00ed la precisi\u00f3n de su proceso de aprendizaje. Adem\u00e1s, en los entornos que consideramos, los agentes suelen acumular datos a un ritmo relativamente similar. Sin embargo, la medida en que las observaciones de otros agentes ser\u00e1n \u00fatiles para un agente determinado depende de la medida en que las distribuciones de las caracter\u00edsticas de sus usuarios est\u00e9n correlacionadas con las del usuario de este agente. No hay garant\u00eda de que la distribuci\u00f3n de dos agentes diferentes est\u00e9 altamente correlacionada positivamente, y mucho menos de que sean iguales. Por lo tanto, para utilizar un enfoque de intercambio de datos, un mecanismo de aprendizaje debe ser capaz de identificar efectivamente el nivel de correlaci\u00f3n entre los datos recopilados por diferentes agentes y sopesar los datos compartidos dependiendo del nivel de correlaci\u00f3n. El dise\u00f1o de un m\u00f3dulo de autonom\u00eda de coordinaci\u00f3n -LRB- CA -RRB- dentro de un sistema de gesti\u00f3n de coordinaci\u00f3n -LRB- como parte del proyecto DARPA Coordinators -LSB- 18 -RSB- -RRB-, en el que los agentes apoyan una tarea de programaci\u00f3n distribuida, proporcion\u00f3 la motivaci\u00f3n inicial y un marco conceptual para este trabajo. Sin embargo, los mecanismos en s\u00ed son generales y pueden aplicarse no s\u00f3lo a otros dominios de ritmo r\u00e1pido, sino tambi\u00e9n en otros entornos de m\u00faltiples agentes en los que los agentes recopilan datos que se superponen hasta cierto punto, a velocidades aproximadamente similares, y en los que el entorno impone las restricciones de uso temprano, limitado y sin estructura definidas anteriormente -LRB-, por ejemplo, exploraci\u00f3n de planetas remotos -RRB-. En particular, nuestras t\u00e9cnicas ser\u00edan \u00fatiles en cualquier entorno en el que un grupo de agentes emprenda una tarea en un entorno nuevo, donde cada agente obtenga observaciones a un ritmo similar de los par\u00e1metros individuales que necesita para su toma de decisiones. En este art\u00edculo, presentamos un mecanismo que se utiliz\u00f3 para aprender caracter\u00edsticas clave del usuario en entornos de ritmo r\u00e1pido.El mecanismo proporciona estimaciones relativamente precisas en per\u00edodos de tiempo cortos al aumentar las observaciones directas de un agente individual con observaciones obtenidas por otros agentes con los que se est\u00e1 coordinando. En particular, nos centramos en los problemas relacionados de estimar el costo de interrumpir a una persona y estimar la probabilidad de que esa persona tenga la informaci\u00f3n requerida por el sistema. El mecanismo fue probado con \u00e9xito utilizando un sistema que simula un entorno de Coordinadores. La siguiente secci\u00f3n del art\u00edculo describe el problema de estimar par\u00e1metros relacionados con el usuario en dominios de ritmo r\u00e1pido. La Secci\u00f3n 3 proporciona una descripci\u00f3n general de los m\u00e9todos que desarrollamos. La implementaci\u00f3n, el entorno emp\u00edrico y los resultados se presentan en las Secciones 4 y 5. En la Secci\u00f3n 6 se ofrece una comparaci\u00f3n con m\u00e9todos relacionados y en la Secci\u00f3n 7 las conclusiones. 6. TRABAJO RELACIONADO Adem\u00e1s de la literatura sobre gesti\u00f3n de interrupciones revisada en la Secci\u00f3n 2, se han publicado varios otros Algunas \u00e1reas de trabajo previo son relevantes para el mecanismo de intercambio selectivo descrito en este documento. El filtrado colaborativo, que realiza predicciones -LRB- filtrado -RRB- sobre los intereses de un usuario -LSB- 7 -RSB-, funciona de forma similar al intercambio selectivo. Sin embargo, los sistemas de filtrado colaborativo presentan un rendimiento deficiente cuando no hay suficiente informaci\u00f3n sobre los usuarios y cuando no hay suficiente informaci\u00f3n sobre un nuevo usuario cuyo gusto el sistema intenta predecir -LSB- 7 -RSB-. El intercambio selectivo se basa en la capacidad de encontrar similitudes entre partes espec\u00edficas de la funci\u00f3n de distribuci\u00f3n de probabilidad asociada con una caracter\u00edstica de diferentes usuarios. Esta capacidad est\u00e1 estrechamente relacionada con la agrupaci\u00f3n y la clasificaci\u00f3n, un \u00e1rea ampliamente estudiada en el aprendizaje autom\u00e1tico. Dadas las consideraciones de espacio, nuestra revisi\u00f3n de esta \u00e1rea se limita a algunos enfoques representativos de agrupaci\u00f3n. De particular importancia es que la CA necesita encontrar similitudes entre funciones, definidas en un intervalo continuo, sin atributos predefinidos distintos. Una dificultad adicional es definir la medida de distancia. En la miner\u00eda de datos se han utilizado muchas t\u00e9cnicas de clustering -LSB- 2 -RSB-, con especial \u00e9nfasis en las actualizaciones incrementales del clustering, debido al gran tama\u00f1o de las bases de datos -LSB- 3 -RSB-. Sin embargo, la aplicabilidad de estos a dominios de ritmo r\u00e1pido es bastante limitada porque dependen de un gran conjunto de datos existentes. El m\u00e9todo m\u00e1s relevante para nuestros prop\u00f3sitos es el \u00edndice de entrop\u00eda relativa de Kullback-Leibler que se utiliza en teor\u00eda de la probabilidad y teor\u00eda de la informaci\u00f3n -LSB- 12 -RSB-. Sin embargo, el m\u00e9todo funcionar\u00e1 mal en escenarios en los que las funciones alternan entre diferentes niveles manteniendo la estructura y los momentos \"generales\". 208 La Sexta Internacional. Conf. Conjunta. sobre Agentes Aut\u00f3nomos y Sistemas Multi-Agente -LRB- AAMAS 07 -RRB-, mientras que nuestro enfoque basado en Wilcoxon les dar\u00e1 el rango m\u00e1s alto en t\u00e9rminos de similitud. Si bien la prueba de Wilcoxon es un procedimiento estad\u00edstico ampliamente utilizado -LSB- 22,14 -RSB-, generalmente se usa para comparar dos conjuntos de datos de una sola variable. Hasta donde sabemos, todav\u00eda no se ha intentado ampliar sus propiedades como infraestructura para determinar con qui\u00e9n y en qu\u00e9 medida se debe compartir la informaci\u00f3n, como se presenta en este documento. En estas aplicaciones, se utiliza principalmente como herramienta de identificaci\u00f3n y criterio de clasificaci\u00f3n.", "keyphrases": ["par\u00e1metro probabilista", "agente", "informar compartir", "tomar decisiones", "ambiente acelerado", "sistema de distribuci\u00f3n multiagente", "aprender mecanica", "seleccionar-compartir", "estimaci\u00f3n de par\u00e1metros"]}
{"file_name": "I-1", "text": "Aborting Tasks in BDI Agents ABSTRACT Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent 's own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system -LRB- such as JACK, Jadex, Jason, or SPARK -RRB-. A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence. 1. INTRODUCTION Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan can not be guaranteed -LSB- 13 -RSB-. Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence -LSB- 21 -RSB-. In agent architectures inspired by the Belief-Desire-Intention -LRB- BDI -RRB- model -LSB- 16 -RSB-, these properties are often characterized by the interactions between beliefs, goals, and plans -LSB- 2 -RSB-.1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success -LSB- 12, 14 -RSB-. Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent 's execution cycle. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. Aborting a task or plan is distinct from its failure. In contrast, aborting says nothing about the ability to perform ; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up. There has been a considerable amount of work on plan failures -LRB- such as detecting and resolving resource conflicts -LSB- 20, 10 -RSB- -RRB- and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct 1One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of `` the task having been performed '', and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use `` task '' to also refer to goals. fails -LRB- common approaches include either letting the other branch run to completion unhindered or dropping it completely -RRB-. In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN -LSB- 23, 17 -RSB-. Our focus is on a single agent, complementary to related work that considers exception handling for single - and multiagent systems -LRB- e.g., -LSB- 22, 5, 6 -RSB- -RRB-. This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work. 5. RELATED WORK Plan failure is handled in the extended version of AgentSpeak found in the Jason system -LSB- 6 -RSB-. Failure `` clean-up '' plans are triggered from goal deletion events --! g. In a goal deletion plan, the programmer can specify any `` undo '' actions and whether to attempt the goal again. If no goal deletion plan is provided, Jason 's default behaviour is to not reattempt the goal. Failure handling is applied only to plans triggered by addition of an achievement or test goal ; in particular, goal deletion events are not posted for failure of a goal deletion plan. The implementation of H \u00a8 ubner et al. -LSB- 6 -RSB- requires Jason 's internal actions. A requirement for implementing our approach is a reflective capability in the BDI agent implementation. All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states. Such meta level facilities are also required by the approach of Unruh et al. -LSB- 21 -RSB-, who define goal-based semantic compensation for an agent. Failure-handling goals are invoked according to failurehandling strategy rules, by a dedicated agent Failure Handling Component -LRB- FHC -RRB- that tracks task execution. These goals are specified by the agent programmer and attached to tasks, much like our FAb -LRB- P, PF, PA -RRB- construct associates failure and abort methods with a plan P. Note, however, that in contrast to both -LSB- 6 -RSB- and our semantics, -LSB- 21 -RSB- attach the failure-handling knowledge at the goal, not plan, level. Their failure-handling goals may consist of stabilization goals that perform localized, immediate `` clean-up '' to restore the agent 's state to a known, stable state, and compensation goals that perform `` undo '' actions. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure -LRB- i.e., if the FHC directs the agent to retry the failed goal and the retry is successful -RRB-. This contrasts with simplistic plan-level failure handling in which the what and how are intermingled in domain task knowledge. While our approach is defined at the plan level, our extended BDI semantics provides for the separation of execution and failure handling. Further, the FHC explicitly maintains data structures to track agent execution. We leverage the existing execution structures and self-reflective ability of a BDI agent to accomplish both aborting and failure handling without additional overhead. FHC 's failure-handling strategy rules -LRB- e.g., whether to retry a failed goal -RRB- are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent 's nature -LRB- e.g., blindly committed -RRB-. The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism -LRB- although not tied to any one agent system -RRB-. 14 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- a state-based protocol. This approach, together with state checkpointing, is used for multi-agent systems in -LSB- 22 -RSB-. The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery. Other work on multi-agent exception handling includes AOEX 's distributed exception handling agents -LSB- 5 -RSB-, and the similar sentinels of -LSB- 8 -RSB-. In both cases, failure-handling logic and knowledge are decoupled from the agents ; by contrast, while separating exception handling from domain-specific knowledge, Unruh et al. 's FHC and our approach both retain failure-handling logic within an agent. 6. CONCLUSION AND FUTURE WORK The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them -LRB- perhaps at the request of another agent to do so -RRB-, or by unbidden factors that lead to failure. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN. We are planning to implement an instance of our approach in the SPARK agent system -LSB- 9 -RSB- ; in particular, the work of this paper will be the basis for SPARK 's abort handling mechanism. An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action. We have assumed the default behaviour of a BDI-style agent, according to its nature : for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain -LRB- in which case to fail the task -RRB-. Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish -LSB- 19, 12 -RSB-. Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process. This would in particular influence the commitment the agent has towards a particular task : the higher the cost, the greater the commitment. A further item of interest is extending our approach to failure and abort to maintenance goals -LSB- 1 -RSB-. For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves.", "keyphrases": ["intellig agent", "failur", "deal", "cleanup method", "abort-method", "oper semant", "task", "goal", "goal construct"]}
{"file_name": "I-9", "text": "Temporal Linear Logic as a Basis for Flexible Agent Interactions ABSTRACT Interactions between agents in an open system such as the Internet require a significant degree of flexibility. A crucial aspect of the development of such methods is the notion of commitments, which provides a mechanism for coordinating interactive behaviors among agents. In this paper, we investigate an approach to model commitments with tight integration with protocol actions. This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments. We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic, which incorporates both temporal and resource-sensitive reasoning. We also discuss the application of this framework to scenarios such as online commerce. 1. INTRODUCTION AND MOTIVATION The agent paradigm has become well suited as a design metaphor to deal with complex systems comprising many components each having their own thread of control and purposes and involved in dynamic and complex interactions. In multi-agent environments, agents often need to interact with each other to fulfill their goals. Protocols are used to regulate interactions. In traditional approaches to protocol specification, like those using Finite State Machines or Petri Nets, protocols are often predetermined legal sequences of interactive behaviors. Therefore, agents are required to adapt their interactive behaviors to succeed and interactions among agents should not be constructed rigidly. To achieve flexibility, as characterized by Yolum and Singh in -LSB- 11 -RSB-, interaction protocols should ensure that agents have autonomy over their interactive behaviors, and be free from any unnecessary constraints. Also, agents should be allowed to adjust their interactive actions to take advantages of opportunities or handle exceptions that arise during interaction. For example, consider the scenario below for online sales. Cus has a goal of obtaining from Mer a cricket bat at some time. There are two options for Cus to pay. If Cus uses credit payment, Mer needs a bank Ebank to check Cus 's credit. If Cus 's credit is approved, Ebank will arrange the credit payment. Otherwise, Cus may then take the option to pay via PayPal. The interaction ends when goods are delivered and payment is arranged. A flexible approach to this example should include several features. Secondly, there should be no unnecessary constraint on the order in which actions are performed, such as which of making payments and sending the cricket bat should come first. Thirdly, choosing a sequence of interactive actions should be based on reasoning about the intrinsic meanings of protocol actions, which are based on the notion of commitment, i.e. which refers to a strong promise to other agent -LRB- s -RRB- to undertake some courses of action. Current approaches -LSB- 11, 12, 10, 1 -RSB- to achieve flexibilities using the notion of commitment make use of an abstract layer of commitments. However, in these approaches, a mapping from protocol actions onto operations on commitments as well as handling and enforcement mechanisms of commitments must be externally provided. Execution of protocol actions also requires concurrent execution of operations on related commitments. As a result, the overhead of processing the commitment layer makes specification and execution of protocols more complicated and error prone. There is also a lack of a logic to naturally express aspects of resources, internal and external choices as well as time of protocols. Rather than creating another layer of commitment outside protocol actions, we try to achieve a modeling of commitments that is integrated with protocol actions. Both commitments and protocol actions can then be reasoned about in one consistent system. In order to achieve that, we specify protocols in a declarative manner, i.e. what is to be achieved rather then how agents should interact. A key to this is using logic. Temporal logic, in particular, is suitable for describing and reasoning about temporal constraints while linear logic -LSB- 3 -RSB- is quite suitable for modeling resources. We suggest using a combination of linear logic and temporal logic to construct a commitment based interaction framework which allows both temporal and resource-related reasoning for interaction protocols. This provides a natural manipulation and reasoning mechanism as well as internal enforcement mechanisms for commitments based on proof search. Section 2 discusses the background material of linear logic, temporal linear logic and commitments. Section 3 introduces our modeling framework and specification of protocols. Section 4 discusses how our framework can be used for an example of online sale interactions between a merchant, a bank and a customer. We then discuss the advantages and limitations of using our framework to model interaction protocols and achieve flexibility in Section 5. Section 6 presents our conclusions and items of further work. 2. BACKGROUND In order to increase the agents ' autonomy over their interactive behaviors, protocols should be specified in terms of what is to be achieved rather than how the agents should act. In other words, protocols should be specified in a declarative manner. Using logic is central to this specification process. 2.1 Linear Logic Logic has been used as formalism to model and reason about agent systems. Linear logic -LSB- 3 -RSB- is well-known for modeling resources as well as updating processes. It has been considered in agent systems to support agent negotiation and planning by means of proof search -LSB- 5, 8 -RSB-. In real life, resources are consumed and new resources are created. In such logic as classical or temporal logic, however, a direct mapping of resources onto formulas is troublesome. If we model resources like A as `` one dollar '' and B as `` a chocolate bar '', then A = * B in classical logic is read as `` from one dollar we can get a chocolate bar ''. In order to resolve such resource - formula mapping issues, Girard proposed the constraints on which formulas will be used exactly once and can no longer be freely added or removed in derivations and hence treating linear logic formulas as resources. In linear logic, a linear implication A -- B, however, allows A to be removed after deriving B, which means the dollar is gone after using one dollar to buy a chocolate bar. Classical conjunction -LRB- and -RRB- and disjunction -LRB- or -RRB- are recast over different uses of contexts - multiplicative as combining and additive as sharing to come up with four connectives. The ability to specify choices via the additive connectives is a particularly useful feature of linear logic. A & -LRB- additive conjunction -RRB- B, stands for one own choice, either of A or B but not both. In agent systems, this duality between inner and outer choices is manifested by one agent having the power to choose between alternatives and the other having to react to whatever choice is made. Moreover, during interaction, the ability to match consumption and supply of resources among agents can simplify the specification of resource allocations. Linear logic is a natural mechanism to provide this ability -LSB- 5 -RSB-. In addition, it is emphasized in -LSB- 8 -RSB- that linear logic is used to model agent states as sets of consumable resources and particularly, linear implication is used to model transitions among states and capabilities of agents. 2.2 Temporal Linear Logic While linear logic provides advantages to modeling and reasoning about resources, it does not deal naturally with time constraints. Temporal logic, on the other hand, is a formal system which addresses the description and reasoning about the changes of truth values of logic expressions over time -LSB- 2 -RSB-. Temporal logic can be used for specification and verification of concurrent and reactive programs -LSB- 2 -RSB-. Temporal Linear Logic -LRB- TLL -RRB- -LSB- 6 -RSB- is the result of introducing temporal logic into linear logic and hence is resourceconscious as well as deals with time. The temporal operators used are Q -LRB- next -RRB-, \u2751 -LRB- anytime -RRB-, and O -LRB- sometime -RRB- -LSB- 6 -RSB-. Formulas with no temporal operators can be considered as being available only at present. Adding Q to a formula A, i.e. QA, means that A can be used only at the next time and exactly once. Similarly, \u2751 A means that A can be used at any time and exactly once. OA means that A can be used once at some time. Though both \u2751 and O refer to a point in time, the choice of which time is different. Regarding \u2751, the choice is an internal choice, as appropriate to one 's own capability. With O, the choice is externally decided by others. 2.3 Commitment The concept of social commitment has been recognized as fundamental to agent interaction. Indeed, social commitment provides intrinsic meanings of protocol actions and states -LSB- 11 -RSB-. In particular, persistence in commitments introduces into agents ' consideration a certain level of predictability of other agents ' actions, which is important when agents deal with issues of inter-dependencies, global constraints or The Sixth Intl.. Joint Conf. resources sharing -LSB- 7 -RSB-. Commitment based approaches associate protocols actions with operations on commitments and protocol states with the set of effective commitments -LSB- 11 -RSB-. Completing the protocol is done via means-end reasoning on commitment operations to bring the current state to final states where all commitments are resolved. From then, the corresponding legal sequences of interactive actions are determined. Hence, the approaches systematically enhance a variety of legal computations -LSB- 11 -RSB-. Commitments can be reduced to a more fundamental form known as pre-commitments. A pre-commitment here refers to a potential commitment that specifies what the owner agent is willing to commit -LSB- 4 -RSB-, like performing some actions or achieving a particular state. Agents can negotiate about pre-commitments by sending proposals of them to others. Once a precommitment is agreed, it then becomes a commitment and the process moves from negotiation phase to commitment phase, in which the agents act to fulfill their commitments.", "keyphrases": ["multi-agent environ", "interact behavior", "tempor constraint", "interact protocol", "linear logic", "multipl conjunct", "classic conjunct", "level of predict", "pre-commit", "linear implic", "emerg protocol", "condit commit", "request messag", "causal relationship"]}
{"file_name": "J-3", "text": "Budget Optimization in Search-Based Advertising Auctions ABSTRACT Internet search companies sell advertisement slots based on users ' search queries via an auction. While there has been previous work on the auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return -LRB- the number of user clicks on their ads -RRB- for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least a 1 \u2212 1/e fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem. 1. INTRODUCTION Online search is now ubiquitous and Internet search companies such as Google, Yahoo! and MSN let companies and * Work done while visiting Google, Inc., New York, NY. individuals advertise based on search queries posed by users. Conventional media outlets, such as TV stations or newspapers, price their ad slots individually, and the advertisers buy the ones they can afford. In contrast, Internet search companies find it difficult to set a price explicitly for the advertisements they place in response to user queries. Thus, they rely on the market to determine suitable prices by using auctions amongst the advertisers. It is a challenging problem to set up the auction in order to effect a stable market in which all the parties -LRB- the advertisers, users as well as the Internet search company -RRB- are adequately satisfied. The perspective in this paper is not of the Internet search company that displays the advertisements, but rather of the advertisers. The challenge from an advertiser 's point of view is to understand and interact with the auction mechanism. The advertiser determines a set of keywords of their interest and then must create ads, set the bids for each keyword, and provide a total -LRB- often daily -RRB- budget. When a user poses a search query, the Internet search company determines the advertisers whose keywords match the query and who still have budget left over, runs an auction amongst them, and presents the set of ads corresponding to the advertisers who `` win '' the auction. The advertiser whose ad appears pays the Internet search company if the user clicks on the ad. The focus in this paper is on how the advertisers bid. For the particular choice of keywords of their interest1, an advertiser wants to optimize the overall effect of the advertising campaign. The Internet search companies are supportive to1The choice of keywords is related to the domain-knowledge of the advertiser, user behavior and strategic considerations. Internet search companies provide the advertisers with summaries of the query traffic which is useful for them to optimize their keyword choices interactively. We do not directly address the choice of keywords in this paper, which is addressed elsewhere -LSB- 13 -RSB-. wards advertisers and provide statistics about the history of click volumes and prediction about the future performance of various keywords. 9 There are complex interactions between keywords because a user query may match two or more keywords, since the advertiser is trying to cover all the possible keywords in some domain. In effect the advertiser ends up competing with herself. As a result, the advertisers face a challenging optimization problem. The focus of this paper is to solve this optimization problem. 1.1 The Budget Optimization Problem We present a short discussion and formulation of the optimization problem faced by advertisers ; a more detailed description is in Section 2. A given advertiser sees the state of the auctions for searchbased advertising as follows. There is a set K of keywords of interest ; in practice, even small advertisers typically have a large set K. There is a set Q of queries posed by the users. For each query q G Q, there are functions giving the clicksq -LRB- b -RRB- and costq -LRB- b -RRB- that result from bidding a particular amount b in the auction for that query, which we model more formally in the next section. There is a bipartite graph G on the two vertex sets representing K and Q. For any query q G Q, the neighbors of q in K are the keywords that are said to `` match '' the query q. 2 The budget optimization problem is as follows. Given graph G together with the functions clicksq -LRB-. -RRB- and costq -LRB-. -RRB- on the queries, as well as a budget U, determine the bids bk for each keyword k G K such that Pq clicksq -LRB- bq -RRB- is maximized subject to Pq costq -LRB- bq -RRB- < U, where the `` effective bid '' bq on a query is some function of the keyword bids in the neighborhood of q. While we can cast this problem as a traditional optimization problem, there are different challenges in practice depending on the advertiser 's access to the query and graph information, and indeed the reliability of this information -LRB- e.g., it could be based on unstable historical data -RRB-. Thus it is important to find solutions to this problem that not only get many clicks, but are also simple, robust and less reliant on the information. In this paper we define the notion of a `` uniform '' strategy which is essentially a strategy that bids uniformly on all keywords. Since this type of strategy obviates the need to know anything about the particulars of the graph, and effectively aggregates the click and cost functions on the queries, it is quite robust, and thus desirable in practice. What is surprising is that uniform strategy actually performs well, which we will prove. 1.2 Our Main Results and Technical Overview We present positive and negative results for the budget optimization problem. In particular, we show : 9 Nearly all formulations of the problem are NP-Hard. In cases slightly more general than the formulation above, where the clicks have weights, the problem is inapproximable better than a factor of 1 -- 1e, unless P = NP. 9 We give a -LRB- 1 -- 1/e -RRB- - approximation algorithm for the budget optimization problem. The strategy found by the algorithm is a two-bid uniform strategy, which means that it randomizes between bidding some value b1 on all keywords, and bidding some other value b2 on all keywords until the budget is exhausted3. We show that this approximation ratio is tight for uniform strategies. We also give a -LRB- 1/2 -RRB- - approximation algorithm that offers a single-bid uniform strategy, only using one value b1. -LRB- This is tight for single-bid uniform strategies. -RRB- These strategies can be computed in time nearly linear in JQJ + JKJ, the input size. Uniform strategies may appear to be naive in first consideration because the keywords vary significantly in their click and cost functions, and there may be complex interaction between them when multiple keywords are relevant to a query. After all, the optimum can configure arbitrary bids on each of the keywords. Even for the simple case when the graph is a matching, the optimal algorithm involves placing different bids on different keywords via a knapsack-like packing -LRB- Section 2 -RRB-. So, it might be surprising that a simple two-bid uniform strategy is 63 % or more effective compared to the optimum. Our proof of the 1 -- 1/e approximation ratio relies on an adversarial analysis. We define a factor-revealing LP -LRB- Section 4 -RRB- where primal solutions correspond to possible instances, and dual solutions correspond to distributions over bidding strategies. We have conducted simulations using real auction data from Google. The results of these simulations, which are highlighted at the end of Section 4, suggest that uniform bidding strategies could be useful in practice. 8. CONCLUDING REMARKS Another interesting generalization is to consider weights on the clicks, which is a way to model conversions. -LRB- A conversion corresponds to an action on the part of the user who clicked through to the advertiser site ; e.g., a sale or an account sign-up. -RRB- Finally, we have looked at this system as a black box returning clicks as a function of bid, whereas in reality it is a complex repeated game involving multiple advertisers. In -LSB- 3 -RSB-, it was shown that when a set of advertisers use a strategy similar to the one we suggest here, under a slightly modified first-price auction, the prices approach a well-understood market equilibrium.", "keyphrases": ["budget optim", "search-base advertis auction", "internet", "advertis", "game theori", "intrigu heurist", "keyword", "uniform bid strategi", "vickrei clark grove", "lp", "gener second price"]}
{"file_name": "I-18", "text": "Collaboration Among a Satellite Swarm ABSTRACT The paper deals with on-board planning for a satellite swarm via communication and negotiation. We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements. We will present the formalization of the problem, a communication protocol, a solving method based on reactive decision rules, and first results. 1. INTRODUCTION Multi-agent architectures have been developed for satellite swarms -LSB- 36, 38, 42 -RSB- but strong assumptions on deliberation and communication capabilities are made in order to build a collective plan. In a multi-agent context, agents that build a collective plan must be able to change their goals, reallocate resources and react to environment changes and to the others ' choices. However, this step needs high communication and computation capabilities. In order to relax communication constraints, coordination based on norms and conventions -LSB- 16 -RSB- or strategies -LSB- 17 -RSB- are considered. Norms constraint agents in their decisions in such a way that the possibilities of conflicts are reduced. Strategies are private decision rules that allow an agent to draw benefit from the knowledgeable world without communication. However, communication is still needed in order to share information and build collective conjectures and plans. Communication can be achieved through a stigmergic approach -LRB- via the environment -RRB- or through message exchange and a protocol. A protocol defines interactions between agents and can not be uncoupled from its goal, e.g. exchanging information, finding a trade-off, allocating tasks and so on. Protocols can be viewed as an abstraction of an interaction -LSB- 9 -RSB-. However, an agent can not always communicate with another agent or the communication possibilites are restricted to short time intervals. At the individual level, agents are deliberative in order to create a local plan but at the collective level, they use normative decision rules in order to coordinate with one another. We will present the features of our problem, a communication protocol, a method for request allocation and finally, collaboration strategies. 7. EXPERIMENTS Satellite swarm simulations have been implemented in JAVA with the JADE platform -LSB- 3 -RSB-. The on-board planner is implemented with linear programming using ILOG CPLEX -LSB- 1 -RSB-. The simulation scenario implements 3 satellites on 6hour orbits. Two scenarios have been considered : the first one with a set of 40 requests with low mutual exclusion and conflict rate and the second one with a set of 74 requests with high mutual exclusion and conflict rate. In the case of low mutual exclusion and conflict rate -LRB- Table 1 -RRB-, centralized and isolated simulations lead to the same number of observations, with the same average priorities. Isolation leading to a lower cost is due to the high number of redundancies : many agents carry out the same request at different costs. The informed simulation reduces the number of redundancies but sligthly increases the average cost for the same reason. We can notice that the use of 5For instance, the rank-1-expert agent withdraws due to the altruist strategy and the cost increases by a in the worst case, then rank-2-expert agent withdraws due to the altruist strategy and the cost increases by e in the worst case. So the cost has increased by 2e in the worst case. 292 The Sixth Intl.. Joint Conf. Table 1 : Scenario 1 - the 40-request simulation results Table 2 : Scenario 2 - the 74-request simulation results collaboration strategies allows the number of redundancies to be much more reduced but the number of observations decreases owing to the constraint created by commitments. Furthermore, the average cost is increased too. Nevertheless each avoided redundancy corresponds to saved resources to realize on-board generated requests during the simulation. In the case of high mutual exclusion and conflict rate -LRB- Table 2 -RRB-, noteworthy differences exist between the centralized and isolated simulations. We can notice that all informed simulations -LRB- with or without strategies -RRB- allow to perform more observations than isolated agents do with less redundancies. Likewise, we can notice that all politics reduce the average cost contrary to the first scenario. The drastic politics is interesting because not only does it allow to perform more observations than isolated agents do but it allows to highly reduce the average cost with the lowest number of redundancies. As far as the number of exchanged messages is concerned, there are 12 meetings between 2 agents during the simulations. In the worst case, at each meeting each agent sends N pieces of information on the requests plus 3N pieces of information on the agents ' intentions plus 1 message for the end of communication, where N is the total number of requests. Consequently, 3864 messages are exchanged in the worst case for the 40-request simulations and 7128 messages for the 74-request simulations. These numbers are much higher than the number of messages that are actually exchanged. We can notice that the informed simulations, that communicate only requests, allow a higher reduction. In the general case, using communication and strategies allows to reduce redundancies and saves resources but increases the average cost : if a request is realized, agents that know it do not plan it even if its cost can be reduce afterwards. It is not the case with isolated agents. Using strategies on little constrained problems such as scenario 1 constrains the agents too much and causes an additional cost increase. Strategies are more useful on highly constrained problems such as scenario 2. Although agents constrain themselves on the number of observations, the average cost is widely reduce. 8. CONCLUSION AND FUTURE WORK An observation satellite swarm is a cooperative multiagent system with strong constraints in terms of communication and computation capabilities. In order to increase the global mission outcome, we propose an hybrid approach : deliberative for individual planning and reactive for collaboration. Agents reason both on requests to carry out and on the other agents ' intentions -LRB- candidacies -RRB-. An epidemic communication protocol uses all communication opportunities to update this information. Reactive decision rules -LRB- strategies -RRB- are proposed to solve conflicts that may arise between agents. Through the tuning of the strategies -LRB- \u03b1, e and \u03bb -RRB- and their plastic interlacing within the protocol, it is possible to coordinate agents without additional communication : the number of exchanged messages remains nearly the same between informed simulations and simulations implementing strategies. Some simulations have been made to experimentally validate these protocols and the first results are promising but raise many questions. What is the trade-off between the constraint rate of the problem and the need of strategies? To what extent are the number of redundancies and the average cost affected by the tuning of the strategies? Future works will focus on new strategies to solve new conflicts, specially those arising when relaxing the independence assumption between the requests. A second point is to take into account the complexity of the initial planning problem. Indeed, the chosen planning approach results in a combinatory explosion with big sets of requests : an anytime or a fully reactive approach has to be considered for more complex problems.", "keyphrases": ["on-board plan", "satellit swarm", "commun and negoti", "reactiv decis rule", "inform system applic", "multiag system", "task and resourc alloc", "objectag architectur", "teamag", "dip", "prospect ant"]}
{"file_name": "H-16", "text": "The Impact of Caching on Search Engines ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. 1. INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial. Caching can be applied at different levels with increasing response latencies or processing requirements. The decision of what to cache is either off-line -LRB- static -RRB- or online -LRB- dynamic -RRB-. A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss. Such online decisions are based on a cache policy, and several different policies have been studied in the past. For a search engine, there are two possible ways to use a cache memory : Caching answers : As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries. Caching terms : As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing. Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers. Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges : when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient. Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time. Figure 1 : One caching level in a distributed search architecture. In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms. More concretely, our main conclusions are that : \u2022 Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists ; \u2022 Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the KNAPSACK problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90 % ; \u2022 Changes of the query distribution over time have little impact on static caching. Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2. RELATED WORK There is a large body of work devoted to query optimization. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists -LSB- 1, 4, 15 -RSB-. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. Markatos -LSB- 10 -RSB- shows the existence of temporal locality in queries, and compares the performance of different caching policies. Fagni et al. follow Markatos ' work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio -LSB- 7 -RSB-. Different from our work, they consider caching and prefetching of pages of results. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system -LSB- 13 -RSB-. Their goal for such systems has been to improve response time for hierarchical engines. In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput. Long and Suel propose a caching system structured according to three different levels -LSB- 9 -RSB-. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy. Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching -LSB- 17 -RSB-, Web caching -LSB- 5 -RSB-, and even caching of posting lists -LSB- 9 -RSB-, but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists. 8. CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching. Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50 % using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12 %. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10 % higher compared these strategies. We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. Figure 14 : Impact of distribution changes on the static caching of posting lists.", "keyphrases": ["effici cach system", "web search engin", "static cach", "dynam cach", "cach queri result", "cach post list", "static cach", "answer and post list", "queri log", "effect of static cach", "distribut of the queri", "data-access hierarchi", "disk layer", "remot server layer"]}
{"file_name": "J-32", "text": "Nash Equilibria in Graphical Games on Trees Revisited * Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents. When the number of players is large, and the underlying graph has low degree, they provide a concise way to represent the players ' payoffs. It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD, indicating that it is unlikely that there is any polynomial-time algorithm for this problem. In this paper, we study the complexity of graphical games with two actions per player on bounded-degree trees. This setting was first considered by Kearns, Littman and Singh, who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games. The running time of their algorithm is exponential, though approximate equilibria can be computed efficiently. Later, Littman, Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time. We show that this modified algorithm is incorrect -- the output is not always a Nash equilibrium. We then propose a new algorithm that is based on the ideas of Kearns et al. and computes all Nash equilibria in quadratic time if the input graph is a path, and in polynomial time if it is an arbitrary graph of maximum degree 2. Moreover, our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees, but the running time can be exponential, even when the tree has bounded degree. We show that this is inevitable -- any algorithm of this type will take exponential time, even on bounded-degree trees with pathwidth 2. It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1, but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete -LRB- so is unlikely to be tractable -RRB-. * This research is supported by the EPSRC research grants `` Algorithmics of Network-sharing Games '' and `` Discontinuous Behaviour in the Complexity of randomized Algorithms ''. 1. INTRODUCTION Graphical games were introduced in the papers of Kearns et al. -LSB- 8 -RSB- and Littman et al. -LSB- 9 -RSB- as a succinct representation of games with a large number of players. The classical normal form -LRB- or matrix form -RRB- representation has a size that is exponential in the number of players, making it unsuitable for large-scale distributed games. A graphical game associates each player with a vertex of an underlying graph G, and the payoff to that player is a function of the actions chosen by himself and his neighbours in G ; if G has low degree, this is a concise way to represent a game with many players. The papers -LSB- 8, 9 -RSB- give a dynamic-programming algorithm for finding Nash equilibria in graphical games where there are two actions per player and G is a tree. The first of these papers describes a generic algorithm for this problem that can be specialized in two ways : as an algorithm that computes approximations to all Nash equilibria in time polynomial in the input size and the approximation quality, or as an exponential-time algorithm that allows the exact computation of all Nash equilibria in G. In -LSB- 9 -RSB-, the authors propose a modification to the latter algorithm that aims to find a single Nash equilibrium in polynomial time. This does not quite work, as we show in Section 3, though it introduces a useful idea. 1.1 Background The generic algorithm of -LSB- 8 -RSB- consists of two phases which we will refer to as the upstream pass and the downstream pass ; 1 the former starts at the leaves of the tree and ends at the root, while the latter starts at the root and ends at the leaves. there is a Nash equilibrium in the graphical game downstream of V -LRB- inclusive -RRB- given that W plays w -LRB- for a more technical definition, the reader is referred to Section 2 -RRB-. The generic algorithm does not address the problem of representing the best response policy ; in fact, the most important difference between the two instantiations of the generic algorithm described in -LSB- 8 -RSB- is in their approach to this issue. The computation is performed inductively : the best response policy for V is computed based on the best response policies of V 's children U1,..., Uk. By the end of the upstream pass, all children of the root have computed their best response policies. In the beginning of the downstream pass, the root selects its strategy and informs its children about its choice. It also selects a strategy for each child. A necessary and sufficient condition for the algorithm to proceed is that the strategy of the root is a best response to the strategies of its children and, for each child, the chosen strategy is one of the pre-computed potential best responses to the chosen strategy of the root. The equilibrium then propagates downstream, with each vertex selecting its children 's actions. The action of the child is chosen to be any strategy from the pre-computed potential best responses to the chosen strategy of the parent. To bound the running time of this algorithm, the paper -LSB- 8 -RSB- shows that any best response policy can be represented as a union of an exponential number of rectangles ; the polynomial time approximation algorithm is obtained by combining this representation with a polynomial-sized grid. 1.2 Our Results One of the main contributions of our paper is to show that the algorithm proposed by -LSB- 9 -RSB- is incorrect. In Section 3 we describe a simple example for which the algorithm of -LSB- 9 -RSB- outputs a vector of strategies that does not constitute a Nash equilibrium of the underlying game. In Sections 4, 5 and 6 we show how to fix the algorithm of -LSB- 9 -RSB- so that it always produces correct output. Section 4 considers the case in which the underlying graph is a path of length n. For this case, we show that the number of rectangles in each of the best response policies is O -LRB- n2 -RRB-. This gives us an O -LRB- n3 -RRB- algorithm for finding a Nash equilibrium, and for computing a representation of all Nash equilibria. -LRB- This algorithm is a special case of the generic algorithm of -LSB- 8 -RSB- -- we show that it runs in polynomial time when the underlying graph is a path. -RRB- We can improve the running time of the generic algorithm using the ideas of -LSB- 9 -RSB-. In particular, we give an O -LRB- n2 -RRB- algorithm for finding a Nash equilibrium of a graphical game on a path of length n. Instead of storing best response policies, this algorithm stores appropriately-defined subsets, which, following -LSB- 9 -RSB-, we call breakpoint policies -LRB- modifying the definition as necessary -RRB-. We obtain the following theorem THEOREM 1. There is an O -LRB- n2 -RRB- algorithm that finds a Nash equilibrium of a graphical game with two actions per player on an n-vertex path. There is an O -LRB- n3 -RRB- algorithm that computes a representation of all Nash equilibria of such a game. In Section 5 we extend the results of Section 4 to general degree2 graphs, obtaining the following theorem. THEOREM 2. There is a polynomial-time algorithm thatfinds a Nash equilibrium of a graphical game with two actions per player on a graph with maximum degree 2. In Section 6 we extend our algorithm so that it can be used to find a Nash equilibrium of a graphical game on an arbitrary tree. Even when the tree has bounded degree, the running time can be exponential. We show that this is inevitable by constructing a family of graphical games on bounded-degree trees for which best response policies of some of the vertices have exponential size, and any twopass algorithm -LRB- i.e., an algorithm that is similar in spirit to that of -LSB- 8 -RSB- -RRB- has to store almost all points of the best response policies. In particular, we show the following. THEOREM 3. There is an infinite family ofgraphical games on bounded-degree trees with pathwidth 2 such that any two-pass algorithm for finding Nash equilibria on these trees requires exponential time and space. It is interesting to note that the trees used in the proof of Theorem 3 have pathwidth 2, that is, they are very close to being paths. It is an open question whether our algorithm runs in polynomial time for graphs of pathwidth 1. This question can be viewed as a generalization of a very natural computational geometry problem -- we describe it in more detail in Section 8. In Section 7, we give a complexity-theoretic intractability result for the problem of finding a Nash equilibrium of a graphical game on a graph with small pathwidth. We prove the following theorem. THEOREM 4. Consider the problem offinding a Nash equilibrium for a graphical game in which the underlying graph has maximum degree 3 and pathwidth k. There is a constant k such that this problem is PPAD-complete. Theorem 4 limits the extent to which we can exploit `` path-like '' properties of the underlying graph, in order to find Nash equilibria. To prove Theorem 4, we use recent PPAD-completeness results for games, in particular the papers -LSB- 7, 4 -RSB- which show that the problem of finding Nash equilibria in graphical games of degree d -LRB- for d > 3 -RRB- is computationally equivalent to the problem of solving r-player normal-form games -LRB- for r > 4 -RRB-, both of which are PPAD-complete. 8. OPEN PROBLEMS The most important problem left open by this paper is whether it is possible to find a Nash equilibrium of a graphical game on a bounded-degree tree in polynomial time. Our construction shows that any two-pass algorithm that explicitly stores breakpoint policies needs exponential time and space. However, it does not preclude the existence of an algorithm that is based on a similar idea, but, instead of computing the entire breakpoint policy for each vertex, uses a small number of additional passes through the graph to decide which -LRB- polynomial-sized -RRB- parts of each breakpoint policy should be computed. In particular, such an algorithm may be based on the approximation algorithm of -LSB- 8 -RSB-, where the value of e is chosen adaptively. Another intriguing question is related to the fact that the graph for which we constructed an exponential-sized breakpoint policy has pathwidth 2, while our positive results are for a path, i.e., a graph of pathwidth 1. It is not clear if for any bounded-degree graph of pathwidth 1 the running time of -LRB- the breakpoint policybased version of -RRB- our algorithm will be polynomial. In particular, it is instructive to consider a `` caterpillar '' graph, i.e., the graph that can be obtained from Tn by deleting the vertices S1,..., Sn. This implies that the problem of bounding the size of the best response policy -LRB- or, alternatively, the breakpoint policy -RRB-, can be viewed as a generalization of the following computational geometry problem, which we believe may be of independent interest : PROBLEM 1. Ifyes, can it be the case that in this set, there is no path with a polynomial number of turns that connects the endpoints of the original segment? This implies that even for a caterpillar, the best response policy can be exponentially large. However, in our example -LRB- which is omitted from this version of the paper due to space constraints -RRB-, there exists a polynomial-size path through the best response policy, i.e., it does not prove that the breakpoint policy is necessarily exponential in size. If one can prove that this is always the case, it may be possible to adapt this proof to show that there can be an exponential gap between the sizes of best response policies and breakpoint policies.", "keyphrases": ["graphic game", "larg-scale distribut network", "nash equilibrium", "degre", "dynam program-base algorithm", "ppad-complet", "bound-degre tree", "gener algorithm", "respons polici", "downstream pass", "breakpoint polici"]}
{"file_name": "I-16", "text": "An Advanced Bidding Agent for Advertisement Selection on Public Displays ABSTRACT In this paper we present an advanced bidding agent that participates in first-price sealed bid auctions to allocate advertising space on BluScreen -- an experimental public advertisement system that detects users through the presence of their Bluetooth enabled devices. Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts, and the auctions that it participates within. It then uses these models to maximise the exposure that its adverts receive. We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy, random allocation, and a centralised optimal allocation with perfect foresight. Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation, and in a mixed population of agents it is able to expose its adverts to 25 % more users than the simple bidding strategy. Moreover, its performance is within 7.5 % of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate. 1. INTRODUCTION Electronic displays are increasingly being used within public environments, such as airports, city centres and retail stores, in order to advertise commercial products, or to entertain and inform passersby. of interactive public displays have been proposed. As such, these systems assume prior knowledge about the target audience, and require either that a single user has exclusive access to the display, or that users carry specific tracking devices so that their presence can be identified -LSB- 6, 11 -RSB-. However, these approaches fail to work in public spaces, where no prior knowledge regarding the users who may view the display exists, and where such displays need to react to the presence of several users simultaneously. By contrast, Payne et al. have developed an intelligent public display system, named BluScreen, that detects and tracks users through the Bluetooth enabled devices that they carry with them everyday -LSB- 8 -RSB-. Within this system, a decentralised multi-agent auction mechanism is used to efficiently allocate advertising time on each public display. Each advert is represented by an individual advertising agent that maintains a history of users who have already been exposed to the advert. This agent then seeks to acquire advertising cycles -LRB- during which it can display its advert on the public displays -RRB- by submitting bids to a marketplace agent who implements a sealed bid auction. The value of these bids is based upon the number of users who are currently present in front of the screen, the history of these users, and an externally derived estimate of the value of exposing an advert to a user. In this paper, we present an advanced bidding agent that significantly extends the sophistication of this approach. In particular, we consider the more general setting in which it is impossible to determine an a priori valuation for exposing an advert to a user. In addition, it is also likely to be the case within new commercial installations where limited market experience makes estimating a valuation impossible. The advertising agent is then simply tasked with using this budget to maximum effect -LRB- i.e. to achieve the maximum possible advert exposure within this time period -RRB-. Now, in order to achieve this goal, the advertising agent must be capable of modelling the behaviour of the users in order to predict the number who will be present in any future advertising cycle. In addition, it must also understand the auction environment in which it competes, in order that it may make best use of its limited budget. Thus, in developing an advanced bidding agent that achieves this, we advance the state of the art in four key ways : 1. We enable the advertising agents to model the arrival and departure of users as independent Poisson processes, and to make maximum likelihood estimates of the rates of these processes based on their observations. We show how these agents can then calculate the expected number of users who will be present during any future advertising cycle. 2. Using a decision theoretic approach we enable the advertising agents to model the probability of winning any given auction when a specific amount is bid. The cumulative form of the gamma distribution is used to represent this probability, and its parameters are fitted using observations of both the closing price of previous auctions, and the bids that that advertising agent itself submits. 3. We show that our explicit assumption that the advertising agent derives no additional benefit by showing an advert to a single user more than once, causes the expected utility of each future advertising cycle to be dependent on the expected outcome of all the auctions that precede it. We thus present a stochastic optimisation algorithm based upon simulated annealing that enables the advertising agent to calculate the optimal sequence of bids that maximises its expected utility. 4. The remainder of this paper is organised as follows : Section 2 discusses related work where agents and auction-based marketplaces are used to allocated advertising space. Section 3 describes the prototype BluScreen system that motivates our work. In section 4 we present a detailed description of the auction allocation mechanism, and in section 5 we describe our advanced bidding strategy for the advertising agents. In section 6 we present an empirical validation of our approach, and finally, we conclude in section 7. 2. RELATED WORK The commercial attractiveness of targeted advertising has been amply demonstrated on the internet, where recommendation systems and contextual banner adverts are the norm -LSB- 1 -RSB-. Attempts to apply these approaches within the real world have been much more limited. Gerding et al. present a simulated system -LRB- CASy -RRB- whereby a Vickrey auction mechanism is used to sell advertising space within a modelled electronic shopping mall -LSB- 2 -RSB-. The auction is used to rank a set of possible advertisements provided by different retail outlets, and the top ranking advertisements are selected for presentation on public displays. Feedback is provided through subsequent sales information, allowing the model to build up a profile of a user 's preferences. However, unlike the BluScreen Figure 1 : A deployed BluScreen prototype. system that we consider here, it is not suitable for advertising to many individuals simultaneously, as it requires explicit interaction with a single user to acquire the user 's preferences. User identification is based on infrared badges and embedded sensors within an office environment. When several users pass by the display, a centralised system compares the user 's profiles to identify common areas of interest, and content that matches this common interest is shown. Thus, whilst CASy is a simulated system that allows advertisers to compete for the attention of single user, GroupCast is a prototype system that detects the presence of groups of users and selects content to match their profiles. Despite their similarities, neither system addresses the settings that interests us here : how to allocate advertising space between competing advertisers who face an audience of multiple individuals about whom there is no a priori profile information. Thus, in the next section we describe the prototype BluScreen system that motivates our work. 7. CONCLUSIONS In this paper, we presented an advanced bidding strategy for use by advertising agents within the BluScreen advertising system. This bidding strategy enabled advertising agents to model and predict the arrival and departure of users, and also to model their success within a first-price sealed bid auction by observing both the bids that they themselves submitted and the winning bid. The ex The Sixth Intl.. Joint Conf. Figure 8 : Comparison of an evenly mixed population of advertising agents using simple and advanced bidding strategies over a range of parameter settings. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. Figure 9 : Comparison of an unevenly mixed population of advertising agents using simple and advanced bidding strategies. Results are averaged over 50 simulation runs and error bars indicate the standard error in the mean. pected utility, measured as the number of users who the advertising agent exposes its advert to, was shown to depend on these factors, and resulted in a complex expression where the expected utility of each auction depended on the success or otherwise of earlier auctions. We presented an algorithm based upon simulated annealing to solve for the optimal bidding strategy, and in simulation, this bidding strategy was shown to significantly outperform a simple bidding strategy that had none of these features. Its performance closely approached that of a central optimal allocation, with perfect knowledge of the arrival and departure of users, despite the uncertain environment in which the strategy must operate. This work will continue to be done in conjunction with the deployment of more BluScreen prototypes in order to gain further real world experience.", "keyphrases": ["advanc bid agent", "bluscreen", "experiment public advertis system", "bluetooth", "probabilist model", "centralis optim alloc", "distribut artifici intellig", "decentralis multi-agent auction mechan", "independ poisson process", "decis theoret approach", "stochast optimis algorithm"]}
{"file_name": "J-18", "text": "Mediators in Position Auctions ABSTRACT A mediator is a reliable entity, which can play on behalf of agents in a given game. A mediator however can not enforce the use of its services, and each agent is free to participate in the game directly. In this paper we introduce a study of mediators for games with incomplete information, and apply it to the context of position auctions, a central topic in electronic commerce. VCG position auctions, which are currently not used in practice, possess some nice theoretical properties, such as the optimization of social surplus and having dominant strategies. These properties may not be satisfied by current position auctions and their variants. We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions. We require that accepting the mediator services, and reporting honestly to the mediator, will form an ex post equilibrium, which satisfies the following rationality condition : an agent 's payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator 's services, or by the agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price -LRB- Google-like -RRB- position auctions, as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k = 1, the self-price position auction, we show that the existence of such mediator depends on the tie breaking rule used in the auction. 1. INTRODUCTION Consider an interaction in a multi-agent system, in which every player holds some private information, which is called the player 's type. For example, in an auction interaction the type of a player is its valuation, or, in more complex auctions, its valuation function. This interaction is modeled as a game with incomplete information. This game is called a Bayesian game, when a commonly known probability measure on the profiles of types is added to the system. Otherwise it is called a pre-Bayesian game. In this paper we deal only with pre-Bayesian games. Consider the following simple example of a pre-Bayesian game, which possesses an ex post equilibrium. The game is denoted by H.", "keyphrases": ["auction", "mediat", "ex post equilibrium", "agent", "posit auction", "electron commerc", "richer class of posit auction", "next-price posit auction", "multi-agent system", "t-strategi", "vcg outcom function", "self-price posit auction"]}
{"file_name": "H-29", "text": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback ABSTRACT Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension : the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given query 's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness -LRB- worst-case performance -RRB- by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method. 1. INTRODUCTION Uncertainty is an inherent feature of information retrieval. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback -LRB- PRF -RRB- tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio 's formula -LSB- 16 -RSB-, or more recent language modeling approaches such as Relevance Models -LSB- 10 -RSB-. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model '. The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model 's weight combines two complementary factors : the model 's probability of generating the query, and the variance of the model, with high-variance models getting lower weight. ` For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings -LRB- see -LSB- 10 -RSB-, p. 62 -RRB-. Figure 1 : Estimating the uncertainty of the feedback model for a single query. 4. RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision. The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. On the document side, recent work by Zhou & Croft -LSB- 21 -RSB- explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. -LSB- 17 -RSB- proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. Greiff, Morgan and Ponte -LSB- 8 -RSB- explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance -- high noise -- increases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models. Estimates of output variance have recently been used for improved text classification. Lee et al. -LSB- 11 -RSB- used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Ando and Zhang proposed a method that they call structural feedback -LSB- 3 -RSB- and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector \u02c6wi of the documents is calculated. Principal component analysis -LRB- PCA -RRB- is then applied to the \u02c6wi to obtain the matrix 4 -RRB- of H left singular vectors \u03c6h that are used to obtain the new, expanded query The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai -LSB- 18 -RSB- describe a method for more robust feedback that allows each document to have a different feedback \u03b1. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter \u03b7 based on a function of some quality measure of feedback documents. 5. CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.", "keyphrases": ["feedback method", "posterior distribut", "enhanc feedback model", "inform retriev", "queri expans", "probabl distribut", "pseudo-relev feedback", "vector space-base algorithm", "risk", "feedback model", "estim uncertainti", "languag model", "feedback distribut"]}
{"file_name": "H-20", "text": "New Event Detection Based on Indexing-tree and Named Entity ABSTRACT New Event Detection -LRB- NED -RRB- aims at detecting from one or multiple streams of news stories that which one is reported on a new event -LRB- i.e. not reported previously -RRB-. With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium -LRB- LDC -RRB- datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems. 1. INTRODUCTION New Event Detection -LRB- NED -RRB- is one of the five tasks in TDT. A Topic is defined as `` a seminal event or activity, along with directly related events and activities '' -LSB- 2 -RSB-. An Event is defined as `` something -LRB- non-trivial -RRB- happening in a certain place at a certain time '' -LSB- 3 -RSB-. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering. In most of state-of-the-art -LRB- currently -RRB- NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. The core problem of NED is to identify whether two stories are on the same topic. Obviously, these systems can not take advantage of topic information. Other systems organize previous stories into clusters -LRB- each cluster corresponds to a topic -RRB-, and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic. On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities -LSB- 10, 11, 12, 13 -RSB-. However, none of the systems have considered that terms of different types -LRB- e.g. Noun, Verb or Person name -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. For example, the names of election candidates -LRB- Person name -RRB- are very important for stories of election class ; the locations -LRB- Location name -RRB- where accidents happened are important for stories of accidents class. -LRB- 2 -RRB- How to make good use of cluster -LRB- topic -RRB- information to improve accuracy? -LRB- 3 -RRB- How to obtain better news story representation by better understanding of named entities. Driven by these problems, we have proposed three approaches in this paper. -LRB- 1 -RRB- To make the detection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news clusters in different hierarchies according to their values of similarity. Comparisons between current story and previous clusters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. -LRB- 2 -RRB- We use the clusters of the first floor in the indexing-tree as news topics, in which term weights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster -LRB- topic -RRB- information is used properly, so the problem of theme decentralization is avoided. -LRB- 3 -RRB- Based on observations on the statistics obtained from training data, we found that terms of different types -LRB- e.g. Noun and Verb -RRB- have different effects for different classes of stories in determining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2. Section 3 presents the basic model for NED that most current systems use. Section 4 describes our new detection procedure based on news indexing-tree. In section 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evaluation metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. 2. RELATED WORK Papka et al. proposed Single-Pass clustering on NED -LSB- 6 -RSB-. When a new story was encountered, it was processed immediately to extract term features and a query representation of the story 's content is built up. Then it was compared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic -LSB- 7 -RSB-. In this manner comparisons happen between stories and clusters. Recent years, most work focus on proposing better methods on comparison of stories and document representation. Good improvements on TDT bench-marks were shown. Stokes et al. -LSB- 9 -RSB- utilized a combination of evidence from two distinct representations of a document 's content. One of the representations was the usual free text vector, the other made use of lexical chains -LRB- created using WordNet -RRB- to build another term vector. Then the two representations are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used. Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities -LSB- 10 -RSB-. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity -LSB- 11 -RSB- -LSB- 12 -RSB-. UMass -LSB- 13 -RSB- research group split document representation into two parts : named entities and non-named entities. And it was found that some classes of news could achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both -LSB- 10 -RSB- and -LSB- 13 -RSB- used text categorization technique to classify news stories in advance. In -LSB- 13 -RSB- news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In -LSB- 10 -RSB- frequent terms for each class are removed from document representation. In their work, effectiveness of different kinds of names -LRB- or terms with different POS -RRB- for NED in different news classes are not investigated. 8. CONCLUSION We have proposed a news indexing-tree based detection procedure in our model. It reduces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensions to the basic TF-IDF model. The first extension is made by adjust term weights based on term distributions between the whole corpus and a cluster story set. And the second extension to basic TF-IDF model is better use of term types -LRB- named entities types and part-of-speed -RRB- according to news categories. Our experimental results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy. For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and their relations within a topic.", "keyphrases": ["new event detect", "stream of new stori", "volum of new", "new index-tree", "term reweight approach", "ned accuraci", "term weight", "statist", "train data", "name entiti reweight mode", "class of stori", "linguist data consortium", "baselin system", "exist system"]}
{"file_name": "H-7", "text": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user 's interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens. 1. INTRODUCTION For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a user 's history. One major personalization topic studied in the information retrieval community is content-based personal recommendation systems '. These systems learn user-specific pro'Content - based recommendation is also called adaptive fil files from user feedback so that they can recommend information tailored to each individual user 's interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems. A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the `` cold start '' problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile. There has been much research on improving classification accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal -LSB- 26 -RSB-. Another approach is using domain knowledge. The third approach is borrowing training data from other resources -LSB- 5 -RSB- -LSB- 7 -RSB-. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data. One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user -LSB- 27 -RSB- -LSB- 25 -RSB-. In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data. A mature recommendation system usually works for millions of users. It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario -LRB- Section 4 -RRB-, we find that the EM tering, or item-based collaborative filtering. In this paper, the words `` filtering '' and `` recommendation '' are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O -LRB- MK -RRB-, where M is the number of users and K is the number of dimensions. This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the `` Modified EM algorithm. '' This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results. The organization of the remaining parts of this paper is as follows : Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6. 2. RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970 's. The approaches that have been used to solve this problem can be roughly classified into two major categories : content based filtering versus collaborative filtering. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user. Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past. Memorybased heuristics and model based approaches have been used in collaborative filtering task -LSB- 15 -RSB- -LSB- 8 -RSB- -LSB- 2 -RSB- -LSB- 14 -RSB- -LSB- 12 -RSB- -LSB- 11 -RSB-. This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks -LSB- 27 -RSB- -LSB- 25 -RSB-. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback. Similar to some other researchers -LSB- 18 -RSB- -LSB- 1 -RSB- -LSB- 21 -RSB-, we found that a recommendation system will be more effective when both techniques are combined. 7. CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors. This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a scenario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users. The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model parameters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems.", "keyphrases": ["model", "content-base", "recommend system", "linear regress", "collabor filter", "paramet", "learn techniqu", "ir", "em algorithm", "classif", "rate"]}
{"file_name": "I-32", "text": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions ABSTRACT Multiagent environments are often not cooperative nor collaborative ; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties -LRB- e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model -RRB-. We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms ' appropriateness. 1. INTRODUCTION Early research in multiagent systems -LRB- MAS -RRB- considered cooperative groups of agents ; because individual agents had MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries. Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states -LRB- e.g., -LSB- 8, 4, 5 -RSB- -RRB-. However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments. In addition, traditional search methods -LRB- like Min-Max -RRB- do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning -LSB- 9, 3, 11 -RSB-. In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans -LSB- 4 -RSB- model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms ; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings. We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players ' behavior using a large set of completed match log 4. RELATED WORK However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents ' behavior in it. The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. Additional Adversarial planning work was done by Willmott et al. -LSB- 13 -RSB-, which provided an adversarial planning approach to the game of GO. The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agent 's advantage. However, the basic limitations of those search methods still apply ; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5. CONCLUSIONS We presented an Adversarial Environment model for a 2These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines. The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment. The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research.", "keyphrases": ["multiag environ", "adversari interact", "adversari environ", "behavior axiom", "bilater and multilater instanti", "evalu function", "benefici action", "connect-four game", "empir studi", "axiomat model", "zero-sum encount", "treatment group", "eval valu", "interact"]}
{"file_name": "I-29", "text": "Distributed Management of Flexible Times Schedules ABSTRACT We consider the problem of managing schedules in an uncertain, distributed environment. We assume a team of collaborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unexpected events will force changes to some prescribed activities and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms : -LRB- 1 -RRB- a `` flexible times '' representation of the agent 's schedule -LRB- using a Simple Temporal Network -RRB- and -LRB- 2 -RRB- an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent 's schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportunities for solution improvement through joint change. Using a simulator to model the environment, we compare the performance of our multi-agent system with that of an expected optimal -LRB- but non-scalable -RRB- centralized MDP solver. 1. INTRODUCTION The practical constraints of many application environments require distributed management of executing plans and schedules. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency -LRB- fall-back -RRB- options. Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. In a flexible-times representation of an agent 's schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. However their use in distributed problem solving settings has been quite sparse -LRB- -LSB- 7 -RSB- is one exception -RRB-, and prior approaches to multi-agent scheduling -LRB- e.g., -LSB- 6, 13, 5 -RSB- -RRB- have generally operated with fixed-times representations of agent schedules. We define an agent architecture centered around incremental management of a flexible times schedule. Local change is ac Figure 1 : A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process -LRB- similar in some respects to -LSB- 5 -RSB- -RRB-, aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans. 8. STATUS AND DIRECTIONS Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods. This scale places much higher computational demands on all of the agent 's components. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems. To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms. To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling -LSB- 7 -RSB- to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules.", "keyphrases": ["manag schedul", "distribut environ", "agent architectur", "schedul", "inter-depend activ", "geograph separ", "flexibl time", "central plan", "manag", "schedul-execut", "slack", "shortest path algorithm", "activ alloc", "conflict-driven approach", "optimist synchron", "inter-agent coordin", "perform"]}
{"file_name": "C-30", "text": "Bullet : High Bandwidth Data Dissemination Using an Overlay Mesh ABSTRACT In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include : i -RRB- an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii -RRB- a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii -RRB- a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node 's parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate. 1. INTRODUCTION In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable -LRB- and hence extensible -RRB- end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree -LSB- 2, 6 -RSB-. However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node 's single parent in the tree. Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically transmit disjoint data sets to various points in the network. Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the `` last object '', which may only be available at a few nodes. To illustrate Bullet 's behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available -LRB- TCP-friendly -RRB- bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data. Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly -LSB- 15 -RSB-. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to `` peer '' with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants ; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure `` high up '' in a multicast overlay tree. In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques -LSB- 1, 18, 21, 24, 34 -RSB-. Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Finally, we use TFRC -LSB- 15 -RSB- to transfer data both down the overlay tree and among peers. One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source -LRB- its parent -RRB-. Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research -LSB- 20, 35 -RSB-, accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system. Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. We have completed a prototype of Bullet running on top of a number of overlay trees. Our evaluation of a 1000-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree -LRB- using an offline algorithm and global network topology information -RRB-, all while remaining TCP friendly. For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 2 presents Bullet 's system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet 's performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions. 5. RELATED WORK Snoeren et al. -LSB- 36 -RSB- use an overlay mesh to achieve reliable and timely delivery of mission-critical data. In this system, every node chooses n `` parents '' from which to receive duplicate packet streams. Since its foremost emphasis is reliability, the system does not attempt to improve the bandwidth delivered to the overlay participants by sending disjoint data at each level. Further, during recovery from parent failure, it limits an overlay router 's choice of parents to nodes with a level number that is less than its own level number. Kazaa nodes are organized into a scalable, hierarchical structure. Individual users search for desired content in the structure and proceed to simultaneously download potentially disjoint pieces from nodes that already have it. Since Kazaa does not address the multicast communication model, a large fraction of users downloading the same file would consume more bandwidth than nodes organized into the Bullet overlay structure. BitTorrent -LSB- 3 -RSB- is another example of a file distribution system currently deployed on the Internet. The tracker poses a scalability limit, as it continuously updates the systemwide distribution of the file. Similar to Bullet, BitTorrent incorporates the notion of `` choking '' at each node with the goal of identifying receivers that benefit the most by downloading from that particular source. FastReplica -LSB- 11 -RSB- addresses the problem of reliable and efficient file distribution in content distribution networks -LRB- CDNs -RRB-. In the basic algorithm, nodes are organized into groups of fixed size -LRB- n -RRB-, with full group membership information at each node. To distribute the file, a node splits it into n equal-sized portions, sends the portions to other group members, and instructs them to download the missing pieces in parallel from other group members. Since only a fixed portion of the file is transmitted along each of the overlay links, the impact of congestion is smaller than in the case of tree distribution. However, since it treats all paths equally, FastReplica does not take full advantage of highbandwidth overlay links in the system. There are numerous protocols that aim to add reliability to IP multicast. In Scalable Reliable Multicast -LRB- SRM -RRB- -LSB- 16 -RSB-, nodes multicast retransmission requests for missed packets. Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. In pbcast -LSB- 2 -RSB-, a node has global group membership, and periodically chooses a random subset of peers to send a digest of its received packets. A node that receives the digest responds to the sender with the missing packets in a last-in, first-out fashion. Since lbpcast does not require an underlying tree for data distribution and relies on the push-gossiping model, its network overhead can be quite high. Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not `` blindly '' request retransmissions from their peers. Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Further, a Bullet node splits the retransmission load between all of its peers. We note that pbcast nodes contain a mechanism to rate-limit retransmitted packets and to send different packets in response to the same digest. However, this does not guarantee that packets received in parallel from multiple peers will not be duplicates. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree. Narada -LSB- 19 -RSB- builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. It then runs a standard routing protocol on top of the overlay mesh to construct forwarding trees using each node as a possible source. Narada nodes maintain global knowledge about all group participants, limiting system scalability to several tens of nodes. Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Overcast -LSB- 21 -RSB- is an example of a bandwidth-efficient overlay tree construction algorithm. In this system, all nodes join at the root and migrate down to the point in the tree where they are still able to maintain some minimum level of bandwidth. Bullet is expected to be more resilient to node departures than any tree, including Overcast. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers. Overcast convergence time is limited by probes to immediate siblings and ancestors. Bullet is able to provide approximately a target bandwidth without having a fully converged tree. In parallel to our own work, SplitStream -LSB- 9 -RSB- also has the goal of achieving high bandwidth data dissemination. It operates by splitting the multicast stream into k stripes, transmitting each stripe along a separate multicast tree built using Scribe -LSB- 34 -RSB-. Perhaps more importantly, SplitStream assumes that there is enough available bandwidth to carry each stripe on every link of the tree, including the links between the data source and the roots of individual stripe trees independently chosen by Scribe. To some extent, Bullet and SplitStream are complementary. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet -LSB- 29 -RSB- considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. In the case of on-demand streaming, CoopNet -LSB- 30 -RSB- addresses the flash-crowd problem at the central server by redirecting incoming clients to a fixed number of nodes that have previously retrieved portions of the same content. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file. 6. CONCLUSIONS Typically, high bandwidth overlay data streaming takes place over a distribution tree. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures. Specifically, this paper makes the following contributions : 9 We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques. 9 We provide a technique for recovering missing data from peers in a scalable and efficient manner. RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants. 9 We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes. 9 A large-scale evaluation of 1000 overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree.", "keyphrases": ["overlai mesh", "data dissemin", "overlai network", "ip multicast", "multipoint commun", "high-bandwidth data distribut", "larg-file transfer", "real-time multimedia stream", "bullet", "bandwidth probe", "peer-to-peer", "ransub", "content deliveri", "tfrc"]}
{"file_name": "I-30", "text": "Distributed Task Allocation in Social Networks ABSTRACT This paper proposes a new variant of the task allocation problem, where the agents are connected in a social network and tasks arrive at the agents distributed over the network. We show that the complexity of this problem remains NPhard. Moreover, it is not approximable within some factor. We develop an algorithm based on the contract-net protocol. Our algorithm is completely distributed, and it assumes that agents have only local knowledge about tasks and resources. We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time. Three different types of networks, namely small-world, random and scale-free networks, are used to represent various social relationships among agents in realistic applications. The results demonstrate that our algorithm works well and that it scales well to large-scale applications. 1. INTRODUCTION Recent years have seen a lot of work on task and resource allocation methods, which can potentially be applied to many real-world applications. However, some interesting applications where relations between agents play a role require a slightly more general model. range of task allocation methods. The question is how VOs are to be dynamically composed and re-composed from individual agents, when different tasks and subtasks need to be performed. This would be done by allocating them to different agents who may each be capable of performing different subsets of those tasks. In this paper, we study the problem of task allocation from the perspective of such a complex interrelated structure. Specifically, therefore, we consider agents to be connected to each other in a social network. Other than modeling the interrelated structure between business partners, the social network introduced in this paper can also be used to represent other types of connections or constraints among autonomous entities that arise from other application domains. The next section gives a formal description of the task allocation problem on social networks. In Section 3, we prove that the complexity of this problem remains NP-hard. We then proceed to develop a distributed algorithm in Section 4, and perform a series of experiments with this algorithm, as described in Section 5. Section 6 discusses related work, and Section 7 concludes. 6. RELATED WORK Task allocation in multiagent systems has been investigated by many researchers in recent years with different assumptions and emphases. However, most of the research to date on task allocation does not consider social connections among agents, and studies the problem in a centralized The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 505 Figure 6 : The quality of the GDAP algorithm for a uniform and a skewed task benefit distribution related to the resource ratio -LRB- the first graph -RRB-, and the network degree -LRB- the second graph -RRB-. setting. For example, Kraus et al. -LSB- 12 -RSB- develop an auction protocol that enables agents to form coalitions with time constraints. It assumes each agent knows the capabilities of all others. The proposed protocol is centralized, where one manager is responsible for allocating the tasks to all coalitions. Manisterski at al. -LSB- 14 -RSB- discuss the possibilities of achieving efficient allocations in both cooperative and noncooperative settings. They propose a centralized algorithm to find the optimal solution. In contrast to this work, we introduce also an efficient completely distributed protocol that takes the social network into account. Task allocation has also been studied in distributed settings by for example Shehory and Kraus -LSB- 18 -RSB- and by Lerman and Shehory -LSB- 13 -RSB-. They propose distributed algorithms with low communication complexity for forming coalitions in large-scale multiagent systems. However, they do not assume the existence of any agent network. The work of Sander et al. -LSB- 16 -RSB- introduces computational geometry-based algorithms for distributed task allocation in geographical domains. Agents are then allowed to move and actively search for tasks, and the capability of agents to perform tasks is homogeneous. In order to apply their approach, agents need to have some knowledge about the geographical positions of tasks and some other agents. Other work -LSB- 17 -RSB- proposes a location mechanism for open multiagent systems to allocate tasks to unknown agents. In this approach each agent caches a list of agents they know. The analysis of the communication complexity of this method is based on lattice-like graphs, while we investigate how to efficiently solve task allocation in a social network, whose topology can be arbitrary. Networks have been employed in the context of task allocation in some other works as well, for example to limit the Figure 8 : The quality of the GDAP algorithm compared to the upper bound. interactions between agents and mediators -LSB- 1 -RSB-. Mediators in this context are agents who receive the task and have connections to other agents. They break up the task into subtasks, and negotiate with other agents to obtain commitments to execute these subtasks. Their focus is on modeling the decision process of just a single mediator. Another approach is to partition the network into cliques of nodes, representing coalitions which the agents involved may use as a coordination mechanism -LSB- 20 -RSB-. The focus of that work is distributed coalition formation among agents, but in our approach, we do not need agents to form groups before allocating tasks. Easwaran and Pitt -LSB- 6 -RSB- study ` complex tasks ' that require ` services ' for their accomplishment. The problem concerns the allocation of subtasks to service providers in a supply chain. Another study of task allocation in supply chains is -LSB- 21 -RSB-, where it is argued that the defining characteristic of Supply Chain Formation is hierarchical subtask decomposition -LRB- HSD -RRB-. HSD is implemented using task dependency networks -LRB- TDN -RRB-, with agents and goods as nodes, and I/O relations between them as edges. Here, the network is given, and the problem is to select a subgraph, for which the authors propose a market-based algorithm, in particular, a series of auctions. Compared to these works, our approach is more general in the sense that we are able to model different types of connections or constraints among agents for different problem domains in addition to supply chain formation. Finally, social networks have been used in the context of team formation. Previous work has shown how to learn which relations are more beneficial in the long run -LSB- 8 -RSB-, and adapt the social network accordingly. We believe these results can be transferred to the domain of task allocation as well, leaving this as a topic for further study. Figure 7 : The run time of the GDAP algorithm. 506 The Sixth Intl.. Joint Conf. on Autonomous Agents and Multi-Agent Systems -LRB- AAMAS 07 -RRB- 7. CONCLUSIONS In this paper we studied the task allocation problem in a social network -LRB- STAP -RRB-, which can be seen as a new, more general, variant of the TAP. We believe it has a great amount of potential for realistic problems. We provided complexity results on computing the efficient solution for the STAP, as well as a bound on possible approximation algorithms. Next, we presented a distributed protocol, related to the contractnet protocol. We also introduced an exponential algorithm to compute the optimal solution, as well as a fast upperbound algorithm. The results presented in this paper show that the distributed algorithm performs well in small-world, scale-free, and random networks, and for many different settings. Also other experiments were done -LRB- e.g. on grid networks -RRB- and these results held up over a wider range of scenarios. Furthermore, we showed that it scales well to large networks, both in terms of quality and of required computation time. The results also suggest that small-world networks are slightly better suited for local task allocation, because there are no nodes with very few neighbors. There are many interesting extensions to our current work. In this paper, we focus on the computational aspect in the design of the distributed algorithm. In our future work, we would also like to address some of the related issues in game theory, such as strategic agents, and show desirable properties of a distributed protocol in such a context. In the current algorithm we assume that agents can only contact their neighbors to request resources, which may explain why our algorithm performs not as good in the scalefree networks as in the small-world networks. Our future work may allow agents to reallocate -LRB- sub -RRB- tasks. We are interested in seeing how such interactions will affect the performance of task allocation in different social networks. A third interesting topic for further work is the addition of reputation information among the agents. This may help to model changing business relations and incentivize agents to follow the protocol. Finally, it would be interesting to study real-life instances of the social task allocation problem, and see how they relate to the randomly generated networks of different types studied in this paper. Acknowledgments.", "keyphrases": ["social network", "social relationship", "task alloc", "util", "alloc", "algorithm", "commun messag", "behavior", "multiag system", "strateg agent", "interact"]}
{"file_name": "J-21", "text": "A Strategic Model for Information Markets ABSTRACT Information markets, which are designed specifically to aggregate traders ' information, are becoming increasingly popular as a means for predicting future events. Recent research in information markets has resulted in two new designs, market scoring rules and dynamic parimutuel markets. We develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection game, that serves as a useful model for information markets. We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets, and also captures the essence of the strategies in market scoring rules. The projection game is tractable to analyze, and has an attractive geometric visualization that makes the strategic moves and interactions more transparent. We use it to prove several strategic properties about the dynamic parimutuel market. We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule, and it is strategically similar to other scoring rules. Finally, we illustrate two applications of the model to analysis of complex strategic scenarios : we analyze the precision of a market in which traders have inertia, and a market in which a trader can profit by manipulating another trader 's beliefs. 1. INTRODUCTION Markets have long been used as a medium for trade. As a side effect of trade, the participants in a market reveal something about their preferences and beliefs. For example, in a financial market, agents would buy shares which they think are undervalued, and sell shares which they think are overvalued. It has long been observed that, because the market price is influenced by all the trades taking place, it aggregates the private information of all the traders. Thus, in a situation in which future events are uncertain, and each trader might have a little information, the aggregated information contained in the market prices can be used to predict future events. This has motivated the creation of information markets, which are mechanisms for aggregating the traders ' information about an uncertain event. Information markets can be modeled as a game in which the participants bet on a number of possible outcomes, such as the results of a presidential election, by buying shares of the outcomes and receiving payoffs when the outcome is realized. As in financial markets, the participants aim to maximize their profit by buying low and selling high. The benefit of well-designed information markets goes beyond information aggregation ; they can also be used as a hedging instrument, to allow traders to insure against risk. Recently, researchers have turned to the problem of designing market structures specifically to achieve better information aggregation properties than traditional markets. Two designs for information markets have been proposed : the Dynamic Parimutuel Market -LRB- DPM -RRB- by Pennock -LSB- 10 -RSB- and the Market Scoring Rules -LRB- MSR -RRB- by Hanson -LSB- 6 -RSB-. Both the DPM and the MSR were designed with the goal of giving informed traders an incentive to trade, and to reveal their information as soon as possible, while also controlling the subsidy that the market designer needs to pump into the market. One version of the DPM was implemented in the Yahoo! Buzz market -LSB- 8 -RSB- to experimentally test the market 's prediction properties. The innovation in the MSR is to use these scoring rules as instruments that can be traded, thus providing traders who have new information an incentive to trade. The MSR was to be used in a policy analysis market in the Middle East -LSB- 15 -RSB-, which was subsequently withdrawn. Information markets rely on informed traders trading for their own profit, so it is critical to understand the strategic properties of these markets. This is not an easy task, because markets are complex, and traders can influence each other 's beliefs through their trades, and hence, can potentially achieve long term gains by manipulating the market. For the DPM, we are not aware of any prior strategic analysis of this nature ; in fact, a strategic hole was discovered while testing the DPM in the Yahoo! Buzz market -LSB- 8 -RSB-. 1.1 Our Results In this paper, we seek to develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection 1 game, that serves as a useful model for information markets. The projection game is conceptually simpler than the MSR and DPM, and thus it is easier to analyze. In addition it has an attractive geometric visualization, which makes the strategic moves and interactions more transparent. We present an analysis of the optimal strategies and profits in this game. We then undertake an analysis of traders ' costs and profits in the dynamic parimutuel market. Remarkably, we find that the cost of a sequence of trades in the DPM is identical to the cost of the corresponding moves in the projection game. Further, if we assume that the traders beliefs at the end of trading match the true probability of the event being predicted, the traders ' payoffs and profits in the DPM are identical to their payoffs and profits in a corresponding projection game. We use the equivalence between the DPM and the projection game to prove that the DPM is arbitrage-free, deduce profitable strategies in the DPM, and demonstrate that constraints on the agents ' trades are necessary to prevent a strategic breakdown. We also prove an equivalence between the projection game and the MSR : We show that play in the MSR is strategically equivalent to play in a restricted projection game, at least for myopic strategies and small trades. This allows us to use the projection game as a conceptual model for market scoring rules. Further, because the restricted projection game corresponds to a DPM with a natural trading constraint, this sheds light on an intriguing connection between the MSR and the DPM. Lastly, we illustrate how the projection game model can be used to analyze the potential for manipulation of information markets for long-term gain.2 We present an example scenario in which such manipulation can occur, and suggest additional rules that might mitigate the possibility of manipulation. We also illustrate another application to analyzing how a market maker can improve the prediction accuracy of a market in which traders will not trade unless their expected profit is above a threshold. 1.2 Related Work Numerous studies have demonstrated empirically that market prices are good predictors of future events, and seem to aggregate the collected wisdom of all the traders -LSB- 2, 3, 12, 1, 5, 16 -RSB-. A number of recent studies have addressed the design of the market structure and trading rules for information markets, as well as the incentive to participate and other strategic issues. However, strategic issues in information markets have also been studied by Mangold et al. -LSB- 8 -RSB- and by Hanson, Oprea and Porter -LSB- 7 -RSB-. An upcoming survey paper -LSB- 11 -RSB- discusses costfunction formulations of automated market makers. Organization of the paper The rest of this paper is organized as follows : In Section 2, we describe the projection game, and analyze the players ' costs, profits, and optimal strategies in this game. In Section 3, we study the dynamic parimutuel market, and show that trade in a DPM is equivalent to a projection game. We establish a connection between the projection game and the MSR in Section 4. In Section 5, we illustrate how the projection game can be used to analyze non-myopic, and potentially manipulative, actions. We present our conclusions, and suggestions for future work, in Section 6. 6. CONCLUSIONS AND FUTURE WORK We have presented a simple geometric game, the projection game, that can serve as a model for strategic behavior in information markets, as well as a tool to guide the design of new information markets. We have used this model to analyze the cost, profit, and strategies of a trader in a dynamic parimutuel market, and shown that both the dynamic parimutuel market and the spherical market scoring rule are strategically equivalent to the restricted projection game under slight distortion of the prior probabilities. The general analysis was based on the assumption that traders do not actively try to mislead other traders for future profit. In section 5, however, we analyze a small example market without this assumption. We demonstrate that the projection game can be used to analyze traders ' strategies in this scenario, and potentially to help design markets with better strategic properties. Our results raise several very interesting open questions. Firstly, the payoffs of the projection game can not be directly implemented in situations in which the true probability is not ultimately revealed. Finally, the existence of long-range manipulative strategies in information markets is of great interest. The example we studied in section 5 merely scratches the surface of this area. A general study of this class of manipulations, together with a characterization of markets in which it can or can not arise, would be very useful for the design of information markets.", "keyphrases": ["inform market", "dynam parimutuel market", "project game model", "predict market", "market score rule", "spheric score rule", "long-rang manipul strategi", "social and behavior scienc-econom", "liquid time"]}
{"file_name": "H-18", "text": "Topic Segmentation with Shared Topic Detection and Alignment of Multiple Documents ABSTRACT Topic detection and tracking -LSB- 26 -RSB- and topic segmentation -LSB- 15 -RSB- play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information -LRB- MI -RRB- and weighted mutual information -LRB- WMI -RRB- that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI -LRB- or WMI -RRB-. Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation. 1. INTRODUCTION Many researchers have worked on topic detection and tracking -LRB- TDT -RRB- -LSB- 26 -RSB- and topic segmentation during the past decade. Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical structure. Topic segmentation tasks usually fall into two categories -LSB- 15 -RSB- : text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics. Traditional approaches perform topic segmentation on documents one at a time -LSB- 15, 25, 6 -RSB-. Most of them perform badly in subtle tasks like coherent document segmentation -LSB- 15 -RSB-. Often, end-users seek documents that have the similar content. At a finer granularity, users may actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest. Thus, the extension of topic segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized. Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment -LSB- 15, 25, 6 -RSB-. However, they usually suffer the issue of identifying stop words. For example, additional document-dependent stop words are removed together with the generic stop words in -LSB- 15 -RSB-. There are two reasons that we do not remove stop words directly. First, identifying stop words is another issue -LSB- 12 -RSB- that requires estimation in each domain. Removing common stop words may result in the loss of useful information in a specific domain. We employ a soft classification using term weights. Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster. Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem. Usually, human readers can identify topic transition based on cue words, and can ignore stop words. Inspired by this, we give each term -LRB- or term cluster -RRB- a weight based on entropy among different documents and different segments of documents. Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words. These words are common in a document. Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed -LSB- 15 -RSB-. Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment. The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used criteria. Also, we have addressed the problem of topic segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents. Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly. Obviously, our approach can handle single documents as a special case when multiple documents are unavailable. It can detect shared topics among documents to judge if they are multiple documents on the same topic. We also introduce the new criterion of WMI based on term weights learned from multiple similar documents, which can improve performance of topic segmentation further. We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice. Some of our prior work is in -LSB- 24 -RSB-. The rest of this paper is organized as follows : In Section 2, we review related work. Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI. In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by us Figure 1 : Illustration of multi-document segmentation and alignment. ing dynamic programming. In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm. Conclusions and some future directions of the research work are discussed in Section 6. 2. PREVIOUS WORK Supervised learning usually has good performance, since it learns functions from labelled training sets. However, often getting large training sets with manual labels on document sentences is prohibitively expensive, so unsupervised approaches are desired. Some approaches also focus on cue words as hints of topic transitions -LSB- 11 -RSB-. While some existing methods only consider information in single documents -LSB- 6, 15 -RSB-, others utilize multiple documents -LSB- 16, 14 -RSB-. There are not many works in the latter category, even though the performance of segmentation is expected to be better with utilization of information from multiple documents. Previous research studied methods to find shared topics -LSB- 16 -RSB- and topic segmentation and summarization between just a pair of documents -LSB- 14 -RSB-. Text classification and clustering is a related research area which categorizes documents into groups using supervised or unsupervised methods. Criteria of these approaches can be utilized in the issue of topic segmentation. Some of those methods have been extended into the area of topic segmentation, such as PLSA -LSB- 5 -RSB- and maximum entropy -LSB- 7 -RSB-, but to our best knowledge, using MI for topic segmentation has not been studied. 6. CONCLUSIONS AND FUTURE WORK We proposed a novel method for multi-document topic segmentation and alignment based on weighted mutual information, which can also handle single-document cases. We used dynamic programming to optimize our algorithm. Our approach outperforms all the previous methods on singledocument cases. Moreover, we also showed that doing segmentation among multiple documents can improve the performance tremendously. Our results also illustrated that using weighted mutual information can utilize the information of multiple documents to reach a better performance. We only tested our method on limited data sets. More data sets especially complicated ones should be tested. More previous methods should be compared with. Moreover, natural segmentations like paragraphs are hints that can be used to find the optimal boundaries. Supervised learning also can be considered.", "keyphrases": ["topic detect", "track", "topic segment", "local and sequenti inform of document", "singl document", "multipl document", "wmu", "share topic", "optim boundari", "singl-document segment", "multi-document segment", "cue term", "stop word", "term weight", "perform of topic segment"]}
{"file_name": "I-6", "text": "Dynamic Semantics for Agent Communication Languages ABSTRACT This paper proposes dynamic semantics for agent communication languages -LRB- ACLs -RRB- as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems. Based on the idea of providing alternative semantic `` variants '' for speech acts and transition rules between them that are contingent on previous agent behaviour, our framework provides an improved notion of grounding semantics in ongoing interaction, a simple mechanism for distinguishing between compliant and expected behaviour, and a way to specify sanction and reward mechanisms as part of the ACL itself. We extend a common framework for commitment-based ACL semantics to obtain these properties, discuss desiderata for the design of concrete dynamic semantics together with examples, and analyse their properties. 1. INTRODUCTION The field of agent communication language -LRB- ACL -RRB- research has long been plagued by problems of verifiability and grounding -LSB- 10, 13, 17 -RSB-. Unable to safeguard themselves against abuse by malicious, deceptive or malfunctioning agents, mentalistic semantics are inherently unreliable and inappropriate for use in open MAS in which agents with potentially conflicting objectives might deliberately exploit their adversaries ' conceptions of message semantics to provoke a certain behaviour. Commitment-based semantics -LSB- 6, 8, 14 -RSB-, on the other hand, define the meaning of messages exchanged among agents in terms of publicly observable commitments, i.e. pledges to bring about a state of affairs or to perform certain actions. Such semantics solve the verifiability problem as they allow for tracing the status of existing commitments at any point in time given observed messages and actions so that any observer can, for example, establish whether an agent has performed a promised action. Further, this implies that the semantics specification does not provide an interface to agents ' deliberation and planning mechanisms and hence it is unclear how rational agents would be able to decide whether to subscribe to a suggested ACL semantics when it is deployed. Finally, none of the existing approaches allows the ACL to specify how to respond to a violation of its semantics by individual agents. Secondly, existing approaches fail to exploit the possibilities of sanctioning and rewarding certain behaviours in a communication-inherent way by modifying the future meaning of messages uttered or received by compliant/deviant agents. In this paper, we propose dynamic semantics -LRB- DSs -RRB- for ACLs as a solution to these problems. Our notion of DS is based on the very simple idea of defining different alternatives for the meaning of individual speech acts -LRB- so-called semantic variants -RRB- in an ACL semantics specification, and transition rules between semantic states -LRB- i.e. collections of variants for different speech acts -RRB- that describe the current meaning of the ACL. These elements taken together result in a FSM-like view of ACL specifications where each individual state provides a complete ACL semantics and state transitions are triggered by observed agent behaviour in order to -LRB- 1 -RRB- reflect future expectations based on previous interaction experience and -LRB- 2 -RRB- sanction or reward certain kinds of behaviour. In defining a DS framework for commitment-based ACLs, this paper makes three contributions : 1. An extension of commitment-based ACL semantics to provide an improved notion of grounding commitments in agent interaction and to allow ACL specifications to be directly used for planning-based rational decision making. 2. A simple way of distinguishing between compliant and expected behaviour with respect to an ACL specification that enables reasoning about the potential behaviour of agents purely from an ACL semantics perspective. 3. A mechanism for specifying how meaning evolves with agent behaviour and how this can be used to describe communication-inherent sanctioning and rewarding mechanisms essential to the design of open MASs.. Furthermore, we discuss desiderata for DS design that can be derived from our framework, present examples and analyse their properties. The remainder of this paper is structured as follows : Section 2 introduces a formal framework for dynamic ACL semantics. In section 3 we present an analysis and discussion of this framework and discuss desiderata for the design of ACLs with dynamic semantics. Section 4 reviews related approaches, and section 5 concludes. 4. RELATED WORK Expectation-based reasoning about interaction was first proposed in -LSB- 2 -RSB-, considering the evolution of expectations described as probabilistic expectations of communication and action sequences. The same authors suggested a more general framework for expectation-based communication semantics -LSB- 9 -RSB-, and argue for a `` consequentialist '' view of semantics that is based on defining the meaning of utterances in terms of their expected consequences and updating these expectations with new observations -LSB- 11 -RSB-. However, their approach does not use an explicit notion of commitments which in our framework mediates between communication and behaviour-based grounding, and provides a clear distinction between a normative notion of compliance and a more empirical notion of expectation. Grounding for -LRB- mentalistic -RRB- ACL semantics has been investigated in -LSB- 7 -RSB- where grounded information is viewed as `` information that is publicly expressed and accepted as being true by all the agents participating in a conversation ''. Like -LSB- 1 -RSB- -LRB- which bases the notion of `` publicly expressed '' on roles rather than internal states of agents -RRB- these authors ' main concern is to provide a verifiable basis for determining the semantics of expressed mental states and commitments. 11In a non-trivial sense, i.e. when some initial transitions are possible in principle 106 The Sixth Intl.. Joint Conf. Our framework is also related to deontic methods for the specification of obligations, norms and sanctions. In this area, -LSB- 16 -RSB- is the only framework that we are aware of which considers dynamic obligations, norms and sanctions. However, as we have described above we solely utilise semantic evolution as a sanctioning and rewarding mechanism, i.e. unlike this work we do not assume that agents can be directly punished or rewarded. 5. CONCLUSION This paper introduces dynamic semantics for ACLs as a method for dealing with some fundamental problems of agent communication in open systems, the simple underlying idea being that different courses of agent behaviour can give rise to different interpretations of meaning of the messages exchanged among agents. Based on a common framework of commitment-based semantics, we presented a notion of grounding for commitments based on notions of compliant and expected behaviour. We then defined dynamic semantics as state transition systems over different semantic states that can be viewed as different `` versions '' of ACL semantics in the traditional sense, and can be easily associated with a planning-based view of reasoning about communication. Thereby, our focus was on simplicity and on providing mechanisms for tracking semantic evolution in a `` down-toearth '', algorithmic fashion to ensure applicability to many different agent designs. We discussed the properties of our framework showing how it can be used as a powerful communication-inherent mechanism for rewarding and sanctioning agent behaviour in open systems without compromising agent autonomy, discussed its integration with agents ' planning processes, complexity issues, and presented a list of desiderata for the design of ACLs with such semantics.", "keyphrases": ["agent commun languag", "dynam semant", "social reason", "commit-base semant", "state transit system", "reput-base adapt", "mutual of expect", "recoveri mechan", "non-redund"]}
{"file_name": "J-13", "text": "On The Complexity of Combinatorial Auctions : Structured Item Graphs and Hypertree Decompositions ABSTRACT The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth -LRB- called structured item graphs -RRB-. Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists -LRB- and if so, computing one -RRB- was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with -LRB- dual -RRB- hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph. 1. INTRODUCTION Combinatorial auctions. Combinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items. This is desirable when a bidder 's valuation of a bundle of items is not equal to the sum of her valuations of the individual items. An outcome for -LRB- Z, B -RRB- is a subset b of B such that item -LRB- Bi -RRB- n item -LRB- Bj -RRB- = 0, for each pair Bi and Bj of bids in b with i = ~ j. The winner determination problem. A crucial problem for combinatorial auctions is to determine the outcome b \u2217 that maximizes the sum of the accepted bid prices -LRB- i.e., Bi \u2208 b \u2217 pay -LRB- Bi -RRB- -RRB- over all the possible outcomes. This problem, called winner determination problem -LRB- e.g., -LSB- 11 -RSB- -RRB-, is known to be intractable, actually NP-hard -LSB- 17 -RSB-, and even not approximable in polynomial time unless NP = ZPP -LSB- 19 -RSB-. Hence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions -LRB- e.g., -LSB- 20, 5, 2, 8, 23 -RSB- -RRB- and to identify classes of instances where solving the winner determination problem is feasible in polynomial time -LRB- e.g., -LSB- 15, 22, 12, 21 -RSB- -RRB-. In fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions. Item graphs. Currently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any Figure 1 : Example MaxWSP problem : -LRB- a -RRB- Hypergraph H -LRB- To, go -RRB-, and a packing h for it ; -LRB- b -RRB- Primal graph for H -LRB- To, go -RRB- ; and, -LRB- c, d -RRB- Two item graphs for H -LRB- To, go -RRB-. bid, the items occurring in it induce a connected subgraph. Indeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure -LSB- 3 -RSB- -- formally bounded treewidth -LSB- 16 -RSB-. To have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H -LRB- T, g -RRB- such that its set of nodes N -LRB- H -LRB- T, g -RRB- -RRB- coincides with set of items I, and where its edges E -LRB- H -LRB- T, g -RRB- -RRB- are precisely the bids of the buyers -LCB- item -LRB- Bi -RRB- | Bi \u2208 B -RCB-. A special item graph for ~ I, B ~ is the primal graph of H -LRB- T, g -RRB-, denoted by G -LRB- H -LRB- T, g -RRB- -RRB-, which contains an edge between any pair of nodes in some hyperedge of H -LRB- T, g -RRB-. Then, any item graph for H -LRB- T, g -RRB- can be viewed as a simplification of G -LRB- H -LRB- T, g -RRB- -RRB- obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge. EXAMPLE 1. The hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB- is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = -LCB- I1,..., I5 -RCB-, and item -LRB- Bi -RRB- = hi, for each 1 \u2264 i \u2264 3. The primal graph for H -LRB- To, go -RRB- is reported in Figure 1. -LRB- b -RRB-, while two example item graphs are reported in Figure 1. -LRB- c -RRB- and -LRB- d -RRB-, where edges required for maintaining the connectivity for h1 are depicted in bold. < Open Problem : Computing structured item graphs efficiently. The above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined. However, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain -LRB- constant -RRB- treewidth exists, and if so, how to compute such a structured item graph efficiently. Weighted Set Packing. Let us note that the hypergraph representation H -LRB- T, g -RRB- of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear -LRB- e.g., -LSB- 17 -RSB- -RRB-. Formally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h ' \u2208 h with h = ~ h ', it holds that h \u2229 h ' = \u2205. Then, the set of the solutions for the weighted set packing problem for H -LRB- T, g -RRB- w.r.t. w -LRB- T, g -RRB- coincides with the set of the solutions for the winner determination problem on ~ I, B ~. EXAMPLE 2. Consider again the hypergraph H -LRB- To, go -RRB- reported in Figure 1. -LRB- a -RRB-. An example packing for H -LRB- To, go -RRB- is h = -LCB- h1 -RCB-, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1. Indeed, the packing Contributions The primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable. Towards this aim, we first study structured item graphs and solve the open problem in -LSB- 3 -RSB-. The result is very bad news : \u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3. More formally, letting C -LRB- ig, k -RRB- denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph -LRB- associated with a combinatorial auction problem -RRB- belongs to C -LRB- ig, 3 -RRB- is NP-complete. In the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem. E -LRB- H -RRB- -RCB- is in E. We show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width -LSB- 7 -RSB- bounded by k -LRB- short : class C -LRB- hw, k -RRB- of hypergraphs -RRB-. Note that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \u00af instead of the auction hypergraph H. In fact, we can show that MaxWSP remains NP-hard even when H is acyclic -LRB- i.e., when it has hypertree width 1 -RRB-, even when each node is contained in 3 hyperedges at most. \u25ba For some relevant special classes of hypergraphs in C -LRB- hw, k -RRB-, we design a higly-parallelizeable algorithm for MaxWSP. Recall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P -LRB- see, e.g., -LSB- 9 -RSB- -RRB-. \u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs. To the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs. In fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach. Intuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition. \u25ba The proof of the above results give us some interesting insight into the notion of structured item graph. Indeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions. The rest of the paper is organized as follows. Section 2 discusses the intractability of structured item graphs. Section 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable. The comparison between the classes C -LRB- ig, k -RRB- and C -LRB- hw, k -RRB- is discussed in Section 4. Finally, in Section 5 we draw our conclusions by also outlining directions for further research. 5. CONCLUSIONS We have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario. The result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3. Motivated by this result, we investigated the use of hypertree decomposition -LRB- on the dual hypergraph associated with the scenario -RRB- and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width. For some special, yet relevant cases, a highly parallelizable algorithm is also discussed. Interestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width -LRB- hence, the reason of their intractability is not their generality -RRB-. In particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions -LRB- on the dual hypergraph -RRB-, called query decompositions -LRB- see, e.g., -LSB- 7 -RSB- -RRB-. In the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.", "keyphrases": ["hypergraph", "combinatori auction", "hypertre decomposit", "well-known mechan for resourc and task alloc", "hypertre-base decomposit method", "hypergraph hg", "complex of structur item graph", "simplif of the primal graph", "structur item graph", "fix treewidth", "accept bid price", "polynomi time"]}
{"file_name": "C-33", "text": "Rewards-Based Negotiation for Providing Context Information ABSTRACT How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly. 1. INTRODUCTION Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates -LSB- 1 -RSB-. Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers -LRB- sensors or other data sources of context information -RRB- with different quality levels. For example, Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized `` arbitrator '' provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers ' interests. Moreover, when such `` arbitrator '' is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide `` good '' context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages : 1. We do not need an `` arbitrator '' provided by the middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware. 2. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized `` arbitrator ''. 3. It can guarantee providers ' interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. 4. This approach can choose an appropriate provider au tomatically. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process -LRB- i.e. rewards -RRB-. It will cost more time to reach agreement. It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. Section 2 presents some assumptions. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers ' strategies. Section 4 evaluates our approach. In section 5 we introduce some related work and conclude in section 6. 5. RELATED WORK In -LSB- 4 -RSB-, Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. Their adaptive middleware uses utility functions to choose the best context provider -LRB- given the QoC requirements of applications and the QoC of alternative means of context acquisition -RRB-. In our negotiation model, the calculation of utility function Uc was inspired by this approach. Henricksen and Indulska propose an approach to modelling and using imperfect information in -LSB- 3 -RSB-. They characterize various types and sources of imperfect context information and present a set of novel context modelling constructs. They also outline a software infrastructure that supports the management and use of imperfect context information. -LSB- 10 -RSB- presents a framework for realizing dynamic context consistency management. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. Most approaches to provide appropriate context utilize a centralized `` arbitrator ''. In our approach, we let distributed context providers themselves decide who can provide appropriate context information. Our approach can reduce the burden of the middleware, because we do not need the middleware to provide a context selection mechanism. Also, it can guarantee context providers ' interests. 6. CONCLUSION AND FUTURE WORK How to provide the appropriate context information is a challenging problem in pervasive computing. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers ' interests by a relatively fair proceeds allocation. In this paper, we only consider how to choose an appropriate context provider from two providers. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem. We assume that the context provider will fulfill its promise of reward in the next negotiation process. In fact, the context provider might deceive its opponent and provide illusive promise. We should solve this problem in the future.", "keyphrases": ["context-awar", "context provid", "negoti", "context-awar comput", "concret negoti model", "distribut applic", "pervas comput", "reput", "qualiti of context", "persuas argument"]}
{"file_name": "H-12", "text": "Fast Generation of Result Snippets in Web Search ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58 % over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache. 1. INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type. In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents. Snippets are short fragments of text extracted from the document content -LRB- or its metadata -RRB-. A query-biased snippet is one selectively extracted on the basis of its relation to the searcher 's query. The addition of informative snippets to search results may substantially increase their value to searchers. Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored. In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searcher 's real information need, such as the contact details of a person or an organization. Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load -LRB- remembering that each search typically generates ten snippets -RRB-. The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required. The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load. Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems. Note that the utility of snippets is by no means restricted to whole-of-Web search applications. Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov -LRB- c. 25 million pages -RRB- and govsearch.australia.gov.au -LRB- c. 5 million pages -RRB- and within large enterprises such as IBM -LSB- 2 -RSB- -LRB- c. 50 million pages -RRB-. Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present. We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets. We report the proportion of time spent for disk seeks, disk reads and cpu processing ; demonstrating that the time for locating each document -LRB- seek time -RRB- dominates, as expected. As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required. However, this is only true if there is no caching of documents in RAM. Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation. In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided : the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists -LSB- 24 -RSB-. As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document. This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2. RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query. Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document. Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms. To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page -LSB- 1 -RSB- only under the heading of future work. Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary -LSB- 20, 21 -RSB-, rather than efficient generation of summaries. Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature. White et al -LSB- 21 -RSB- report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. N/M documents. The total amount of RAM required by a single machine, therefore, would be N/M -LRB- 8.192 + 10.24 + 8 -RRB- bytes. Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine. These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache. In this work we have deliberately avoided committing to one particular scoring method for sentences in documents. Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets -LSB- 20 -RSB-. The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here can not be used. As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper. We will explore alternate compression schemes in future work.", "keyphrases": ["search engin", "snippet gener", "document cach", "link graph measur", "perform", "web summari", "special-purpos filesystem", "ram", "document compact", "text fragment", "precomput final result page", "vbyte code scheme", "semi-static compress"]}
{"file_name": "H-10", "text": "Regularized Clustering for Documents * ABSTRACT In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization -LRB- CLGR -RRB-. We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods. 1. INTRODUCTION Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies. In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful. Generally, document clustering methods can be mainly categorized into two classes : hierarchical methods and partitioning methods. The hierarchical methods group the data points into a hierarchical tree structure using bottom-up or top-down approaches. For example, hierarchical agglomerative clustering -LRB- HAC -RRB- -LSB- 13 -RSB- is a typical bottom-up hierarchical clustering method. It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster. On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions. For instance, K-means -LSB- 13 -RSB- is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers. In this paper, we will focus on the partitioning methods. In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods -LSB- 19 -RSB- -LSB- 28 -RSB-. Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community. The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points. Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph. After some relaxations, these criterions can usually be optimized via eigen-decompositions, which is guaranteed to be global optimal. In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph. In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e. the final cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. So we call our method Clustering with Local and Global Regularization -LRB- CLGR -RRB-. The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi-supervised learning -LSB- 31 -RSB-, and our experimental evaluations on several real document datasets show that CLGR performs better than many state-of-the-art clustering methods. The rest of this paper is organized as follows : in section 2 we will introduce our CLGR algorithm in detail. The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. 4. CONCLUSIONS AND FUTURE WORKS In this paper, we derived a new clustering algorithm called clustering with local and global regularization. Our method preserves the merit of local learning algorithms and spectral clustering. Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm.", "keyphrases": ["document cluster", "regular", "global regular", "cluster hierarchi", "spectrum", "specifi search", "hierarch method", "partit method", "label predict", "function estim", "manifold"]}
{"file_name": "C-32", "text": "BuddyCache : High-Performance Object Storage for Collaborative Strong-Consistency Applications in a WAN * ABSTRACT Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area networks because of high network latency. BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers. We have implemented a BuddyCache prototype and evaluated its performance. Analytical results, confirmed by measurements of the BuddyCache prototype using the multiuser 007 benchmark indicate that for typical Internet latencies, e.g. ranging from 40 to 80 milliseconds round trip time to the storage server, peers using BuddyCache can reduce by up to 50 % the latency of access to shared objects compared to accessing the remote servers directly. 1. INTRODUCTION Nevertheless, distributed applications may perform poorly in wide-area network environments. Network bandwidth problems will improve in the foreseeable future, but improvement in network latency is fundamentally limited. BuddyCache is a new object caching technique that addresses the network latency problem for collaborative applications in wide-area network environment. Collaborative applications provide a shared work environment for groups of networked users collaborating on a common task, for example a team of engineers jointly overseeing a construction project. Strong-consistency collaborative applications, for example CAD systems, use client/server transactional object storage systems to ensure consistent access to shared persistent data. Up to now however, users have rarely considered running consistent network storage systems over wide-area networks as performance would be unacceptable -LSB- 24 -RSB-. For transactional storage systems, the high cost of wide-area network interactions to maintain data consistency is the main cost limiting the performance and therefore, in wide-area network environments, collaborative applications have been adapted to use weaker consistency storage systems -LSB- 22 -RSB-. Adapting an application to use weak consistency storage system requires significant effort since the application needs to be rewritten to deal with a different storage system semantics. If shared persistent objects could be accessed with low-latency, a new field of distributed strong-consistency applications could be opened. Cooperative web caching -LSB- 10, 11, 15 -RSB- is a well-known approach to reducing client interaction with a server by allowing one client to obtain missing objects from a another client instead of the server. However, cooperative web caching techniques do not provide two important properties needed by collaborative applications, strong consistency and efficient access to fine-grained objects. Cooperative object caching systems -LSB- 2 -RSB- provide these properties. However, they rely on interaction with the server to provide fine-grain cache coherence that avoids the problem of false sharing when accesses to unrelated objects appear to conflict because they occur on the same physical page. Interaction with the server increases latency. The contribution of this work is extending cooperative caching techniques to provide strong consistency and efficient access to fine-grain objects in wide-area environments. The engineers use a collaborative CAD application to revise and update complex project design documents. The shared documents are stored in transactional repository servers at the company home site. The engineers use workstations running repository clients. The workstations are interconnected by a fast local Ethernet but the network connection to the home repository servers is slow. To improve access latency, clients fetch objects from repository servers and cache and access them locally. A coherence protocol ensures that client caches remain consistent when objects are modified. The performance problem facing the collaborative application is coordinating with the servers consistent access to shared objects. With BuddyCache, a group of close-by collaborating clients, connected to storage repository via a high-latency link, can avoid interactions with the server if needed objects, updates or coherency information are available in some client in the group. BuddyCache presents two main technical challenges. One challenge is how to provide efficient access to shared finegrained objects in the collaborative group without imposing performance overhead on the entire caching system. The other challenge is to support fine-grain cache coherence in the presence of slow and failed nodes. BuddyCache uses a '' redirection '' approach similar to one used in cooperative web caching systems -LSB- 11 -RSB-. A redirector server, interposed between the clients and the remote servers, runs on the same network as the collaborating group and, when possible, replaces the function of the remote servers. If the client request can not be served locally, the redirector forwards it to a remote server. When one of the clients in the group fetches a shared object from the repository, the object is likely to be needed by other clients. BuddyCache redirects subsequent requests for this object to the caching client. Similarly, when a client creates or modifies a shared object, the new data is likely to be of potential interest to all group members. BuddyCache uses redirection to support peer update, a lightweight '' application-level multicast '' technique that provides group members with consistent access to the new data committed within the collaborating group without imposing extra overhead outside the group. Nevertheless, in a transactional system, redirection interferes with shared object availability. Solo commit, is a validation technique used by BuddyCache to avoid the undesirable client dependencies that reduce object availability when some client nodes in the group are slow, or clients fail independently. A salient feature of solo commit is supporting fine-grained validation using inexpensive coarse-grained coherence information. We designed and implemented a BuddyCache prototype and studied its performance benefits and costs using analytical modeling and system measurements. We compared the storage system performance with and without BuddyCache and considered how the cost-benefit balance is affected by network latency. These strong performance gains could make transactional object storage systems more attractive for collaborative applications in wide-area environments. 2. RELATED WORK Cooperative caching techniques -LSB- 20, 16, 13, 2, 28 -RSB- provide access to client caches to avoid high disk access latency in an environment where servers and clients run on a fast local area network. These techniques use the server to provide redirection and do not consider issues of high network latency. Cooperative Web caching techniques, -LRB- e.g. -LSB- 11, 15 -RSB- -RRB- investigate issues of maintaining a directory of objects cached in nearby proxy caches in wide-area environment, using distributed directory protocols for tracking cache changes. This work does not consider issues of consistent concurrent updates to shared fine-grained objects. This multicast transport level solution is geared to the single writer semantics of web objects. In contrast, BuddyCache uses '' application level '' multicast and a sender-reliable coherence protocol to provide similar access latency improvements for transactional objects. Application level multicast solution in a middle-ware system was described by Pendarakis, Shi and Verma in -LSB- 27 -RSB-. The schema supports small multi-sender groups appropriate for collaborative applications and considers coherence issues in the presence of failures but does not support strong consistency or fine-grained sharing. The protocol uses leases to provide fault-tolerant call-backs and takes advantage of nearby caches to reduce the cost of lease extensions. The study uses simulation to investigate latency and fault tolerance issues in hierarchical avoidance-based coherence scheme. In contrast, our work uses implementation and analysis to evaluate the costs and benefits of redirection and fine grained updates in an optimistic system. Anderson, Eastham and Vahdat in WebFS -LSB- 29 -RSB- present a global file system coherence protocol that allows clients to choose on per file basis between receiving updates or invalidations. Updates and invalidations are multicast on separate channels and clients subscribe to one of the channels. The protocol exploits application specific methods e.g. last-writer-wins policy for broadcast applications, to deal with concurrent updates but is limited to file systems. BuddyCache provides similar bandwidth improvements when objects are available in the group cache. 7. CONCLUSION Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide-area network because of high network latency. This paper described BuddyCache, a new transactional cooperative caching -LSB- 20, 16, 13, 2, 28 -RSB- technique that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The technique improves performance yet provides strong correctness and availability properties in the presence of node failures and slow clients. Redirection, however, can interfere with object availability. Solo commit, is a new validation technique that allows a client in a group to commit independently of slow or failed peers. It provides fine-grained validation using inexpensive coarse-grain version information. We have designed and implemented BuddyCache prototype in Thor distributed transactional object storage system -LSB- 23 -RSB- and evaluated the benefits and costs of the system over a range of network latencies. fine-grain strong-consistency access in high-latency environments, 2. an implementation of the system prototype that yields strong performance gains over the base system, 3. analytical and measurement based performance evaluation of the costs and benefits of the new techniques capturing the dominant performance cost, high network latency.", "keyphrases": ["object storag system", "collabor strong-consist applic", "wide-area network", "cooper web cach", "fine-grain share", "transact", "fault-toler properti", "buddycach", "domin perform cost", "optimist system", "peer fetch", "multi-user oo7 benchmark"]}
{"file_name": "J-17", "text": "Truthful Mechanism Design for Multi-Dimensional Scheduling via Cycle Monotonicity ABSTRACT We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design, where the machines are the strategic players. This is a multidimensional scheduling domain, and the only known positive results for makespan minimization in such a domain are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We study a well-motivated special case of this problem, where the processing time of a job on each machine may either be `` low '' or `` high '', and the low and high values are public and job-dependent. This preserves the multidimensionality of the domain, and generalizes the restricted-machines -LRB- i.e., -LCB- pj, \u221e -RCB- -RRB- setting in scheduling. We give a general technique to convert any c-approximation algorithm to a 3capproximation truthful-in-expectation mechanism. This is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion. When the low and high values are the same for all jobs, we devise a deterministic 2-approximation truthful mechanism. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Our constructions are novel in two respects. First, we do not utilize or rely on explicit price definitions to prove truthfulness ; instead we design algorithms that satisfy cycle monotonicity. Cycle monotonicity -LSB- 23 -RSB- is a necessary and sufficient condition for truthfulness, is a generalization of value monotonicity for multidimensional domains. However, whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains, ours is the first work that leverages cycle monotonicity in the multidimensional setting. Second, our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem, and then converting it into a truthfulin-expectation mechanism. This builds upon a technique of -LSB- 16 -RSB-, and shows the usefulness of fractional mechanisms in truthful mechanism design. 1. INTRODUCTION Mechanism design studies algorithmic constructions under the presence of strategic players who hold the inputs to the algorithm. Algorithmic mechanism design has focused mainly on settings were the social planner or designer wishes to maximize the social welfare -LRB- or equivalently, minimize social cost -RRB-, or on auction settings where revenuemaximization is the main goal. In this paper, we consider such an alternative goal in the context of machine scheduling, namely, makespan minimization. There are n jobs or tasks that need to be assigned to m machines, where each job has to be assigned to exactly one machine. Hence, we approach the problem via mechanism design : the social designer, who holds the set of jobs to be assigned, needs to specify, in addition to a schedule, suitable payments to the players in order to incentivize them to reveal their true processing times. Such a mechanism is called a truthful mechanism. Instead, it corresponds to maximizing the minimum welfare and the notion of max-min fairness, and appears to be a much harder problem from the viewpoint of mechanism design. In particular, the celebrated VCG -LSB- 26, 9, 10 -RSB- family of mechanisms does not apply here, and we need to devise new techniques. The possibility of constructing a truthful mechanism for makespan minimization is strongly related to assumptions on the players ' processing times, in particular, the `` dimensionality '' of the domain. Nisan and Ronen considered the setting of unrelated machines where the pij values may be arbitrary. This is a multidimensional domain, since a player 's private value is its entire vector of processing times -LRB- pij -RRB- j. Very few positive results are known for multidimensional domains in general, and the only positive results known for multidimensional scheduling are O -LRB- m -RRB- - approximation truthful mechanisms -LSB- 22, 20 -RSB-. We emphasize that regardless of computational considerations, even the existence of a truthful mechanism with a significantly better -LRB- than m -RRB- approximation ratio is not known for any such scheduling domain. On the negative side, -LSB- 22 -RSB- showed that no truthful deterministic mechanism can achieve approximation ratio better than 2, and strengthened this lower bound to m for two specific classes of deterministic mechanisms. Recently, -LSB- 20 -RSB- extended this lower bound to randomized mechanisms, and -LSB- 8 -RSB- improved the deterministic lower bound. In stark contrast with the above state of affairs, much stronger -LRB- and many more -RRB- positive results are known for a special case of the unrelated machines problem, namely, the setting of related machines. Here, we have pij = pj/si for every i, j, where pj is public knowledge, and the speed si is the only private parameter of machine i. This assumption makes the domain of players ' types single-dimensional. Truthfulness in such domains is equivalent to a convenient value-monotonicity condition -LSB- 21, 3 -RSB-, which appears to make it significantly easier to design truthful mechanisms in such domains. Archer and Tardos -LSB- 3 -RSB- first considered the related machines setting and gave a randomized 3-approximation truthful-in-expectation mechanism. The gap between the single-dimensional and multidimensional domains is perhaps best exemplified by the fact that -LSB- 3 -RSB- showed that there exists a truthful mechanism that always outputs an optimal schedule. -LRB- Recall that in the multidimensional unrelated machines setting, it is impossible to obtain a truthful mechanism with approximation ratio better than 2. -RRB- Various follow-up results -LSB- 2, 4, 1, 13 -RSB- have strengthened the notion of truthfulness and/or improved the approximation ratio. Such difficulties in moving from the single-dimensional to the multidimensional setting also arise in other mechanism design settings -LRB- e.g., combinatorial auctions -RRB-. Thus, in addition to the specific importance of scheduling in strategic environments, ideas from multidimensional scheduling may also have a bearing in the more general context of truthful mechanism design for multidimensional domains. In this paper, we consider the makespan-minimization problem for a special case of unrelated machines, where the processing time of a job is either `` low '' or `` high '' on each machine. We call this model the `` jobdependent two-values '' case. This model generalizes the classic `` restricted machines '' setting, where pij \u2208 -LCB- Lj, \u221e -RCB- which has been well-studied algorithmically. A special case of our model is when Lj = L and Hj = H for all jobs j, which we denote simply as the `` two-values '' scheduling model. Both of our domains are multidimensional, since the machines are unrelated : one job may be low on one machine and high on the other, while another job may follow the opposite pattern. Thus, the private information of each machine is a vector specifying which jobs are low and high on it. Thus, they retain the core property underlying the hardness of truthful mechanism design for unrelated machines, and by studying these special settings we hope to gain some insights that will be useful for tackling the general problem. Our Results and Techniques We present various positive results for our multidimensional scheduling domains. Our first result is a general method to convert any capproximation algorithm for the job-dependent two values setting into a 3c-approximation truthful-in-expectation mechanism. This is one of the very few known results that use an approximation algorithm in a black-box fashion to obtain a truthful mechanism for a multidimensional problem. Our result implies that there exists a 3-approximation truthfulin-expectation mechanism for the Lj-Hj setting. Our second result applies to the twovalues setting -LRB- Lj = L, Hj = H -RRB-, for which we improve both the approximation ratio and strengthen the notion of truthfulness. We obtain a deterministic 2-approximation truthful mechanism -LRB- along with prices -RRB- for this problem. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Complementing this, we observe that even this seemingly simple setting does not admit truthful mechanisms that return an optimal schedule -LRB- unlike in the case of related machines -RRB-. By exploiting the multidimensionality of the domain, we prove that no truthful deterministic mechanism can obtain an approximation ratio better than 1.14 to the makespan -LRB- irrespective of computational considerations -RRB-. The main technique, and one of the novelties, underlying our constructions and proofs, is that we do not rely on explicit price specifications in order to prove the truthfulness of our mechanisms. Instead we exploit certain algorithmic monotonicity conditions that characterize truthfulness to first design an implementable algorithm, i.e., an algorithm for which prices ensuring truthfulness exist, and then find these prices -LRB- by further delving into the proof of implementability -RRB-. This kind of analysis has been the method of choice in the design of truthful mechanisms for singledimensional domains, where value-monotonicity yields a convenient characterization enabling one to concentrate on the algorithmic side of the problem -LRB- see, e.g., -LSB- 3, 7, 4, 1, 13 -RSB- -RRB-. Our work is the first to leverage monotonicity conditions for truthful mechanism design in arbitrary domains. The monotonicity condition we use, which is sometimes called cycle monotonicity, was first proposed by Rochet -LSB- 23 -RSB- -LRB- see also -LSB- 11 -RSB- -RRB-. It is a generalization of value-monotonicity and completely characterizes truthfulness in every domain. Our methods and analyses demonstrate the potential benefits of this characterization, and show that cycle monotonicity can be effectively utilized to devise truthful mechanisms for multidimensional domains. Consider, for example, our first result showing that any c-approximation algorithm can be `` exported '' to a 3c-approximation truthful-in-expectation mechanism. At the level of generality of an arbitrary approximation algorithm, it seems unlikely that one would be able to come up with prices to prove truthfulness of the constructed mechanism. But, cycle monotonicity does allow us to prove such a statement. In fact, some such condition based only on the underlying algorithm -LRB- and not on the prices -RRB- seems necessary to prove such a general statement. The method for converting approximation algorithms into truthful mechanisms involves another novel idea. Our randomized mechanism is obtained by first constructing a truthful mechanism that returns a fractional schedule. Moving to a fractional domain allows us to `` plug-in '' truthfulness into the approximation algorithm in a rather simple fashion, while losing a factor of 2 in the approximation ratio. We then use a suitable randomized rounding procedure to convert the fractional assignment into a random integral assignment. This preserves truthfulness, but we lose another additive factor equal to the approximation ratio. Our construction uses and extends some observations of Lavi and Swamy -LSB- 16 -RSB-, and further demonstrates the benefits of fractional mechanisms in truthful mechanism design. Related Work Nisan and Ronen -LSB- 22 -RSB- first considered the makespan-minimization problem for unrelated machines. They gave an m-approximation positive result and proved various lower bounds. This been improved in -LSB- 2, 4, 1, 13 -RSB- to : a 2-approximation randomized mechanism -LSB- 2 -RSB- ; an FPTAS for any fixed number of machines given by Andelman, Azar and Sorani -LSB- 1 -RSB-, and a 3-approximation deterministic mechanism by Kov \u00b4 acs -LSB- 13 -RSB-. The algorithmic problem -LRB- i.e., without requiring truthfulness -RRB- of makespan-minimization on unrelated machines is well understood and various 2-approximation algorithms are known. Lenstra, Shmoys and Tardos -LSB- 18 -RSB- gave the first such algorithm. Shmoys and Tardos -LSB- 25 -RSB- later gave a 2approximation algorithm for the generalized assignment problem, a generalization where there is a cost cij for assigning a job j to a machine i, and the goal is to minimize the cost subject to a bound on the makespan. Recently, Kumar, Marathe, Parthasarathy, and Srinivasan -LSB- 14 -RSB- gave a randomized rounding algorithm that yields the same bounds. We use their procedure in our randomized mechanism. The characterization of truthfulness for arbitrary domains in terms of cycle monotonicity seems to have been first observed by Rochet -LSB- 23 -RSB- -LRB- see also Gui et al. -LSB- 11 -RSB- -RRB-. This generalizes the value-monotonicity condition for single-dimensional domains which was given by Myerson -LSB- 21 -RSB- and rediscovered by -LSB- 3 -RSB-. As mentioned earlier, this condition has been exploited numerous times to obtain truthful mechanisms for single-dimensional domains -LSB- 3, 7, 4, 1, 13 -RSB-. For convex domains -LRB- i.e., each players ' set of private values is convex -RRB-, it is known that cycle monotonicity is implied by a simpler condition, called weak monotonicity -LSB- 15, 6, 24 -RSB-. But even this simpler condition has not found much application in truthful mechanism design for multidimensional problems. Objectives other than social-welfare maximization and revenue maximization have received very little attention in mechanism design. In the context of combinatorial auctions, the problems of maximizing the minimum value received by a player, and computing an envy-minimizing allocation have been studied briefly. Lavi, Mu'alem, and Nisan -LSB- 15 -RSB- showed that the former objective can not be implemented truthfully ; Bezakova and Dani -LSB- 5 -RSB- gave a 0.5-approximation mechanism for two players with additive valuations. These lower bounds were strengthened in -LSB- 20 -RSB-. 2. PRELIMINARIES 2.1 The scheduling domain In our scheduling problem, we are given n jobs and m machines, and each job must be assigned to exactly one machine. In the unrelated-machines setting, each machine i is characterized by a vector of processing times -LRB- pij -RRB- j, where pij E R \u2265 0 U -LCB- oo -RCB- denotes i 's processing time for job j with the value oo specifying that i can not process j. We consider two special cases of this problem : 1. The job-dependent two-values case, where pij E -LCB- Lj, Hj -RCB- for every i, j, with Lj < Hj, and the values Lj, Hj are known. This generalizes the classic scheduling model of restricted machines, where Hj = oo. 2. We say that a job j is low on machine i if pij = Lj, and high if pij = Hj. We will use the terms schedule and assignment interchangeably. We will also consider randomized algorithms and algorithms that return a fractional assignment. We denote the load of machine i -LRB- under a given assignj xijpij, and the makespan of a schedule is defined as the maximum load on any machine, i.e., maxi li. The goal in the makespan-minimization problem is to assign the jobs to the machines so as to minimize the makespan of the schedule. 2.2 Mechanism design We consider the makespan-minimization problem in the above scheduling domains in the context of mechanism design. Mechanism design studies strategic settings where the social designer needs to ensure the cooperation of the different entities involved in the algorithmic procedure. Following the work of Nisan and Ronen -LSB- 22 -RSB-, we consider the machines to be the strategic players or agents. The social designer holds the set of jobs that need to be assigned, but does not know the -LRB- true -RRB- processing times of these jobs on the different machines. Each machine is a selfish entity, that privately knows its own processing time for each job. We consider direct-revelation mechanisms : each machine reports its -LRB- possibly false -RRB- vector of processing times, the mechanism then computes a schedule and hands out payments to the players -LRB- i.e., machines -RRB- to compensate them for the cost they incur in processing their assigned jobs. A -LRB- direct-revelation -RRB- mechanism thus consists of a tuple -LRB- x, P -RRB- : x specifies the schedule, and P = -LCB- Pi -RCB- specifies the payments handed out to the machines, where both x and the Pis are functions of the reported processing times p = -LRB- pij -RRB- i, j. The mechanism must therefore incentivize the machines/players to truthfully reveal their processing times via the payments. This is made precise using the notion of dominant-strategy truthfulness. To put it in words, in a truthful mechanism, no machine can improve its utility by declaring a false processing time, no matter what the other machines declare. We will also consider fractional mechanisms that return a fractional assignment, and randomized mechanisms that are allowed to toss coins and where the assignment and the payments may be random variables. The notion of truthfulness for a fractional mechanism is the same as in Definition 2.1, where x1, x2 are now fractional assignments. For a randomized mechanism, we will consider the notion of truthfulness in expectation -LSB- 3 -RSB-, which means that a machine -LRB- player -RRB- maximizes her expected utility by declaring her true processing-time vector. For our two scheduling domains, the informational assumption is that the values Lj, Hj are publicly known. The private information of a machine is which jobs have value Lj -LRB- or L -RRB- and which ones have value Hj -LRB- or H -RRB- on it. We emphasize that both of our domains are multidimensional, since each machine i needs to specify a vector saying which jobs are low and high on it.", "keyphrases": ["mechan design", "approxim algorithm", "schedul", "multi-dimension schedul", "cycl monoton", "makespan minim", "algorithm", "random mechan", "us of fraction mechan", "truth mechan design", "fraction domain"]}
{"file_name": "I-26", "text": "Sequential Decision Making in Parallel Two-Sided Economic Search ABSTRACT This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships. In each search stage, each of the agents is randomly matched with several other agents in parallel, and makes a decision whether to accept a potential partnership with one of them. The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized -LRB- instantaneous -RRB- decision protocol and can sequentially accept and reject partnerships within the same search stage. We analyze the dynamics which drive the agents ' strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model. By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments. 1. INTRODUCTION A two-sided economic search is a distributed mechanism for forming agents ' pairwise partnerships -LSB- 5 -RSB-.1 On every stage of the process, each of the agents is randomly matched with another agent 1Notice that the concept of '' search '' here is very different from the classical definition of '' search '' in AI. While AI search is an active process in which an agent finds a sequence of actions that will bring it from the initial state to a goal state, economic search refers to the identification of the best agent to commit to a partnership with. and the two interact bilaterally in order to learn the benefit encapsulated in a partnership between them. The interaction does not involve bargaining thus each agent merely needs to choose between accepting or rejecting the partnership with the other agent. A typical market where this kind of two-sided search takes place is the marriage market -LSB- 22 -RSB-. Recent literature suggests various software agent-based applications where a two-sided distributed -LRB- i.e., with no centralized matching mechanisms -RRB- search takes place. An important class of such applications includes secondary markets for exchanging unexploited resources. For example, through a twosided search, agents, representing different service providers, can exchange unused bandwidth -LSB- 21 -RSB- and communication satellites can transfer communication with a greater geographical coverage. Twosided agents-based search can also be found in applications of buyers and sellers in eMarkets and peer-to-peer applications. The twosided nature of the search suggests that a partnership between a pair of agents is formed only if it is mutually accepted. By forming a partnership the agents gain an immediate utility and terminate their search. When resuming the search, on the other hand, a more suitable partner might be found however some resources will need to be consumed for maintaining the search process. In this paper we focus on a specific class of two-sided search matching problems, in which the performance of the partnership applies to both parties, i.e., both gain an equal utility -LSB- 13 -RSB-. The equal utility scenario is usually applicable in domains where the partners gain from the synergy between them. In all these applications, any two agents can form a partnership and the performance of any given partnership depends on the skills or the characteristics of its members. Furthermore, the equal utility scenario can also hold whenever there is an option for side-payments and the partnership 's overall utility is equally split among the two agents forming it -LSB- 22 -RSB-. While the two-sided search literature offers comprehensive equilibrium analysis for various models, it assumes that the agents ' search is conducted in a purely sequential manner : each agent locates and interacts with one other agent in its environment at a time -LSB- 5, 22 -RSB-. Nevertheless, when the search is assigned to autonomous software agents a better search strategy can be used. Here an agent can take advantage of its unique inherent filtering and information processing capabilities and its ability to efficiently -LRB- in comparison to people -RRB- maintain concurrent interactions with several other agents at each stage of its search. Such use of parallel interactions in search is favorable whenever the average cost2 per interaction with another agent, when interacting in parallel with a batch of other agents, is smaller than the cost of maintaining one interaction at a time -LRB- i.e., advantage to size -RRB-. For example, the analysis of the costs associated with evaluating potential partnerships between service providers reveals both fixed and variable components when using the parallel search, thus the average cost per interaction decreases as the number of parallel interactions increases -LSB- 21 -RSB-. Despite the advantages identified for parallel interactions in adjacent domains -LRB- e.g., in one-sided economic search -LSB- 7, 16 -RSB- -RRB-, a first attempt for modeling a repeated pairwise matching process in which agents are capable of maintaining interaction with several other agents at a time was introduced only recently -LSB- 21 -RSB-. However, the agents in that seminal model are required to synchronize their decision making process. Thus each agent, upon reviewing the opportunities available in a specific search stage, has to notify all other agents of its decision whether to commit to a partnership -LRB- at most with one of them -RRB- or reject the partnership -LRB- with the rest of them -RRB-. This inherent restriction imposes a significant limitation on the agents ' strategic behavior. In our model, the agents are free to notify the other agents of their decisions in an asynchronous manner. The asynchronous approach allows the agents to re-evaluate their strategy, based on each new response they receive from the agents they interact with. The new model is a much more realistic pairwise model and, as we show in the analysis section, is always preferred by any single agents participating in the process. In the absence of other economic two-sided parallel search models, we use the model that relies on an instantaneous -LRB- synchronous -RRB- decision making process -LSB- 21 -RSB- -LRB- denoted I-DM throughout the rest of the paper -RRB- as a benchmark for evaluating the usefulness of our proposed sequential -LRB- asynchronous -RRB- decision making strategy -LRB- denoted S-DM -RRB-. The main contributions of this paper are threefold : First, we formally model and analyze a two-sided search process in which the agents have no temporal decision making constraints concerning the rejection of or commitment to potential partnerships they encounter in parallel -LRB- the S-DM model -RRB-. This model is a general search model which can be applied in various -LRB- not necessarily software agents-based -RRB- domains. Second, we prove that the agents ' SDM strategy weakly dominates the I-DM strategy, thus every agent has an incentive to deviate to the S-DM strategy when all other agents are using the I-DM strategy. Finally, by using an innovative recursive presentation of the acceptance probabilities of different potential partnerships, we identify unique characteristics of the equilibrium strategies in the new model. These are used for supplying an appropriate computational means that facilitates the calculation of the agents ' equilibrium strategy. This latter contribution is We manage to extract the agents ' new equilibrium strategies without increasing the computational complexity in comparison to the I-DM model. Throughout the paper we demonstrate the different properties of the new model and compare it with the I-DM model using an artificial synthetic environment. In the following section we formally present the S-DM model. An equilibrium analysis and computational means for finding the equilibrium strategy are provided in Section 3. In Section 4 we review related MAS and economic search theory literature. 4. RELATED WORK The two-sided economic search for partnerships in AI literature is a sub-domain of coalition formation8. As in the general 8The use of the term '' partnership '' in this context refers to the agreement between two individual agents to cooperate in a pre-defined manner. For example, in the buyer-seller application a partnership is defined as an agreed transaction between the two-parties -LSB- 9 -RSB-. coalition formation case, agents have the incentive to form partnerships when they are incapable of executing a task by their own or when the partnership can improve their individual utilities -LSB- 14 -RSB-. Various centralized matching mechanisms can be found in the literature -LSB- 6, 2, 8 -RSB-. However, in many MAS environments, in the absence of any reliable central matching mechanism, the matching process is completely distributed. While the search in agent-based environments is well recognized to be costly -LSB- 11, 21, 1 -RSB-, most of the proposed coalition formation mechanisms assume that an agent can scan as many partnership opportunities in its environment as needed or have access to central matchers or middle agents -LSB- 6 -RSB-. The incorporation of costly search in this context is quite rare -LSB- 21 -RSB- and to the best of our knowledge, a distributed two-sided search for partners model similar to the S-DM model has not been studied to date. Classical economic search theory -LRB- -LSB- 15, 17 -RSB-, and references therein -RRB- widely addresses the problem of a searcher operating in a costly environment, seeking to maximize his long term utility. In these models, classified as one-sided search, the focus is on establishing the optimal strategies for the searcher, assuming no mutual search activities -LRB- i.e., no influence on the environment -RRB-. Here the sequential search procedure is often applied, allowing the searcher to investigate a single -LSB- 15 -RSB- or multiple -LSB- 7, 19 -RSB- opportunities at a time. While the latter method is proven to be beneficial for the searcher, it was never used in the '' two-sided '' search models that followed -LRB- where dual search activities are modeled -RRB- -LSB- 22, 5, 18 -RSB-. Therefore, in these models, the equilibrium strategies are always developed based on the assumption that the agents interact with others sequentially -LRB- i.e., with one agent at a time -RRB-. A first attempt to integrate the parallel search into a two-sided search model is given in -LSB- 21 -RSB-, as detailed in the introduction section. The models presented in this area do not associate the coalition formation process with search costs, which is the essence of the analysis that economic search theory aims to supply. Furthermore, even in repeated pairwise bargaining -LSB- 10 -RSB- models the agents are always limited to initiating a single bargaining interaction at a time.", "keyphrases": ["pairwis partnership", "decis", "peer-to-peer applic", "inform process", "util", "search cost", "multi-equilibrium scenario", "equilibrium strategi", "parallel interact", "bound methodolog", "coalit format", "partnership format", "partnership", "costli environ", "search perform", "instantan decis make", "sequenti decis make", "two-side search"]}
{"file_name": "H-8", "text": "Robust Test Collections for Retrieval Evaluation ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable : the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability : with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. 1. INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments? Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem : for most retrieval tasks, it is impossible to judge the relevance of every document ; there are simply too many of them. The solution used by NIST at TREC -LRB- Text REtrieval Conference -RRB- is the pooling method -LSB- 19, 20 -RSB- : all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool -LSB- 21 -RSB-. This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of -LRB- paid -RRB- annotator time. As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems. Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be `` reusable ''. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. We need a more careful definition of reusability. Specifically, the question of reusability is not how accurately we can evaluate new systems. A `` malicious adversary '' can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree. Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done `` in-house '' for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time. 6. CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of `` reusability '' of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments : focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judg Table 5 : Accuracy, W, mean \u03c4, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100 % confidence in every evaluation -- and there is a full test collection for the task. It could be applied to evaluation on a dynamic test collection as defined by Soboroff -LSB- 18 -RSB-. The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration. We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust : with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.", "keyphrases": ["inform retriev", "evalu", "relev judgement", "reusabl", "lowerest-confid comparison", "mtc", "rtc", "expect", "varianc", "distribut of relev"]}
{"file_name": "H-21", "text": "Robust Classification of Rare Queries Using Web Knowledge ABSTRACT We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in realtime with the query volume of a commercial web search engine. We use a blind feedback technique : given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience. 1. INTRODUCTION One thing, however, has remained constant : people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not -LRB- yet! -RRB- omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. In this study we present a methodology for query classification, where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in today 's search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. However, the `` tail '' queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries -LSB- 22 -RSB-. More recent studies -LSB- 18, 21 -RSB- also attempted to gather some additional knowledge from the Web. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising -LSB- 11 -RSB-. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure ; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge -LRB- e.g., using search summaries vs. entire crawled pages -RRB-. We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification -LSB- 20 -RSB-, but is supported by research in mainstream text classification -LSB- 5 -RSB-. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri -LSB- 22 -RSB-, or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results -LSB- 16, 15 -RSB-. More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. -LSB- 10 -RSB- found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. -LSB- 1 -RSB- used semi-supervised learning as well as unlabeled data -LSB- 2 -RSB-. Gravano et al. -LSB- 6 -RSB- classified queries with respect to geographic locality in order to determine whether their intent is local or global. The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories -LSB- 11, 18, 20, 9, 21 -RSB-. The KDD task specification provided a small taxonomy -LRB- 67 nodes -RRB- along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are -LRB- 1 -RRB- how to build a document classifier, -LRB- 2 -RRB- how to translate its classifications into the target taxonomy, and -LRB- 3 -RRB- how to determine the query class based on document classifications. The winning solution of the KDD Cup -LSB- 18 -RSB- proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue -LRB- 1 -RRB- above, their solution used the Open Directory Project -LRB- ODP -RRB- to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper -LSB- 19 -RSB-, Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set -LRB- ODP -RRB- is used. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies -LRB- we used a much larger taxonomy -RRB-, our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. When using search results, one can either use only summaries of the results provided by 3Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages -LRB- Table 1 -RRB-. These results are consistent with prior studies -LSB- 5 -RSB-, which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. -LSB- 19 -RSB-, who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the -LRB- fixed -RRB- taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. We plan to further investigate this direction in our future work. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.", "keyphrases": ["queri classif", "search engin", "search advertis", "machin learn", "relev feedback", "vote scheme", "crawl", "topic taxonomi", "affin score", "condit probabl", "adapt", "inform retriev"]}
{"file_name": "C-4", "text": "Intra-flow Loss Recovery and Control for ABSTRACT `` Best effort '' packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold : on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal out is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss lengths with which we can describe the loss distribution within a These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs -LRB- PCM -RRB- with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs -LRB- G. 729 -RRB- on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may `` break '' at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data to minimize the impact of packet loss. We designate these methods as loss recovery and control. At the end-to-end level, identification of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow to -LRB- statistically -RRB- trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both ets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term. 1. INTRODUCTION Considering that a real-time may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity : Temporal sensitivity : Loss of which is correlated in time may lead to disruptions in the service. For voice, as a single packet contains typically several -LRB- voice frames -RRB- this effect is thus more significant than e.g. for video. It translates basically to isolated packet losses versus losses that occur in bursts. Figure 1 : Schematic utility functions dependent on the loss of more and less -LRB- -1 -RRB- important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance -LRB- we same size, frequency and no interdependence between the frames -RRB-. Under the loss of 50 % of the packets, the perceptual quality varies hugely between the where the 50 % of the frames with high perceptual importance are received and the where the 50 % less important frames received. Network support for real-time multimedia flows can on one hand aim at offering a service, which, however, to be implemented within pa & et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. Let us now consider the case that 50 % of packets of flow identified more important -LRB- designated by or less important due to any of the above sensitivity constraints. Figure 1 a -RRB- shows a generic utility function describing the level Quality of Service dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the and -1 packets are equal. Due to the over-proportional sensitivity of the packets to loss as well as the dependence of the end loss recovery performance on the packets, the utility function is decreasing significantly in a non-linear way -LRB- approximated in the figure by piece-wise linear functions -RRB- with an increasing loss rate. Figure 1 b -RRB- presents the where all packets are protected at the expense of -1 The decay of the utility function -LRB- for loss rates < 50 % -RRB- is reduced, because the packets are protected and the endto-end loss recovery can thus operate properly a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application 's utility. Note that the higher the non-linearity of the utility contribution of the packets is -LRB- deviation from the dotted curve in Fig. 1 a -RRB-, the higher is the potential gain in utility when the protection for is enabled. Results for actual perceived quality utility for multimedia applications exhibit such non-linear behavior *. As mechanisms have to be implemented within the network -LRB- hopby-hop -RRB- and/or in the end systems -LRB- end-to-end -RRB-, we have another axis of classification. The adaptation of the sender 's to the current network congestion state an scheme -LRB- loss avoidance, is difficult to apply to voice. Considering that voice flows have very low the relative cost of transmitting the feedback information is -LRB- when compared e.g. to a video flow -RRB-. The major however, the lack of a codec is truly scalable in terms of its output and corresponding perceptual quality. when the availability of computing power is assumed, the lowest codec can be chosen permanently without actually decreasing the perceptual quality. For loss on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction -LRB- FEC -RRB- have been proposed While attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. Using redundancy has also implications to the delay adaptation -LRB- -LSB- lo -RSB- -RRB- employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to we have obtained results which confirm the shape of the `` overall utility '' curve shown in Fig. 1, clearly the utility functions of the `` sub ''. flows and their relationship are more complex and only approximately additive. Table 1 : State and transition probabilities computed for an end-to-end Internet trace using a general Markov model -LRB- third order -RRB- by Yajnik et. al.. which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but can not come close to eliminate them. Therefore in this work we assume that the lowest possible trate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss -LRB- sender -RRB- as well as loss concealment -LRB- receiver -RRB- takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual and measure their utility in the presence of packet loss using objective speech quality measurement. The paper is structured as follows : Section 2 introduces packet - and user-level metrics. We employ these metrics to describe the sensitivity of traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and by-hop level. Section 6 concludes the paper.", "keyphrases": ["end-to-end model", "sampl-base codec", "loss recoveri and control", "loss sensit", "network support for real-time multimedia", "qualiti of servic", "end-to-end loss recoveri", "voip traffic", "intra-flow loss control", "packet-level metric", "gener markov model", "sensit of voip traffic", "queue manag algorithm", "frame-base codec"]}
{"file_name": "C-22", "text": "Runtime Metrics Collection for Middleware Supported Adaptation of Mobile Applications ABSTRACT This paper proposes, implements, and evaluates in terms of worst case performance, an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host managers, runtimes and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead. 1. INTRODUCTION Effective adaptation requires detailed and up to date information about both the system and the software itself. Metrics related to system wide information -LRB- e.g. processor, memory and network load -RRB- are referred to as environmental metrics -LSB- 5 -RSB-, while metrics representing application behaviour are referred as software metrics -LSB- 8 -RSB-. Furthermore, the type of metrics required for performing adaptation is dependent upon the type of adaptation required. For example, service-based adaptation, in which service quality or service behaviour is modified in response to changes in the runtime environment, generally requires detailed environmental metrics but only simple software metrics -LSB- 4 -RSB-. On the other hand, adaptation via object mobility -LSB- 6 -RSB-, also requires detailed software metrics -LSB- 9 -RSB- since object placement is dependent on the execution characteristics of the mobile objects themselves. With the exception of MobJeX -LSB- 6 -RSB-, existing mobile object systems such as Voyager -LSB- 10 -RSB-, FarGo -LSB- 11, 12 -RSB-, and JavaParty -LSB- 13 -RSB- do not provide automated adaptation, and therefore lack the metrics collection process required to support this process. In the case of MobJeX, although an adaptation engine has been implemented -LSB- 5 -RSB-, preliminary testing was done using synthetic pre-scripted metrics since there is little prior work on the dynamic collection of software metrics in mobile object frameworks, and no existing means of automatically collecting them. Consequently, the main contribution of this paper is a solution for dynamic metrics collection to support adaptation via object mobility for mobile applications. This problem is non-trivial since typical mobile object frameworks consist of multiple application and middleware components, and thus metrics collection must be performed at different locations and the results efficiently propagated to the adaptation engine. The rest of this paper is organised as follows : Section 2 describes the general structure and implementation of mobile object frameworks in order to understand the challenges related to the collection, propagation and delivery of metrics as described in section 3. Section 4 describes some initial testing and results and section 5 closes with a summary, conclusions and discussion of future work. 2. BACKGROUND In general, an object-oriented application consists of objects collaborating to provide the functionality required by a given problem domain. Mobile object frameworks allow some of these objects to be tagged as mobile objects, providing middleware support for such objects to be moved at runtime to other hosts. At a minimum, a mobile object framework with at least one running mobile application consists of the following components : runtimes, mobile objects, and proxies -LSB- 14 -RSB-, although the terminology used by individual frameworks can differ -LSB- 6, 10-13 -RSB-. A runtime is a container process for the management of mobile objects. For example, in FarGo -LSB- 15 -RSB- this component is known as a core and in most systems separate runtimes are required to allow different applications to run independently, although this is not the case with MobJeX, which can run multiple applications in a single runtime using threads. The applications themselves comprise mobile objects, which interact with each other through proxies -LSB- 14 -RSB-. Upon migration, proxy objects move with the source object. The Java based system MobJeX, which is used as the implementation platform for the metrics collection solution described in this paper, adds a number of additional middleware components. Firstly, a host manager -LRB- known as a service in MobJeX -RRB- provides a central point of communication by running on a known port on a per host basis, thus facilitating the enumeration or lookup of components such as runtimes or mobile objects. Secondly, MobJeX has a per-application mobile object container called a transport manager -LRB- TM -RRB-. As such the host and transport managers are considered in the solution provided in the next section but could be omitted in the general case. Finally, depending on adaptation mode, MobJeX can have a centralised system controller incorporating a global adaptation engine for performing system wide optimisation. 5. SUMMARY AND CONCLUSIONS Given the challenges of developing mobile applications that run in dynamic/heterogeneous environments, and the subsequent interest in application adaptation, this paper has proposed and implemented an online metrics collection strategy to assist such adaptation using a mobile object framework and supporting middleware. Controlled lab studies were conducted to determine worst case performance, as well as show the reduction in collection overhead when applying simple collection criteria. In addition, further testing provided an initial indication of the characteristics of application objects -LRB- based on method execution time -RRB- that would be good candidates for adaptation using the worst case implementation of the proposed metrics collection strategy. A key feature of the solution was the specification of multiple configurable criteria to control the propagation of metrics through the system, thereby reducing collection overhead. Furthermore, such a temporal history could also facilitate intelligent decisions regarding the collection of metrics since for example a metric that is known to be largely constant need not be frequently measured. Future work will also involve the evaluation of a broad range of adaptation scenarios on the MobJeX framework to quantity the gains that can be made via adaptation through object mobility and thus demonstrate in practise, the efficacy of the solution described in this paper. Finally, the authors wish to explore applying the metrics collection concepts described in this paper to a more general and reusable context management system -LSB- 20 -RSB-.", "keyphrases": ["data", "object-orient applic", "mobil object framework", "mobjex", "java", "metricscontain", "metric collect", "proxi", "perform and scalabl", "measur", "propag and deliveri", "framework"]}
{"file_name": "H-26", "text": "A Support Vector Method for Optimizing Average Precision ABSTRACT Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision -LRB- MAP -RRB-, despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora -LRB- WT10g -RRB-, comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores. 1. INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision -LRB- MAP -RRB-. Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy -LSB- 17, 15 -RSB-, ROCArea -LSB- 1, 5, 10, 11, 13, 21 -RSB- or modifications of ROCArea -LSB- 4 -RSB-, and NDCG -LSB- 2, 3 -RSB-. Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance -LSB- 7 -RSB-. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft -LSB- 16 -RSB- and Caruana et al. -LSB- 6 -RSB-, our technique is computationally efficient while finding a globally optimal solution. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We have also developed a software package implementing our algorithm that is available for public user. 6. CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance. The learning framework used by our method is fairly general.", "keyphrases": ["machin learn", "rank retriev system", "learn techniqu", "mean averag precis", "optim solut", "relax of map", "inform retriev system", "probabl", "surrog measur", "loss function", "supervis learn"]}
{"file_name": "H-25", "text": "Term Feedback for Information Retrieval with Language Models ABSTRACT I n t hi s paper w e s t udy t er m - based f eedback f or i nf or mat i on r etrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top. 1. INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents -LSB- 25, 13 -RSB-. This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user 's assistance for query model construction, in the sense that the refined query model -LRB- based on terms -RRB- is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query `` Hubble telescope achievements '', when a relevant document talks more about the telescope 's repair than its discoveries, irrelevant terms such as `` spacewalk '' can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a -LRB- reasonable -RRB- number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms : he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. This is especially helpful for interactive adhoc search. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user 's information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6. Section 7 concludes this paper. 2. RELATED WORK Relevance feedback -LSB- 17, 19 -RSB- has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the user 's information need better than the original one, which is often just a short keyword query. A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback -LSB- 5, 16 -RSB- and usually still brings performance improvement. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance -LSB- 1, 23 -RSB-. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as -LSB- 12 -RSB-, there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. In many cases term relevance feedback has been found to effectively improve retrieval performance -LSB- 6, 22, 12, 4, 10 -RSB-. For example, the study in -LSB- 12 -RSB- shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit -LSB- 3, 14 -RSB-, even if the user likes interacting with expansion terms. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. Our work differs from the previous ones in two important aspects. The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 7. CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C 's performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top. We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment. Third, we have plans to incorporate term feedback into our UCAIR toolbar -LSB- 20 -RSB-, an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents.", "keyphrases": ["term-base feedback", "inform retriev", "languag model", "queri expans process", "queri model", "interact adhoc search", "retriev perform", "probabl", "kl-diverg", "present term"]}
{"file_name": "H-9", "text": "Learn from Web Search Logs to Organize Search Results ABSTRACT Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well : -LRB- 1 -RRB- the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user 's perspective ; and -LRB- 2 -RRB- the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by -LRB- 1 -RRB- learning `` interesting aspects '' of a topic from Web search logs and organizing search results accordingly ; and -LRB- 2 -RRB- generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels. 1. INTRODUCTION The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly. Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization. The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results ; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document. As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively -LSB- 9, 15, 26, 27, 28 -RSB-. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. However, this clustering strategy has two deficiencies which make it not always work well : First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the user 's perspective. But the clusters discovered by the current methods may partition the results into `` local codes '' and `` international codes. '' Such clusters would not be very useful for users ; even the best cluster would still have a low precision. Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem : -LRB- 1 -RRB- The clusters are not corresponding to a user 's interests, so their labels would not be very meaningful or useful. For example, the ambiguous query `` jaguar '' may mean an animal or a car. A cluster may be labeled as `` panthera onca. '' In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following : First, we will learn `` interesting aspects '' of similar topics from search logs and organize search results based on these `` interesting aspects ''. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as `` jaguar '' we can expect to see some clear clusters corresponding different senses of `` jaguar ''. Such aspects can be very useful for organizing future search results about `` car ''. Second, we will generate more meaningful cluster labels using past query words entered by users. Thus they can be better labels than those extracted from the ordinary contents of search results. To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm -LSB- 2 -RSB- to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster. We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches. The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7. 2. RELATED WORK Our work is closely related to the study of clustering search results. In -LSB- 9, 15 -RSB-, the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis -LSB- 20 -RSB- that relevant documents tend to form clusters. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm -LRB- STC -RRB- was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in -LSB- 28 -RSB-, supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In -LSB- 13 -RSB-, the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo -LSB- 22 -RSB-. However, in all these works, the clusters are generated solely based on the search results. Thus the obtained clusters do not necessarily reflect users ' preferences and the generated labels may not be informative from a user 's viewpoint. Methods of organizing search results based on text categorization are studied in -LSB- 6, 8 -RSB-. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query. Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions -LRB- FAQ -RRB- is studied in -LSB- 24, 4 -RSB-. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm -LSB- 2 -RSB-, which is a graph partition based approach, to learn interesting aspects from search logs given a new query. 7. CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users ' perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse. Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results. There are several interesting directions for further extending our work : First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user -LRB- e.g., the aspect chosen by a user to view -RRB-. It would thus be interesting to study how to further improve the organization of the results based on such feedback information. Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user.", "keyphrases": ["retriev model", "rank function", "ambigu", "cluster view", "meaning cluster label", "histori collect", "past queri", "clickthrough", "star cluster algorithm", "suffix tree cluster algorithm", "search result snippet", "monothet cluster algorithm", "pseudo-document", "pairwis similar graph", "similar threshold paramet", "centroid-base method", "cosin similar", "centroid prototyp", "reciproc rank", "log-base method", "mean averag precis"]}
{"file_name": "H-5", "text": "Utility-based Information Distillation Over Temporally Sequenced Documents ABSTRACT This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs -LRB- ` tasks ' with multiple queries -RRB-. Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys -LRB- nuggets -RRB- were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components. 1. INTRODUCTION Tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. Adaptive filtering -LRB- AF -RRB- is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. Based on the initial query and a few positive examples -LRB- if available -RRB-, an AF system maintains a profile for each such topic of interest, and constantly updates it based on feedback from the user. Despite substantial achievements in recent adaptive filtering research, significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. Specifically, the following issues may seriously limit the true utility of AF systems in real-world applications : adaptive filtering setup -- he or she reacts to the system only when the system makes a ` yes ' decision on a document, by confirming or rejecting that decision. A more ` active ' alternative would be to allow the user to issue multiple queries for a topic, review a ranked list of candidate documents -LRB- or passages -RRB- per query, and provide feedback on the ranked list, thus refining their information need and requesting updated ranked lists. The latter form of user interaction has been highly effective in standard retrieval for ad hoc queries. How to deploy such a strategy for long-lasting information needs in AF settings is an open question for research. 2. However, a real user may be willing to provide more informative, fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant, instead of labeling the entire document as relevant. Effectively leveraging such fine-grained feedback could substantially enhance the quality of an AF system. For this, we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents. 3. System-selected documents are often highly redundant. A conventional AF system would select all these redundant news stories for user feedback, wasting the user 's time while offering little gain. Clearly, techniques for novelty detection can help in principle -LSB- 25, 2, 22 -RSB- for improving the utility of the AF systems. However, the effectiveness of such techniques at passage level to detect novelty with respect to user 's -LRB- fine-grained -RRB- feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user. We call the new process utility-based information distillation. Note that conventional benchmark corpora for AF evaluations, which have relevance judgments at the document level and do not define tasks with multiple queries, are insufficient for evaluating the new approach. Therefore, we extended a benchmark corpus -- the TDT4 collection of news stories and TV broadcasts -- with task definitions, multiple queries per task, and answer keys per query. We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1. To automatically evaluate the system-returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi-automatic procedure for acquiring rules that can match nuggets against system responses. Moreover, we propose an extension of NDCG -LRB- Normalized Discounted Cumulated Gain -RRB- -LSB- 9 -RSB- for assessing the utility of ranked passages as a function of both relevance and novelty. Section 2 outlines the information distillation process with a concrete example. Section 3 describes the technical cores of our system called CAF \u00b4 E -- CMU Adaptive Filtering Engine. Section 4 discusses issues with respect to evaluation methodology and proposes a new scheme. Section 5 describes the extended TDT4 corpus. Section 6 presents our experiments and results. Section 7 concludes the study and gives future perspectives. 7. CONCLUDING REMARKS This paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. Our system, called CAF \u00b4 E, combines adaptive filtering, novelty detection and antiredundant passage ranking in a unified framework for utility optimization. We developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. We also proposed an extension of the NDCG metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off.", "keyphrases": ["util-base inform distil", "tempor order document", "passag rank", "adapt filter", "ad-hoc retriev", "novelti detect", "new evalu methodolog", "answer kei", "nugget-match rule", "unifi framework", "ndcg metric"]}
{"file_name": "C-8", "text": "Operation Context and Context-based Operational Transformation ABSTRACT Operational Transformation -LRB- OT -RRB- is a technique for consistency maintenance and group undo, and is being applied to an increasing number of collaborative applications. The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems, as well as the quality of those solutions. The theory of causality has been the foundation of all prior OT systems, but it is inadequate to capture essential correctness requirements. Past research had invented various patches to work around this problem, resulting in increasingly intricate and complicated OT algorithms. After having designed, implemented, and experimented with a series of OT algorithms, we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems, reducing its complexity, and supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The COT algorithm is capable of supporting both do and undo of any operations at anytime, without requiring transformation functions to preserve Reversibility Property, Convergence Property 2, Inverse Properties 2 and 3. The COT algorithm is not only simpler and more efficient than prior OT control algorithms, but also simplifies the design of transformation functions. We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications. 1. INTRODUCTION Operational Transformation -LRB- OT -RRB- was originally invented for consistency maintenance in plain-text group editors -LSB- 4 -RSB-. To effectively and efficiently support existing and new applications, we must continue to improve the capability and quality of OT in solving both old and new problems. The soundness of the theoretical foundation for OT is crucial in this process. However, the theory of causality is inadequate to capture essential OT conditions for correct transformation. The limitation of the causality theory had caused correctness problems from the very beginning of OT. The dOPT algorithm was the first OT algorithm and was based solely on the concurrency relationships among operations -LSB- 4 -RSB- : a pair of operations are transformable as long as they are concurrent. However, later research discovered that the concurrency condition alone is not sufficient to ensure the correctness of transformation. Another condition is that the two concurrent operations must be defined on the same document state. This puzzle was solved in various ways, but the theory of causality as well as its limitation were inherited by all follow-up OT algorithms. The causality theory limitation became even more prominent when OT was applied to solve the undo problem in group editors. The concept of causality is unsuitable to capture the relationships between an inverse operation -LRB- as an interpretation of a meta-level undo command -RRB- and other normal editing operations. In fact, the causality relation is not defined for inverse operations -LRB- see Section 2 -RRB-. Various patches were invented to work around this problem, resulting in more intricate complicated OT algorithms -LSB- 18, 21 -RSB-. supporting its continual evolution. In this paper, we report the main results of this effort : the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. First, we define causal-dependency / - independency and briefly describe their limitations in Section 2. Then, we present the key elements of the operation context theory, including the definition of operation context, context-dependency / - independency relations, context-based conditions, and context vectors in Section 3. In Section 4, we present the basic COT algorithm for supporting consistency maintenance -LRB- do -RRB- and group undo under the assumption that underlying transformation functions are able to preserve some important transformation properties. Then, these transformation properties and their pre-conditions are discussed in Section 5. The COT solutions to these transformation properties are presented in Section 6. Comparison of the COT work to prior OT work, OT correctness issues, and future work are discussed in Section 7. Finally, major contributions of this work are summarized in Section 8. 8. CONCLUSIONS We have contributed the theory of operation context and the COT -LRB- Context-based OT -RRB- algorithm. The theory of operation context is capable of capturing essential relationships and conditions for all types of operation in an OT system ; it provides a new foundation for better understanding and resolving OT problems. The COT algorithm provides uniformed solutions to both consistency maintenance and undo problems ; it is simpler and more efficient than prior OT control algorithms with similar capabilities ; and it significantly simplifies the design of transformation functions. The COT algorithm has been implemented in a generic collaboration engine and used for supporting a range of novel collaborative applications -LSB- 24 -RSB-. Real-world applications provide exciting opportunities and challenges to future OT research. The theory of operation context and the COT algorithm shall serve as new foundations for addressing the technical challenges in existing and emerging OT applications.", "keyphrases": ["oper transform", "cot", "context-base ot", "causal-depend", "concurr condit", "concurr relat", "invers oper", "document state", "origin oper", "transform oper", "invers cluster", "vector represent of oper context", "histori buffer", "exclus transform"]}
{"file_name": "J-4", "text": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions ABSTRACT Keyword auctions lie at the core of the business models of today 's leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google 's auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders ' values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword. 1. INTRODUCTION Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for * This work was done while the author was at Yahoo! Research. `` iPod '', the highest paying advertisers -LRB- for example, Apple or Best Buy -RRB- for that keyword may appear in a separate `` sponsored '' section of the page above or to the right of the algorithmic results. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions. Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture 's success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft 's MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. The search engine evaluates the advertisers ' bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine can not directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. Until recently, Yahoo! ranked bidders in decreasing order of advertisers ' stated values per click, while Google ranks in decreasing order of advertisers ' stated values per impression. We refer to these rules as `` rank-by-bid '' and `` rank-by-revenue '', respectively. ' We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank ` These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. ing rules where bidders are ranked in decreasing order of score eqb, where e denotes an advertiser 's click-through rate -LRB- normalized for position -RRB- and b his bid. Notice that q = 0 corresponds to Yahoo! 's rank-by-bid rule and q = 1 corresponds to Google 's rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz -LSB- 3 -RSB- and Varian -LSB- 11 -RSB-. We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! keyword auctions. We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders ' values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results. Related work. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement `` locally envy-free equilibrium '' and `` symmetric equilibrium '', respectively. Varian also provides some empirical analysis. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani -LSB- 1 -RSB-. That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar -LSB- 5 -RSB- study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting. They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always -LRB- weakly -RRB- dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations. Lahaie -LSB- 8 -RSB- gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture. 8. CONCLUSIONS In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo! and Google models as special cases. In practice, it should be very simple to move between rules within the family : this simply involves changing the exponent q applied to advertiser effects. We also showed that, in principle, the same effect could be obtained by using bidding credits. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices. On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders. In principle the minimum revenue in Nash equilibrium can be found by linear programming. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept. Larger classes of ranking rules could be relevant. For instance, it is possible to introduce discounts ds and rank according to wsbs \u2212 ds ; the equilibrium analysis generalizes to this case as well. With this larger class the virtual score can equal the score, e.g. in the case of a uniform marginal distribution over values. Figure 4 : Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1.", "keyphrases": ["revenu", "keyword auction", "revenu-optim rank", "rank rule", "search engin", "advertis", "sponsor search", "rank-by-bid", "rank-by-revenu", "profit", "advertis revenu", "price search keyword", "optim auction design problem"]}
{"file_name": "C-31", "text": "Apocrita : A Distributed Peer-to-Peer File Sharing System for Intranets ABSTRACT Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Another problem arises when a document is made available on a user 's machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets. 1. INTRODUCTION The Peer-to-Peer -LRB- P2P -RRB- computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user 's machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8. 7. RELATED WORK Several decentralized P2P systems -LSB- 1, 2, 3 -RSB- exist today that Apocrita features some of their functionality. However, Apocrita also has unique novel searching and indexing features that make this system unique. For example, Majestic-12 -LSB- 4 -RSB- is a distributed search and indexing project designed for searching the Internet. Each user would install a client, which is responsible for indexing a portion of the web. A central area for querying the index is available on the Majestic-12 web page. The index itself is not distributed, only the act of indexing is distributed. The distributed indexing aspect of this project most closely relates Apocrita goals. YaCy -LSB- 6 -RSB- is a peer-to-peer web search application. YaCy is designed to maintain a distributed index of the Internet. It used a distributed hash table -LRB- DHT -RRB- to maintain the index. The local node is used to query but all results that are returned are accessible on the Internet. YaCy used many peers and DHT to maintain a distributed index. Apocrita will also use a distributed index in future implementations and may benefit from using an implementation of a DHT. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita. 8. CONCLUSIONS AND FUTURE WORK We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet. It can help organizations with no network file server or necessary network infrastructure to share documents. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems.", "keyphrases": ["peer-to-peer", "file share system", "intranet", "author", "document", "apocrita", "jxta", "distribut index", "peer-to-peer distribut model", "idl queri", "index file", "incom file", "p2p search"]}
{"file_name": "C-20", "text": "Live Data Center Migration across WANs : A Robust Cooperative Context Aware Approach ABSTRACT A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocate a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities. We make use of server virtualization technologies to enable the replication and migration of server functions. We propose new network functions to enable server migration and replication across wide area networks -LRB- e.g., the Internet -RRB-, and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives. 1. INTRODUCTION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. A relatively minor outage can disrupt and inconvenience a large number of users. Today these services are almost exclusively hosted in data centers. Recent advances in server virtualization technologies -LSB- 8, 14, 22 -RSB- allow for the live migration of services within a local area network -LRB- LAN -RRB- environment. In the LAN environment, these technologies have proven to be a very effective tool to enable data center management in a non-disruptive fashion. Not only can it support planned maintenance events -LSB- 8 -RSB-, but it can also be used in a more dynamic fashion to automatically balance load between the physical servers in a data center -LSB- 22 -RSB-. When using these technologies in a LAN environment, services execute in a virtual server, and the migration services provided by the underlying virtualization framework allows for a virtual server to be migrated from one physical server to another, without any significant downtime for the service or application. In particular, since the virtual server retains the same network address as before, any ongoing network level interactions are not disrupted. Similarly, in a LAN environment, storage requirements are normally met via either network attached storage -LRB- NAS -RRB- or via a storage area network -LRB- SAN -RRB- which is still reachable from the new physical server location to allow for continued storage access. Unfortunately in a wide area environment -LRB- WAN -RRB-, live server migration is not as easily achievable for two reasons : First, live migration requires the virtual server to maintain the same network address so that from a network connectivity viewpoint the migrated server is indistinguishable from the original. Second, while fairly sophisticated remote replication mechanisms have been developed in the context of disaster recovery -LSB- 20, 7, 11 -RSB-, these mechanisms are ill suited to live data center migration, because in general the available technologies are unaware of application/service level semantics. In this paper we outline a design for live service migration across WANs. Our design makes use of existing server virtualization technologies and propose network and storage mechanisms to facilitate migration across a WAN. The essence of our approach is cooperative, context aware migration, where a migration management system orchestrates the data center migration across all three subsystems involved, namely the server platforms, the wide area network and the disk storage system. While conceptually similar in nature to the LAN based work described above, using migration technologies across a wide area network presents unique challenges and has to our knowledge not been achieved. Our main contribution is the design of a framework that will allow the migration across a WAN of all subsystems involved with enabling data center services. We describe new mechanisms as well as extensions to existing technologies to enable this and outline the cooperative, context aware functionality needed across the different subsystems to enable this. 4. RELATED WORK Prior work on this topic falls into several categories : virtual machine migration, storage replication and network support. At the core of our technique is the ability of encapsulate applications within virtual machines that can be migrated without application downtimes -LSB- 15 -RSB-. As indicated earlier, these techniques assume that migration is being done on a LAN. VM migration has also been studied in the Shirako system -LSB- 10 -RSB- and for grid environments -LSB- 17, 19 -RSB-. Current virtual machine software support a suspend and resume feature that can be used to support WAN migration, but with downtimes -LSB- 18, 12 -RSB-. Recently live WAN migration using IP tunnels was demonstrated in -LSB- 21 -RSB-, where an IP tunnel is set up from the source to destination server to transparently forward packets to and from the application ; we advocate an alternate approach that assumes edge router support. An excellent description of these and others, as well as a detailed taxonomy of the different approaches for replication can be found in -LSB- 11 -RSB-. The Ursa Minor system argues that no single fault model is optimal for all applications and proposed supporting data-type specific selections of fault models and encoding schemes for replication -LSB- 1 -RSB-. In the context of network support, our work is related to the RouterFarm approach -LSB- 2 -RSB-, which makes use of orchestrated network changes to realize near hitless maintenance on provider edge routers. In addition to being in a different application area, our approach differs from the RouterFarm work in two regards. Second, due to the stringent timing requirements of live migration, we expect that our approach would require new router functionality -LRB- as opposed to being realizable via the existing configuration interfaces -RRB-. In a similar spirit to ROC, we advocate using mechanisms from live VM migration to storage replication to support planned and unplanned outages in data centers -LRB- rather than full replication to mask such failures -RRB-. 5. CONCLUSION A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocated a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We sought to achieve high availability of data center services in the face of both planned and incidental outages of data center facilities. We advocated using server virtualization technologies to enable the replication and migration of server functions. We proposed new network functions to enable server migration and replication across wide area networks -LRB- such as the Internet or a geographically distributed virtual private network -RRB-, and finally showed the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "keyphrases": ["internet-base servic", "data center migrat", "wan", "lan", "virtual server", "storag replic", "synchron replic", "asynchron replic", "network support", "storag", "voic-over-ip", "voip", "databas"]}
{"file_name": "J-20", "text": "Clearing Algorithms for Barter Exchange Markets : Enabling Nationwide Kidney Exchanges ABSTRACT In barter-exchange markets, agents seek to swap their items with one another, in order to improve their own utilities. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. We focus mainly on the upcoming national kidney-exchange market, where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed. Long cycles are forbidden, since, for incentive reasons, all transplants in a cycle must be performed simultaneously. Also, in barter-exchanges generally, more agents are affected if one drops out of a longer cycle. We prove that the clearing problem with this cycle-length constraint is NP-hard. Solving it exactly is one of the main challenges in establishing a national kidney exchange. We present the first algorithm capable of clearing these markets on a nationwide scale. The key is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop techniques that dramatically improve both runtime and memory usage. We conclude that column generation scales drastically better than constraint generation. Our algorithm also supports several generalizations, as demanded by real-world kidney exchanges. Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges. The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted. 1. INTRODUCTION The role of kidneys is to filter waste from blood. Kidney failure results in accumulation of this waste, which leads to death in months. One treatment option is dialysis, in which the patient goes to a hospital to have his/her blood filtered by an external machine. Several visits are required per week, and each takes several hours. The quality of life on dialysis can be extremely low, and in fact many patients opt to withdraw from dialysis, leading to a natural death. Only 12 % of dialysis patients survive 10 years -LSB- 23 -RSB-. Instead, the preferred treatment is a kidney transplant. Kidney transplants are by far the most common transplant. Unfortunately, the demand for kidneys far outstrips supply. In the United States in 2005, 4,052 people died waiting for a life-saving kidney transplant. During this time, almost 30,000 people were added to the national waiting list, while only 9,913 people left the list after receiving a deceaseddonor kidney. For many patients with kidney disease, the best option is to find a living donor, that is, a healthy person willing to donate one of his/her two kidneys. In 2005, there were 6,563 live donations in the US. and his intended recipient are blood-type or tissue-type incompatible. In the past, the incompatible donor was sent home, leaving the patient to wait for a deceased-donor kidney. However, there are now a few regional kidney exchanges in the United States, in which patients can swap their incompatible donors with each other, in order to each obtain a compatible donor. These markets are examples of barter exchanges. In a barter-exchange market, agents -LRB- patients -RRB- seek to swap their items -LRB- incompatible donors -RRB- with each other. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. Barter exchanges are ubiquitous : examples include Peerflix -LRB- DVDs -RRB- -LSB- 11 -RSB-, Read It Swap It -LRB- books -RRB- -LSB- 12 -RSB-, and Intervac -LRB- holiday houses -RRB- -LSB- 9 -RSB-. For many years, there has even been a large shoe exchange in the United States -LSB- 10 -RSB-. People with different-sized feet use this to avoid having to buy two pairs of shoes. Leg amputees have a separate exchange to share the cost of buying a single pair of shoes. We can encode a barter exchange market as a directed graph G = -LRB- V, E -RRB- in the following way. Construct one vertex for each agent. Add a weighted edge e from one agent vi to another vj, if vi wants the item of vj. The weight we of e represents the utility to vi of obtaining vj 's item. A cycle c in this graph represents a possible swap, with each agent in the cycle obtaining the item of the next agent. The weight wc of a cycle c is the sum of its edge weights. An exchange is a collection of disjoint cycles. The weight of an exchange is the sum of its cycle weights. A social welfare maximizing exchange is one with maximum weight. Figure 1 illustrates an example market with 5 agents, -LCB- v1, v2,..., v5 -RCB-, in which all edges have weight 1. The market has 4 cycles, c1 = -LRB- v1, v2 -RRB-, c2 = -LRB- v2, v3 -RRB-, c3 = -LRB- v3, v4 -RRB- and c4 = -LRB- v1, v2, v3, v4, v5 -RRB-, and two -LRB- inclusion -RRB- maximal exchanges, namely M1 = -LCB- c4 -RCB- and M2 = -LCB- c1, c3 -RCB-. Exchange M1 has both maximum weight and maximum cardinality -LRB- i.e., it includes the most edges/vertices -RRB-. Figure 1 : Example barter exchange market. The clearing problem is to find a maximum-weight exchange consisting of cycles with length at most some small constant L. This cycle-length constraint arises naturally for several reasons. For example, in a kidney exchange, all operations in a cycle have to be performed simultaneously ; otherwise a donor might back out after his incompatible partner has received a kidney. Due to such resource constraints, the upcoming national kidney exchange market will likely allow only cycles of length 2 and 3. Another motivation for short cycles is that if the cycle fails to exchange, fewer agents are affected. For example, last-minute testing in a kidney exchange often reveals new incompatibilities that were not detected in the initial testing -LRB- based on which the compatibility graph was constructed -RRB-. In Section 3, we show that -LRB- the decision version of -RRB- the clearing problem is NP-complete for L > 3. One approach then might be to look for a good heuristic or approximation algorithm. However, for two reasons, we aim for an exact algorithm based on an integer-linear program -LRB- ILP -RRB- formulation, which we solve using specialized tree search. 9 First, any loss of optimality could lead to unnecessary patient deaths. 9 Second, an attractive feature of using an ILP formula tion is that it allows one to easily model a number of variations on the objective, and to add additional constraints to the problem. Or, if for various -LRB- e.g., ethical -RRB- reasons one requires a maximum cardinality exchange, one can at least in a second pass find the solution -LRB- out of all maximum cardinality solutions -RRB- that has the fewest 3-cycles. Other variations one can solve for include finding various forms of `` fault tolerant '' -LRB- non-disjoint -RRB- collections of cycles in the event that certain pairs that were thought to be compatible turn out to be incompatible after all. In this paper, we present the first algorithm capable of clearing these markets on a nationwide scale. Straight-forward ILP encodings are too large to even construct on current hardware -- not to talk about solving them. The key then is incremental problem formulation. We adapt two paradigms for the task : constraint generation and column generation. For each, we develop a host of -LRB- mainly problemspecific -RRB- techniques that dramatically improve both runtime and memory usage. 1.1 Prior Work Several recent papers have used simulations and marketclearing algorithms to explore the impact of a national kidney exchange -LSB- 13, 20, 6, 14, 15, 17 -RSB-. For example, using Edmond 's maximum-matching algorithm -LSB- 4 -RSB-, -LSB- 20 -RSB- shows that a national pairwise-exchange market -LRB- using length-2 cycles only -RRB- would result in more transplants, reduced waiting time, and savings of $ 750 million in heath care costs over 5 years. Those results are conservative in two ways. Firstly, the simulated market contained only 4,000 initial patients, with 250 patients added every 3 months. It has been reported to us that the market could be almost double this size. Secondly, the exchanges were restricted to length-2 cycles -LRB- because that is all that can be modeled as maximum matching, and solved using Edmonds 's algorithm -RRB-. Allowing length-3 cycles leads to additional significant gains. This has been demonstrated on kidney exchange markets with 100 patients by using CPLEX to solve an integer-program encoding of the clearing problem -LSB- 15 -RSB-. In this paper, we present an alternative algorithm for this integer program that can clear markets with over 10,000 patients -LRB- and that same number of willing donors -RRB-. Allowing cycles of length more than 3 often leads to no improvement in the size of the exchange -LSB- 15 -RSB-. -LRB- Furthermore, in a simplified theoretical model, any kidney exchange can be converted into one with cycles of length at most 4 -LSB- 15 -RSB-. -RRB- Whilst this does not hold for general barter exchanges, or even for all kidney exchange markets, in Section 5.2.3 we make use of the observation that short cycles suffice to dramatically increase the speed of our algorithm. At a high-level, the clearing problem for barter exchanges is similar to the clearing problem -LRB- aka winner determination problem -RRB- in combinatorial auctions. In both settings, the idea is to gather all the pertinent information about the agents into a central clearing point and to run a centralized clearing algorithm to determine the allocation. Both problems are NP-hard. Both are best solved using tree search techniques. Since 1999, significant work has been done in computer science and operations research on faster optimal tree search algorithms for clearing combinatorial auctions. However, the kidney exchange clearing problem -LRB- with a limit of 3 or more on cycle size -RRB- is different from the combinatorial auction clearing problem in significant ways. The most important difference is that the natural formulations of the combinatorial auction problem tend to easily fit in memory, so time is the bottleneck in practice. In contrast, the natural formulations of the kidney exchange problem -LRB- with L = 3 -RRB- take at least cubic space in the number of patients to even model, and therefore memory becomes a bottleneck much before time does when using standard tree search, such as branch-andcut in CPLEX, to tackle the problem. Therefore, the approaches that have been developed for combinatorial auctions can not handle the kidney exchange problem. 1.2 Paper Outline The rest of the paper is organized as follows. Section 2 discusses the process by which we generate realistic kidney exchange market data, in order to benchmark the clearing algorithms. Section 3 contains a proof that the market clearing decision problem is NP-complete. Sections 4 and 5 each contain an ILP formulation of the clearing problem. We also detail in those sections our techniques used to solve those programs on large instances. Section 6 presents experiments on the various techniques. Section 7 discusses recent fielding of our algorithm. Finally, we present our conclusions in Section 8, and suggest future research directions. 7. FIELDING THE TECHNOLOGY Our algorithm and implementation replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges, in December 2006. We conduct a match run every two weeks, and the first transplants based on our solutions have already been conducted. While there are -LRB- for political/inter-personal reasons -RRB- at least four kidney exchanges in the US currently, everyone understands that a unified unfragmented national exchange would save more lives. We are in discussions with additional kidney exchanges that are interested in adopting our technology. This way our technology -LRB- and the processes around it -RRB- will hopefully serve as a substrate that will eventually help in unifying the exchanges. At least computational scalability is no longer an obstacle. 8. CONCLUSION AND FUTURE RESEARCH In this work we have developed the most scalable exact algorithms for barter exchanges to date, with special focus on the upcoming national kidney-exchange market in which patients with kidney disease will be matched with compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease. Our work presents the first algorithm capable of clearing these markets on a nationwide scale. It optimally solves the kidney exchange clearing problem with 10,000 donordonee pairs. The best prior technology -LRB- vanilla CPLEX -RRB- can not handle instances beyond about 900 donor-donee pairs because it runs out of memory. The key to our improvement is incremental problem formulation. We adapted two paradigms for the task : constraint generation and column generation. For each, we developed a host of techniques that substantially improve both runtime and memory usage. Some of the techniques use domain-specific observations while others are domain independent. We conclude that column generation scales dramatically better than constraint generation. Undoubtedly, further parameter tuning and perhaps additional speed improvement techniques could be used to make the algorithm even faster. Our algorithm also supports several generalizations, as desired by real-world kidney exchanges. Because we use an ILP methodology, we can also support a variety of side constraints, which often play an important role in markets in practice -LSB- 19 -RSB-. We can also support forcing part of the allocation, for example, `` This acutely sick teenager has to get a kidney if possible. '' Our work has treated the kidney exchange as a batch problem with full information -LRB- at least in the short run, kidney exchanges will most likely continue to run in batch mode every so often -RRB-. Two important directions for future work are to explicitly address both online and limited-information aspects of the problem. The online aspect is that donees and donors will be arriving into the system over time, and it may be best to not execute the myopically optimal exchange now, but rather save part of the current market for later matches.", "keyphrases": ["barter-exchang market", "match", "column gener", "kidnei", "transplant", "market characterist", "instanc gener", "solut approach", "edg formul", "cycl formul"]}
{"file_name": "H-2", "text": "Personalized Query Expansion for the Web ABSTRACT The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user 's Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach. 1. INTRODUCTION The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web. Yet keyword queries are * Part of this work was performed while the author was visiting Yahoo! Research, Barcelona, Spain. inherently ambiguous. The query `` canon book '' for example covers several different areas of interest : religion, photography, literature, and music. Clearly, one would prefer search output to be aligned with user 's topic -LRB- s -RRB- of interest, rather than displaying a selection of popular URLs from each category. Studies have shown that more than 80 % of the users would prefer to receive such personalized search results -LSB- 33 -RSB- instead of the currently generic ones. Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly. It has been shown to perform very well over large data sets, especially with short input queries -LRB- see for example -LSB- 19, 3 -RSB- -RRB-. This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the user 's Personal Information Repository -LRB- PIR -RRB-, i.e., the personal collection of text documents, emails, cached Web pages, etc.. Several advantages arise when moving Web search personalization down to the Desktop level -LRB- note that by `` Desktop '' we refer to PIR, and we use the two terms interchangeably -RRB-. First is of course the quality of personalization : The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user. Our algorithms expand Web queries with keywords extracted from user 's PIR, thus implicitly personalizing the search output. After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1. We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best. In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co-occurrence metrics and external thesauri. The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG -LSB- 15 -RSB- improvements of up to 51.28 %. In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query. This yields an additional improvement of 8.47 % over the previously identified best algorithm. We conclude and discuss further work in Section 5. 2. PREVIOUS WORK This paper brings together two IR areas : Search Personalization and Automatic Query Expansion. There exists a vast amount of algorithms for both domains. In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms. 2.1 Personalized Search Personalized search comprises two major components : -LRB- 1 -RRB- User profiles, and -LRB- 2 -RRB- The actual search algorithm. This section splits the relevant background according to the focus of each article into either one of these elements. Approaches focused on the User Profile. Sugiyama et al. -LSB- 32 -RSB- analyzed surfing behavior and generated user profiles as features -LRB- terms -RRB- of the visited pages. Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile. Qiu and Cho -LSB- 26 -RSB- used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic-Sensitive PageRank -LSB- 13 -RSB-. User profiling based on browsing history has the advantage of being rather easy to obtain and process. This is probably why it is also employed by several industrial search engines -LRB- e.g., Yahoo! MyWeb2 -RRB-. However, it is definitely not sufficient for gathering a thorough insight into user 's interests. Moreover, none of these investigated the adaptive application of personalization. Approaches focused on the Personalization Algorithm. Haveliwala -LSB- 13 -RSB- computed a topicoriented PageRank, in which 16 PageRank vectors biased on each of the main topics of the Open Directory were initially calculated off-line, and then combined at run-time based on the similarity between the user query and each of the 16 topics. 2.2 Automatic Query Expansion Automatic query expansion aims at deriving a better formulation of the user query in order to enhance retrieval. It is based on exploiting various social or collection specific characteristics in order to generate additional terms, which are appended to the original in put keywords before identifying the matching documents returned as output. In this section we survey some of the representative query expansion works grouped according to the source employed to generate additional terms : -LRB- 1 -RRB- Relevance feedback, -LRB- 2 -RRB- Collection based co-occurrence statistics, and -LRB- 3 -RRB- Thesaurus information. Some other approaches are also addressed in the end of the section. Relevance Feedback Techniques. The main idea of Relevance Feedback -LRB- RF -RRB- is that useful information can be extracted from the relevant documents returned for the initial query. First approaches were manual -LSB- 28 -RSB- in the sense that the user was the one choosing the relevant results, and then various methods were applied to extract new terms, related to the query and the selected documents. Efthimiadis -LSB- 11 -RSB- presented a comprehensive literature review and proposed several simple methods to extract such new keywords based on term frequency, document frequency, etc.. We used some of these as inspiration for our Desktop specific techniques. Chang and Hsu -LSB- 5 -RSB- asked users to choose relevant clusters, instead of documents, thus reducing the amount of interaction necessary. RF has also been shown to be effectively automatized by considering the top ranked documents as relevant -LSB- 37 -RSB- -LRB- this is known as Pseudo RF -RRB-. Lam and Jones -LSB- 21 -RSB- used summarization to extract informative sentences from the top-ranked documents, and appended them to the user query. Finally, Yu et al. -LSB- 38 -RSB- selected the expansion terms from vision-based segments of Web pages in order to cope with the multiple topics residing therein. Co-occurrence Based Techniques. Terms highly co-occurring with the issued keywords have been shown to increase precision when appended to the query -LSB- 17 -RSB-. We have also investigated three such approaches in order to identify query relevant keywords from the rich, yet rather complex Personal Information Repository. Thesaurus Based Techniques. A broadly explored method is to expand the user query with new terms, whose meaning is closely related to the input keywords. Just as for the co-occurrence methods, initial experiments with this approach were controversial, either reporting improvements, or even reductions in output quality -LSB- 36 -RSB-. We also use WordNet based expansion terms. However, we base this process on analyzing the Desktop level relationship between the original query and the proposed new keywords. Other Techniques. There are many other attempts to extract expansion terms. Though orthogonal to our approach, two works are very relevant for the Web environment : Cui et al. -LSB- 8 -RSB- generated word correlations utilizing the probability for query terms to appear in each document, as computed over the search engine logs. Kraft and Zien -LSB- 19 -RSB- showed that anchor text is very similar to user queries, and thus exploited it to acquire additional keywords. 5. CONCLUSIONS AND FURTHER WORK In this paper we proposed to expand Web search queries by exploiting the user 's Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to user 's interests, personalizing the search output. In this context, the paper includes the following contributions : \u2022 We proposed five techniques for determining expansion terms from personal documents. Each of them produces additional query keywords by analyzing user 's Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co-occurrence statistics and external thesauri. Figure 1 : Relative NDCG gain -LRB- in % -RRB- for each algorithm overall, as well as separated per query category. \u2022 We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios. We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28 %. \u2022 We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level. \u2022 Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47 % over the previously identified best approach. We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms. We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data. Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms.", "keyphrases": ["short keyword queri", "web retriev", "web queri", "person inform repositori", "search output", "addit queri keyword", "granular level", "term and compound level analysi", "global co-occurr statist", "extern thesauru", "extens empir analysi", "ambigu queri", "qualiti", "output rank", "person search framework", "expans process", "variou featur of each queri", "adapt algorithm", "signific improv", "static expans approach"]}
{"file_name": "J-30", "text": "Implementation with a Bounded Action Space ABSTRACT While traditional mechanism design typically assumes isomorphism between the agents ' type - and action spaces, in many situations the agents face strict restrictions on their action space due to, e.g., technical, behavioral or regulatory reasons. We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces. Our contribution is threefold. First, we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies, and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost. Second, we identify necessary conditions for the optimality of action-bounded mechanisms, and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives. Finally, we prove that for any multilinear social-choice rule, the optimal mechanism with k actions incurs an expected loss of O -LRB- k21 -RRB- compared to the optimal mechanisms with unrestricted action spaces. Our results apply to various economic and computational settings, and we demonstrate their applicability to signaling games, public-good models and routing in networks. 1. INTRODUCTION Mechanism design is a sub-field of game theory that studies how to design rules of games resulting in desirable outcomes, when the players are rational. In a standard setting, players hold some private information -- their `` types '' -- and choose `` actions '' from their action spaces to maximize their utilities. The social planner wishes to implement a social-choice function, which maps each possible state of the world -LRB- i.e., a profile of the players ' types -RRB- to a single alternative. For example, a government that wishes to undertake a public-good project -LRB- e.g., building a bridge -RRB- only if the total benefit for the players exceeds its cost. Much of the literature on mechanism design restricts attention to direct revelation mechanisms, in which a player 's action space is identical to his type space. This focus is owing to the revelation principle that asserts that if some mechanism achieves a certain result in an equilibrium, the same result can be achieved in a truthful one -- an equilibrium where each agent simply reports his private type -LSB- 15 -RSB-. Nonetheless, in many environments, direct-revelation mechanisms are not viable since the actions available for the players have a limited expressive power. Consider, for example, the well-studied `` screening '' model, where an insurance firm wishes to sell different types of policies to different drivers based on their caution levels, which is their private information. There are various reasons for such strict restrictions on the action spaces. The buyers in such environemnts face only two actions -- to buy or not to buy -- although they may have an infinite number of possible values for the item. In many similar settings, players might be also reluctant to reveal their accurate types, but willing to disclose partial information about them. For example, agents will typically be unwilling to reveal their types, even if it is beneficial for them in the short run, since it might harm them in future transactions. Agents may also not trust the mechanism to keep their valuations private -LSB- 16 -RSB-, or not even know their exact type while computing it may be expensive -LSB- 12 -RSB-. Consider for example a public-good model : a social planner needs to decide whether to build a bridge. The two players in the game have some privately known benefits \u03b81, \u03b82 \u2208 -LSB- 0, 1 -RSB- from using this bridge. The social planner aims to build the bridge only if the sum of these benfits exceeds the construction cost of the bridge. The social planner can not access the private data of the players, and can only learn about it from the players ' actions. When direct revelation is allowed, the social planner can run the well-known VCG mechanism, where the players have incentives to report their true data ; hence, the planner can elicit the exact private information of the players and build the bridge only when it should be built. Assume now that the players can not send their entire secret data, but can only choose an action out of two possible actions -LRB- e.g., `` 0 '' or `` 1 '' -RRB-. Now, the social planner will clearly no longer be able to always build the bridge according to her objective function, due to the limited expressivness of the players ' messages. In this work we try to analyze what can be achieved in the presence of such restrictions. Restrictions on the action space, for specific models, were studied in several earlier papers. They studied single-item auctions where bidders are allowed to send messages with severely bounded size. They characterized the optimal mechanisms under this restriction, and showed that nearly optimal results can be achieved even with very strict limitations on the action space. Our work generalizes the main results of Blumrosen et al. to a general mechanism-design framework that can be applied to a multitude of models. A standard mechanism design setting is composed of agents with private information -LRB- their `` types '' -RRB-, and a social planner, who wishes to implement a social choice function, c -- a function that maps any profile of the agents ' types into a chosen alternative. A classic result in this setting says that under some monotonicity assumption on the agents ' preferences -- the `` single-crossing '' assumption -LRB- see definition below -RRB- -- a social-choice function is implementable in dominant strategies if and only if it is monotone in the players ' types. However, in environments with restricted action spaces, the social planner can not typically implement every social-choice function due to inherent informational constraints. That is, for some realizations of the players ' types, the decision of the social planner will be incompatible with the social-choice function c. In order to quantitatively measure how well bounded-action mechanisms can approximate the original social-choice functions, we follow a standard assumption that the social choice function is derived from a social-value function, g, which assigns a real value for every alternative A and realization of the players ' types. The social-choice function c will therefore choose an alternative that maximizes the social value function, given the type \u2192 \u2212 \u03b8 = -LRB- \u03b81,. . Observe that the social-value function is not necessarily the social welfare function -- the social welfare function is a special case of g in which g is defined to be the sum of the players ' valuations for the chosen alternative. Following are several simple examples of social-value functions : \u2022 Public goods. A government wishes to build a bridge only if the sum of the benefits that agents gain from it exceeds its construction cost C. The social value functions in a 2-player game will therefore be : g -LRB- \u03b81, \u03b82, `` build '' -RRB- = \u03b81 + \u03b82-C and g -LRB- \u03b81, \u03b82, `` do not build '' -RRB- = 0. \u2022 Routing in networks. Consider a network that is composed of two links in parallel. Each link has a secret probability pi of transferring a message successfully. A sender wishes to send his message through the network only if the probability of success is greater than, say, 90 percent - the known probability in an alternate network. \u2022 Single-item auctions. Consider a 2-player auction, where the auctioneer wishes to allocate the item to the player who values it the most. The social choice function is given by : g -LRB- \u03b81, \u03b82, `` player 1 wins '' -RRB- = \u03b81 and for the second alternative is g -LRB- \u03b81, \u03b82, `` player 2 wins '' -RRB- = \u03b82. 1.1 Our Contribution In this paper, we present a general framework for the study of mechanism design in environments with a limited number of actions. We assume a Bayesian model where players have one-dimensional private types, independently distributed on some real interval. The main question we ask is : when agents are only allowed to use k different actions, which mechanisms achieve the optimal expected social-value? Note that this question is actually composed of two separate questions. The first question is an information-theoretic question : what is the optimal result achievable when the players can only reveal information using these k actions -LRB- recall that their type space may be continuous -RRB-. The other question involves gametheoretic considerations : what is the best result achievable with k actions, where this result should be achieved in a dominant-strategy equilibrium. These questions raise the question about the `` price of implementation '' : can the optimal information-theoretic result always be implemented in a dominant-strategy equilibrium? And if not, to what extent does the dominant-strategy requirement degrades the optimal result? Our first contribution is the characterization of sufficient conditions for implementing the optimal informationtheoretic social-choice rule in dominant strategies. We show that for the family of multilinear social-value functions -LRB- that Theorem : Given any multilinear single-crossing socialvalue function, and for any number of alternatives and players, the social choice rule that is information-theoretically optimal is implementable in dominant strategies. Multilinear social-value functions capture many important and well-studied models, and include, for instance, the routing example given above, and any social welfare function in which the players ' valuations are linear in their types -LRB- such as public-goods and auctions -RRB-. The implementability of the information-theoretically optimal mechanisms enables us to use a standard routine in Mechanism Design and first determine the optimal socialchoice rule, and then calculate the appropriate payments that ensure incentive compatibility. To show this result, we prove a useful lemma that gives another characterization for social-choice functions whose `` price of implementation '' is zero. We show that for any social-choice function, incentive compatibility in action-bounded mechanisms is equivalent to the property that the optimal expected social value is achieved with non-decreasing strategies -LRB- or threshold strategies -RRB-.1 In other words, this lemma implies that one can always implement, with dominant strategies, the best socialchoice rule that is achievable with non-decreasing strategies. Our second contribution is in characterizing the optimal action-bounded mechanisms. We identify some necessary conditions for the optimality of mechanisms in general, and using these conditions, we fully characterize the optimal mechanisms in environments with two players and two alternatives. We complete the characterization of the optimal mechanisms with the depiction of the optimal strategies -- strategies that are `` mutually maximizers ''. Since the payments in a dominantstrategy implementation are uniquely defined by a monotone allocation and a profile of strategies, this also defines the payments in the mechanism. We give an intuitive proof for the optimality of such strategies, generalizing the concept of optimal `` mutually-centered '' strategies from -LSB- 4 -RSB-. Surprisingly, as opposed to the optimal auctions in -LSB- 4 -RSB-, for some non-trivial social-value functions, the optimal `` diagonal '' mechanism may not utilize all the k available actions. Theorem : For any multilinear single-crossing social-value function over two alternatives, the informationally optimal 2-player k-action mechanism is diagonal, and the optimal dominant strategies are mutually-maximizers. Achieving a full characterization of the optimal actionbounded mechanism for multi-player or multi-alternative environments seems to be harder. To support this claim, we observe that the number of mechanisms that satisfy the necessary conditions above is growing exponentially in the number of players. 1The restriction to non-decreasing strategies is very common in the literature. One remarkable result by Athey -LSB- 1 -RSB- shows that when a non-decreasing strategy is a best response for any other profile of non-decreasing strategies, a pure Bayesian-Nash equilibrium must exist. Our next result compares the expected social-value in k-action mechanisms to the optimal expected social value when the action space is unrestricted. For any number of players or alternatives, and for any profile of independent distribution functions, we construct mechanisms that are nearly optimal -- up to an additive difference of O -LRB- k21 -RRB-. This result is achieved in dominant strategies. Theorem : For any multilinear social-value function, the optimal k-action mechanism incurs an expected social loss of O -LRB- k21 -RRB-. Note that there are social-choice functions that can be implemented with k actions with no loss at all -LRB- for example, the rule `` always choose alternative A '' -RRB-. However, we know that in some settings -LRB- e.g., auctions -LSB- 5 -RSB- -RRB- the optimal loss may be proportional to 1k2, thus a better general upper bound is impossible. Finally, we present our results in the context of several natural applications. First, we give an explicit solution for a public-good game with k-actions. This is a natural application in our context since education levels are often discrete -LRB- e.g., B.A, M.A and PhD -RRB-. The latter example illustrates how our results apply to settings where the goal of the social planner is not welfare maximization -LRB- nor variants of it like `` affine maximizers '' -RRB-. The rest of the paper is organized as follows : our model and notations are described in Section 2. We then describe our general results regarding implementation in multi-player and multi-alternative environments in Section 3, including the asymptotic analysis of the social-value loss. In Section 4, we fully characterize the optimal mechanisms for 2player environments with two alternative. In Section 5, we conclude with applying our general results to several wellstudied models. 5. EXAMPLES Our results apply to a variety of economic, computational and networked settings. In this section, we demonstrate the applicability of our results to public-good models, signaling games and routing applications. 5.1 Application 1 : Public Goods The public-good model deals with a social planner -LRB- e.g., government -RRB- that needs to decide whether to supply a public good, such as building a bridge. Let Yes and No denote the respective alternatives of building and not building the bridge. v = v1,..., vn is the vector of the players ' types -- the values they gain from using the bridge. The decision that maximizes the social welfare is to build the bridge if and only if P is built, the social welfare is P i vi is greater than its cost, denoted by C. The utility of player i under payment pi is ui = vi -- pi if the bridge is built, and 0 otherwise. It is well-known that under no restriction on the action space, it is possible to induce truthful revelation by VCG mechanisms, therefore full efficiency can be achieved. Obviously, when the action set is limited to k actions, we can not achieve full efficiency due to the informational constraints. Hence, the information-theoretically optimal kaction mechanism is implementable in dominant strategies. Moreover, as Theorem 3 suggests, in the k-action 2-player public-good game, we can fully characterize the optimal mechanisms. In the proof of Theorem 3, we saw that when for both players g -LRB- \u03b8i, \u03b8i, A -RRB- = g -LRB- \u03b8i, \u03b8i, B -RRB-, the mechanism is non-degenerate with respect to both players.6 This condition clearly holds here -LRB- 1 + 0 -- C = 0 + 1 -- C -RRB-, therefore the optimal mechanisms will use all k actions. 1. Allocation : Build the bridge if j '' b1 + b2 > k. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1, 2. Allocation : Build the bridge if j '' b1 + b2 > k -- 1. Strategies : Threshold strategies based on the vectors \u2192 -- x, -- \u2192 y where for every 1 < i < k-1 : Recall that we define the optimal mechanisms by their allocation scheme and by the optimal strategies for the players. It is well known, that the allocation scheme in monotone mechanisms uniquely defines the payments that ensure incentive-compatibility. In public-good games, these payments satisfy the rule that a player pays his lowest value for which the bridge is built, when the action of the other player is fixed. Therefore, the payments for the players 1 and 2 reporting the actions b1 and b2 are as follows : in mechanism 1 from Proposition 3, p1 = xb2 and p2 = yb1 ; in mechanism 2 from Proposition 3, p1 = xb2 -- 1 and p2 = yb1 -- 1. We now show a more specific example that assumes uniform distributions. The example shows how the optimal mechanism is determined by the cost C : for low costs, mechanism of type 1 is optimal, and for high costs the optimal mechanism is of type 2. An additional interesting feature of the optimal mechanisms in the example is that they are symmetric with respect to the players. This come as opposed to the optimal mechanisms in the auction model -LSB- 5 -RSB- that are asymmetric -LRB- even when the players ' values are drawn from identical distributions -RRB-. Figure 2 : Optimal mechanisms in a 2-player, 2-alternative, 2-action public-goods game, when the types are uniformly distributed in -LSB- 0, 1 -RSB-. The mechanism on the left is optimal when C < 1 and the other is optimal when C > 1. EXAMPLE 1. Suppose that the types of both players are uniformly distributed on -LSB- 0, 1 -RSB-. Figure 2 illustrates the optimal mechanisms for k = 2, and shows how both the allocation scheme and the payments depend on the construction cost C. Then, the welfare-maximizing mechanisms are : 5.2 Application 2 : Signaling We now study a signaling model in labor markets. In this model, the type of each worker, \u03b8i \u2208 -LSB- \u03b8, \u03b8 -RSB-, describes the worker 's productivity level. The firm wants to make her hiring decisions according to a decision function f -LRB- \u2212 \u2192 \u03b8 -RRB-. For example, the firm may want to hire the most productive worker -LRB- like the auction model -RRB-, or hire a group of workers only if their sum of productivities is greater than some threshold -LRB- similar to the public-good model -RRB-. However, the worker 's productivity is invisible to the firm ; the firm only observes the worker 's education level e that should convey signals about her productivity level. Note that the assumption here is that acquiring education, at any level, does not affect the productivity of the worker, but only signals about the worker 's skills. A main component in this model, is the fact that as the worker is more productive, it is easier for him to acquire high-level education. In addition, the cost of acquiring education increases with the education level. More formally, a continuous function C -LRB- e, \u03b8 -RRB- describes the cost to a worker from acquiring each education level as a function of his productivity. An action for a worker in this game is the education level he chooses to acquire. In standard models, this action space is continuous, and then a `` fully separating equilibrium '' exists -LRB- under the single-crossing conditions on the cost function -RRB-. That is, there exists an equilibrium in which every type is mapped into a different education level ; thus, the firm can induce the exact productivity levels of the workers by this signaling mechanism. However, it is hard to imagine a world with a continuum of education levels. It is usually the case that there are only several discrete education levels -LRB- e.g., BSc, MSc, PhD -RRB-. With k education levels, the firm may not be able to exactly follow the decision function f. For achieving the best result in k actions, the firm may want the workers to play according to specific threshold strategies. It turns out that the standard condition, the single-crossing condition on the cost function, suffices for ensuring that these threshold strategies will be dominant for the players. COROLLARY 4. Consider a multilinear decision function f, and a single-crossing cost function for the players. With k education levels, the firm can implement in dominant strategies a decision function that incurs a loss of O -LRB- k21 -RRB- compared with the decision function f. 5.3 Application 3 : Routing In our last example, we show the applicability of our results to routing in lossy networks. In such systems, a sender needs to decide through which network to transmit his message. In this example, we focus on parallel-path networks. The edges in these networks are controlled by different selfish agents, and each edge appears only in one of the networks. Suppose that the sender, who wishes to send a message from the source to the sink, knows the topology of each network, but the probability of success on each link, pi, is the link 's private information. The problem of the sender is to decide whether to send a message through the network N1 or through an alternate network N2. Obviously, the sender wishes to send the message through N1 only if the total probability of success in N1 is greater than the success probability in N2. Let f N -LRB- \u2212 \u2192 p -RRB- denote the probability of success in network N with a successprobability vector \u2192 \u2212 p. The social choice function in this example is thus : c -LRB- \u2212 \u2192 p -RRB- \u2208 argmax -LCB- N1, N2 -RCB- -LCB- fN1 -LRB- \u2212 \u2192 p -RRB-, f N2 -LRB- \u2212 \u2192 p -RRB- -RCB-. Figure 3 : An example for a parallel-path network, where each link has a probability pi for transmission success. We show that the overall probability of success in such networks is multilinear in pi, and thus the optimal k-action social-choice function is dominant-strategy implementable. In this example, we assume that every agent has a singlecrossing valuation function over the alternatives. That is, each player wishes that the message will be sent through his network, and his benefit is positively correlated with his secret data -LRB- e.g., the valuation of player i may be exactly pi -RRB-. We would like to emphasize that the social planner in this example -LRB- the sender -RRB- does not aim to maximize the social welfare. That is, the social value is not the sum of the players ' types nor any weighted sum of the types -LRB- `` affine maximizer '' -RRB-. The success probability of sending a message through a parallel-path network is multilinear, since it can be expressed by the following multilinear formula -LRB- where P denotes the set of all paths between the source and the sink -RRB- : Note that for every link i, the partial derivative in pi of the success probability written in Equation 3 is positive. In all the other networks, that do not contain link i, the partial derivative is clearly zero. Therefore, the social-value function is single crossing and our general results can be applied. COROLLARY 5. For any social-choice function that maximizes the success probability over parallel-path networks, the informationally optimal k-action social-choice function is implementable -LRB- for any k -RRB-. Acknowledgment. The work of the second author is also supported by the Lady Davis Trust Fellowship.", "keyphrases": ["bound action space", "implement", "domin strategi", "social-choic function", "decis function", "singl-cross condit", "multilinear function", "optim mechan", "action-bound mechan", "probabl of success"]}
{"file_name": "C-19", "text": "Service Interface : A New Abstraction for Implementing and Composing Protocols * ABSTRACT In this paper we compare two approaches to the design of protocol frameworks -- tools for implementing modular network protocols. The most common approach uses events as the main abstraction for a local interaction between protocol modules. We argue that an alternative approach, that is based on service abstraction, is more suitable for expressing modular protocols. It also facilitates advanced features in the design of protocols, such as dynamic update of distributed protocols. We then describe an experimental implementation of a service-based protocol framework in Java. 1. INTRODUCTION They allow complex protocols to be implemented by decomposing them into several modules cooperating together. This approach facilitates code reuse and customization of distributed protocols in order to fit the needs of different applications. Moreover, protocol modules can be plugged in to the system dynamically. All these features of protocol frameworks make them an interesting enabling technology for implementing adaptable systems -LSB- 14 -RSB- - an important class of applications. Most protocol frameworks are based on events -LRB- all frameworks cited above are based on this abstraction -RRB-. Events are used for asynchronous communication between different modules on the same machine. For instance, the composition of modules may require connectors to route events, which introduces burden for a protocol composer -LSB- 4 -RSB-. Protocol frameworks such as Appia and Eva extend the event-based approach with channels. However, in our opinion, this solution is not satisfactory since composition of complex protocol stacks becomes more difficult. In this paper, we propose a new approach for building modular protocols, that is based on a service abstraction. We compare this new approach with the common, event-based approach. We show that protocol frameworks based on services have several advantages, e.g. allow for a fairly straightforward protocol composition, clear implementation, and better support of dynamic replacement of distributed protocols. To validate our claims, we have implemented SAMOA -- an experimental protocol framework that is purely based on the service-based approach to module composition and implementation. The framework allowed us to compare the service - and event-based implementations of an adaptive group communication middleware. Section 2 defines general notions. Section 3 presents the main characteristics of event-based frameworks, and features that are distinct for each framework. Section 4 describes our new approach, which is based on service abstraction. Section 5 discusses the advantages of a service-based protocol framework compared to an event-based protocol framework. The description of our experimental implementation is presented in Section 6. Finally, we conclude in Section 7. 7. CONCLUSION In the paper, we proposed a new approach to the protocol composition that is based on the notion of Service Interface, instead of events. We believe that the service-based framework has several advantages over event-based frameworks. A prototype implementation allowed us to validate our ideas.", "keyphrases": ["protocol framework", "distribut algorithm", "distribut system", "servic interfac", "network", "commun", "event-base framework", "stack", "modul", "request", "repli"]}
{"file_name": "I-21", "text": "Interactions between Market Barriers and Communication Networks in Marketing Systems ABSTRACT We investigate a framework where agents search for satisfying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents. Local behaviour incorporates a satisficing model of choice, a set of rules governing the interactions between agents, including learning about the trustworthiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products. We use various degree distributions dictating the extent of connectivity, and incorporate both small-world effects and the notion of preferential attachment in our network models. We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effectiveness changes with the imposition of market barriers. 1. INTRODUCTION Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product -LSB- 14 -RSB- and the availability of more attractive alternatives -LSB- 17 -RSB-. However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves -LRB- in the guise of formal or informal contracts -RRB-, or other so-called ` switching costs ' or market barriers -LSB- 12, 5 -RSB-. The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. While the level of satisfaction with a currently-used product will largely be a function of one 's own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 2. BACKGROUND 2.1 Word-of-mouth communication The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. Simulation-based investigations of wordof-mouth -LSB- 6, 13 -RSB- have focused on developing strategies for ensuring that a system reaches an equilibrium level where all agents are satisfied, largely by learning about the effectiveness of others ' referrals or by varying the degree of inertia in individual behaviour. The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in -LSB- 6 -RSB- in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects. 2.2 Market barriers The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. A useful typology of market barriers distinguishes ` transactional ' barriers associated with the monetary cost of changing -LRB- e.g. in financial services -RRB-, ` learning ' barriers associated with deciding to replace well-known existing products, and ` contractual ' barriers imposing legal constraints for the term of the contract -LSB- 12 -RSB-. A different typology -LSB- 5 -RSB- introduces the additional aspect of ` relational ' barriers arising from personal relationships that may be interwoven with the use of a particular product. There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. Burnham et al. -LSB- 5 -RSB- find that perceived market barriers account for nearly twice the variance in intention to stay with a product than that explained by satisfaction with the product -LRB- 30 % and 16 % respectively -RRB-, and that so-called relational barriers are considerably more influential than either transactional or learning barriers. Further, they find that switching costs are perceived by consumers to exist even in markets which are fluid and where barriers would seem to be weak. Simply put, market barriers appear to play a greater role in what people do than satisfaction ; and their presence may be more pervasive than is generally thought. 5. CONCLUSIONS AND RELATED WORK Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components : networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world. It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving. In poorly-connected networks, the effect of barriers on market shares is slight. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share. Products with small market share -LRB- which, in the context of our simulations, is generally due to the product offering poor performance -RRB- are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case. Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two ` modules ' comprising network structure and local behaviour. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions.", "keyphrases": ["referr system", "purchas behaviour", "word-of-mouth commun", "market system", "defect behaviour", "psycholog affin", "switch behaviour", "agent-base model", "social psycholog", "market barrier", "consum choic", "switch cost"]}
{"file_name": "C-34", "text": "Researches on Scheme of Pairwise Key Establishment for Distributed Sensor Networks ABSTRACT Security schemes of pairwise key establishment, which enable sensors to communicate with each other securely, play a fundamental role in research on security issue in wireless sensor networks. A new kind of cluster deployed sensor networks distribution model is presented, and based on which, an innovative Hierarchical Hypercube model - H -LRB- k, u, m, v, n -RRB- and the mapping relationship between cluster deployed sensor networks and the H -LRB- k, u, m, v, n -RRB- are proposed. By utilizing nice properties of H -LRB- k, u, m, v, n -RRB- model, a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed, which combines the idea of KDC -LRB- Key Distribution Center -RRB- and polynomial pool schemes. Furthermore, the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected. Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key, compared with previous related works. 1. INTRODUCTION Security communication is an important requirement in many sensor network applications, so shared secret keys are used between communicating nodes to encrypt data. As one of the most fundamental security services, pairwise key establishment enables the sensor nodes to communicate securely with each other using cryptographic techniques. However, due to the sensor nodes ' limited computational capabilities, battery energy, and available memory, it is not feasible for them to use traditional pairwise key establishment techniques such as public key cryptography and key distribution center -LRB- KDC -RRB-. Several alternative approaches have been developed recently to perform pairwise key establishment on resource-constrained sensor networks without involving the use of traditional cryptography -LSB- 14 -RSB-. Eschenauer and Gligor proposed a basic probabilistic key predistribution scheme for pairwise key establishment -LSB- 1 -RSB-. In the scheme, each sensor node randomly picks a set of keys from a key pool before the deployment so that any two of the sensor nodes have a certain probability to share at least one common key. Chan et al. further extended this idea and presented two key predistribution schemes : a q-composite key pre-distribution scheme and a random pairwise keys scheme. The q-composite scheme requires any two sensors share at least q pre-distributed keys. The random scheme randomly picks pair of sensors and assigns each pair a unique random key -LSB- 2 -RSB-. Based on such a framework, they presented two pairwise key pre-distribution schemes : a random subset assignment scheme and a grid-based scheme. A polynomial pool is used in those schemes, instead of using a key pool in the previous techniques. The random subset assignment scheme assigns each sensor node the secrets generated from a random subset of polynomials in the polynomial pool. The gridbased scheme associates polynomials with the rows and the columns of an artificial grid, assigns each sensor node to a unique coordinate in the grid, and gives the node the secrets generated from the corresponding row and column polynomials. Based on this grid, each sensor node can then identify whether it can directly establish a pairwise key with another node, and if not, what intermediate nodes it can contact to indirectly establish the pairwise key. A similar approach to those schemes described by Liu et al was independently developed by Du et a. -LSB- 5 -RSB-. Rather than on Blundo 's scheme their approach is based on Blom 's scheme -LSB- 6 -RSB-. All of those schemes above improve the security over the basic probabilistic key pre-distribution scheme. However, the pairwise key establishment problem in sensor networks is still not well solved. For the basic probabilistic and the q-composite key predistribution schemes, as the number of compromised nodes increases, the fraction of affected pairwise keys increases quickly. As a result, a small number of compromised nodes may affect a large fraction of pairwise keys -LSB- 3 -RSB-. Though the random pairwise keys scheme doses not suffer from the above security problem, it incurs a high memory overhead, which increases linearly with the number of nodes in the network if the level of security is kept constant -LSB- 2 -RSB- -LSB- 4 -RSB-. For the random subset assignment scheme, it suffers higher communication and computation overheads. In 2004, Liu proposed a new hypercube-based pairwise key predistribution scheme -LSB- 7 -RSB-, which extends the grid-based scheme from a two dimensional grid to a multi-dimensional hypercube. The analysis shows that hypercube-based scheme keeps some attractive properties of the grid-based scheme, including the guarantee of establishing pairwise keys and the resilience to node compromises. Also, when perfect security against node compromise is required, the hypercube-based scheme can support a larger network by adding more dimensions instead of increasing the storage overhead on sensor nodes. Though hypercube-based scheme -LRB- we consider the grid-based scheme is a special case of hypercube-based scheme -RRB- has many attractive properties, it requires any two nodes in sensor networks can communication directly with each other. This strong assumption is impractical in most of the actual applications of the sensor networks. In this paper, we present a kind of new cluster-based distribution model of sensor networks, and for which, we propose a new pairwise key pre-distribution scheme. Based on the topology, we propose a novel cluster distribution based hierarchical hypercube model to establish the pairwise key. We develop a kind of new pairwise key establishment algorithm with our hierarchical hypercube model. The structure of this paper is arranged as follows : In section 3, a new distribution model of cluster deployed sensor networks is presented. In section 4, a new Hierarchical Hypercube model is proposed. In section 5, the mapping relationship between the clusters deployed sensor network and Hierarchical Hypercube model is discussed. In section 6 and section 7, new pairwise key establishment algorithm are designed based on the Hierarchical Hypercube model and detailed analyses are described. Finally, section 8 presents a conclusion. 2. PRELIMINARY Definition 1 -LRB- Key Predistribution -RRB- : The procedure, which is used to encode the corresponding encryption and decryption algorithms in sensor nodes before distribution, is called Key Predistribution. Definition 2 -LRB- Pairwise Key -RRB- : For any two nodes A and B, if they have a common key E, then the key E is called a pairwise key between them. 8. CONCLUSION A new hierarchical hypercube model named H -LRB- k, u, m, v, n -RRB- is proposed, which can be used for pairwise key predistribution for cluster deployed sensor networks. And Based on the H -LRB- k, u, m, v, n -RRB- model, an innovative pairwise key predistribution scheme and algorithm are designed respectively, by combing the good properties of the Polynomial Key and Key Pool encryption schemes. So, the traditional pairwise key predistribution algorithm based on hypercube model -LSB- 7 -RSB- is only a special case of the new algorithm proposed in this paper. Theoretical and experimental analyses show that the newly proposed algorithm is an efficient pairwise key establishment algorithm that is suitable for the cluster deployed sensor networks.", "keyphrases": ["sensor network", "kei pool", "kei predistribut", "hierarch hypercub model", "secur", "pairwis kei establish algorithm", "cluster-base distribut model", "polynomi kei", "encrypt", "node code", "high fault-toler"]}
{"file_name": "H-32", "text": "Interesting Nuggets and Their Impact on Definitional Question Answering ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a `` Human Interest Model '' from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering. 1. DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003. The Definition questions, also called Other questions in recent years, are defined as follows. Given a question topic X, the task of a definitional QA system is akin to answering the question `` What is X? '' . The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic. Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as `` informative nuggets ''. Each informative nugget is a sentence fragment that describe some factual information about the topic. From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets can not simply be described as informative nuggets. Rather, these topic nuggets have a trivia-like quality associated with them. Typically, these are out of the ordinary pieces of information about a topic that can pique a human reader 's interest. For this reason, we decided to define answer nuggets that can evoke human interest as `` interesting nuggets ''. In essence, interesting nuggets answer the questions `` What is X famous for? '' , `` What defines X? '' . We now have two very different perspective as to what constitutes an answer to Definition questions. An answer can be some important factual information about the topic or some novel and interesting aspect about the topic. This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of `` George Foreman ''. Certain answer nuggets are more informative while other nuggets are more interesting in nature. Informative Nuggets - Became oldest world champion in boxing history. Interesting Nuggets - Returned to boxing after 10 yr hiatus. As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets. In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets. A '' Human Interest Model '' definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system. We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2. RELATED WORK There are currently two general methods for Definitional Question Answering. The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. -LSB- 1 -RSB- and Xu et al. -LSB- 14 -RSB-. Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets. For example, Xu et al. used 40 manually defined `` structured patterns '' in their 2003 definitional question answering system. Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created. A recent system by Harabagiu et al. -LSB- 6 -RSB- created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains. As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information. Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences. Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a person 's birthdate, or the name of a company 's CEO. However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations. This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities. For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being. Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets. This leads to the exploration of the second relevance-based approach that has been used in definitional question answering. Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets -LSB- 1 -RSB-. A similar approach has also been used as a baseline system for TREC 2003 -LSB- 14 -RSB-. More recently, Chen et al. -LSB- 3 -RSB- adapted a bi-gram or bi-term language model for definitional Question Answering. Generally, the relevance-based approach requires a `` definitional corpus '' that contain documents highly relevant to the topic. The baseline system in TREC 2003 simply uses the topic words as its definitional corpus. Blair-Goldensohn et al. -LSB- 1 -RSB- uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional. Chen et al. -LSB- 3 -RSB- collect snippets from Google to build its definitional corpus. From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected. This centroid vector or set of centroid words is taken to be highly indicative of the topic. Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic. BlairGoldensohn et al. -LSB- 1 -RSB- uses Cosine similarity to rank sentences by `` centrality ''. As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus. However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences. Thus such methods identify relevant sentences and not sentences containing definitional nuggets. Yet, the TREC 2003 baseline system -LSB- 14 -RSB- outperformed all but one other system. The bi-term language model -LSB- 3 -RSB- is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach. At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin -LSB- 7 -RSB-. We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords. This may explain why relevance-based method can perform competitively in definitional question answering. However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner. Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets. We will describe how we expand upon such methods to identify interesting nuggets in the next section. 7. CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets. Interesting nuggets are uncommon pieces of information about the topic that can evoke a human reader 's curiosity. The notion of an '' average human reader '' is an important consideration in our approach. This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering. Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings. Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems. We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers. What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic. Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic. Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features. As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model. We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers. Although the methods we used are simple, they have been shown experimentally to be effective. Our approach may also provide some insight into a few anomalies in past definitional question answering 's trials. For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets. We suspect the main contributor to the system 's performance Table 3 : TREC 2005 Topics Grouped by Entity Type In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers.", "keyphrases": ["us of linguist", "extern knowledg", "comput of human interest", "new corpu", "question topic", "inform nugget", "sentenc fragment", "human reader", "interest", "interest nugget", "uniqu qualiti", "surpris factor", "lexic pattern", "manual labor", "baselin system"]}
{"file_name": "I-4", "text": "Meta-Level Coordination for Solving Negotiation Chains in Semi-Cooperative Multi-Agent Systems ABSTRACT A negotiation chain is formed when multiple related negotiations are spread over multiple agents. In order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility, we present an extension to a singleagent concurrent negotiation framework. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. We introduce a pre-negotiation phase that allows agents to transfer meta-level information. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context. The agent can also use this information to allocate appropriate time for each negotiation, hence to find a good ordering of all related negotiations. The experimental data shows that these mechanisms improve the agents ' and the system 's overall performance significantly. 1. INTRODUCTION Sophisticated negotiation for task and resource allocation is crucial for the next generation of multi-agent systems -LRB- MAS -RRB- applications. Groups of agents need to efficiently negotiate over multiple related issues concurrently in a complex, distributed setting where there are deadlines by which the negotiations must be completed. This is an important research area where there has been very little work done. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility ; however, the performance of each individual agent is tightly related to other agent 's cooperation and the system 's overall performance. There is no single global goal in such systems, either because each agent represents a different organization/user, or because it is difficult/impossible to design one single global goal. This issue arises due to multiple concurrent tasks, resource constrains and uncertainties, and thus no agent has sufficient knowledge or computational resources to determine what is best for the whole system -LSB- 11 -RSB-. To accomplish tasks continuously arriving in the virtual organization, cooperation and sub-task relocation are needed and preferred. There is no single global goal since each agent may be involved in multiple virtual organizations. Meanwhile, the performance of each individual agent is tightly related to other agents ' cooperation and the virtual organization 's overall performance. The negotiation in such systems is not a zero-sum game, a deal that increases both agents ' utilities can be found through efficient negotiation. Additionally, there are multiple encounters among agents since new tasks are arriving all the time. In such negotiations, price may or may not be important, since it can be fixed resulting from a long-term contract. Other factors like quality and delivery time are important too. Reputation mechanisms in the system makes cheating not attractive from a long term viewpoint due to multiple encounters among agents. Another major difference between this work and other work on negotiation is that negotiation, here, is not viewed as a stand-alone process. Rather it is one part of the agent 's activity which is tightly interleaved with the planning, scheduling and executing of the agent 's activities, which also may relate to other negotiations. Based on this recognition, this work on negotiation is concerned more about the meta-level decision-making process in negotiation rather than the basic protocols or languages. the negotiations be performed. These macro-strategies are different from those micro-strategies that direct the individual negotiation thread, such as whether the agent should concede and how much the agent should concede, etc -LSB- 3 -RSB-. In this paper we extend a multi-linked negotiation model -LSB- 10 -RSB- from a single-agent perspective to a multi-agent perspective, so that a group of agents involved in chains of interrelated negotiations can find nearly-optimal macro negotiation strategies for pursuing their negotiations. Section 2 describes the basic negotiation process and briefly reviews a single agent 's model of multi-linked negotiation. Section 3 introduces a complex supply-chain scenario. Section 4 details how to solve those problems arising in the negotiation chain. Section 5 reports on the experimental work. Section 6 discusses related work and Section 7 presents conclusions and areas of future work. 2. BACKGROUND ON MULTI-LINKED NEGOTIATION This process can go back and forth until an agreement is reached or the agents decide to stop. If an agreement is reached and one agent can not fulfill the commitment, it needs to pay the other party a decommitment penalty as specified in the commitment. A negotiation starts with a proposal, which announces that a task -LRB- t -RRB- needs to be performed includes the following attributes : 1. deadline -LRB- dl -RRB- : the latest finish time of the task ; the task needs to be finished before the deadline dl. 3. minimum quality requirement -LRB- minq -RRB- : the task needs to be finished with a quality achievement no less than minq. 4. regular reward -LRB- r -RRB- : if the task is finished as the contract requested, the contractor agent will get reward r. 5. early finish reward rate -LRB- e -RRB- : if the contractor agent can finish the task earlier than dl, it will get the extra early finish reward proportional to this rate. 6. The multi-linked negotiation problem has two dimensions : the negotiations, and the subjects of negotiations. The negotiations are interrelated and the subjects are interrelated ; the attributes of negotiations and the attributes of the subjects are interrelated as well. This two-dimensional complexity of interrelationships distinguishes it from the classic project management problem or scheduling problem, where all tasks to be scheduled are local tasks and no negotiation is needed. 1. negotiation duration -LRB- \u03b4 -LRB- v -RRB- -RRB- : the maximum time allowed for negotiation v to complete, either reaching an agreed upon proposal -LRB- success -RRB- or no agreement -LRB- failure -RRB-. 2. negotiation start time -LRB- \u03b1 -LRB- v -RRB- -RRB- : the start time of negotiation v. \u03b1 -LRB- v -RRB- is an attribute that needs to be decided by the agent. 3. negotiation deadline -LRB- e -LRB- v -RRB- -RRB- : negotiation v needs to be finished before this deadline e -LRB- v -RRB-. The negotiation is no longer valid after time e -LRB- v -RRB-, which is the same as a failure outcome of this negotiation. 4. It depends on a set of attributes, including both attributes-in-negotiation -LRB- i.e. reward, flexibility, etc. -RRB- and attributes-of-negotiation -LRB- i.e. negotiation start time, negotiation deadline, etc. -RRB-. An agent involved in multiple related negotiation processes needs to reason on how to manage these negotiations in terms of ordering them and choosing the appropriate values for features. This is the multi-linked negotiation problem -LSB- 10 -RSB- : \u03c1 -LRB- v -RRB- -RRB-, which describes the relationship between negotiation v and its children. The AND relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires all its children nodes have successful accomplishments. The OR relationship associated with a negotiation v means the successful accomplishment of the commitment on v requires at least one child node have successful accomplishment, where the multiple children nodes represent alternatives to accomplish the same goal. Multi-linked negotiation problem is a local optimization problem. To solve a multi-linked negotiation problem is to find a negotiation solution -LRB- 0, \u03d5 -RRB- with optimized expected utility Elf -LRB- 0, \u03d5 -RRB-, which is defined as : A negotiation ordering 0 defines a partial order of all negotiation issues. A feature assignment \u03d5 is a mapping function that assigns a value to each attribute that needs to be decided in the negotiation. A negotiation outcome \u03c7 for a set of negotiations -LCB- vj 1, -LRB- j = 1,..., n -RRB- specifies the result for each negotiation, either success or failure. There are a total of 2n different outcomes for n negotiations : -LCB- chii1, -LRB- i = 1,..., 2n -RRB-. P -LRB- \u03c7i, \u03d5 -RRB- denotes the probability of the outcome \u03c7i given the feature assignment \u03d5, which is calculated based on the success probability of each negotiation. The Sixth Intl.. Joint Conf. Figure 1 : A Complex Negotiation Chain Scenario A heuristic search algorithm -LSB- 10 -RSB- has been developed to solve the single agent 's multi-linked negotiation problem that produces nearly-optimal solutions. This algorithm is used as the core of the decision-making for each individual agent in the negotiation chain scenario. In the rest of the paper, we present our work on how to improve the local solution of a single agent in the global negotiation chain context. 6. RELATED WORK Fatima, Wooldridge and Jennings -LSB- 1 -RSB- studied the multiple issues in negotiation in terms of the agenda and negotiation procedure. However, this work is limited since it only involves a single agent 's perspective without any understanding that the agent may be part of a negotiation chain. Mailler and Lesser -LSB- 4 -RSB- have presented an approach to a distributed resource allocation problem where the negotiation chain scenario occurs. It models the negotiation problem as a distributed constraint optimization problem -LRB- DCOP -RRB- and a cooperative mediation mechanism is used to centralize relevant portions of the DCOP. In our work, the negotiation involves more complicated issues such as reward, penalty and utility ; also, we adopt a distribution approach where no centralized control is needed. A mediator-based partial centralized approach has been applied to the coordination and scheduling of complex task network -LSB- 8 -RSB-, which is different from our work since the system is a complete cooperative system and individual utility of single agent is not concerned at all. A combinatorial auction -LSB- 2, 9 -RSB- could be another approach to solving the negotiation chain problem. However, in a combinatorial auction, the agent does not reason about the ordering of negotiations. This would lead to a problem similar to those we discussed when the same-deadline policy is used. 7. CONCLUSION AND FUTURE WORK In this paper, we have solved negotiation chain problems by extending our multi-linked negotiation model from the perspective of a single agent to multiple agents. Instead of solving the negotiation chain problem in a centralized approach, we adopt a distributed approach where each agent has an extended local model and decisionmaking process. We have introduced a pre-negotiation phase that allows agents to transfer meta-level information on related negotiation issues. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability. This more accurate model helps the agent in choosing the appropriate negotiation solution. The experimental data shows that these mechanisms improve the agent 's and the system 's overall performance significantly. In future extension of this work, we would like to develop mechanisms to verify how reliable the agents are.", "keyphrases": ["multipl agent", "negoti framework", "negoti chain", "semi-cooper multi-agent system", "pre-negoti", "multi-link negoti", "agent", "distribut set", "multipl concurr task", "virtual organ", "sub-task reloc", "reput mechan", "complex suppli-chain scenario"]}
{"file_name": "I-14", "text": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems ABSTRACT The dominant existing routing strategies employed in peerto-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions. Specifically, agents maintain estimates on the downstream agents ' abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies. 1. INTRODUCTION Over the last few years there have been increasing interests in studying how to control the search processes in peer-to-peer -LRB- P2P -RRB- based information retrieval -LRB- IR -RRB- systems -LSB- 6, 13, 14, 15 -RSB-. In this line of research, one of the core problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents. In the absence of global information, the dominant strategies in addressing this problem are content-similarity based approaches -LSB- 6, 13, 14, 15 -RSB-. While the content similarity between queries and local nodes appears to be a creditable indicator for the number of relevant documents residing on each node, these approaches are limited by a number of factors. Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the historical information of the past search sessions, that provide valuable information for the query routing algorithms. In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms. Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' capabilities of providing relevant documents for specific types of incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. This process is conducted in an iterative manner. The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents. This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time. Another way of viewing this paper is that our basic approach to distributed IR search is to construct a hierarchical overlay network -LRB- agent organization -RRB- based on the contentsimilarity measure among agents ' document collections in a bottom-up fashion. In the past work, we have shown that this organization improves search performance significantly. The intention of the reinforcement learning is to adapt the agents ' routing decisions to the dynamic network situations and learn from past search sessions. Specifically, the contributions of this paper include : -LRB- 1 -RRB- a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents ; -LRB- 2 -RRB- two strategies to speed up the learning process. The remainder of this paper is organized as follows : Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology. Section 3 describes a reinforcement learning based approach to direct the routing process ; Section 4 details the experimental settings and analyze the results. Section 5 discusses related studies and Section 6 concludes the paper. 5. RELATED WORK The content routing problem differs from the networklevel routing in packet-switched communication networks in that content-based routing occurs in application-level networks. In addition, the destination agents in our contentrouting algorithms are multiple and the addresses are not known in the routing process. IP-level Routing problems have been attacked from the reinforcement learning perspective -LSB- 2, 5, 11, 12 -RSB-. There are two major classes of adaptive, distributed packet routing algorithms in the literature : distance-vector algorithms and link-state algorithms. While this line of studies carry a certain similarity with our work, it has mainly focused on packet-switched communication networks. Each agent maintains estimations, probabilistically or deterministically, on the distance to a certain destination through its neighbors. A variant of Q-Learning techniques is deployed The Sixth Intl.. Joint Conf. to update the estimations to converge to the real distances. It has been discovered that the locality property is an important feature of information retrieval systems in user modeling studies -LSB- 3 -RSB-. The learning based approach is perceived to be more beneficial for real distributed information retrieval systems which exhibit locality property. This is because the users ' traffic and query patterns can reduce the state space and speed up the learning process. Related work in taking advantage of this property include -LSB- 7 -RSB-, where the authors attempted to address this problem by user modeling techniques. 6. CONCLUSIONS In this paper, a reinforcement-learning based approach is developed to improve the performance of distributed IR search algorithms. Particularly, agents maintain estimates, namely expected utility, on the downstream agents ' ability to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on the updated expected utility information, the agents modify their routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies. The experiments on two different distributed IR datasets illustrates that the reinforcement learning approach improves considerably the cumulative utility over time.", "keyphrases": ["peer-to-peer inform retriev system", "reinforc learn", "distribut search algorithm", "rout decis", "util", "network", "learn algorithm", "rout polici", "queri"]}
{"file_name": "C-23", "text": "Implementation of a Dynamic Adjustment Mechanism with Efficient Replica Selection in Data Grid Environments ABSTRACT The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers. Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers. In this paper, we propose a dynamic coallocation scheme, namely Recursive-Adjustment Co-Allocation scheme, to improve the performance of data transfer in Data Grids. Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide an effective scheme for reducing the cost of reassembling data blocks. 1. INTRODUCTION Data Grids aggregate distributed resources for solving large-size dataset management problems. Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment. Certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access. Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity. One way to improve download speeds is to determine the best replica locations using replica selection techniques -LSB- 19 -RSB-. This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet. Another way is to use co-allocation technology -LSB- 17 -RSB- to download data. Co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel. Several co-allocation strategies were provided in previous work -LSB- 17 -RSB-. An idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block. Therefore, it is important to reduce the differences in finishing time among replica servers. In this paper, we propose a dynamic co-allocation scheme based on co-allocation Grid data transfer architecture called RecursiveAdjustment Co-Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance -LSB- 24 -RSB-. Experimental results show that our approach is superior to previous methods and achieved the best overall performance. We also discuss combination cost and provide an effective scheme for reducing it. Related background review and studies are presented in Section 2 and the co-allocation architecture and related work are introduced in Section 3. In Section 4, an efficient replica selection service is proposed by us. Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6. Section 7 concludes this research paper. 2. BACKGROUND 2.1 Data Grid Data Grids -LSB- 1, 2, 16 -RSB- federate a lot of storage resources. Large collections of measured or computed data are emerging as important resources in many data intensive applications. 2.1.1 Replica Management Replica management involves creating or removing replicas at a data grid site -LSB- 19 -RSB-. In other words, the role of a replica manager is to create or delete replicas, within specified storage systems. Most often, these replicas are exact copies of the original files, created only to harness certain performance benefits. A replica manager typically maintains a replica catalog containing replica site addresses and the file instances. The replica management service is responsible for managing the replication of complete and partial copies of datasets, defined as collections of files. The replica management service is just one component in a Data Grid environment that provides support for high-performance, data-intensive applications. A replica or location is a subset of a collection that is stored on a particular physical storage system. There may be multiple possibly overlapping subsets of a collection stored on multiple storage systems in a Data Grid. These Grid storage systems may use a variety of underlying storage technologies and data movement protocols, which are independent of replica management. 2.1.2 Replica Catalog As mentioned above, the purpose of the replica catalog is to provide mappings between logical names for files or collections and one or more copies of the objects on physical storage systems. The replica catalog includes optional entries that describe individual logical files. Logical files are entities with globally unique names that may have one or more physical instances. The catalog may optionally contain one logical file entry in the replica catalog for each logical file in a collection. A Data Grid may contain multiple replica catalogs. For example, a community of researchers interested in a particular research topic might maintain a replica catalog for a collection of data sets of mutual interest. It is possible to create hierarchies of replica catalogs to impose a directory-like structure on related logical collections. In addition, the replica manager can perform access control on entire catalogs as well as on individual logical files. 2.1.3 Replica Selection The purpose of replica selection -LSB- 16 -RSB- is to select a replica from among the sites which constitute a Data Grid -LSB- 19 -RSB-. The criteria of selection depend on characteristics of the application. By using this mechanism, users of the Data Grid can easily manage replicas of data sets at their sites, with better performance. Much previous effort has been devoted to the replica selection problem. The common process of replica selection consists of three steps : data preparation, preprocessing and prediction. Then, applications can select a replica according to its specific attributes. Replica selection is important to data-intensive applications, and it can provide location transparency. When a user requests for accessing a data set, the system determines an appropriate way to deliver the replica to the user. 2.2 Globus Toolkit and GridFTP The Globus Project -LSB- 9, 11, 16 -RSB- provides software tools collectively called The Globus Toolkit that makes it easier to build computational Grids and Grid-based applications. Many organizations use the Globus Toolkit to build computational Grids to support their applications. The composition of the Globus Toolkit can be pictured as three pillars : Resource Management, Information Services, and Data Management. GRAM implements a resource management protocol, MDS implements an information services protocol, and GridFTP implements a data transfer protocol. The Globus alliance proposed a common data transfer and access protocol called GridFTP that provides secure, efficient data movement in Grid environments -LSB- 3 -RSB-. This protocol, which extends the standard FTP protocol, provides a superset of the features offered by the various Grid storage systems currently in use. In order to solve the appearing problems, the Data Grid community tries to develop a secure, efficient data transport mechanism and replica management services. GridFTP is a reliable, secure and efficient data transport protocol which is developed as a part of the Globus project. There is another key technology from Globus project, called replica catalog -LSB- 16 -RSB- which is used to register and manage complete and partial copies of data sets. The replica catalog contains the mapping information from a logical file or collection to one or more physical files. 2.3 Network Weather Service The Network Weather Service -LRB- NWS -RRB- -LSB- 22 -RSB- is a generalized and distributed monitoring system for producing short-term performance forecasts based on historical performance measurements. The goal of the system is to dynamically characterize and forecast the performance deliverable at the application level from a set of network and computational resources. 2.4 Sysstat Utilities The Sysstat -LSB- 15 -RSB- utilities are a collection of performance monitoring tools for the Linux OS. The Sysstat package incorporates the sar, mpstat, and iostat commands. The sar command collects and reports system activity information, which can also be saved in a system activity file for future inspection. The iostat command reports CPU statistics and I/O statistics for tty devices and disks. 7. CONCLUSIONS The co-allocation architecture provides a coordinated agent for assigning data blocks. A previous work showed that the dynamic co-allocation scheme leads to performance improvements. However, it can not handle the idle time of faster servers, which must wait for the slowest server to deliver its final block. We proposed the Recursive-Adjustment Co-Allocation scheme to improve data transfer performances using the co-allocation architecture in -LSB- 17 -RSB-. In this approach, the workloads of selected replica servers are continuously adjusted during data transfers, and we provide a function that enables users to define a final block threshold, according to their data grid environment. Experimental results show the effectiveness of our proposed technique in improving transfer time and reducing overall idle time spent waiting for the slowest server. We also discussed the re-combination cost and provided an effective scheme for reducing it.", "keyphrases": ["distribut resourc", "data grid applic", "replic", "co-alloc", "larg dataset", "resourc manag protocol", "replica", "co-alloc strategi", "server", "perform"]}
{"file_name": "J-28", "text": "Approximately-Strategyproof and Tractable Multi-Unit Auctions ABSTRACT We present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem. The bidding language in our auctions allows marginal-decreasing piecewise constant curves. First, we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem, which computes a -LRB- 1 + e -RRB- approximation in worst-case time T = O -LRB- n3/e -RRB-, given n bids each with a constant number of pieces. Second, we embed this approximation scheme within a Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism and compute payments to n agents for an asymptotic cost of O -LRB- T log n -RRB-. The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by e / -LRB- 1 + e -RRB- V, where V is the total surplus in the efficient outcome. 1. INTRODUCTION In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce ; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services -LRB- e.g. circuit boards, power suppliers, toner cartridges -RRB-, with flexible-sized lots. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. We consider the computational complexity of implementing the Vickrey-Clarke-Groves -LSB- 22, 5, 11 -RSB- mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves -LRB- VCG -RRB- mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer can not be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design -LSB- 17 -RSB-. The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to -LRB- weakly -RRB- intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the 1However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a -LRB- 1 + ~ -RRB- - approximation in worst-case time T = O -LRB- n3 / \u03b5 -RRB-, where each bid has a fixed number of piecewise constant pieces. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O -LRB- nT -RRB-. This upper-bound tends to 1 as the number of sellers increases. The approximate VCG mechanism is -LRB- \u03b5 1 + \u03b5 -RRB- - strategyproof for an approximation to within -LRB- 1 + ~ -RRB- of the optimal allocation. This means that a bidder can gain at most -LRB- \u03b5 1 + \u03b5 -RRB- V from a nontruthful bid, where V is the total surplus from the efficient allocation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about \u03b5-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme. Section 4 defines the approximation scheme for the payments in the VCG mechanism. Section 5 concludes. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem -LRB- CAP -RRB- is both NP-complete and inapproximable -LRB- e.g. -LSB- 6 -RSB- -RRB-. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language -- the bid taker in our setting is allowed to accept at most one point on the bid curve. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. For instance, Lehmann et al. -LSB- 15 -RSB- propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Feigenminimum-lot size constraints from the buyers. baum & Shenker -LSB- 8 -RSB- have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Eso et al. -LSB- 7 -RSB- have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. Kalagnanam et al. -LSB- 12 -RSB- address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different : it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel -LSB- 1 -RSB- has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values -LSB- 1 -RSB-, with an interpretation as a primal-dual algorithm -LSB- 2 -RSB-. 5. CONCLUSIONS We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language. Our scheme is both approximately efficient and approximately strategyproof within any specified factor \u03b5 > 0. As such it is an example of computationally tractable \u03b5-dominance result, as well as an example of a non-trivial but approximable allocation problem. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O -LRB- T log n -RRB-, where T is the time complexity to compute the solution to a single allocation problem.", "keyphrases": ["approxim-effici and approximatelystrategyproof auction mechan", "singl-good multi-unit alloc problem", "fulli polynomi-time approxim scheme", "vickrei-clark-grove", "forward auction", "revers auction", "equilibrium", "margin-decreas piecewis constant curv", "bid languag", "dynam program"]}
{"file_name": "C-40", "text": "Edge Indexing in a Grid for Highly Dynamic Virtual Environments \u2217 ABSTRACT Newly emerging game -- based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real -- time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client -- server based grid subspace division where the virtual worlds are partitioned into manageable sub -- worlds. In each sub -- world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user 's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real -- time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments. The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real -- time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next -- generation virtual frameworks that may merge into existing web -- based services in the near future. \u2217 This research has been funded in part by NSF grants EEC9529152 -LRB- IMSC ERC -RRB- and IIS-0534761, and equipment gifts from Intel Corporation, Hewlett-Packard, Sun Microsystems and Raptor Networks Technology. Categories and Subject Descriptors : C. 2.4 -LSB- Computer -- Com 1. INTRODUCTION Recently, Massively Multiplayer Online Games -LRB- MMOGs -RRB- have been studied as a framework for next -- generation virtual environments. In this paper, we mainly focus on the first two requirements. Dynamic extensibility allows regular game -- users to deploy their own created content. This is a powerful concept, but unfortunately, user -- created content tends to create imbalances among the existing scene complexity, causing system -- wide performance problems. Another important requirement is scalability. By carefully partitioning the world into multiple sub -- worlds or replicating worlds at geographically dispersed locations, massive numbers of concurrent users can be supported. Second Life -LSB- 4 -RSB- is the first successfully deployed MMOG system that meets both requirements. But we acknowledge that these requirements are also valid for new virtual environments. Figure 1 : Object popping occurred as a user moves forward -LRB- screenshots from Second Life -RRB- where \u0394 = 2 seconds. employs a client/server based 3D object streaming model -LSB- 5 -RSB-. In this model, a server continuously transmits both update events and geometry data to every connected user. As a result, this extensible gaming environment has accelerated the deployment of user -- created content and provides users with unlimited freedom to pursue a navigational experience in its space. One of the main operations in MMOG applications that stream 3D objects is to accurately calculate all objects that are visible to a user. The traditional visibility determination approach, however, has an object popping problem. For example, a house outside a user 's visible range is not drawn at time t, illustrated in Figure 1 -LRB- a -RRB-. As the user moves forward, the house will suddenly appear at time -LRB- t + \u0394 -RRB- as shown in Figure 1 -LRB- b -RRB-. The visibility calculation for each user not only needs to be accurate, but also fast. This challenge is illustrated by the fact that the maximum number of concurrent users per server of Second Life is still an order of magnitude smaller than for stationary worlds. To address these challenges, we propose a method that identifies the most relevant visible objects from a given geometry database -LRB- view model -RRB- and then put forth a fast indexing method that computes the visible objects for each user -LRB- spatial indexing -RRB-. Our two novel methods represent the main contributions of this work. Section 2 presents related work. Section 3 describes our new view method. In Section 4, we present assumptions on our target application and introduce a new spatial indexing method designed to support real -- time visibility computations. We also discuss its optimization issues. Section 5 reports on the quantitative analysis and Section 6 presents preliminary results of our simulation based experiments. Finally, we conclude and address future research directions in Section 7. 2. RELATED WORK Visibility determination has been widely explored in the field of 3D graphics. Various local rendering algorithms have been proposed to eliminate unnecessary objects before rendering or at any stage in the rendering pipeline. However, these algorithms assume that all the candidate visible objects have been stored locally. If the target objects are stored on remote servers, the clients receive the geometry items that are necessary for rendering from the server databases. However, these online optimization algorithms fail to address performance issue at the server in highly crowded environments. On the other hand, our visibility computation model, a representative of this category, is based on different assumptions on the data representation of virtual entities. In the graphics area, there has been little work on supporting real -- time visibility computations for a massive number of moving objects and users. Here we recognize that such graphics related issues have a very close similarity to spatial database problems. Recently, a number of publications have addressed the scalability issue on how to support massive numbers of objects and queries in highly dynamic environments. To support frequent updates, two partitioning policies have been studied in depth : -LRB- 1 -RRB- R-tree based spatial indexing, and -LRB- 2 -RRB- grid -- based spatial indexing. The grid -- based partitioning model is a special case of fixed partitioning. Recently, it has been re -- discovered since it can be efficient in highly dynamic environments. Q-Index -LSB- 13, 11 -RSB- is one of the earlier work that re -- discovers the usefulness of grid -- based space partitioning for emerging moving object environments. In contrast to traditional spatial indexing methods that construct an index on the moving objects, it builds an index on the continuous range queries, assuming that the queries move infrequently while the objects move freely. The basic idea of the Q+R tree -LSB- 14 -RSB- is to separate indexing structures for quasi -- stationary objects and moving objects : fast -- moving objects are indexed in a Quadtree and quasi -- stationary objects are stored in an R \u2217 - tree. SINA -LSB- 10 -RSB- was proposed to provide efficient query evaluations for any combination of stationary/moving objects and stationary/moving queries. Specifically, this approach only detects newly discovered -LRB- positive -RRB- or no longer relevant -LRB- negative -RRB- object updates efficiently. Unlike other spatial indexing methods that focus on reducing the query evaluation cost, Hu et al. -LSB- 12 -RSB- proposed a general framework that minimizes the communication cost for location updates by maintaining a rectangular area called a safe region around moving objects. As long as any object resides in this region, all the query results are guaranteed to be valid in the system. If objects move out of their region, location update requests should be delivered to the database server and the affected queries are re -- evaluated on the fly. Our indexing method is very similar to the above approaches. The major difference is that we are more concentrating on real -- time visibility determination while others assume loose timing constraints. 6. EVALUATION This section presents two simulation setups and their performance results. Section 6.1 examines whether our new view approach is superior to existing view models, in spite of its higher indexing complexity. Section 6.2 discusses the degree of practicality and scalability of our indexing method that is designed for our new view model. 6.1 Justification of Object-initiated View Model 6.1.1 Evaluation Metrics P is the ratio of relevant, retrieved items to all retrieved items. A lower value of P implies that the query result set contains a large number of unnecessary objects that do not have to be delivered to a client. A higher P value means a higher network traffic load than required. R is the ratio of relevant, retrieved items to all relevant items. A lower R value means that more objects that should be recognized are ignored. From the R measure, we can quantitatively estimate the occurrence of object popping. In addition to the P and R metrics, we use a standardized single -- valued query evaluation metric that combines P and R, called E -- measure -LSB- 15 -RSB-. The E -- measure is defined as : If \u03b2 is less than 1, P becomes more important. Otherwise, R will affect the E -- measure significantly. A lower E -- measure value implies that the tested view model has a higher quality. The best E -- measure value is zero, where the best values for P and R are both ones. 6.1.2 Simulation Setup We tested four query processing schemes, which use either a user -- initiated or an object -- initiated view model : \u2022 User-initiated visibility computation -- RQ -- OP : Region Query -- Object Point \u2022 Object-oriented visibility computation -- PQ-OR : Point Query -- Object Region -- RQ-OR : Region Query -- Object Region -- ACQ-OR : Approximate Cell Query -- Object Region RQ -- OP is the typical computation scheme that collects all objects whose location is inside a user defined AOI. PQ -- OR collects a set of objects whose AOI intersects with a given user point, formally -LCB- o | q.P \u2208 o.R -RCB-. RQ -- OR, an imaginary computation scheme, is the combination of RQ -- OP and PQ -- OR where the AOI of an object intersects with that of a user, -LCB- o | o.R \u2229 q.R = ~ \u2205 -RCB-. Lastly, ACQ -- OR, an approximate visibility computation model, is a special scheme designed for grid -- based space partitioning, which is our choice of cell evaluation methodology for edge indexing. If a virtual space is partitioned into tiled cells and a user point belongs to one of the cells, the ACQ -- OR searches the objects whose AOI Table 5 : P and R computations of different visibility determination schemes. Table 6 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a slowly moving environment would intersect with the region of the corresponding grid cell. It identifies any object o satisfying the condition c.R n o.R _ ~ 0 where the cell c satisfies q.P E c.R as well. Our simulation program populated 100K object entities and 10K user entities in a 2D unit space, -LSB- 0, 1 -RRB- x -LSB- 0, 1 -RRB-. The populated entities are uniformly located in the unit space. The program performs intersection tests between all user and all object entities exhaustively and computes the P, R, and E -- measure values -LRB- shown in Table 5 -RRB-. 6.1.3 Experimental Results Distribution of P and R measure : Figure 7 shows the distribution of P and R for RQ -- OP. We can observe that P and R are roughly inversely proportional to each other when varying a user AOI range. A smaller side length leads to higher accuracy but lower comprehensiveness. For example, 5 % of the side length of a user AOI detects all objects whose side length of the AOI is at least 5 %. Thus, every object retrieved by RQ -- OP is guaranteed to be all rendered at the client. But RQ -- OP can not detect the objects outside the AOI of the user, thus suffering from too many missing objects that should be rendered. Similarly, the user whose AOI is wider than any other AOI can not miss any objects that should be rendered, but detects too many unnecessary objects. To remove any object popping problem, the side length of any AOI should be greater than or equal to the maximum visible distance of any object in the system, which may incur significant system degradation. E-measure Distribution : Figure 8 reveals two trends. First, the precision values of RQ -- OP lie in between those of ACQ -- OR -LRB- 100 x 100 grid -RRB- and RQ -- OR. Second, the tendency curve of the Precision -- to -- E -- measure plot of RQ -- OR shows resemblance to that of ACQ -- OR. Effect of Different Grid Size : Figure 9 shows the statistical difference of E -- measure values of seven different grid partitioning schemes -LRB- using ACQ -- OR -RRB- and one RQ -- OP model. We use a box -- and -- whisker plot to show both median values and the variances of E-measure distributions and the outliers of each scheme. We also draw the median value of the RQ -- OP E -- measures -LRB- green line -RRB- for comparison purposes. While the ACQ -- OR schemes have some outliers, their E-measure values are heavily concentrated around the median values, thus, they are less sensitive to object AOI. As expected, fine-grained grid partitioning showed a smaller E-measure value. The RQ -- OP scheme showed a wider variance of its quality than other schemes, which is largely attributable to different user side lengths. As the R measure becomes more important, the query quality of ACQ -- OR is improved more evidently than that of RQ -- OP. From Figure 9, the 20x20 grid scheme had a better E-measure Table 7 : Measured elapsed time -LRB- seconds -RRB- of 100K moving objects and 10K moving users in a highly dynamic environment -LRB- value in a prioritized environment than in an equal-prioritized environment. As a result, we can roughly anticipate that at least the 20x20 grid cell partitioning retrieves a higher quality of visible sets than the RQ -- OP. 6.2 Evaluation of Edge Indexing In this section, we present the preliminary results of the simulations that examine the applicability of our edge indexing implementation. To estimate the degree of real -- time support of our indexing method, we used the total elapsed time of updating all moving entities and computing visible sets for every cell. We also experimented with different grid partitioning policies and compared them with exhaustive search solutions. 6.2.1 Simulation Setup We implemented edge indexing algorithms in C and ran the experiments on a 64-bit 900MHz Itanium processor with 8 GBs of memory. We implemented a generalized hash table mechanism to store node and edge structures. 6.2.2 Experimental Results Periodic Monitoring Cost : Tables 6 and 7 show the performance numbers of different edge indexing methods by varying v. The moving speed of entities was also uniformly assigned between 0 and v. However, the two -- table method showed a slightly higher evaluation time than the two single -- table methods because of its sequential token removal. Table 7 exemplified the elapsed time of index updates and cell evaluations in a highly dynamic environment where slowly moving and dynamically moving objects co -- exist. Compared with the results shown in Table 6, the two -- table approach produced similar performance numbers regardless of the underlying moving environments. However, the performance gain obtained by the incremental policy of the single -- table is decreased compared with that in the slowly moving environment. Effect of Different Grid Size : How many object updates and cell evaluations can be supported in a given time period is an important performance metric to quantify system throughput. In this section, we evaluate the performance results of three different visibility computation models : two computation -- driven exhaustive search methods ; and one two -- table edge indexing method with different grid sizes. Figure 7 : Distribution of P and R measured by RQ -- OP. Figure 8 : E -- measure value as a function of Figure 9 : E -- measure value as a function of ACQ -- QR grid partitioning scheme when Figure 10 : Total elapsed time of different indexing schemes. Exhaustive search methods do not maintain any intermediate results. They simply compute whether a given user point is inside a given object AOI. They can tolerate unpredictable behavior of object movement. Figure 10 reveals the performance difference between the exhaustive solutions and the two -- table methods, a difference of up to two orders of magnitude. As shown in Section 5, the total elapsed time of object updates and cell evaluations is linear with respect to the average side length of object AOI. Because the side length is represented by cell units, an increase in the number of cells increases the side lengths proportionally. Figure 10 illustrates that the measured simulation results roughly match the expected performance gain computed from the analysis. 7. CONCLUSION AND FUTURE WORK To support dynamic extensibility and scalability in highly dynamic environments, we proposed a new view paradigm, the object-initiated view model, and its efficient indexing method, edge indexing. Compared with the traditional view model, our new view model promises to eliminate any object popping problem that can easily be observed in existing virtual environments at the expense of increased indexing complexity. Our edge indexing model, however, can overcome such higher indexing complexity by indexing spatial extensions at edge -- level not at node -- level in a grid partitioned sub -- world and was validated through quantitative analyses and simulations. However, for now our edge indexing still retains a higher complexity, even in a two -- dimensional domain. Currently, we are developing another edge indexing method to make the indexing complexity constant. Once indexing complexity becomes constant, we plan to index 3D spatial extensions and multi -- resolutional geometry data. We expect that our edge indexing can contribute to successful deployment of next -- generation gaming environments.", "keyphrases": ["edg index", "dynam virtual environ", "game-base applic", "mutabl virtual content", "spatial databas", "spatial index method", "real-time visibl test", "object-initi view model", "object pop", "3d spatial extens"]}
{"file_name": "C-86", "text": "Addressing Strategic Behavior in a Deployed Microeconomic Resource Allocator ABSTRACT While market-based systems have long been proposed as solutions for distributed resource allocation, few have been deployed for production use in real computer systems. Towards this end, we present our initial experience using Mirage, a microeconomic resource allocation system based on a repeated combinatorial auction. Mirage allocates time on a heavily-used 148-node wireless sensor network testbed. In particular, we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects. Based on these results, we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. Finally, we propose refinements to the system 's current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction. 1. INTRODUCTION Market-based systems have long been proposed as solutions for resource allocation in distributed systems including computational Grids -LSB- 2, 20 -RSB-, wide-area network testbeds -LSB- 9 -RSB-, and peer-to-peer systems -LSB- 17 -RSB-. Yet, while the theoretical underpinnings of market-based schemes have made significant strides in recent years, practical integration of market-based mechanisms into real computer systems and empirical observations of such systems under real workloads has remained an elusive goal. Towards this end, we have designed, implemented, and deployed a microeconomic resource allocation system called Mirage -LSB- 3 -RSB- for scheduling testbed time on a 148-node wireless sensor network -LRB- SensorNet -RRB- testbed at Intel Research. The system, which employs a repeated combinatorial auction -LSB- 5, 14 -RSB- to schedule allocations, has been in production use for over four months and has scheduled over 312,148 node hours across 11 research projects to date. In designing and deploying Mirage, we had three primary goals. First, we wanted to validate whether a market-based resource allocation scheme was necessary at all. An economic problem only exists when resources are scarce. Therefore, a key goal was to first measure both resource contention and the range of underlying valuations users place on the resources during periods of resource scarcity. Second, we wanted to observe how users would actually behave in a market-based environment. With Mirage, we wanted to observe to what extent rationality held and in what ways users would attempt to strategize and game the system. Finally, we wanted to identify what other practical problems would emerge in a deployment of a market based system. In this paper, we report briefly on our first goal while focusing primarily on the second. In deploying Mirage, we made the early decision to base the system on a repeated combinatorial auction known not to be strategyproof. That is, self-interested users could attempt to increase their personal gain, at the expense of others, by not revealing their true value to the system. We made this decision mainly because designing a strategyproof mechanism remains an open, challenging problem and we wanted to deploy a working system and gain experience with real users to address our three goals in a timely manner. Deploying a non-strategyproof mechanism also had the benefit of testing rationality and seeing how and to what extent users would try to game the system. The key contribution of this paper is an analysis of such strategic behavior as observed over a four-month time period and proposed refinements for mitigating such behavior en route to building an approximately strategyproof repeated combinatorial auction. The rest of this paper is organized as follows. In Section 2, we present an overview of Mirage including high-level observations on usage over a four-month period. In Section 3, we examine strategic user behavior, focusing on the four primary types of strategies employed by users in the system. Based on these results, Section 4 presents a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. As a first step in addressing some of these challenges, we describe refinements to Mirage 's current auction scheme that mitigate the strategies observed to date and also comment on some initial steps towards building an approximately strategyproof repeated combinatorial auction for Mirage. Finally, in Section 5, we conclude the paper. 5. CONCLUSION Despite initially using a repeated combinatorial auction known not to be strategyproof, Mirage has shown significant promise as a vehicle for SensorNet testbed allocation. Fully realizing these gains, however, requires addressing key problems in strategyproof mechanism design and combinatorial optimization. The temporal nature of computational resources and the combinatorial resource demands of distributed applications adds an additional layer of complexity.", "keyphrases": ["resourc alloc system", "combinatori auction", "market-base system", "distribut system", "strateg behavior", "ration", "auction-base scheme", "mirag system", "sensornet testb", "node-hour price", "usabl overhead", "batch schedul", "distribut applic"]}
